nama_dokumen,kalimat,summary
Alfharizky Fauzi_Kualifikasi.txt,pada metodologi penelitian ini menjelaskan mengenai bagaimana proses dari analisis system perancangan dan analisis program yang dilakukan pada penelitian ini. berikut analisis dan perancangan pada penelitian ini. 3.1 tahapan penelitian gambar 3. 1 tahapan penelitian dokumentasi peneliti tahapan penelitian dapat dilihat pada gambar 3.1. tahapan penelitian yang dilakukan terdiri dari 9 tahapan yaitu dimulai dari studi literatur sebagai dasar penelitian analisis kebutuhan pada system yang akan dibangun pengumpulan dataset preprocessing data membangun model training model evaluasi model deployme nt model dan implementasi model yang telah dibuat ke dalam smartphone. saat program telah dijalankan program akan mengakuisisi dataset kemudian dataset akan melalui tahap preprocessing untuk m enormalkan data kemudian setelah melalui tahap preprocessing selanjutnya mentraining dataset yang sudah didapatkan jika dataset berhasil dilatih dan juga divalidasi maka berlanjut ke tahap berikutnya yaitu tahapan test ing dengan menerapkan model yang dibuat kedalam mobile phone atau smartphone. tahap selanjutnya jika camera telah menyala maka artinya sudah siap untuk mendeteksi objek jenis penyakit kulit . pada tahap terakhir yaitu saat ada objek jenis penyakit kulit yang masuk atau terdeteksi oleh camera m aka citra tersebut sudah dapat dilakukan proses klasifikasi kemudian divalidasikan bahwa data tersebut sama dengan yang ada pada database untuk memunculkan label nama pada dataset serta memunculkan nilai confidence pada citra jenis penyakit kulit yang terdete ksi. 3.2 analisis kebutuhan analisis kebutuhan merupakan menganalisis komponen yang diperlukan dalam pembuatan dan menjalankan program proses ini mencakup evaluasi identifikasi dan pemetaan kebutuhan dari berbagai perangkat yang terlibat dalam pembuatan system dan program pada penelitian ini. berikut analisis kebutuhan dari penelitian yang dibuat . 3.2.1. analisis kebutuhan perangkat keras perangkat keras yang digunakan dalam penelitian identifikasi penyakit kulit pada manusia menggunakan laptop acer predator helios neo 16 dan mobile phone atau smartphone xiaomi redmi note 7 dengan bahasa pemrograman python dengan spesifikasi yang dapat dilihat pada tabel 3.1. tabel 3. 1 daftar perangkat keras no perangkat qty spesifikasi 1 laptop acer predator helios neo 16 1 13th gen intelr core tm i7 13700hx 24 cpus 2.1ghz. random acces memory 8gb. graphics card nvidia geforce rtx 4060 8gb solid state drive 2tb. 2 mobile phone smartphone 1 camera hd 48mp 169 1280x720 f1.8 wide dual led flash hdr panorama rgb red green blue 3.2.2. analisis kebutuhan perangkat lunak perangkat lunak yang digunakan dalam penelitian identifikasi penyakit kulit pada manusia menggunakan operating system windows jupyter lab dengan bahasa pemrograman python dan visual studio sebagai text editor yang dapat dilihat pada tabel 3.2. tabel 3. 2 daftar perangkat keras no perangkat lunak version 1 operating system windows 11 pro single language 64 bit 10.0 build 22631 2 python 3.7.0 3 jupyter notebook labs 7.2.1 4.2.2 4 visual studio code may 2024 version 1.90 3.2.3. analisis objek program dengan menggunakan metode bidirectional image text matching deep learning ini mempunyai beberapa objek yang diterapkan pada penelitian ini yaitu 1. identifikasi berbagai macam jenis penyakit kulit dengan memunculkan citra gambar yang didapat dan deskripsi mengenai penyakit kulit yang teridentifikasi dibawah citra gambar untuk setiap objek penyakit kulit yang terdeteksi data yang digunakan memiliki variasi jenis penyakit kulit dengan kategori 2 penyakit kulit menular candidiasis dan molluscum dan 2 penyakit kulit tidak menular eczhema dan melanoma dengan masing masing kelas memiliki 1000 citra penyakit kulit yang di dapat pada website international dermnet nz dermnetnz.org 2024 dan the international skin imaging collaboration isic isicarchive.com 2024 . 2. program identifikasi berbagai macam objek penyakit kulit pada manusia ditampilkan secara real time menggunakan file upload kamera mobile phone. 3.3 akuisisi data set proses akuisisi citra dilakukan dengan melakukan pengunduhan data dari berbagai sumber online international skin disease seperti pada website dermnetnz.org dan www.isic archive.com yang merupakan referensi gratis berbasis website untuk informasi tentang berbagai kondisi kulit. website ini menyediakan gambar gambar resolusi tinggi dari berbagai penyakit kulit baik yang menular maupun tidak menular serta memberikan deskripsi lengkap tentang penyakti tersebut meliputi gejala dan pengobatan. citra yang diperoleh kemudian diseleksi berdasarkan fokus penelitian yaitu identifikasi penyakit kulit menular candidiasis dan molluscum dan tidak menular eczhema dan melanoma . data citra yang digunakan berasal dari pasien dewasa dan anak anak dengan kondisi kulit yang jelas menunjukkan ge jala atau kelainan seperti lesi atau ruam. contoh citra yang akan digunakan pada penelitian seperti terlihat pada gambar 3. 2 gambar 3. 2 citra penyakit kulit yang berasal dari website isic isicarchive.com 2024 3.3.1. dataset penyakit kulit dataset pada penelitian ini dibagi menjadi 2 bagian yaitu 80 data training dan 20 data testing objek jenis penyakit kulit . dataset bersumber dari citra data image dan deskripsi data teks beberapa jenis penyakit kulit sejumlah 4000 citra dengan 4 jenis penyakit kulit yang terdiri dari echzema melanoma candidiasis dan molluscum dengan memiliki 1000 citra berbeda setiap jenis penyakit kulit . dari keempat jenis penyakit kulit tersebut dibagi menjadi 2 kelompok sebagai penyakit kulit menular dan tidak menular. 3.3.1.1. data gambar data image ini mencakup berbagai jenis gambar yang menampilkan gejala dan karakteristik penyakit kulit yang digunakan pada peneltian ini eczhema melanoma candidiasis dan molluscum seperti ruam bintik bintik lepuhan atau lesi kulit lainnya . ukuran citra asli yang didapat berukuran 294 x 222 yang akan diproses menjadi 256 x 256 sehingga ukuran gambar menjadi presisi dan pengambilan gambar diambil dari berbagai posisi yang berbeda sehingga posisi dalam proses training data akan mendapat banyak posisi p engenalan 1 jenis penyakit kulit dengan format citra jpeg joint photographic experts group serta pengambilan gambar dengan kamera . penggunaan data gambar sangat penting dalam penelitian ini untuk membandingkan dan mempelajari pola visual yang terkait dengan berbagai penyakit kulit. data image pada penelitian ini terdiri 4000 gambar dari 4 jenis penyakit kulit yaitu eczhema melanoma candidiasis dan molluscum yang dibagi menjadi 2 kelompok menular dan tidak menular. data gambar dapat dilihat pada gambar 3.3 . gambar 3. 3 data gambar penyakit kulit 3.3.1.2. data teks data teks penyakit kulit merujuk kepada informasi tertulis yang berisi deskripsi dan karakteristik berbagai kondisi dermatologis. data pada penelitian ini meliputi penjelasan tentang gejala gejala khas seperti gatal gatal perubahan warna kulit tekstur dan lokasi lesi serta penjelasan mengenai cara penanganan maupun pengobatan yang dapat dilakukan pasien . informasi ini penting untuk diagnosis dan pemahaman lebih lanjut tentang berbagai penyakit kulit seperti dermatitis eksim psoriasis dan infeksi jamur kulit. pada penelitian ini data teks diproses menggunakan teknik pengolahan bahasa alami atau natural language processing nlp untuk mengidentifikasi kata kunci dan pola yang terkait dengan setiap kondisi kulit. berikut data teks yang digunakan pada penelitian ini dapat dilihat pada tabel 3.3 tabel 3. 3 data teks penyakit kulit 3.4 pre processing data pada tahapan ini data gambar penyakit kulit preprocessing mencakup berbagai teknik seperti pengubahan ukuran gambar normalisasi piksel peningkatan kontras penghapusan noise serta melakukan segmentasi dan fitur ekstraksi. teknik ini bertujuan untuk meningkatkan kualitas gambar dan memastikan konsistensi data sehingga fitur fitur penting dapat diekstraksi dengan lebih efektif oleh algoritma analisis atau mode l kecerdasan buatan. sedangkan pada data teks penyakit kulit preprocessing melibatkan beberapa tahap seperti tokenisasi penghapusan stop words stemming lemmatization dan tagging . langkah langkah ini membantu dalam menyederhanakan teks mengurangi dime nsionalitas dan meningkatkan efisiensi analisis teks. dengan preprocessing yang tepat data gambar dan teks menjadi lebih bersih dan terstruktur memungkinkan model machine learning untuk menghasilkan prediksi yang lebih akurat dan andal. tahapan preprocessing dapat dilihat pada gambar 3.4. gambar 3. 4. tahapan preprocessing data 3.4.1. preprocessing data gambar proses ini melibatkan beberapa teknik utama. pertama pengubahan ukuran resizing gambar dilakukan untuk memastikan bahwa semua gambar memiliki dimensi yang seragam yaitu 256 x 256 yang penting untuk pengolahan batch dan integrasi dalam model. kedua normalisasi piksel diterapkan untuk mengatur nilai piksel dalam rentang tertentu biasanya antara 0 dan 1 guna meningkatkan stabilitas dan kecepatan konvergensi model. ketiga peningk atan kontras contrast enhancement dan penghapusan noise bertujuan untuk memperjelas fitur fitur penting dalam gambar seperti tepi atau tekstur yang mungkin relevan untuk diagnosis penyakit kulit. keempat segmentasi data untuk memisahkan area kulit yang terkena penyakit dari bagian yang sehat . kelima fitur ekstraksi memungkinkan identifikasi karakteristik spesifik dari kondisi kulit seperti ukuran dan bentuk lesi distribusi warna dan tekstur permukaan kulit. preprocessing data gambar dapat dilihat pada gambar 3.5. gambar 3. 5 preprocessing data gambar 3.4.1.1. resizing data pada tahap resize da ta ini betujuan untuk mengubah ukuran citra penyakit kulit menjadi resolusi tetap 256x256 piksel. langkah ini penting untuk memastikan bahwa semua citra memiliki ukuran yang konsisten sebelum digunakan dalam proses analisis data atau pelatihan model pembelajaran mesin. skrip ini menggunakan pustaka opencv untuk memuat mengubah ukuran dan menyimpan citra. dapat dilihat pada algoritma 3.1. algoritma 3.1 algoritma resize citra input citra penyakit kulit dengan ukuran asli ouput citra penyakit kulit dengan ukuran sama 256x256 proses 1. inisialisasi citra 2. periksa dan buat direktori output 3. iterasi melalui citra dalam direktori input 4. muat citra 5. ubah ukuran citra 6. simpan citra yang telah diubah ukurannya ukuran dan bentuk citra hasil resizing disimpan pada folder output masing masing penyakit kulit yang selanjutnya akan diproses pada tahap berikutnya. algoritma 3.1 diatas dapat dikonversi kedalam pseudo code 1 yang dapat diimplementasikan pada pemrograman python. pseudocode 1. resize citra def resize_imageimage size256 256 resized_image cv2.resizeimage size interpolationcv2.inter_area return resized_image sehingga tampilan hasil program terlihat pada gambar 3. 6 berikut. seperti terlihat pada gambar proses resize ditujukan pada ukuran gambar yang terlihat presisi dan sama yaitu 256x256. gambar 3. 6. hasil resize data gambar 3.4.1.2. normalisasi data pada tahapan ini data yang telah di resize pada tahap sebelumya dinormalisasi. melalui tahap normalisasi data bertujuan untuk mengubah nilai piksel citra ke dalam rentang yang konsisten biasanya antara 0 dan 1 atau 1 dan 1. proses ini membantu dalam mengurangi variasi yang tidak diinginkan antar citra seperti perbedaan pencahayaan dan kontras sehingga fitur yang relevan menjadi lebih menonjol. normalisasi dilakukan dengan membagi nilai piksel setiap citra dengan nilai maksimum piksel biasanya 255 untuk citra 8 bit sehingga setiap piksel memiliki nilai yang proporsional dalam rentang yang diinginkan. langkah langkah normalisasi data d apat di lihat pada algoritma 3.2. algoritma 3. 2 algoritma no rmalisasi citra input citra penyakit kulit hasil resize ouput citra penyakit kulit dengan hasil normalisasi proses 1. inisialisasi citra 2. muat data citra 3. ubah tipe data citra 4. normalisasi nilai piksel 5. simpan dan gunakan hasil normalisasi citra hasil normalisasi disimpan yang selanjutnya akan diproses pada tahap berikutnya. algoritma 3. 2 diatas dapat dikonversi kedalam pseudo code 2 yang dapat diimplementasikan pada pemrograman python. pseudocode 2. normalisasi citra def normalize_imageimage convert image to float32 type for normalization image image.astypenp.float32 normalize the image normalized_image image 255.0 return normalized_image sehingga tampilan hasil program terlihat pada gambar 3.7 berikut. seperti terlihat pada gambar proses normalisasi ditujukan mengubah nilai piksel citra ke dalam rentang yang konsisten biasanya antara 0 dan 1 atau 1 dan 1. gambar 3. 7 hasil normalisasi data citra 3.4.1.3. peningkatan kontras data pada tahap ini dilakuakn peningkatan kontras pada data citra yang telah di normalisasi bertujuan untuk meningkatkan perbedaan antara nilai intensitas piksel yang berdekatan. dengan meningkatkan perbedaan antara nilai intensitas piksel proses ini membantu dalam meningkatkan ketajaman citra dan membuatnya lebih mudah untuk dianalisis. proses ini tidak hanya membuat citra lebih tajam dan lebih jelas tetapi juga dapat meningkatkan kemampuan sistem analisis citra seperti deteksi objek atau segmentasi yang lebih baik. langkah langkah peningkatan kontras dapat dilihat pada algoritma 3.3 . algoritma 3. 3 algoritma peningkatan kontras input citra penyakit kulit hasil normalisasi ouput citra penyakit kulit dengan peningkatan kontras proses 1. inisialisasi citra 2. muat data citra 3. ubah tipe data citra 4. hitung rata rata intensitas piksel 5. peningkatan kontras 6. simpan hasil peningkatan kontras citra hasil peningkatan kontras disimpan yang selanjutnya akan diproses pada tahap berikutnya. algoritma 3. 3 diatas dapat dikonversi kedalam pseudo code 3 yang dapat diimplementasikan pada pemrograman python. pseudocode 3. peningkatan kontras citra def normalize_imageimage convert image to float32 type for normalization image image.astypenp.float32 normalize the image normalized_image image 255.0 return normalized_image sehingga tampilan hasil program terlihat pada gambar 3.8 berikut. seperti terlihat pada gambar proses peningkatan kontras ditujukan untuk meningkatkan perbedaan antara nilai intensitas piksel yang berdekatan. dengan meningkatkan perbedaan antara nilai intensitas piksel proses ini membantu dalam meningkatkan ketajaman citra dan membuatnya lebih mudah untuk dianalisis. gambar 3. 8 hasil peningkatan kontras 3.4.1.4. penghapusan noise data pada tahap ini dilakukan penghapusan noise yang bertujuan untuk menghilangkan noise pada citra . noise pada citra kulit dapat muncul karena berbagai alasan seperti kualitas kamera yang rendah kondisi pencahayaan yang buruk atau bahkan gangguan selama pengambilan gambar. untuk membersihkan gambar dari gangguan ini digunakan berbagai teknik penghapusan noise. filter median misalnya sangat baik untuk mengatasi noise jenis salt andpepper dengan menggantikan nilai setiap piksel dengan median dari piksel piksel sekitarnya sementara filter gaussian menghaluskan gambar dengan mempertahankan tepi dan detail penting . dengan menghilangkan noise gambar kulit menjadi lebih bersih dan detail penting seperti warna bentuk dan tekstur lesi menjadi lebih jelas. ini sangat membantu dokter atau sistem analisis otomatis untuk mengidentifikasi dan mengevaluasi kondisi kulit dengan lebih akurat memastikan diagnosis dan rencana perawatan yang lebih efektif. langkah langkah penghapusan noise menggunakan median dan gaussian filter dapat dilihat pada algoritma 3.4. algoritma 3. 4 algoritma penghapusan noise input citra penyakit kulit hasil peningkatan kontras ouput citra penyakit kulit dengan penghapusan noise proses 1. inisialisasi citra 2. muat data citra 3. penghapusan noise menggunakan filter median 4. penghapusan noise menggunakan filter gaussian 5. tampilakan dan simpan hasil citra hasil penghapusan noise menggunakan median filter dan gaussian filter disimpan yang selanjutnya akan diproses pada tahap berikutnya. algoritma 3. 4 diatas dapat dikonversi kedalam pseudo code 4 yang dapat diimplementasikan pada pemrograman python. pseudocode 4. penghapusan noise def denoise_medianimage kernel_size3 denoised_image cv2.medianblurimage kernel_size return denoised_image def denoise_gaussianimage kernel_size3 denoised_image cv2.gaussianblurimage kernel_size kernel_size 0 return denoised_image sehingga tampilan hasil program terlihat pada gambar 3. 9 berikut. seperti terlihat pada gambar proses penghapusan noise menggunakan gabungan median filter dan gaussian filter ditujukan untuk m enghilangkan objek objek yang tidak terpakai dengan menggunakan kernel rendah citra yang dihasilkan tidak terlalu mendapatkan blur yang sangat singnifikan sehingga objek suatu penyakit kulit masih dapat terlihat jelas tanpa adanya noise yang tidak terpakai . dengan menghapus noise maka citra yang dihasilkan menjadi lebih bersih proses ini membantu dalam meningkatkan fokus citra terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. gambar 3. 9 hasil penghapusan noise 3.4.1.5. segmentasi data pada tahap ini dilakukan segmentasi dengan thresholding atau penghapusan bagian yang tidak diperlukan seperti background untuk mendapatkan objek penyakit kulit yang digunakan pada penelitian serta menambahkan active contour untuk mendapatkan objek yang ditandai sebagai penyakit kulit . proses ini melibatkan beberapa tahapan penting. pertama citra awal dimuat dan mungkin diubah menjadi citra skala abu abu untuk mempermudah analisis intensitas piksel. selanjutnya nilai ambang dipilih atau dihitung berdasarkan karakteristik citra sepert i histogram int ensitas piksel. pada tahap thresholding piksel dalam citra yang melebihi nilai ambang akan diberi warna atau nilai putih 255 sementara piksel yang lebih rendah akan diberi warna atau nilai hitam 0 menghasilkan citra biner. langkah langkah segmentasi menggunakan thresholding atau penghapusan bagian yang tidah dibutuhkan dapat dilihat pada algoritma 3.5. algoritma 3. 5 algoritma segmentasi input citra penyakit kulit hasil penghapusan noise ouput citra penyakit kulit hasil segmentasi thresholding proses 1. inisialisasi citra 2. muat data citra 3. konversi ke citra grayscale 4. tentukan nilai threshold 5. segmentasi dengan thresholding 6. inversi citra hasil thresholding 7. pemulihan warna asli 8. simpan hasil citra hasil segmentasi menggunakan thresholding disimpan yang selanjutnya akan diproses pada tahap berikutnya. algoritma 3. 5 diatas dapat dikonversi kedalam pseudo code 5 yang dapat diimplementasikan pada pemrograman python. pseudocode 5. segmentasi def segment_with_thresholdimage threshold_value _ segmented_image cv2.thresholdimage threshold_value 255 cv2.thresh_binary return segmented_image def invert_imageimage inverted_image cv2.bitwise_notimage return inverted_image def find_contoursimage contours _ cv2.findcontoursimage cv2.retr_external cv2.chain_approx_simple return contours def draw_contoursimage contours image_with_contours image.copy cv2.drawcontoursimage_with_contours contours 1 0 255 0 2 return image_with_contours def restore_colororiginal_image inverted_segmented_image mask cv2.mergeinverted_segmented_image inverted_segmented_image inverted_segmented_image inverted_mask cv2.bitwise_notmask restored_image cv2.bitwise_ororiginal_image inverted_mask return restored_image sehingga tampilan hasil program terlihat pada gambar 3.10 berikut. seperti terlihat pada gambar proses segmentasi menggunakan thresholding dan active contour ditujukan untuk menghilangkan objek objek yang tidak digunakan dan memberi tanda pada objek yang digunakan untuk proses selanjutnya . dengan menghapus nilainilai pada citra yang tidak terpakai maka citra yang dihasilkan menjadi lebih bersih proses ini membantu dalam menentukan focus objek terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. gambar 3. 10 hasil segmentasi 3.4.1.6. ekstraksi fitur tahapan ini melibatkan pengambilan informasi relevan dari citra yang dapat digunakan untuk mengklasifikasikan dan mendiagnosis kondisi kulit. setelah citra tersegmentasi dengan baik langkah berikutnya yaitu mengekstraksi fitur fitur yang relevan dari setiap area tersegmentasi. fitur fitur ini berupa tekstur bentuk dan warna yang dapat membedakan antara lesi kulit yang berbeda. dalam beberapa kasus tidak semua fitur yang diekstraksi diperlukan. proses seleksi fitur membantu dalam memilih subset fitur terbaik yang paling bermakna untuk klasifikasi atau diagnosa yang akurat. 3.4.1.6.1. ekstraksi fitur warna tahap ini dimulai dengan memuat citra dalam format yang sesuai seperti jpeg atau png dan memisahkan informasi warna menjadi tiga kanal utama merah red hijau green dan biru blue. setiap kanal ini mewakili intensitas cahaya pada panjang gelombang yang berbeda dan memiliki rentang nilai dari 0 hingga 255 dalam skala 8 bit. langkah langkah ekstraksi fitur warna dapat dilihat pada algoritma 3.6 . algoritma 3. 6 ekstraksi fitur warna input citra penyakit kulit hasil segmentasi ouput nilai fitur ekstraksi warna proses 1. inisialisasi citra 2. muat data citra 3. pisahkan kanal warna r g b 4. hitung statistik kanal a ratarata mean b standar deviasi standard deviation 5. simpan fitur nilai hasil ektraksi fitur warna menggunakan rgb disimpan yang selanjutnya akan diproses pada tahap berikutnya. algoritma 3. 6 diatas dapat dikonversi kedalam pseudo code 6 yang dapat diimplementasikan pada pemrograman python. pseudocode 6. ektraksi fitur warna def extract_rgb_featuresimage split the image into rgb channels b g r cv2.splitimage calculate mean and standard deviation for each channel r_mean b.mean g_mean g.mean b_mean r.mean r_std b.std g_std g.std b_std r.std return r_mean g_mean b_mean r_std g_std b_std sehingga tampilan hasil program terlihat pada gambar 3.11 berikut. seperti terlihat pada gambar proses ektraksi fitur menggunakan rgb dan menunjukan hasil histogram ditujukan untuk memisahkan informasi warna menjadi tiga kanal utama merah red hijau green dan biru blue. dengan mendapatkan nilai nilai pada setiap kanal rgb maka informasi yang didapat akan semakin kompleks proses ini membantu dalam menentukan setiap warna yang paling dominan pada objek terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. gambar 3. 11 hasil ektraksi fitur warna 3.4.1.6.2. ektraksi fitur bentuk tahapan ini dimulai dengan pra pemrosesan citra untuk meningkatkan kualitas dan mempersiapkannya untuk ekstraksi fitur. langkah pertama biasanya melibatkan segmentasi objek dari latar belakang yang dapat dilakukan dengan metode seperti thresholding atau d eteksi tepi. setelah objek tersegmentasi berbagai fitur geometris seperti luas keliling bentuk dan orientasi dapat diekstraksi. langkah langkah ekstraksi fitur bentuk dapat dilihat pada algoritma 3.7. algoritma 3. 7 ekstraksi fitur bentuk input citra penyakit kulit hasil segmentasi ouput nilai fitur ekstraksi bentuk proses 1. inisialisasi citra 2. muat data citra 3. ekstraksi kontur 4. ekstraksi fitur geometris 5. simpan hasil nilai hasil ektraksi fitur bentuk menggunakan contour dan geometris disimpan yang selanjutnya akan diproses pada tahap berikutnya. algoritma 3. 7 diatas dapat dikonversi kedalam pseudo code 7 yang dapat diimplementasikan pada pemrograman python. pseudocode 7. ektraksi fitur bentuk def extract_shape_featuresimage gray cv2.cvtcolorimage cv2.color_bgr2gray _ thresh cv2.thresholdgray 0 255 cv2.thresh_binary cv2.thresh_otsu contours _ cv2.findcontoursthresh cv2.retr_external cv2.chain_approx_simple areas perimeters circularities eccentricities for contour in contours area cv2.contourareacontour perimeter cv2.arclengthcontour true circularity 4 np.pi area perimeter 2 if perimeter 0 else 0 if lencontour 5 ellipse cv2.fitellipsecontour center axes orientation ellipse major_axis maxaxes minor_axis minaxes eccentricity np.sqrt1 minor_axis 2 major_axis 2 if major_axis 0 else 0 else eccentricity 0 areas.appendarea perimeters.appendperimeter circularities.appendcircularity eccentricities.appendeccentricity avg_area np.meanareas avg_perimeter np.meanperimeters avg_circularity np.mean circularities avg_eccentricity np.meaneccentricities return avg_area avg_perimeter avg_circularity avg_eccentricity sehingga tampilan hasil program terlihat pada gambar 3.12 berikut. seperti terlihat pada gambar proses ektraksi fitur menggunakan bentuk contour dan geometris menunjukan hasil nilai untuk setiap citra ditujukan untuk memisahkan informasi bentuk menjadi area perimeter circularity dan exccentricity . dengan mendapatkan nilainilai bentuk maka informasi yang didapat akan semakin kompleks proses ini membantu dalam menentukan setiap bentuk yang paling dominan pada objek terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. gambar 3. 12 hasil ektraksi fitur bentuk 3.4.1.6.3. ektraksi fitur tekstur pada tahapan ekstraksi fitur tekstur melibatkan beberapa langkah kunci untuk menggambarkan dan menganalisis tekstur citra secara sistematis . tahap awal mencakup pemilihan glcm sebagai metode utama untuk mengekstraksi fitur tekstur. setelah glcm terbentuk tahap selanjutnya adalah ekstraksi fitur fitur statistik dari matriks glcm. fitur fitur ini mungkin mencakup energi kontras homogenitas dan korelasi yang masing masing memberikan informasi tentang struktur dan pola tekstur dalam citra yang d ianalisis. langkah langkah ekstraksi fitur tekstur dengan menggunakan metode glcm sebagai acuan tekstur dapat dilihat pada algoritma 3. 8. algoritma 3. 8 ekstraksi fitur tektur input citra penyakit kulit hasil segmentasi ouput nilai fitur ekstraksi tekstur proses 1. inisialisasi citra 2. muat data citra 3. pembentukan glcm 4. normalisasi glcm 5. ekstraksi fitur statistik 6. simpan hasil nilai hasil ektraksi fitur tekstur menggunakan glcm disimpan yang selanjutnya akan diproses pada tahap berikutnya. algoritma 3. 8 diatas dapat dikonversi kedalam pseudo code 8 yang dapat diimplementasikan pada pemrograman python . pseudocode 8. ektraksi fitur tektur def calculate_glcm_featuresimage_path load gambar dan konversi ke grayscale image io.imreadimage_path gray_image color.rgb2grayimage gray_image img_as_ubytegray_image konversi ke tipe data uint8 hitung glcm dengan jarak dan arah yang ditentukan distances 1 2 3 jarak d angles 0 np.pi4 np.pi2 3np.pi4 arah θ glcm graycomatrixgray_image distancesdistances anglesangles symmetrictrue normedtrue ekstraksi fitur tekstur dari glcm contrast graycopropsglcm contrast dissimilarity graycopropsglcm dissimilarity homogeneity graycopropsglcm homogeneity energy graycopropsglcm energy correlation graycopropsglcm correlation mengembalikan hasil fitur sebagai tuple return contrast.mean dissimilarity.mean homogeneity.mean energy.mean correlation.mean sehingga tampilan hasil program terlihat pada gambar 3.13 berikut. seperti terlihat pada gambar proses ektraksi fitur menggunakan glcm menunjukan hasil nilai untuk setiap citra ditujukan untuk memisahkan informasi tekstur menjadi contrast dissimilarity homogeneity energy dan correlation. dengan mendapatkan nilai nilai tekstur maka informasi yang didapat akan semakin kompleks proses ini membantu dalam menentukan setiap tekstur yang paling dominan pada objek terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. gambar 3. 13 hasil ekstraksi fitur tekstur dengan glcm 3.4.2. preprocessing data teks tahap pre processing data teks dilakukan serangkaian langkah penting dalam pengolahan informasi teks yang bertujuan untuk membersihkan merapihkan dan mempersiapkan data sebelum dilakukan analisis lebih lanjut. proses ini krusial karena data teks sering kali tidak terstruktur dan dapat mengandu ng berbagai jenis noise atau informasi yang tidak relevan yang dapat mempengaruhi hasil analisis. beberapa tahap yang dilakukan pada preprocessing data teks ini meliputi pertama tokenisasi dilakukan untuk memecah te ks menjadi unitunit yang lebih kecil seperti kata kata atau kalimat. setelah itu langkah pembersihan cleaning dilakukan untuk menghilangkan elemen elemen yang tidak relevan seperti karakter khusus atau token seperti stopwords yang tidak memberikan banyak informasi. selanjutnya stemming atau lemmatisasi proses ini mengubah kata kata menjadi bentuk dasarnya lemmas atau akar kata stems untuk mengurangi variasi kata yang memiliki arti yang sama. contohnya,1 tahapan penelitian dokumentasi peneliti tahapan penelitian dapat dilihat pada gambar 3.1. tahapan penelitian yang dilakukan terdiri dari 9 tahapan yaitu dimulai dari studi literatur sebagai dasar penelitian analisis kebutuhan pada system yang akan dibangun pengumpulan dataset preprocessing data membangun model training model evaluasi model deployme nt model dan implementasi model yang telah dibuat ke dalam smartphone. saat program telah dijalankan program akan mengakuisisi dataset kemudian dataset akan melalui tahap preprocessing untuk m enormalkan data kemudian setelah melalui tahap preprocessing selanjutnya mentraining dataset yang sudah didapatkan jika dataset berhasil dilatih dan juga divalidasi maka berlanjut ke tahap berikutnya yaitu tahapan test ing dengan menerapkan model yang dibuat kedalam mobile phone atau smartphone. tahap selanjutnya jika camera telah menyala maka artinya sudah siap untuk mendeteksi objek jenis penyakit kulit . pada tahap terakhir yaitu saat ada objek jenis penyakit kulit yang masuk atau terdeteksi oleh camera m aka citra tersebut sudah dapat dilakukan proses klasifikasi kemudian divalidasikan bahwa data tersebut sama dengan yang ada pada database untuk memunculkan label nama pada dataset serta memunculkan nilai confidence pada citra jenis penyakit kulit yang terdete ksi. 3.2 analisis kebutuhan analisis kebutuhan merupakan menganalisis komponen yang diperlukan dalam pembuatan dan menjalankan program proses ini mencakup evaluasi identifikasi dan pemetaan kebutuhan dari berbagai perangkat yang terlibat dalam pembuatan system dan program pada penelitian ini. 3.2.1. analisis kebutuhan perangkat keras perangkat keras yang digunakan dalam penelitian identifikasi penyakit kulit pada manusia menggunakan laptop acer predator helios neo 16 dan mobile phone atau smartphone xiaomi redmi note 7 dengan bahasa pemrograman python dengan spesifikasi yang dapat dilihat pada tabel 3.1. tabel 3. 2 mobile phone smartphone 1 camera hd 48mp 169 1280x720 f1.8 wide dual led flash hdr panorama rgb red green blue 3.2.2. analisis kebutuhan perangkat lunak perangkat lunak yang digunakan dalam penelitian identifikasi penyakit kulit pada manusia menggunakan operating system windows jupyter lab dengan bahasa pemrograman python dan visual studio sebagai text editor yang dapat dilihat pada tabel 3.2. tabel 3. 2 daftar perangkat keras no perangkat lunak version 1 operating system windows 11 pro single language 64 bit 10.0 build 22631 2 python 3.7.0 3 jupyter notebook labs 7.2.1 4.2.2 4 visual studio code may 2024 version 1.90 3.2.3. analisis objek program dengan menggunakan metode bidirectional image text matching deep learning ini mempunyai beberapa objek yang diterapkan pada penelitian ini yaitu 1. identifikasi berbagai macam jenis penyakit kulit dengan memunculkan citra gambar yang didapat dan deskripsi mengenai penyakit kulit yang teridentifikasi dibawah citra gambar untuk setiap objek penyakit kulit yang terdeteksi data yang digunakan memiliki variasi jenis penyakit kulit dengan kategori 2 penyakit kulit menular candidiasis dan molluscum dan 2 penyakit kulit tidak menular eczhema dan melanoma dengan masing masing kelas memiliki 1000 citra penyakit kulit yang di dapat pada website international dermnet nz dermnetnz.org 2024 dan the international skin imaging collaboration isic isicarchive.com 2024 . 2. program identifikasi berbagai macam objek penyakit kulit pada manusia ditampilkan secara real time menggunakan file upload kamera mobile phone. website ini menyediakan gambar gambar resolusi tinggi dari berbagai penyakit kulit baik yang menular maupun tidak menular serta memberikan deskripsi lengkap tentang penyakti tersebut meliputi gejala dan pengobatan. citra yang diperoleh kemudian diseleksi berdasarkan fokus penelitian yaitu identifikasi penyakit kulit menular candidiasis dan molluscum dan tidak menular eczhema dan melanoma . 2 citra penyakit kulit yang berasal dari website isic isicarchive.com 2024 3.3.1. dataset penyakit kulit dataset pada penelitian ini dibagi menjadi 2 bagian yaitu 80 data training dan 20 data testing objek jenis penyakit kulit . dataset bersumber dari citra data image dan deskripsi data teks beberapa jenis penyakit kulit sejumlah 4000 citra dengan 4 jenis penyakit kulit yang terdiri dari echzema melanoma candidiasis dan molluscum dengan memiliki 1000 citra berbeda setiap jenis penyakit kulit . dari keempat jenis penyakit kulit tersebut dibagi menjadi 2 kelompok sebagai penyakit kulit menular dan tidak menular. 3.3.1.1. data gambar data image ini mencakup berbagai jenis gambar yang menampilkan gejala dan karakteristik penyakit kulit yang digunakan pada peneltian ini eczhema melanoma candidiasis dan molluscum seperti ruam bintik bintik lepuhan atau lesi kulit lainnya . penggunaan data gambar sangat penting dalam penelitian ini untuk membandingkan dan mempelajari pola visual yang terkait dengan berbagai penyakit kulit. data image pada penelitian ini terdiri 4000 gambar dari 4 jenis penyakit kulit yaitu eczhema melanoma candidiasis dan molluscum yang dibagi menjadi 2 kelompok menular dan tidak menular. informasi ini penting untuk diagnosis dan pemahaman lebih lanjut tentang berbagai penyakit kulit seperti dermatitis eksim psoriasis dan infeksi jamur kulit. 3 data teks penyakit kulit 3.4 pre processing data pada tahapan ini data gambar penyakit kulit preprocessing mencakup berbagai teknik seperti pengubahan ukuran gambar normalisasi piksel peningkatan kontras penghapusan noise serta melakukan segmentasi dan fitur ekstraksi. teknik ini bertujuan untuk meningkatkan kualitas gambar dan memastikan konsistensi data sehingga fitur fitur penting dapat diekstraksi dengan lebih efektif oleh algoritma analisis atau mode l kecerdasan buatan. dapat dilihat pada algoritma 3.1. algoritma 3.1 algoritma resize citra input citra penyakit kulit dengan ukuran asli ouput citra penyakit kulit dengan ukuran sama 256x256 proses 1. inisialisasi citra 2. periksa dan buat direktori output 3. iterasi melalui citra dalam direktori input 4. muat citra 5. ubah ukuran citra 6. simpan citra yang telah diubah ukurannya ukuran dan bentuk citra hasil resizing disimpan pada folder output masing masing penyakit kulit yang selanjutnya akan diproses pada tahap berikutnya. 3 algoritma peningkatan kontras input citra penyakit kulit hasil normalisasi ouput citra penyakit kulit dengan peningkatan kontras proses 1. inisialisasi citra 2. muat data citra 3. ubah tipe data citra 4. hitung rata rata intensitas piksel 5. peningkatan kontras 6. simpan hasil peningkatan kontras citra hasil peningkatan kontras disimpan yang selanjutnya akan diproses pada tahap berikutnya. 4 algoritma penghapusan noise input citra penyakit kulit hasil peningkatan kontras ouput citra penyakit kulit dengan penghapusan noise proses 1. inisialisasi citra 2. muat data citra 3. penghapusan noise menggunakan filter median 4. penghapusan noise menggunakan filter gaussian 5. tampilakan dan simpan hasil citra hasil penghapusan noise menggunakan median filter dan gaussian filter disimpan yang selanjutnya akan diproses pada tahap berikutnya. dengan menghapus noise maka citra yang dihasilkan menjadi lebih bersih proses ini membantu dalam meningkatkan fokus citra terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. 5 algoritma segmentasi input citra penyakit kulit hasil penghapusan noise ouput citra penyakit kulit hasil segmentasi thresholding proses 1. inisialisasi citra 2. muat data citra 3. konversi ke citra grayscale 4. tentukan nilai threshold 5. segmentasi dengan thresholding 6. inversi citra hasil thresholding 7. pemulihan warna asli 8. simpan hasil citra hasil segmentasi menggunakan thresholding disimpan yang selanjutnya akan diproses pada tahap berikutnya. dengan menghapus nilainilai pada citra yang tidak terpakai maka citra yang dihasilkan menjadi lebih bersih proses ini membantu dalam menentukan focus objek terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. 6 ekstraksi fitur warna input citra penyakit kulit hasil segmentasi ouput nilai fitur ekstraksi warna proses 1. inisialisasi citra 2. muat data citra 3. pisahkan kanal warna r g b 4. hitung statistik kanal a ratarata mean b standar deviasi standard deviation 5. simpan fitur nilai hasil ektraksi fitur warna menggunakan rgb disimpan yang selanjutnya akan diproses pada tahap berikutnya. dengan mendapatkan nilai nilai pada setiap kanal rgb maka informasi yang didapat akan semakin kompleks proses ini membantu dalam menentukan setiap warna yang paling dominan pada objek terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. 11 hasil ektraksi fitur warna 3.4.1.6.2. ektraksi fitur bentuk tahapan ini dimulai dengan pra pemrosesan citra untuk meningkatkan kualitas dan mempersiapkannya untuk ekstraksi fitur. 7 ekstraksi fitur bentuk input citra penyakit kulit hasil segmentasi ouput nilai fitur ekstraksi bentuk proses 1. inisialisasi citra 2. muat data citra 3. ekstraksi kontur 4. ekstraksi fitur geometris 5. simpan hasil nilai hasil ektraksi fitur bentuk menggunakan contour dan geometris disimpan yang selanjutnya akan diproses pada tahap berikutnya. algoritma 3. pseudocode 7. ektraksi fitur bentuk def extract_shape_featuresimage gray cv2.cvtcolorimage cv2.color_bgr2gray _ thresh cv2.thresholdgray 0 255 cv2.thresh_binary cv2.thresh_otsu contours _ cv2.findcontoursthresh cv2.retr_external cv2.chain_approx_simple areas perimeters circularities eccentricities for contour in contours area cv2.contourareacontour perimeter cv2.arclengthcontour true circularity 4 np.pi area perimeter 2 if perimeter 0 else 0 if lencontour 5 ellipse cv2.fitellipsecontour center axes orientation ellipse major_axis maxaxes minor_axis minaxes eccentricity np.sqrt1 minor_axis 2 major_axis 2 if major_axis 0 else 0 else eccentricity 0 areas.appendarea perimeters.appendperimeter circularities.appendcircularity eccentricities.appendeccentricity avg_area np.meanareas avg_perimeter np.meanperimeters avg_circularity np.mean circularities avg_eccentricity np.meaneccentricities return avg_area avg_perimeter avg_circularity avg_eccentricity sehingga tampilan hasil program terlihat pada gambar 3.12 berikut. seperti terlihat pada gambar proses ektraksi fitur menggunakan bentuk contour dan geometris menunjukan hasil nilai untuk setiap citra ditujukan untuk memisahkan informasi bentuk menjadi area perimeter circularity dan exccentricity . dengan mendapatkan nilainilai bentuk maka informasi yang didapat akan semakin kompleks proses ini membantu dalam menentukan setiap bentuk yang paling dominan pada objek terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. 12 hasil ektraksi fitur bentuk 3.4.1.6.3. ektraksi fitur tekstur pada tahapan ekstraksi fitur tekstur melibatkan beberapa langkah kunci untuk menggambarkan dan menganalisis tekstur citra secara sistematis . tahap awal mencakup pemilihan glcm sebagai metode utama untuk mengekstraksi fitur tekstur. langkah langkah ekstraksi fitur tekstur dengan menggunakan metode glcm sebagai acuan tekstur dapat dilihat pada algoritma 3. 8. algoritma 3. 8 ekstraksi fitur tektur input citra penyakit kulit hasil segmentasi ouput nilai fitur ekstraksi tekstur proses 1. inisialisasi citra 2. muat data citra 3. pembentukan glcm 4. normalisasi glcm 5. ekstraksi fitur statistik 6. simpan hasil nilai hasil ektraksi fitur tekstur menggunakan glcm disimpan yang selanjutnya akan diproses pada tahap berikutnya. algoritma 3. pseudocode 8. ektraksi fitur tektur def calculate_glcm_featuresimage_path load gambar dan konversi ke grayscale image io.imreadimage_path gray_image color.rgb2grayimage gray_image img_as_ubytegray_image konversi ke tipe data uint8 hitung glcm dengan jarak dan arah yang ditentukan distances 1 2 3 jarak d angles 0 np.pi4 np.pi2 3np.pi4 arah θ glcm graycomatrixgray_image distancesdistances anglesangles symmetrictrue normedtrue ekstraksi fitur tekstur dari glcm contrast graycopropsglcm contrast dissimilarity graycopropsglcm dissimilarity homogeneity graycopropsglcm homogeneity energy graycopropsglcm energy correlation graycopropsglcm correlation mengembalikan hasil fitur sebagai tuple return contrast.mean dissimilarity.mean homogeneity.mean energy.mean correlation.mean sehingga tampilan hasil program terlihat pada gambar 3.13 berikut. seperti terlihat pada gambar proses ektraksi fitur menggunakan glcm menunjukan hasil nilai untuk setiap citra ditujukan untuk memisahkan informasi tekstur menjadi contrast dissimilarity homogeneity energy dan correlation. dengan mendapatkan nilai nilai tekstur maka informasi yang didapat akan semakin kompleks proses ini membantu dalam menentukan setiap tekstur yang paling dominan pada objek terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. 13 hasil ekstraksi fitur tekstur dengan glcm 3.4.2. preprocessing data teks tahap pre processing data teks dilakukan serangkaian langkah penting dalam pengolahan informasi teks yang bertujuan untuk membersihkan merapihkan dan mempersiapkan data sebelum dilakukan analisis lebih lanjut. proses ini krusial karena data teks sering kali tidak terstruktur dan dapat mengandu ng berbagai jenis noise atau informasi yang tidak relevan yang dapat mempengaruhi hasil analisis.
Alifurrohman_Kualifikasi.txt,"3.1 Kerangka Umum Penelitian
     Berikut ini merupakan kerangka penelitian yang menjelaskan tahapan yang dilakukan dalam penelitan ini. Berikut gambar 3.1 diagram alir penelitian

Persiapan Data
Definisikan Ukuran Input dan Parameter
Definisikan Multi-Head Attention
Desain Model
Definisikan DQN dengan Lapisan Tersembunyi
Output Layer untuk Q-values
Fungsi untuk Memilih Tindakan menggunakan
Strategi e-greedy
Fungsi untuk Memperbarui Model dengan
Pengalaman dari Replay Buffer
Gambar 3.1 Diagram Alir Penelitian


3.2 Pengumpulan Data
     Langkah awal adalah mengumpulkan dataset yang akurat dan relevan. Dataset didapatkan dari data sekunder, dataset ini merupakan hal yang penting dari simulasi dan eksperimen, mencakup koordinat lokasi yang mungkin meliputi lokasi depot dan titik pengiriman, jendela waktu untuk setiap pengiriman yang menentukan batas awal dan akhir kapan pengiriman harus dilakukan, serta jumlah kendaraan. Data ini harus mencerminkan situasi dunia nyata untuk memastikan bahwa model yang dikembangkan dapat diaplikasikan secara praktis. 3.3 Persiapan Data
     Langkah berikutnya adalah persiapan data. Pada persiapan data dilakukan normalisasi data. Normalisasi merupakan proses penting untuk menyamakan skala data, memastikan bahwa model dapat memprosesnya dengan efisien. Normalisasi min-max digunakan pada penelitian ini. Min-max adalah teknik yang mengubah skala nilai data ke dalam rentang baru seperti 0 hingga 1 atau -1 hingga 1. Teknik ini memastikan bahwa setiap fitur atau kolom data memberikan kontribusi yang seimbang dalam analisis tanpa membiarkan fitur dengan skala besar mendominasi. Pengecekan matriks korelasi dilakukan untuk memahami hubungan antara variabel-variabel dalam dataset. Korelasi membantu mengidentifikasi fitur-fitur yang saling terkait dan memberikan wawasan tentang bagaimana setiap fitur dapat mempengaruhi model prediksi rute. Koefisien Korelasi Pearson digunakan untuk
mengukur hubungan linear antara fitur. 3.4 Desain model
     Implementasi Deep Q-Network (DQN) dengan mekanisme attention untuk Dynamic Vehicle Routing Problem with Time Windows (DVRPTW) melibatkan beberapa langkah utama, mulai dari pemilihan kerangka kerja hingga pembuatan lingkungan simulasi. 1. Pemilihan Kerangka Kerja
     Kerangka kerja yang digunakan yaitu TensorFlow, dimana kerangka kerja ini menawarkan lingkungan yang komprehensif dengan TensorBoard untuk visualisasi, serta dukungan terhadap TPU untuk akselerasi komputasi. TensorFlow mungkin lebih cocok untuk produksi dan skala besar. 2. Desain model DQN dan Multi header-attention
     DQN adalah algoritma pembelajaran penguatan yang menggunakan jaringan saraf tiruan untuk memperkirakan fungsi nilai Q, yang merepresentasikan nilai maksimum hadiah kumulatif yang diharapkan, diberikan sebuah state dan semua strategi yang mungkin diambil. Implementasi DQN melibatkan beberapa komponen utama:
Jaringan Q: Jaringan ini memperkirakan nilai Q untuk setiap aksi dari state tertentu. Dalam kasus DVRPTW, input bisa berupa representasi dari state saat ini (misalnya, lokasi kendaraan, status pengiriman) dan output adalah nilai Q untuk setiap kemungkinan aksi (misalnya, memilih lokasi pengiriman berikutnya). Memory Replay: Untuk meningkatkan stabilitas dan efisiensi pembelajaran, DQN menggunakan teknik memory replay, di mana transisi (state, aksi, reward, state baru) disimpan dalam sebuah buffer. Batch transisi ini kemudian digunakan untuk melatih jaringan Q, memungkinkan pengalaman dari masa lalu digunakan kembali. Strategi Eksplorasi: Seperti e-greedy, di mana aksi acak dipilih dengan probabilitas e untuk mendorong eksplorasi lingkungan. Mekanisme attention terdiri dari tiga matriks utama: Query (Q), Key (K), dan Value (V) untuk setiap head i. Adapun langkah-langkahnya implementasinya sebagai berikut:
a. Definisikan Ukuran Input dan Parameter
      Mentukan jumlah fitur input, dimensi embedding, jumlah heads untuk mekanisme attention, dan jumlah unit dalam lapisan tersembunyi DQN. Serta jumlah tindakan yang mungkin dilakukan oleh agen. b. Definisikan Multi-Header Attention
      Menerapkan mekanisme Multi-Header Attention pada representasi vektor dari embedding layer. Multi-Header Attention menggunakan Query (Q), Key (K), dan Value (V) untuk menangkap hubungan kontekstual dalam data. Proses ini membantu model untuk fokus pada aspek-aspek penting dari data input. Attention Score dihitung dengan mengalikan Query dengan Key, kemudian membaginya dengan skala (biasanya akar dari dimensi Key) dan menerapkan fungsi softmax untuk mendapatkan bobot attention. Output Attention diperoleh dengan mengalikan bobot perhatian dengan Value. Multi-Header Attention melakukan proses ini beberapa kali secara paralel (dengan beberapa ""heads"") dan hasilnya digabungkan untuk data input. c. Definisikan DQN dengan Lapisan Tersembunyi
      Membuat beberapa lapisan tersembunyi (hidden layers) menggunakan fungsi aktivasi ReLU. Lapisan tersembunyi ini memungkinkan jaringan untuk belajar representasi yang kompleks dari data input. Output dari mekanisme attention diberikan sebagai input ke DQN. DQN memperkirakan Q-values untuk setiap tindakan yang mungkin dilakukan oleh agen berdasarkan representasi state yang telah diperkaya. d. Output Layer untuk Q-values
      Lapisan output menghasilkan Q-values untuk setiap tindakan yang mungkin dilakukan oleh agen. Q-values ini menunjukkan seberapa baik setiap tindakan dalam memaksimalkan reward di masa depan. Misalnya, jika agen memiliki 5 kemungkinan tindakan, lapisan output akan menghasilkan 5 Q-values, satu untuk setiap tindakan. e. Memilih Tindakan menggunakan Strategi e-greedy
      Implementasikan strategi e-greedy untuk memastikan agen mengeksplorasi lingkungan sekaligus mengeksploitasi pengetahuan yang ada. Dengan probabilitas e, agen memilih tindakan secara acak untuk eksplorasi, dan dengan probabilitas 1- e, agen memilih tindakan dengan Q-value tertinggi untuk eksploitasi. f. Memperbarui Model dengan Pengalaman dari Replay Buffer
      Menggunakan replay buffer untuk menyimpan transisi (state, action, reward, next state) dan menggunakannya untuk melatih model. Batch transisi diambil secara acak dari replay buffer untuk mengurangi korelasi antara sampel pelatihan dan meningkatkan stabilitas pelatihan. 3.5 Pelatihan Model
     Selama fase pelatihan, model secara berulang kali dihadapkan pada berbagai skenario dari masalah rute kendaraan. Untuk setiap episode, model mengambil serangkaian aksi berdasarkan policy atau kebijakan saat ini yang awalnya adalah kebijakan acak dengan tujuan meminimalkan jarak total dan memenuhi jendela waktu pengiriman. Setelah mengambil aksi, model menerima feedback dari lingkungan berupa reward yang merupakan ukuran dari performa aksi tersebut dan state baru yang mencerminkan kondisi terkini dari lingkungan setelah aksi diambil. Informasi ini digunakan untuk memperbarui kebijakan model dengan cara mengoptimalkan parameter jaringan sehingga meningkatkan estimasi nilai Q, yang merepresentasikan hadiah kumulatif yang diharapkan. Untuk meningkatkan stabilitas dan efisiensi pelatihan, teknik seperti experience replay dan target networks digunakan. Experience replay memungkinkan model untuk belajar dari pengalaman masa lalu yang disimpan dalam memory replay, sedangkan target networks membantu mengurangi pergeseran target yang bergerak selama proses pembelajaran. Melalui interaksi yang berulang dan proses optimisasi ini, model secara bertahap belajar untuk memprediksi nilai Q yang lebih akurat untuk setiap kombinasi state dan aksi, yang mengarah pada pembentukan kebijakan rute yang lebih optimal. Selain itu pada tahap pelatihan model ini juga dilakukan penyetelan hyperparameter untuk menemukan nilai optimal hyperparameter guna meningkatkan kinerja model. Ini merupakan langkah penting dalam machine learning karena dapat menghasilkan peningkatan akurasi, efisiensi, dan generalizability model. Penyetelan hyperparameter menggunakan teknik random search, yaitu teknik yang digunakan untuk penyetelan hyperparameter yang melibatkan pemilihan acak dari ruang yang ditentukan untuk menemukan kombinasi terbaik yang mengoptimalkan kinerja model. Teknik ini lebih efektif dan efisien untuk penyetelan hyperparameter terutama pada kasus dengan ruang pencarian yang besar, serta dapat menemukan solusi yang baik dalam waktu yang lebih singkat. 3.6 Evaluasi Model
     Setelah fase pelatihan model Deep Q-Network (DQN) dengan multi-header attention untuk Dynamic Vehicle Routing Problem with Time Windows (DVRPTW) selesai, langkah evaluasi menjadi penting untuk memahami seberapa efektif model dalam menyelesaikan masalah yang ditargetkan. Evaluasi dilakukan dengan menguji model terhadap kumpulan data pengujian yang tidak terlibat selama proses pelatihan, memberikan masukan penting tentang kemampuan generalisasi model terhadap skenario baru dan belum pernah dilihat. Dalam konteks DVRPTW, metrik yang relevan seperti total jarak tempuh oleh semua kendaraan dan kepatuhan terhadap jendela waktu pengiriman menjadi fokus utama. Total jarak tempuh mencerminkan efisiensi rute yang dihasilkan, sementara kepatuhan terhadap jendela waktu mencerminkan kualitas layanan yang dapat dijamin oleh model. 3.7 Analisis dan Penyempurnaan
     Langkah terakhir yaitu analisis secara mendalam kinerja model pada dataset pengujian. Penyempurnaan dilakukan untuk mengatasi kelemahan yang telah dianalisis sebelumnya seperti penyempurnaan pada tuning hyperparameter untuk peningkatan kinerja, modifikasi arsitektur dan pelatihan ulang model. 3.8 Jadwal Penelitian
     Jadwal penelitian digunakan untuk meningkatkan efektivitas dalam proses penelitian. Adanya jadwal penelitian ini setiap proses penelitian sudah terjadwal dalam tabel 3.1 sehingga penelitian lebih efektif dan optimal. Tabel 3.1 Jadwal Penelitian","3.1 Kerangka Umum Penelitian
     Berikut ini merupakan kerangka penelitian yang menjelaskan tahapan yang dilakukan dalam penelitan ini. Berikut gambar 3.1 diagram alir penelitian

Persiapan Data
Definisikan Ukuran Input dan Parameter
Definisikan Multi-Head Attention
Desain Model
Definisikan DQN dengan Lapisan Tersembunyi
Output Layer untuk Q-values
Fungsi untuk Memilih Tindakan menggunakan
Strategi e-greedy
Fungsi untuk Memperbarui Model dengan
Pengalaman dari Replay Buffer
Gambar 3.1 Diagram Alir Penelitian


3.2 Pengumpulan Data
     Langkah awal adalah mengumpulkan dataset yang akurat dan relevan. Dataset didapatkan dari data sekunder, dataset ini merupakan hal yang penting dari simulasi dan eksperimen, mencakup koordinat lokasi yang mungkin meliputi lokasi depot dan titik pengiriman, jendela waktu untuk setiap pengiriman yang menentukan batas awal dan akhir kapan pengiriman harus dilakukan, serta jumlah kendaraan. Data ini harus mencerminkan situasi dunia nyata untuk memastikan bahwa model yang dikembangkan dapat diaplikasikan secara praktis. Normalisasi merupakan proses penting untuk menyamakan skala data, memastikan bahwa model dapat memprosesnya dengan efisien. Korelasi membantu mengidentifikasi fitur-fitur yang saling terkait dan memberikan wawasan tentang bagaimana setiap fitur dapat mempengaruhi model prediksi rute. 3.4 Desain model
     Implementasi Deep Q-Network (DQN) dengan mekanisme attention untuk Dynamic Vehicle Routing Problem with Time Windows (DVRPTW) melibatkan beberapa langkah utama, mulai dari pemilihan kerangka kerja hingga pembuatan lingkungan simulasi. Informasi ini digunakan untuk memperbarui kebijakan model dengan cara mengoptimalkan parameter jaringan sehingga meningkatkan estimasi nilai Q, yang merepresentasikan hadiah kumulatif yang diharapkan. Untuk meningkatkan stabilitas dan efisiensi pelatihan, teknik seperti experience replay dan target networks digunakan. Selain itu pada tahap pelatihan model ini juga dilakukan penyetelan hyperparameter untuk menemukan nilai optimal hyperparameter guna meningkatkan kinerja model. Ini merupakan langkah penting dalam machine learning karena dapat menghasilkan peningkatan akurasi, efisiensi, dan generalizability model. 3.6 Evaluasi Model
     Setelah fase pelatihan model Deep Q-Network (DQN) dengan multi-header attention untuk Dynamic Vehicle Routing Problem with Time Windows (DVRPTW) selesai, langkah evaluasi menjadi penting untuk memahami seberapa efektif model dalam menyelesaikan masalah yang ditargetkan. Total jarak tempuh mencerminkan efisiensi rute yang dihasilkan, sementara kepatuhan terhadap jendela waktu mencerminkan kualitas layanan yang dapat dijamin oleh model. 3.7 Analisis dan Penyempurnaan
     Langkah terakhir yaitu analisis secara mendalam kinerja model pada dataset pengujian. Penyempurnaan dilakukan untuk mengatasi kelemahan yang telah dianalisis sebelumnya seperti penyempurnaan pada tuning hyperparameter untuk peningkatan kinerja, modifikasi arsitektur dan pelatihan ulang model. 3.8 Jadwal Penelitian
     Jadwal penelitian digunakan untuk meningkatkan efektivitas dalam proses penelitian. Adanya jadwal penelitian ini setiap proses penelitian sudah terjadwal dalam tabel 3.1 sehingga penelitian lebih efektif dan optimal. Tabel 3.1 Jadwal Penelitian"
Armando Tirta Dwilaga_Kualifikasi.txt,3.1 gambaran umum penelitian penelitian ini digunakan untuk mengatasi sensitivitas terhadap cacat pada gambar ban dengan melibatkan penggunaan jaringan syaraf menggunakan algoritma convolutional neural network cnn dan membangun model atau kerangka kerja menggunakan keras. berikut adalah gambar 3.1 blok diagram gambaran umum penelitian. data preparation data augmentasi data splitting model training forward pass model evaluation backward passfinetuning if neededinput unit processing unit output unit model deployment inference12 gambar 3.1 blok diagram gambaran umum penelitian berdasarkan gambar 3.1 blok diagram gambaran umum penelitian maka dapat dijelaskan di blok tersebut terbagi menjadi 3 bagian yaitu bagian pertama adalah unit masukan berisikan data preparation di mana gambar ban dimuat diubah menjadi format yang sesuai dipersiapkan untuk pelatihan model convolutional neural network cnn seperti pemrosesan gambar ban selanjutnya data augmentation di mana data dibuat lebih ber variasi dari training data yang ada sehingga dapat meningkatkan keberagaman training data tanpa h arus mengambil data baru mencakup rotasi pergeseran horizontalvertikal perbesar gambar perubahan kecerahan gambar sampai mengubah nilai pixel selanjutnya data di mana dataset yang telah di augmentasi dan disiapkan dibagi menjadi subset yang berbeda untuk training untuk melatih model validation untuk menyempurnakan model serta me mvalidasi performanya selama pelatihan dan testing untuk mengevaluasi kinerja model akhir . data set dibagi menjadi training data validation data dan testing data dalam proporsi tertentu. bagian kedua adalah unit pemrosesan yang bertindak adalah model training forward pass tahap di mana input diproses melalui model untuk menghasilkan prediksi tujuannya melatih model convolutional neural network cnn menggunakan dataset pelatihan di mana data dari unit masukan diteruskan melalui jaringan neural di lakukan transformasi linier konvulasi dan non linier fungsi aktivasi dilakukan pada data di setiap lapisan untuk menghasilkan output prediksi yang melibatkan komputasi di setiap neuron dan lapisan jaringan yang merupakan inti dari proses pembelajaran dalam jaringan saraf. selanjutnya unit pemrosesan finetuning tujuannya dilakukan untuk menyempurnakan model lebih lanjut setelah pelatihan awal dengan dataset yang lebih kecil atau lebih spesifik nantinya. proses di dalam finetuning menyesuaikan bobot menggunakan kumpulan data yang lebih kecil untuk menyesuaikan bobot model untuk performa yang lebih baik pelatihan khusus fokus pada fitur data yang lebih relevan dengan objek . bagian ketiga adalah unit keluaran yang bertindak ada proses model evaluatioan backward pass tahap di mana gradien memperbarui parameter model dalam arah yang akan mengurangi fungsi loss dari fungsi loss metrik yang mengukur seberapa baik atau buruk model melakukan prediksi dibandingkan nilai aktualnya dihitung dan digunakan untuk memperbarui parameter model selama pelatihan tujuannya mengevaluasi performa model yang dilatih dan model dievaluasi menggunakan metrik yang relevan accuracy precision recall dan f1 score berdasarkan prediksi yang dihasilkan dari model terhadap validasi atau uji data. output dari proses ini adalah tentang hasil evaluasi model yang memberikan informasi kinerja model. selanjutnya ada dua alur pilihan yang bisa dilakukan alur pertama jika hasil prediksi sudah sesuai dengan keinginan maka bisa langsung masuk ke model deployment inference dan alu r kedua jika hasil prediksi masih perlu diperbaiki pada bagian unit pemrosesan terlebih dahulu fine tuning untuk penggunaan data set lebih kecil jika menunjukan model belum mencapai performa yang diharapkan baru masuk ke model deployment inference tujuannya menerapkan model terlatih untuk membuat prediksi pada data baru yang belum terlihat. model deployment inference yang telah dilatih digunakan untuk membuat prediksi pada data baru atau dalam situasi dunia nyata tahap di mana model menerima input baru dan menghasilkan output berdasarkan pada pembelajaran yang dilakukan selama proses pelatihan dan merupakan output akhir dari keseluruhan proses di mana model mengambil keputusan atau membuat prediksi berdasarkan pada pengalaman yang telah diperoleh selama pelatihan. 3.2. tahapan penelitian penelitian ini di dalamnya terdapat tahapan tahapan yang dilakukan untuk membentuk satu kesatuan yang utuh dari awal sampai akhir dan membentuk kerangka penelitian mengenai klasifikasi pada produk ban menggunakan algoritma convolutional neural network cnn . berikut gambar 3.2 tahapan penelitian. study of literature data acquisition data augmentation data splitting model buildingdata preprocessing model evaluation testing gambar 3.2 tahapan penelitian berdasarkan gambar 3.2 tahapan penelitian maka dapat dijelaskan proses yang terlibat di dalamnya ada 8 yaitu studi literatur data aquisition data preprocessing data augmentation texture feature extraction data splitting model building dan model evaluation testing di mana tahap ke dua sampai lima merupakan tahap proses menyiapkan sebuah data sebelum dilakukan pemodelan. 3.2.1 studi literatur tahap pertama adalah studi literatur di mana studi yang dilakukan berasal dari artikel ilmiah dan buku yang menunjang dalam menganalisis terkait dengan metode pen gukuran kualitas mengenai klasifikasi produk ban meninjau penggunaan pembelajaran mesin algoritma convolutional neural network cnn dari beberapa tahun ke belakang dalam konteks peng ukuran kualitas untuk klasifikasi terhadap kondisi kondisi produk ban . sehingga dapat menemukan teknik terbaik yang dapat diaplikasikan pada masalah yang ada. berikut merupakan gambar 3. 3 tahapan study literature . study of literature load libraries initialize imagedatagenerator set image directory and parameters create test training and validation dataset gambar 3. 3 tahapan study literature 3.2.2 data aquisition tahap k edua adalah data aquisition dengan mengumpulkan kumpulan data sesuai tujuan penelitian dengan target untuk kumpulan data gambar ban untuk training data validation data dan testing data memastikan bahwa kumpulan data tersebut memiliki varian yang secara akurat memang mewakili kondisi produk ban dan diperoleh dari sumber sumber terpercaya . berikut merupakan gambar 3. 4 tahapan data aquisition . data acquisition normal defecttire dataset gambar 3. 4 tahapan data aquisition 3.2.3 data preprocessing tahap ketiga adalah data preprocessing melakukan pra pemrosesan data untuk menyiapkan gambar untuk model pelatihan dan pengujian proses ini meliputi normalisasi dan penskalaan dengan fitur dalam program image data generator . bermaksud merapikan menata dan menyiapkan data untuk pemeriksaan tambahan. normalisasi data pengkodean variabel mengatasi nilai yang hilang menghapus data yang tidak relevan atau hilang dan modifikasi data lainnya untuk memenuhi persyaratan analisis a dalah persiapan data. berikut merupakan gambar 3.5 tahapan data preprocessing . data preprocessing data normalization data scalinginitiation split data into training and validation sets rescale target_size gambar 3. 5 tahapan data preprocessing 3.2.4 data augmentation tahap keempat adalah data augmentation meningkatkan variasi dalam dataset dengan teknik augmentasi data menggunakan operasi seperti rotasi pergeserarn horizontalvertikal perbesar gambar perubahan kecerahan gambar sampai mengubah nilai pixel untuk memperkaya dataset dan mengurangi overfitting saat disajikan dengan data baru yang belum pernah dilihat sebelumnya performa model akan menurun drastis karena model tersebut dapat menyesuaikan diri dengan kumpulan training data dengan sangat efektif. augmentasi data dilakukan dengan dua cara secara statis dan dinamis yang artinya secara statis yaitu menambah data secara fisiknya dan dinamis tidak menambah secara fisik tetapi secara k uantitas dataset yang dapat diakses secara fisik di komputer tidak bertambah ketika image data generator digunakan pada dataset. sebaliknya pada saat runtime hanya menghasilkan variasi dari gambar yang sudah ada dibuat secara dinami s dan cukup bagi model untuk berlatih dari berbagai kondisi gambar ban yang ada pada kenyataaanya. secara lebih jelas nilai teknik augmentasi pertama dilakukan dengan manual menggunakan bantuan dari website roboflow dengan resize gambar menjadi 640 x 640 pada augmentasinya menggunakan model flip horizontal dan vertikal 90 pemutaran searah jarum jam berlawanan arah jarum jam dan terbalik rotasi 45 dan 45 shear 5 horizontal dan 5 vertikal brightness 20 sampai 20. data asli pada dataset berjumlah 1 .028 data gambar ban setelah dilakukan augmentasi secara fisik menggunakan website roboflow ada data yang tidak dapat diidentifikasi ada 3 gambar sehingga total gambar asli yang berhasil di upload dan dijadikan data asli yang tetap berjumlah 1.025 data ga mbar dan setelah di augmentasi bertambah menjadi 2 .050 data gambar ban. rinciannya pada data asli training adalah 560 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 1.121 gambar. rincian data asli pada validation data berjumlah 140 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 279 gambar. rincian data asli pada testing data berjumlah 328 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 6 50 gambar. testing data pada prosesnya sebenarnya tidak mengalami augmentasi karena pada proses pengujian atau evaluasi mode l ingin menggunakan data asli yang sebenarnya untuk melihat kinerja model pada kasus kasus yang belum pernah dilihat sebelumnya. augmentasi kedua yaitu dilakukan rotasi melakukan pemutaran gambar secara penuh dan secara acak dengan nilai 360 atau rentang nilai 0 360 derajat kedua width shift range yang menggeser gambar secara acak ke kiri atau kanan dengan nilai 0 .05 atau gambar dapat digeser sampai 5 dari lebar aslinya . ketiga height shift range gambar dapat digeser secara vertikal dengan nilai 0 .05 atau gambar dapat digeser sampai 5 dari tinggi aslinya . keempat shear range untuk menggeser gambar dengan sudut geser berlawanan arah jarum jam dengan nilai 0.05. kelima zoom range memperbesar gambar sebanyak 0.05 atau gambar dapat diperbesar sampai 5. keenam horizontal flip adalah memberikan variasi tambahan dengan mengubah orientasi gambar secara horizontal acak dengan keterangan nilai true. ketujuh vertikal flip adalah memberikan variasi tambahan dengan mengubah orientasi gambar secara vertikal acak dengan keterangan nilai true. kedelapan brightness range mengubah atau menentukan kecerahan pada gambar secara acak dengan nilai rentan 0.75 1.25 atau kecerahan dapat diubah mulai dari rentnag 75 sampai 125 dari kecerahan asli gambarnya. kesembilan resecale mengubah nilai skala piksel 0.1 dengan membaginya setiap nilai piksel pada nilai 255 sehingga dapat membant u untuk normalisasi data. kesepuluh validation split mengatur pembagian data untuk validasi dengan nilai 0.2 atau 20 data dari keseluruhan data untuk alokasi validation data dan 80 un tuk alokasi training data. merupakan pendekatan augmentasi awal di mana sebelum data masuk ke model untuk proses pelatihan dan akan diperbesar sebelum pembagian dataset menjadi batch untuk setiap epoch nya sehingga model akan dilatih menggunakan dataset yang telah diaugmentsi sejak awal dan seluruh augmentasi akan diterapkan pada setiap epoch nya dengan penggunaan ukuran batch 64 dengan jumlah batch training 36 dan validasi 10. rinciannya data asli pada training data berjumlah 1.121 gambar dan setelah dilakukan augmentasi bertambah sebanyak 1.152 gambar sehingga data pada training data berjumlah total menjadi 2 .273 gambar. rincian data asli pada validation data berjumlah 2 79 gambar dan setelah dilakukan augmentasi bertambah sebanyak 320 gambar sehingga data pada validation data berjumlah total menjadi 59 9 gambar. testing data tidak mengalami augmentasi karena pada proses pengujian atau evaluasi mode l ingin menggunakan data asli yang sebenarnya untuk melihat kinerja model pada kasus kasus yang belum pernah dilihat sebelumnya. berikut merupakan gambar 3. 6 tahapan data augmentation . data augmentation flip rotation shear brightness gambar 3. 6 tahapan data augmentation 3.2.5 data splitting tahap ke lima data splitting dengan membagi file dataset menjadi subset training data validation data dan testing data berisikan gambar ban normal dan gambar ban tidak normal sehingga subset training data digunakan untuk melatih model sedangkan subset validation data digunakan untuk menguji kinerja model. sebenarnya langkah langkah dalam proses pra pemrosesan data yang mempersiapkan data mentah untuk digunakan dalam pelatihan model adalah tahapan yang sudah disebutkan sebelumnya data aquisition data prerocessing data augmentation dan splitting data. prosedur yang disebutkan di atas berkonsentrasi pada pengumpulan sanitasi pengorganisasian dan penambahan jumlah data yang diperlukan untuk pelatihan model. rinciannya yaitu file yang tersimpan di dalam komputer total data gambar sebanyak 2.050 gambar yang dibagi menjadi dua pertama adalah file testing data dengan jumlah data tersimpan sebanyak 650 gambar yang dibagi menjadi sub file crack berjumlah 420 dan sub file normal berjumlah 230 data. kedua adalah file training data dengan jumlah data tersimpan sebanyak 1400 gambar yang dibagi menjadi sub file crack berjumlah 654 dan sub file normal berjumlah 746 data. maka ketika dilakukan data splitting pada program secara otomatis yang pada data augmentasi diatur menjadi pembagian 80 untuk training data dan 20 untuk validation data yaitu untuk train data sebanyak 1.121 gambar dengan 2 kelas validation data sebanyak 279 gambar dengan 2 kelas dan test data sebanyak 650 gambar dengan 2 kelas. testing data bernilai tetap h al ini bertujuan agar kuantitas data awal yang telah ditentukan sebelumnya tetap terjaga dan testing data tidak terpengaruh oleh prosedur pemisahan. setelah model dilatih dan divalidasi testing data digunakan untuk mengevaluasi performa akhir model. akibatnya testing data tidak terbagi dan rincian asli 648 foto masih berlaku. berikut merupakan gambar 3.7 tahapan splitting data . data splitting training data validation data testing data gambar 3. 7 tahapan splitting data 3.2.6 model building tahap ke enam adalah model building membangun model convolutional neural network cnn dengan keras membangun arsitektur model convolutional neural network cnn menggunakan keras mengatur lapisan lapisan seperti convolutional maxpooling2d flatten dan dense untuk membangun model. learning rate dalam penggunaan algoritma optimasi menggunakan adaptive momentum adam untuk menghasilkan pembelajaran yang adaptif pemilihan penggunaan adaptive momentum adam jika dibandingkan dengan learning rate lain seperti stochastic gradient descent sgd karena kecepatan pembelajaran adaptif untuk adaptive momentum adam bisa secara otomatis menyesuaikan learning rate untuk setiap parameter dalam model klasifikasi ban sedangkan stochastic gradient descent sgd memiliki learning rate tetap selama pelatihan model klasifikasi ban yang penentuannya dari user dan tidak bisa menyesuaikan learning rate secara otomatis berdasarkan kondisi a ktual dari setiap parameter. selanjutnya secara kestabilan dan konvergensi adaptive momentum adam menyambung dari awal dapat mengubah kecepatan pembelajaran secara adaptif sehingga membuatnya lebih stabil dan kecil kemungkinannya terjebak pada tingkat minimum lokal nilai yang dianggap sebagai titik terendah dari loss function dalam model sehingga adaptive momentum adam cenderung mencapai konvergensi tingkat kinerja yang diharapkan lebih cepat dan andal dalam berbagai keadaan sedangkan stochastic gradient descent sgd mungkin lebih stuck pada nilai minimum atau terjebak pada nilai minimum lokal ya ng disebabkan oleh kemungkinan bergantung pada seberapa tepat kecepatan pemelajaran dipilih kecepatan pemelajaran yang tetap dapat membuat model mencapai konvergensi terlalu cepat atau terlalu lambat. adapun melakukan pendekatan kedua penam bahan data keti ka masuk ke model building dan terjadi proses pemodelan setelah menggunakan epoch . augment asi data diterapkan setelah data melewati beberapa epoch selama proses pelatihan sehingga variasi data yang dihasilkan akan berbeda beda pada setiap epoch dan model dapat terus menerus terlatih dengan variasi data yang lebih besar . menggunakan 100 epoch sehingga total training data yang diproses menjadi 230.400 gambar dan validation data menjadi 6.400 gambar. sehingga jumlah data yang diproses selama pelatihan menjadi sangat besar dan pada akhirnya nanti akan menyiapkan m odel kompilasi dalam mengatur pengoptimal adam fungsi kerugian biner crossentropy dan metrik evaluasi akurasi. berikut merupakan gambar 3. 8 tahapan building model . model building define cnn model compile model gambar 3 .8 tahapan building model berdasarkan hasil analisis sebelumnya maka dapat diketahui untuk j umlah data asli training data adalah 1.121 jumlah data asli validation data adalah 279 jumlah data asli training data setelah augmentasi adalah 1152 jumlah data asli validation data setelah augmentasi adalah 320 jumlah epoch yang digunakan sebanyak 100 dan ukuran batch adalah 64. berikut merupakan perhitungan manualnya ketika masuk ke model building dan terjadi proses pemodelan setelah menggunakan epoch . 1. jumlah batch per epoch untuk training data . 𝑆𝑡𝑒𝑝𝑝𝑒𝑟𝑒𝑝𝑜𝑐ℎjumlahdatatrainjumlahdataaugmentasi train 𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒 11211152 64 2273 64 35515636𝑏𝑎𝑡𝑐ℎ 2. jumlah batch per epoch untuk validation data. 𝑆𝑡𝑒𝑝𝑝𝑒𝑟𝑒𝑝𝑜𝑐ℎjumlahdatavalidjumlahdataaugmentasi valid 𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒 279320 64 599 64 9359310𝑏𝑎𝑡𝑐ℎ 3. total jumlah data setelah augmentasi untuk semua epoch . a. training data total𝑇𝑟𝑎𝑖𝑛𝑖𝑛𝑔 datajumlah𝐵𝑎𝑡𝑐ℎx𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒xjumlah𝐸𝑝𝑜𝑐ℎ𝑠 36x64x100 230.400 b. validation data total𝑉𝑎𝑙𝑖𝑑𝑎𝑡𝑖𝑜𝑛 datajumlah𝐵𝑎𝑡𝑐ℎx𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒xjumlah𝐸𝑝𝑜𝑐ℎ𝑠 10x64x100 64.000 3.2.7 model evaluation testing tahap kedelapan adalah model evaluation testing digunakan sebagai bahan terusan pada model building yang dibuat untuk melakukan evaluasi performanya dengan menggunakan bagian pengujian dan parameter yang digunakan pada metrik evaluasi seperti akurasi presisi recall dan f1score . berikut merupakan gambar 3. 9 tahapan model evaluation testing . model evaluation testing accuracy precision recall f1score gambar 3. 9 tahapan model evaluation testing 3.3 arsitektur convolutional neural network cnn convolutional neural network cnn yang dibangun menggunakan model atau kerangka kerja yang pada dasarnya menggunakan keras dan juga tensorflow dengan menambahkan beberapa model lapisan lapisan seperti lapisan convolutional conv2d laposan pooling maxpooling 2d flatten dan lapisan fully connected dense . berikut merupakan gambar 3. 10 tahapan convolutional neural network cnn dengan model keras. conv2d filter 128 ukuran filter 3 atau 3x3 strides 2 input size 189x189 jumlah neuron 1280maxpooling2d pool size 2 strides 2 input size 94x94 jumlah neuron 1280conv2d 2nd filter 64 ukuran filter 3 atau 3x3 strides 2 input size 46x46 jumlah neuron 73792maxpooling2d 2nd pool size 2 strides 2 input size 23x23 jumlah neuron 73792 conv2d 3nd filter 32 ukuran filter 3 atau 3x3 strides 2 input size 11x11 jumlah neuron 18464maxpooling2d 2nd pool size 2 strides 2 input size 5x5 jumlah neuron 18464conv2d 4nd filter 16 ukuran filter 3 atau 3x3 strides 2 input size 2x2 jumlah neuron 4624maxpooling2d 4nd pool size 2 strides 2 input size 1x1 jumlah neuron 4624 flatten input size 1x1x16 jumlah neuron 16dense layer 1 dengan aktivasi relu jumlah neuron 128 jumlah parameter 2176dropout layer 1 rate 0.2 20dense layer 2 dengan aktivasi relu jumlah neuron 64 jumlah parameter 8256 dropout layer 2 rate 0.2 20dense layer 3 dengan aktivasi sigmoid jumlah neuron 1 biner sigmoid jumlah parameter 65 training fit callbacks optimizer adam evaluation confusion matrix report epochs 100 gambar 3. 10 tahapan convolutional neural network cnn dengan model keras berdasarkan gambar 3. 10 tahapan convolutional neural network cnn dengan model keras maka dapat dijelaskan mulai dari yang mencakup lapisan lapisan konvolusi yang telah dilatih pada dataset besar seperti imagenet untuk mengekstrak fitur dari gambar gambar penggunaan image size diatur dengan 379 379 batch size 64 kernel size 3 strides 2 untuk cov2d dan 2 untuk maxpooling2d dan pool size 2. selanjutnya conv2d yang merupakan convolutional layer pertama yang berfungsi untuk mengekstrak fitur fitur visual dari gambar. filter convolutional layer pertama yang berfungsi untuk mengekstrak fitur fitur visual diterapkan pada gambar untuk menghasilkan fitur fitur yang lebih abstrak formula untuk mengetahui jumlah training datanya dengan . selanjutnya max pooling 2d di mana tahap pooling digunakan untuk mengurangi dimensi spasial dari setiap feature map yang dihasilkan oleh layer sebelumnya. max pooling memilih nilai maksimum di dalam jendela pooling untuk m engurangi ukuran fitur dan mempertahankan informasi penting. selanjutnya conv2d dan max pooling 2d diulang sampai 4 layer karena untuk terus mengekstrak fitur fitur yang semakin kompleks dari gambar. selanjutnya flatten digunakan untuk mengubah tensor multi dimensi menjadi tensor satu dimensi di mana setelah serangkaian layer konvolusi dan pooling masukan dari layer terakhir perlu diubah menjadi vektor tunggal sebelum dimasukkan ke dalam layer dense . flatten layer melakukan hal ini dengan mengubah matriks output menjadi array satu dimensi. selanjutnya dense layers lapisan dense digunakan sebagai lapisan output dalam model klasifikasi di mana jumlah neuron dalam lapisan output sesuai dengan jumlah kelas yang harus diprediksi di mana ada tiga lapisan dense ditambahkan dengan fungsi pertama dan kedua menggunakan relu sebagai 0 f x max x yang artinya menunjukkan bahwa keluarannya nol jika masukannya negatif atau nol dan output x jika masukannya positif dengan 128 unit neuron dan pada dense kedua 64 unit neuron karena tugasnya mengurangi dimensi representasi pada lapisan dense pertama maka model dapat mempelajari pola yang lebih rumit dan mendalam dari data dengan menambahkan lapisan yang lebih padat yang dapat meningkatkan performa model dalam tugas klasifikasi gambar. lapisan dense ketiga dengan fungsi aktivasi sigmoid untuk output biner dengan menunjukan kelas prediksi dari gambar yaitu normal atau crack . di antara tig a lapisan dense di ikuti dengan lapisan dropout untuk mencega h overfitting di mana model pembelajaran mesin terlalu menghafal pola dari training data yang tersedia sehingga kinerjanya menurun secara signifikan saat diuji dengan data baru yang tidak dilihat sebelumnya juga dimasukkan setelah setiap lapisan dense untuk mencegah overfitting dengan secara acak menonaktifkan seba gian unit sebanyak 0.2 atau 20 dari neuron selama pelatihan. selanjutnya training di mana model diterapkan pada training data dengan menggunakan metode fit dan callback . model fit digunakan untuk melatih model dengan training data dan model callback menggunakan modelcheckpoint untuk menyimpan model terba ik selama pelatihan berkaitan dengan performa pada validation data mengontrol proses pelatihan. terakhir evaluation di mana performa model pada testing data dinilai menggunakan hasil klasifikasi dan confusion matrix untuk memahami kinerjanya testing data. banyaknya parameter atau bobot dan jumlah data yang harus dipelajari selama pelatihan bergantung pada jumlah neuron pada lapisan. jumlah data yang harus dipelajari model selama pelatihan tercermin dalam jumlah parameter ini. berikut merupakan perhitungan dalam mengetahui total neuron yang dikerjakan oleh setiap lapisan. 1. first conv2d totalneuronukuranfilterxjumlah𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝐼𝑛𝑝𝑢𝑡1xfilter 3x3x11x128 10x128 1280 kedalaman gambar yang diproses lapisan konvolusi sebenarnya ditunjukkan oleh jumlah saluran masukan. tiga saluran merah hijau dan biru membentuk sebuah gambar jika diwarnai artinya ada tiga saluran masukan. karena kata grayscale digunakan untuk mendeskripsikan gambar ini hanya ada satu saluran warna dan bernilai 1 . sehingga jumlah neuronnya 1280 yang berarti ada 1280 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan . selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 379x379 menjadi 189x189 sebagai berikut. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝐾𝑒𝑟𝑛𝑒𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 3793 11 376 21 189 2. first maxpooling2d tidak ada parameter baru yang ditambahkan dan jumlah neuron dalam contoh ini lapisan konvolusi pertama tetap sama. setiap filter diubah menjadi setengah dari ukuran inputnya 189x189 menjadi 94x94 dan jumlah neuronnya 1280 mengikuti lapisan konvolusi pertama . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 1892 21 187 21 94 3. second cov2d totalneuronukuranfilterxjumlah𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝐼𝑛𝑝𝑢𝑡1xfilter 3x3x1281x64 1153x128 73792 jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. sehingga jumlah neuronnya 73792 yang berarti ada 73792 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan . selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 94x94 menjadi 46x46 sebagai berikut. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝐾𝑒𝑟𝑛𝑒𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 943 21 91 21 46 4. second maxpooling2d tidak ada parameter baru yang ditambahkan dan jumlah neuron dalam contoh ini lapisan konvolusi kedua tetap sama. setiap filter diubah menjadi setengah dari ukuran inputnya 46x46 menjadi 23x23 dan jumlah neuronnya 73792 mengikuti lapisan konvolusi kedua . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 462 21 44 21 23 5. third cov2d totalneuronukuranfilterxjumlah𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝐼𝑛𝑝𝑢𝑡1xfilter 3x3x641x32 577x32 18464 jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. sehingga jumlah neuronnya 18464 yang berarti ada 18464 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan . selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 23x23 menjadi 11x11 sebagai berikut. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝐾𝑒𝑟𝑛𝑒𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 233 21 20 21 11 6. third maxpooling2d tidak ada parameter baru yang ditambahkan dan jumlah neuron dalam contoh ini lapisan konvolusi ketiga tetap sama. setiap filter diubah menjadi setengah dari ukuran inputnya 11x11 menjadi 5x5 dan jumlah neuronnya 18464 mengikuti lapisan konvolusi ketiga . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 112 21 9 21 5 7. fourth cov2d totalneuronukuranfilterxjumlah𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝐼𝑛𝑝𝑢𝑡1xfilter 3x3x321x16 289x16 4624 jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. sehingga jumlah neuronnya 4624 yang berarti ada 4624 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan . selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 5x5 menjadi 2x2 sebagai berikut. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝐾𝑒𝑟𝑛𝑒𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 53 21 1 21 1.52 8. fourth maxpooling2d tidak ada parameter baru yang ditambahkan dan jumlah neuron dalam contoh ini lapisan konvolusi keempat tetap sama. setiap filter diubah menjadi setengah dari ukuran inputnya 2x2 menjadi 1x1 dan jumlah neuronnya 4624 mengikuti lapisan konvolusi keempat . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 12 21 1 21 0.51 9. flatten tidak mengubah parameter yang ada karena fungsi flatten hanya mengubah matriks multidimensi menjadi vektor tunggal berdasarkan hasil dari jumlah filter pada lapisan lima cov2d yaitu 8 dan maxpooling2d dengan ukuran inputnya 1x1 sehingga menjadi matriks multidimensi 1 1 16 diubah menjadi nilai vektor tunggal dengan panjang 16 atau menjadi jumlah neuron sebanyak 16. 10. dense layer 1 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 161x128 17x128 2176 11. dropout layer 1 menggunakan 0.2 yang artinya sebanyak 20 dari neuron dalam dense layer 1 akan dinonaktifkan secara acak. 12. dense layer 2 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 1281x64 129x64 8256 13. dropout layer 2 menggunakan 0.2 yang artinya sebanyak 20 dari neuron dalam dense layer 2 akan dinonaktifkan secara acak. 14. dense layer 2 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 641x1 65x1 65 ketika dimensi spasial tinggi dan lebar dikurangi menggunakan operasi lapisan pooling seperti maxpooling jumlah neuron di setiap lapisan pooling akan menurun. misalnya dimensi spasial setiap filter tinggi dan lebar di lapisan maxpooling disesuaikan menjadi setengah dari dimensi masukannya. karena hanya separuh dari masukan yang diproses lebih lanjut hal ini juga menyebabkan berkurangnya jumlah neuron pada lapisan tersebut. sedangkan penurunan pada dense terjadi karena penentuan jumlah neuron.,3.1 gambaran umum penelitian penelitian ini digunakan untuk mengatasi sensitivitas terhadap cacat pada gambar ban dengan melibatkan penggunaan jaringan syaraf menggunakan algoritma convolutional neural network cnn dan membangun model atau kerangka kerja menggunakan keras. berikut adalah gambar 3.1 blok diagram gambaran umum penelitian. data preparation data augmentasi data splitting model training forward pass model evaluation backward passfinetuning if neededinput unit processing unit output unit model deployment inference12 gambar 3.1 blok diagram gambaran umum penelitian berdasarkan gambar 3.1 blok diagram gambaran umum penelitian maka dapat dijelaskan di blok tersebut terbagi menjadi 3 bagian yaitu bagian pertama adalah unit masukan berisikan data preparation di mana gambar ban dimuat diubah menjadi format yang sesuai dipersiapkan untuk pelatihan model convolutional neural network cnn seperti pemrosesan gambar ban selanjutnya data augmentation di mana data dibuat lebih ber variasi dari training data yang ada sehingga dapat meningkatkan keberagaman training data tanpa h arus mengambil data baru mencakup rotasi pergeseran horizontalvertikal perbesar gambar perubahan kecerahan gambar sampai mengubah nilai pixel selanjutnya data di mana dataset yang telah di augmentasi dan disiapkan dibagi menjadi subset yang berbeda untuk training untuk melatih model validation untuk menyempurnakan model serta me mvalidasi performanya selama pelatihan dan testing untuk mengevaluasi kinerja model akhir . data set dibagi menjadi training data validation data dan testing data dalam proporsi tertentu. bagian kedua adalah unit pemrosesan yang bertindak adalah model training forward pass tahap di mana input diproses melalui model untuk menghasilkan prediksi tujuannya melatih model convolutional neural network cnn menggunakan dataset pelatihan di mana data dari unit masukan diteruskan melalui jaringan neural di lakukan transformasi linier konvulasi dan non linier fungsi aktivasi dilakukan pada data di setiap lapisan untuk menghasilkan output prediksi yang melibatkan komputasi di setiap neuron dan lapisan jaringan yang merupakan inti dari proses pembelajaran dalam jaringan saraf. selanjutnya unit pemrosesan finetuning tujuannya dilakukan untuk menyempurnakan model lebih lanjut setelah pelatihan awal dengan dataset yang lebih kecil atau lebih spesifik nantinya. proses di dalam finetuning menyesuaikan bobot menggunakan kumpulan data yang lebih kecil untuk menyesuaikan bobot model untuk performa yang lebih baik pelatihan khusus fokus pada fitur data yang lebih relevan dengan objek . bagian ketiga adalah unit keluaran yang bertindak ada proses model evaluatioan backward pass tahap di mana gradien memperbarui parameter model dalam arah yang akan mengurangi fungsi loss dari fungsi loss metrik yang mengukur seberapa baik atau buruk model melakukan prediksi dibandingkan nilai aktualnya dihitung dan digunakan untuk memperbarui parameter model selama pelatihan tujuannya mengevaluasi performa model yang dilatih dan model dievaluasi menggunakan metrik yang relevan accuracy precision recall dan f1 score berdasarkan prediksi yang dihasilkan dari model terhadap validasi atau uji data. output dari proses ini adalah tentang hasil evaluasi model yang memberikan informasi kinerja model. selanjutnya ada dua alur pilihan yang bisa dilakukan alur pertama jika hasil prediksi sudah sesuai dengan keinginan maka bisa langsung masuk ke model deployment inference dan alu r kedua jika hasil prediksi masih perlu diperbaiki pada bagian unit pemrosesan terlebih dahulu fine tuning untuk penggunaan data set lebih kecil jika menunjukan model belum mencapai performa yang diharapkan baru masuk ke model deployment inference tujuannya menerapkan model terlatih untuk membuat prediksi pada data baru yang belum terlihat. model deployment inference yang telah dilatih digunakan untuk membuat prediksi pada data baru atau dalam situasi dunia nyata tahap di mana model menerima input baru dan menghasilkan output berdasarkan pada pembelajaran yang dilakukan selama proses pelatihan dan merupakan output akhir dari keseluruhan proses di mana model mengambil keputusan atau membuat prediksi berdasarkan pada pengalaman yang telah diperoleh selama pelatihan. 3.2. tahapan penelitian penelitian ini di dalamnya terdapat tahapan tahapan yang dilakukan untuk membentuk satu kesatuan yang utuh dari awal sampai akhir dan membentuk kerangka penelitian mengenai klasifikasi pada produk ban menggunakan algoritma convolutional neural network cnn . 3.2.1 studi literatur tahap pertama adalah studi literatur di mana studi yang dilakukan berasal dari artikel ilmiah dan buku yang menunjang dalam menganalisis terkait dengan metode pen gukuran kualitas mengenai klasifikasi produk ban meninjau penggunaan pembelajaran mesin algoritma convolutional neural network cnn dari beberapa tahun ke belakang dalam konteks peng ukuran kualitas untuk klasifikasi terhadap kondisi kondisi produk ban . 5 tahapan data preprocessing 3.2.4 data augmentation tahap keempat adalah data augmentation meningkatkan variasi dalam dataset dengan teknik augmentasi data menggunakan operasi seperti rotasi pergeserarn horizontalvertikal perbesar gambar perubahan kecerahan gambar sampai mengubah nilai pixel untuk memperkaya dataset dan mengurangi overfitting saat disajikan dengan data baru yang belum pernah dilihat sebelumnya performa model akan menurun drastis karena model tersebut dapat menyesuaikan diri dengan kumpulan training data dengan sangat efektif. sebaliknya pada saat runtime hanya menghasilkan variasi dari gambar yang sudah ada dibuat secara dinami s dan cukup bagi model untuk berlatih dari berbagai kondisi gambar ban yang ada pada kenyataaanya. secara lebih jelas nilai teknik augmentasi pertama dilakukan dengan manual menggunakan bantuan dari website roboflow dengan resize gambar menjadi 640 x 640 pada augmentasinya menggunakan model flip horizontal dan vertikal 90 pemutaran searah jarum jam berlawanan arah jarum jam dan terbalik rotasi 45 dan 45 shear 5 horizontal dan 5 vertikal brightness 20 sampai 20. data asli pada dataset berjumlah 1 .028 data gambar ban setelah dilakukan augmentasi secara fisik menggunakan website roboflow ada data yang tidak dapat diidentifikasi ada 3 gambar sehingga total gambar asli yang berhasil di upload dan dijadikan data asli yang tetap berjumlah 1.025 data ga mbar dan setelah di augmentasi bertambah menjadi 2 .050 data gambar ban. 7 tahapan splitting data 3.2.6 model building tahap ke enam adalah model building membangun model convolutional neural network cnn dengan keras membangun arsitektur model convolutional neural network cnn menggunakan keras mengatur lapisan lapisan seperti convolutional maxpooling2d flatten dan dense untuk membangun model. selanjutnya secara kestabilan dan konvergensi adaptive momentum adam menyambung dari awal dapat mengubah kecepatan pembelajaran secara adaptif sehingga membuatnya lebih stabil dan kecil kemungkinannya terjebak pada tingkat minimum lokal nilai yang dianggap sebagai titik terendah dari loss function dalam model sehingga adaptive momentum adam cenderung mencapai konvergensi tingkat kinerja yang diharapkan lebih cepat dan andal dalam berbagai keadaan sedangkan stochastic gradient descent sgd mungkin lebih stuck pada nilai minimum atau terjebak pada nilai minimum lokal ya ng disebabkan oleh kemungkinan bergantung pada seberapa tepat kecepatan pemelajaran dipilih kecepatan pemelajaran yang tetap dapat membuat model mencapai konvergensi terlalu cepat atau terlalu lambat. 9 tahapan model evaluation testing 3.3 arsitektur convolutional neural network cnn convolutional neural network cnn yang dibangun menggunakan model atau kerangka kerja yang pada dasarnya menggunakan keras dan juga tensorflow dengan menambahkan beberapa model lapisan lapisan seperti lapisan convolutional conv2d laposan pooling maxpooling 2d flatten dan lapisan fully connected dense . 10 tahapan convolutional neural network cnn dengan model keras. filter convolutional layer pertama yang berfungsi untuk mengekstrak fitur fitur visual diterapkan pada gambar untuk menghasilkan fitur fitur yang lebih abstrak formula untuk mengetahui jumlah training datanya dengan . max pooling memilih nilai maksimum di dalam jendela pooling untuk m engurangi ukuran fitur dan mempertahankan informasi penting. selanjutnya dense layers lapisan dense digunakan sebagai lapisan output dalam model klasifikasi di mana jumlah neuron dalam lapisan output sesuai dengan jumlah kelas yang harus diprediksi di mana ada tiga lapisan dense ditambahkan dengan fungsi pertama dan kedua menggunakan relu sebagai 0 f x max x yang artinya menunjukkan bahwa keluarannya nol jika masukannya negatif atau nol dan output x jika masukannya positif dengan 128 unit neuron dan pada dense kedua 64 unit neuron karena tugasnya mengurangi dimensi representasi pada lapisan dense pertama maka model dapat mempelajari pola yang lebih rumit dan mendalam dari data dengan menambahkan lapisan yang lebih padat yang dapat meningkatkan performa model dalam tugas klasifikasi gambar. terakhir evaluation di mana performa model pada testing data dinilai menggunakan hasil klasifikasi dan confusion matrix untuk memahami kinerjanya testing data. bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 23x23 menjadi 11x11 sebagai berikut. setiap filter diubah menjadi setengah dari ukuran inputnya 11x11 menjadi 5x5 dan jumlah neuronnya 18464 mengikuti lapisan konvolusi ketiga . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 112 21 9 21 5 7. fourth cov2d totalneuronukuranfilterxjumlah𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝐼𝑛𝑝𝑢𝑡1xfilter 3x3x321x16 289x16 4624 jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 5x5 menjadi 2x2 sebagai berikut. setiap filter diubah menjadi setengah dari ukuran inputnya 2x2 menjadi 1x1 dan jumlah neuronnya 4624 mengikuti lapisan konvolusi keempat . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 12 21 1 21 0.51 9. flatten tidak mengubah parameter yang ada karena fungsi flatten hanya mengubah matriks multidimensi menjadi vektor tunggal berdasarkan hasil dari jumlah filter pada lapisan lima cov2d yaitu 8 dan maxpooling2d dengan ukuran inputnya 1x1 sehingga menjadi matriks multidimensi 1 1 16 diubah menjadi nilai vektor tunggal dengan panjang 16 atau menjadi jumlah neuron sebanyak 16. 10. dense layer 1 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 161x128 17x128 2176 11. dropout layer 1 menggunakan 0.2 yang artinya sebanyak 20 dari neuron dalam dense layer 1 akan dinonaktifkan secara acak. 12. dense layer 2 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 1281x64 129x64 8256 13. dropout layer 2 menggunakan 0.2 yang artinya sebanyak 20 dari neuron dalam dense layer 2 akan dinonaktifkan secara acak. 14. dense layer 2 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 641x1 65x1 65 ketika dimensi spasial tinggi dan lebar dikurangi menggunakan operasi lapisan pooling seperti maxpooling jumlah neuron di setiap lapisan pooling akan menurun. misalnya dimensi spasial setiap filter tinggi dan lebar di lapisan maxpooling disesuaikan menjadi setengah dari dimensi masukannya. karena hanya separuh dari masukan yang diproses lebih lanjut hal ini juga menyebabkan berkurangnya jumlah neuron pada lapisan tersebut. sedangkan penurunan pada dense terjadi karena penentuan jumlah neuron.
Devi Resviani_KUALIFIKASI.txt,"3.1 Tahapan Penelitian
Tahapan penelitian merupakan serangkaian langkah-langkah yang dilakukan dalam penelitian. Gambaran mengenai tahapan penelitian ini dapat dilihat pada Gambar 3.1. Tahapan penelitian ini terdiri dari studi literatur untuk memahami keadaan yang terfokus terhadap tentang mesin kompresor reciprocating, metode prediksi pemeliharaan mesin, predictive maintenance, dan machine learning. Identifikasi permasalahan secara spesifik yang akan diatasi oleh penelitian, termasuk mendefinisikan ruang lingkup serta tujuan dari penelitian. Pengumpulan data yang relevan dan melakukan analisis awal untuk memahami karakteristik dan pola dalam data. Preprocessing data melibatkan pembersihan data, normalisasi, dan transformasi data agar siap digunakan dalam model machine learning. Tahap ini bertujuan untuk mengatasi masalah data yang hilang, outliers, dan memastikan data berada dalam format yang sesuai. Pemilihan algoritma machine learning yang efektif untuk membantu mencapai akurasi yang lebih tinggi dan efisiensi dalam prediksi. Pengembangan dan melatih model machine learning menggunakan algoritma yang telah dipilih dengan data yang telah diperoses, tahap ini melibatkan pembagian data menjadi set pelatihan dan set pengujian, serta termasuk mengatur parameter model untuk mencapai kinerja terbaik. Sistem peringatan pemeliharaan prediktif menggunakan model machine learning untuk memprediksi kegagalan mesin, sistem ini bertujuan untuk memberikan peringatan dini sebelum terjadinya kerusakan atau kegagalan mesin. Integrasi model dengan sistem peringatan pemeliharaan prediktif, tahap ini memastikan bahwa model dapat bekerja secara real-time dan memberikan peringatan yang akurat kepada pengguna. Serta tahapan terakhir adalah evaluasi kinerja sistem secara keseluruhan untuk memastikan bahwa sistem peringatan pemeliharaan prediktif berfungsi dengan baik dan mencapai tujuan yang diinginkan. 3.2 Pengumpulan dan Analisis Data
Observasi data dilakukan pada platform terpercaya yang menyediakan berbagai dataset publik yaitu Kaggle. Data yang digunakan yaitu dataset mesin kompresor reciprocating yang divisualisasikan oleh akun kaggle bernama Ahmet Okudan. Dataset tersebut berisi data operasional dari mesin kompresor reciprocating dengan total 1000 sampel. Setiap sampel dalam dataset tersebut mencakup 26 kolom yang berisi informasi dapat dilihat pada Tabel 3.1. Analisis data penelitian ini menggunakan metode Exploratory Data Analysis (EDA) bertujuan untuk memberikan gambaran umum tentang data dan mengidentifikasi pola atau anomali yang mungkin tidak terlihat dengan metode lain. Penerapan metode EDA dalam dataset mesin kompresor reciprocating menggunakan software python open access di notebook jupyter, gambaran mengenai alur proses analisis data dapat dilihat pada Gambar 3.2. 3.3 Preprocessing Data
      Preprocessing data sangat penting untuk memastikan bahwa data yang digunakan untuk pelatihan model machine learning memiliki kualitas yang tinggi, relevan, dan sesuai. Alur proses data preprocessing pada penelitian ini dapat dilihat pada Gambar 3.3. 3.4 Sistem Peringatan Pemeliharaan Prediktif
      Sistem peringatan pemeliharaan prediktif adalah sistem yang menggunakan teknik-teknik analisis data, terutama machine learning untuk memantau kondisi mesin dan memprediksi kegagalan yang mungkin terjadi. Penelitian ini mengusulkan sistem untuk memberikan peringatan dini kepada operator atau tim pemeliharaan sehingga tindakan preventif dapat dilakukan sebelum kegagalan terjadi, mengurangi downtime, dan biaya pemeliharaan. Gambaran alur kerja usulan sistem peringatan pemeliharaan prediktif dapat dilihat pada Gambar 3.4. Tahap awal adalah data operasional mesin dikumpulkan secara menerus menggunakan sensor, bertujuan memastikan data terkini tersedia untuk dianalisis guna mendeteksi
tanda-tanda awal kegagalan mesin. Tahap kedua menginput data dari pemantauan real-time ke dalam model prediksi pemeliharaan yang telah dilatih. Tahap ketiga mendeteksi adanya anomali atau pola yang tidak biasa agar tindakan pencegahan dapat diambil sebelum kerusakan terjadi. Tahap keempat analisis prediksi untuk memperkirakan kapan dan bagaimana kegagalan akan terjadi, sehingga pemeliharaan dapat direncanakan dengan tepat. Tahap kelima alert atau peringatan untuk memberikan informasi kepada tim pemeliharaan agar dapat segera mengambil tindakan. Tahap terakhir menjadwalkan tindakan pemeliharaan yang diperlukan berdasarkan peringatan untuk menghindari kegagalan mendadak dan meminimalkan downtime.","3.1 Tahapan Penelitian
Tahapan penelitian merupakan serangkaian langkah-langkah yang dilakukan dalam penelitian. Gambaran mengenai tahapan penelitian ini dapat dilihat pada Gambar 3.1. Tahapan penelitian ini terdiri dari studi literatur untuk memahami keadaan yang terfokus terhadap tentang mesin kompresor reciprocating, metode prediksi pemeliharaan mesin, predictive maintenance, dan machine learning. Preprocessing data melibatkan pembersihan data, normalisasi, dan transformasi data agar siap digunakan dalam model machine learning. Pengembangan dan melatih model machine learning menggunakan algoritma yang telah dipilih dengan data yang telah diperoses, tahap ini melibatkan pembagian data menjadi set pelatihan dan set pengujian, serta termasuk mengatur parameter model untuk mencapai kinerja terbaik. Tahap keempat analisis prediksi untuk memperkirakan kapan dan bagaimana kegagalan akan terjadi, sehingga pemeliharaan dapat direncanakan dengan tepat. Tahap kelima alert atau peringatan untuk memberikan informasi kepada tim pemeliharaan agar dapat segera mengambil tindakan. Tahap terakhir menjadwalkan tindakan pemeliharaan yang diperlukan berdasarkan peringatan untuk menghindari kegagalan mendadak dan meminimalkan downtime."
Erfiana Wahyuningsih_UK.txt,"3.1 KONSEP PENELITIAN
Untuk mempermudah dalam melakukan penelitian, maka dibuat sebuah flowchart agar penelitian tidak menyimpang dan salah. Berikut flowchart penelitian untuk rangkaian SRAM 6T Low power dan High read stability dengan metode m- GDI. Dalam memulai desain SRAM 6T dengan menggunakan metode m-GDI, diperlukan studi literatur terkait beberapa penelitian dengan metode atau hasil serupa. Setelah mempelajari seluruh penelitian terkait, maka dilakukan desain
rangkaian SRAM dengan metode konvensional sebagai referensi untuk dilakukan proses m-GDI. Referensi rangkaian diperlukan untuk melihat hasil sebagai pembanding dengan rangkaian baru yang didesain dengan metode m-GDI. Dipilih desain berdasarkan penelitian sebelumnya yang dilakukan oleh Ebrahim Abiri dan Abdolreza Darabi (2015) SRAM 8T dengan Low Power dan High Read Stability menggunakan metode m-GDI. Berdasarkan penelitian yang dilakukan, rangkaian SRAM ini terbagi menjadi 3 Blok, yakni Write Block, SRAM 8T Block dan Read Block, seperti pada gambar 11 berikut,
Proses write pada rangkaian SRAM 8T dimulai ketika word line (WL) mencapai tegangan tinggi, yang menyebabkan transistor akses (access transistors) menjadi aktif (on). Pada saat itu, data disimpan dengan cepat pada node q dan qb yang terhubung ke gerbang dari transistor pusat sel (ENR dan ENL). Setelah itu, data
mencapai keadaan permanen dengan bantuan sel-sel m-GDI . Selama siklus write, sel m-GDI berperan dalam memastikan bahwa data yang ditulis ke dalam sel memori SRAM disimpan dengan stabil dan cepat, yang merupakan bagian penting dari desain SRAM low power
Dalam blok rangkaian SRAM 8T, input utama termasuk word line (WL), bit lines (BL dan BLB), dan sinyal-sinyal kontrol untuk operasi pembacaan dan penulisan. Output dari rangkaian ini adalah data yang dibaca dari sel memori (D_Out dan D_Outb). Ketika melakukan operasi penulisan, data yang akan ditulis ke dalam sel memori disuplai melalui bit lines (BL dan BLB). Sinyal WL diaktifkan untuk menghubungkan sel memori dengan bit lines, memungkinkan data untuk ditransfer ke dalam sel. Setelah data ditulis, WL dinonaktifkan untuk mengisolasi sel dari bit lines dan menjaga data yang telah disimpan. Selama operasi pembacaan, WL diaktifkan untuk menghubungkan sel memori dengan bit lines, memungkinkan data yang tersimpan di dalam sel untuk ditransfer keluar. Data yang dibaca kemudian muncul pada output D_Out dan D_Outb dengan swing tegangan maksimum pada output inverter.","3.1 KONSEP PENELITIAN
Untuk mempermudah dalam melakukan penelitian, maka dibuat sebuah flowchart agar penelitian tidak menyimpang dan salah. Berikut flowchart penelitian untuk rangkaian SRAM 6T Low power dan High read stability dengan metode m- GDI. Dalam memulai desain SRAM 6T dengan menggunakan metode m-GDI, diperlukan studi literatur terkait beberapa penelitian dengan metode atau hasil serupa. Referensi rangkaian diperlukan untuk melihat hasil sebagai pembanding dengan rangkaian baru yang didesain dengan metode m-GDI."
Fitriana Indah Pramitasari_Kualifikasi.txt,3.1 alur penelitian alur penelitian menggambarkan alur dari awal hingga akhir penelitian dilaksanakan. alur penelitian ini diuraikan pada gambar 3.1 di bawah ini. gambar 3.1 alur penelitian 27 3.2 identifikasi masalah identifikasi masalah adalah salah satu langkah pertama yang dilakukan sebelum melakukan penelitian. identifikasi masalah merupakan suatu proses mencari dan mengetahui masalah yang ingin diselesaikan. identifikasi masalah ini membantu penelitian untuk memah ami tantangan yang dihadapi oleh petani kentang skala nasional dan merancang solusi yang tepat sesuai dengan kebutuhan mereka. identifikasi masalah pada penelitian ini berfokus pada mengidentifikasi proses perancangan model koperasi petani mengidentifikas i metode prediksi permintaan dengan ann di dalam blockchain yang digunakan untuk mengoptimalkan permintaan pelanggan di masa depan selama periode tertentu dan mengidentifikasi metode safety stock di dalam blockchain yang digunakan agar dapat mengoptimalkan stok dan permintaan. identifikasi masalah pada penelitian ini peneliti dapat lebih memahami kendala dan kebutuhan petani kentang skala nasional. perancangan model platform koperasi untuk meningkatkan efisiensi dan kerjasama antarpetani dengan koperasi sebagai mitranya. sement ara itu metode prediksi permintaan dengan menggunakan artificial neural network ann diharapkan dapat membantu petani mengelola produksi secara lebih tepat sesuai dengan kebutuhan pasar dan koperasi dapat menyesuaikan persediaan stok dan permintaan secar a dinamis dari hasil prediksi permintaan. selain itu identifikasi masalah juga mencakup penerapan metode safety stock untuk mengoptimalkan manajemen stok memastikan ketersediaan barang dan meningkatkan responsibilitas terhadap fluktuasi permintaan pasar. dengan penerapan ann dan metode safety stock di dalam blockchain semua prediksi dan manajemen stok dapat dicatat di dlaam buku besar yang tidak dapat diubah sehingga meningkatkan transparansi dan keamanan data dalam rantai pasok. sehingga koperasi ini dapat melakukan perencanaan yang lebih akurat meminimalkan pemborosan dan meningkatkan ketersediaan kentang sesuai dengan kebutuhan pelanggan. dengan demikian platform koperasi menjadi responsif terhadap perubahan permintaan pasar mendukung pertumbuhan ekonomi para petani memperkuat kolaborasi antar anggota koperasi serta memiliki transparansi dan keamanan pada rantai pasok. 28 3.3 studi literatur studi literatur yang dilakukan pada penelitian engembangan platform koperasi petani ini dimulai dari pencarian dan review literatur literatur terbaru dan relevan yang telah diterbitkan. studi literatur juga dapat dari teori teori buku yang relevan dengan metode yang digunakan. analisis literatur membantu untuk mengidentifikasi kerangka kerja metode dan teknologi yang telah digunakan pada penelitian sebelumnya. studi literatur dapat digunakan sebagai mencari solusi dan menganalisa penelitian yang dilakukan . studi literatur juga membantu dalam mengetahui tantangan dan peluang yang mungkin dihadapi dalam pengembangan platform koperasi petani kentang. sehingga informasi tersebut dapat memberikan sebuah wawasan terkait dengan penelitian yang dilakukan. 3.4 pengumpulan data pengumpulan data yang digunakan sebagai bahan dalam mengolah data. sehingga penelitian ini akan menghasilkan data yang sesuai dengan tujuan penelitian. penelitian ini mengumpulkan data sekunder dan data primer. pengumpulan data pada penelitian ini terdiri dari beberapa proses sebagai berikut. pengumpulan data sekunder memanfaatkan sumber informasi yang sudah ada seperti literatur ilmiah dokumen resmi dan data statistik yang relevan. proses ini memungkinkan peneliti untuk memahami konteks yang telah ada sebelumnya dan memanfaatkan pengetahua n serta data yang telah dihasilkan sebelumnya. berdasarkan data yang diperoleh dari badan pusat statistik bps tahun 2022 menjelaskan data produksi kentang di berbagai wilayah indonesia. beberapa wilayah indonesia berhasil dalam produksi kentang dan beberapa wilayah indonesia yang tidak dapat mempro duksi kentang. data tersebut memberikan gambaran lengkap mengenai kegiatan pertanian kentang di berbagai wilayah indonesia pada tahun 2022. berikut data bps tahun 2022 produksi kentang di berbagai wilayah indonesia. 29 tabel 3.1 data lokasi produksi kentang 2022 sumber badan pusat statistik 2023 pengumpulan data primer yaitu melakukan pencarian secara langsung untuk mengumpulkan data serta informasi baru sesuai dengan tujuan penelitian. metode ini seperti pengambilan data survei wawancara observasi atau eksperimen dengan tujuan untuk kebutuha n penelitian. data primer yang akan digunakan pada penelitian ini adalah kebutuhan pengguna aliran data dari petani dengan koperasi sebagai mitranya data musim data historis penjualan data produksi kentang dan data harga kentang. pengambilan data primer dilakukan di wonosobo jawa tengah. berdasarkan informasi yang didapatkan dari salah satu petani di wonosobo jawa tengah disana terdapat banyak petani kentang dan sayuran lainnya. menurut bps kabupaten wonosobo jawa tengah adalah wilayah yang terbanyak memproduksi kentang di provinsi jawa tengah. pola distribusi kentang di wonosobo jawa tengah terdiri dari 3 pola sebagai berikut zaenuri et al 2023. 30 gambar 3.2 pola distribusi kentang 3.5 blockchain pada penelitian ini untuk meningkatkan keamanan dan transparansi maka menggunakan teknologi blockchain untuk rantai pasok kentang. berikut flowchart kecerdasan buatan safety stock yang dikombinasikan di dalam blockchain. gambar 3.3 flowchart blockchain berdasarkan gambar di atas menggambarkan kombinasi ann dan safety stock di dalam blockchain. data rantai pasok yang telah dikumpulkan kemudian 31 dimasukkan ke dalam database. data tersebut diverifikasi dalam blockchain dengan proses pembuatan blok baru yang melibatkan perhitungan hash blok sebelumnya menyusun blok baru menghitung hash blok baru dan mencapai konsensus untuk menambahkan blok ke ra ntai. data yang diverifikasi kemudian diproses menggunakan model artificial neural network ann. tahapan dalam ann meliputi praproses data inisialisasi model pelatihan model validasi model dan evaluasi kinerja. hasil prediksi permintaan disimpan dalam blockchain dengan proses pembuatan blok baru yang sama seperti langkah sebelumnya. selanjutnya permintaan data diverifikasi dan jika valid safety stock dihitung menggunakan rumus safety stock yang sudah ada. hasil perhitungan safety stock disimpan dalam database dan dicatat dalam blockchain dengan pembuatan blok baru. semua data dari proses tersebut dicatat dalam blockchain untuk memastikan transparansi dan keamanan. proses validasi memastikan bahwa data permintaan dan pengelolaan stok selalu diperbarui dan valid sebelum digunakan untuk pengambilan keputusan. 3.6 design sistem dengan uml pengembangan platform koperasi petani kentang menggunakan metode unified modeling language uml untuk menggambarkan struktur fungsi dan interaksi komponen sistem secara visual. dimana proses metode uml ini diawali dengan identifikasi kebutuhan sistem dan pemahaman terhadap fungsionalitas yang terkait dengan economic sharing dan prinsip prinsip perkoperasian. 3.6.1 analisis kebutuhan sistem analisis kebutuhan sistem bertujuan untuk mengidentifikasi kebutuhan dari pengguna dan stakeholder sistem. pengumpulan informasi pada proses ini mengenai detail cara kerja sistem dan batasan batasan yang ada. pengumpulan data dilakukan melalui wawancara a tau survei. analisis kebutuhan sistem dapat menentukan arah dan ruang lingkup proyek pengembang sistem serta memastikan bahwa produk akhir akan memenuhi harapan dan memecahkan masalah yang dihadapi oleh pengguna. 32 gambar 3. 4 multi stakeholder cooperative gambar 3. 4 menjelaskan proses multi stakeholder cooperative dimana anggota koperasi termasuk dari workers community producers dan consumers . mereka memilih board of director dari para anggotanya. board of director merupakan struktur organisasi yang bertanggung jawab dalam mengawasi manajemen yang dijalankan oleh koperasi dengan setiap anggotanya memiliki tugas khusus sesuai dengan tujuan koperasi. dewan direksi berperan penting dalam menjaga keberlanjutan dan kese imbangan antara berbagai kepentingan dalam konteks multi stakeholder cooperative . gambar 3. 5 pengguna platform gambar 3. 5 mendeskripsikan pengguna platform koperasi petani yang melibatkan sejumlah pihak. pengguna platform ini terdiri dari consumers yang dapat mengakses produk pertanian secara langsung farmers yang memanfaatkan platform untuk memasarkan hasil panen companies yang terlibat dalam dukungan pengembangan teknologi dan partner cooperatives yang menjadi bagian dari 33 kolaborasi kerjasama antar koperasi untuk meningkatkan kesejahteraan bersama. keterlibatan seluruh pihak ini diharapkan platform koperasi petani menciptakan lingkungan yang saling mendukung dan berkelanjutan memperkuat konektivitas antar anggota untuk me ncapai tujuan bersama dalam dunia pertanian. 3.6.2 use case diagram model pertama uml adalah pemodelan use case diagram dimana menggambarkan skenario skenario utama pengguna platform koperasi. use case diagram digunakan untuk menunjukkan hubungan dan struktur kelas kelas yang terlibat dalam sistem termasuk entitas entitas seperti data permintaan stok kentang dan pengguna. gambar 3. 6 use case diagram gambar 3. 6 adalah diagram use case untuk platform koperasi petani yang menunjukkan berbagai interaksi antara pengguna dan sistem. pada diagram use 34 case terdapat aktor yang terdiri dari petani konsumen anggota koperasi dan admin. registrasi dilakukan oleh petani anggota koperasi dan konsumen. proses login untuk mengakses fitur yang terdapat pada website koperasi dapat dilakukan oleh petani anggot a koperasi konsumen dan admin. proses mencatat produksi hanya dilakukan oleh petani dimana petani mencatat data produksi kentang mereka yang kemudian akan dicatat di blockchain. proses melacak produksi dan distribusi menggunakan blockchain. proses verifi kasi produk kentang yang dihasilkan oleh petani. proses verifikasi dilakukan oleh admin dan anggota koperasi. proses melakukan pembelian melalui website dilakukan oleh konsumen. proses mengelola transaksi yang terjadi di dalam sistem memastikan semua tran saksi tercatat di blockchain dilakukan oleh admin. proses melihat laporan dan statistik dari data produksi distribusi dan transaksi yang terjadi di dalam sistem dilakukan oleh admin anggota koperasi dan petani. 3.6.3 activity diagram model kedua adalah activity diagram untuk menggambarkan alur kerja atau proses proses yang terjadi dalam platform koperasi. activity diagram dapat membantu dalam menguraikan langkah langkah yang diperlukan dari pemesanan kentang hingga manajemen stok. gambar 3. 7 activity diagram mencatat produksi 35 diagram pada gambar 3. 7 adalah proses mencatat produksi yang diawali dengan login seorang petani ke dalam platform koperasi petani. petani memulai dengan membuka platform dan memilih opsi untuk login. setelah login berhasil petani memasukkan data produksi kentang. sistem akan m emverifikasi data. selanjutnya sistem mencatat data di blockchain dan sistem memberikan notifikasi ke petani. gambar 3. 8 activity diagram melacak produksi dan distribusi diagram pada gambar 3. 8 merupakan alur proses melacak produksi dan distribusi. proses ini dilakukan dari anggota melakukan login ke sistem. selanjutnya anggota koperasi memilih menu pelacak produksi dan distribusi. sistem akan menampilkan tampilan menu pelacak produksi dan distr ibusi dan anggota koperasi memasukkan id produk. sistem akan mengambil data dari blockchain dan jika data telah dikirim oleh blockchain selanjutnya sistem akan menampilkan informasi pelacak. 36 gambar 3. 9 activity diagram pembelian diagram pada gambar 3. 9 menggambarkan proses pembelian yang dilakukan oleh konsumen. konsumen melakukan login di sistem koperasi petani. selanjutnya konsumen mencari produk yang ingin dibeli dan memilih produk serta memasukkan jumlah pembelian. sistem akan memverifikasi stok pr oduk. konsumen selanjutnya memasukkan informasi pembayaran. sistem akan mencatat transaksi di blockchain dan sistem akan mengirimkan notifikasi pembelian. gambar 3. 10 activity diagram kelola transaksi diagram pada gambar 3. 10 menggambarkan alur proses mengelola transaksi. pertama admin melakukan login pada sistem website koperasi. admin 37 memilih menu manajemen transaksi dan sistem menampilkan daftar transaksi yang terjadi. admin memverifikasi transaksi yang belum diverifikasi lalu mengubah status transaksi sesuai hasil verifikasi. sistem memperbarui data di blockchain. sistem akan menampil kan laporan transaksi yang berhasil diperbarui di blockchain. 3.6.4 sequence diagram model ketiga adalah sequence diagram dimana diagram ini dapat membantu dalam menggambarkan urutan peristiwa atau interaksi antar komponen dalam sistem seperti bagaimana data pemesanan diteruskan dan diproses. gambar 3. 11 sequence diagram mencatat produksi diagram pada gambar 3. 11 menggambarkan proses mencatat produksi kentang petani di website koperasi yang terintegrasi dengan blockchain. pertama petani memilih menu login dan mengisi form login ke website koperasi. website memverifikasi dan mengirimkan notifikasi login sukses. pe tani selanjutnya memilih menu data produksi dan melakukan input data produksi seperti nama produk kuantitas tanggal produksi dan sebagainya. website akan memverifikasi data yang di masukkan oleh petani. setelah data diverifikasi website koperasi mengir imkan data produksi ke blockchain. blockchain akan memverifikasi data tersebut melalui node node yang ada. data produksi yang telah diverifikasi oleh node blockchain selanjutnya dicatat di dalam blockchain. kemudian website 38 koperasi mengirimkan notifikasi ke petani bahwa data produksi telah berhasil tercatat gambar 3.1 2 sequence diagram melacak produksi dan distribusi diagram pada gambar 3.1 2 menunjukkan tahapan yang dilalui oleh seorang petani untuk melacak produksi dan distribusi kentang melalui website koperasi yang terintegrasi dengan blockchain. pertama anggota koperasi. mengirimkan permintaan login ke website koperasi. website akan mem verifikasi dan mengirimkan notifikasi login sukses. anggota koperasi memilih menu pelacak produksi dan distribusi pada website. anggota koperasi memasukkan id produk yang ingin dilacak. website koperasi mengirimkan permintaan untuk mengambil data pelacakan dari blockchain. blockchain mengirimkan data pelacakan yang diminta. selanjutnya website koperasi akan menampilkan informasi pelacakan yang diperoleh dari blockchain kepada anggota koperasi. gambar 3.1 3 sequence diagram pembelian 39 diagram pada gambar 3.1 3 menunjukkan alur proses pembelian yang dilakukan oleh konsumen untuk membeli produk kentang melalui website koperasi yang terintegrasi dengan blockchain. konsumen mengirimkan permintaan login ke website koperasi. website koperasi akan memverifikasi data l ogin yang diinput oleh konsumen dan mengirimkan notifikasi login sukses. konsumen memilih menu produk dan input nama produk yang akan dicari di sistem website. website koperasi akan menampilkan daftar produk yang diinginkan. konsumen memilih produk dan mem asukkan jumlah pembelian. website akan memverifikasi stok produk yang tersedia. setelah stok produk diverifikasi website akan mengirimkan informasi stok tersedia dan konsumen memasukkan informasi pembayaran. website koperasi akan mengirimkan data transaks i ke blockchain untuk dicatat. blockchain memverifikasi dan mencatat transaksi. website koperasi akan mengirimkan informasi kepada konsumen bahwa transaksi telah berhasil dicatat. gambar 3.1 4 sequence diagram mengelola transaksi diagram yang ditampilkan pada gambar 3.1 4 menggambarkan langkah langkah dalam mengelola transaksi di website koperasi. admin mengirimkan permintaan login ke website koperasi. website koperasi akan memverifikasi dan mengirimkan notifikasi login sukses. setelah berhasil login admin memilih menu 40 manajemen transaksi. admin melihat daftar transaksi yang terjadi. selanjutnya admin memilih transaksi yang belum diverifikasi dan melakukan verifikasi. website koperasi mengubah status transaksi berdasarkan hasil verifikasi. website mengirimkan permintaan untuk memperbarui status transaksi di blockchain. blockchain memverifikasi dan memperbarui status transaksi. admin melihat laporan transaksi yang telah diperbarui. website mengambil data laporan dari blockchain dan menampilkan kepada admin. 3.7 prediksi permintaan dengan ann metode prediksi permintaan dalam penelitian ini bertujuan untuk memprediksi jumlah kentang yang diminta oleh pasar atau konsumen pada periode waktu tertentu. metode yang digunakan pada prediksi permintaan adalah metode ann. input data yang akan digunakan a dalah data kuantitatif dan kualitatif yang dapat mempengaruhi permintaan di masa depan sehingga agar hasil prediksi permintaan dapat lebih akurat. dengan menerapkan metode ann pada prediksi permintaan ini penelitian dapat memberikan prediksi yang lebih tepat terkait kebutuhan pasar di masa mendatang sehingga dapat meningkatnya efektivitas rantai pasok. prediksi ini juga dapat memberikan petani wawas an yang berharga terkait potensi pasar dan membantu mereka mengoptimalkan produksi serta mitra koperasi dapat merencanakan strategi pemasaran yang lebih efektif. dengan menerapkan metode prediksi permintaan kentang menggunakan metode ann penelitian ini da pat memberikan kontribusi signifikan dalam mendukung keberlanjutan dan efisiensi dalam pertanian kentang. langkah pra pemrosesan data melibatkan pengumpulan data historis relevan seperti data penjualan sebelumnya data harga data produksi kentang serta data musim sebagai faktor eksternal yang dapat memengaruhi permintaan. kemudian dilakukan data cleaning di normalisasi dan di transformasi untuk memastikan bahwa ann yang akan dibangun dapat bekerja dengan efektif dan menghasilkan prediksi yang akurat. 41 langkah selanjutnya adalah desain dan pelatihan model ann. tahapan ini melibatkan proses pemilihan arsitektur jaringan yang tepat termasuk jumlah lapisan tersembunyi jumlah neuron per lapisan fungsi aktivasi dan algoritma pembelajaran. pelatihan model merupakan proses ann menyesuaikan bobotnya berdasarkan kesalahan prediksi melalui metode seperti backpropagation. pada tahapan ini penyesuaian parameter seperti kecepatan belajar dan momentum dilakukan untuk memperbaiki proses pembelajaran model. setelah dilatih dengan baik model akan mampu mengenali pola kompleks dan hubungan non linear dalam data. tahapan evaluasi model ann adalah tahapan dimana model yang telah dilatih dan diuji menggunakan dataset yang belum pernah dilihat sebelumnya untuk menilai kemampuannya dalam memprediksi permintaan dengan akurat. matrik evaluasi seperti mean square error mse atau mean absolute percentage error mape merupakan matrik yang sering digunakan untuk mengukur kinerja model. berdasarkan hasil evaluasi model prediksi yang akurat dari model ann ini berguna untuk perusahan dalam membuat keputusan strategis seperti inventory management . proses prediksi permintaan dengan ann akan menghasilkan data permintaan yang diharapkan informasi tersebut digunakan untuk proses inventory management . data prediksi permintaan tersebut akan digunakan sebagai dasar perhitungan safety stock . sehingga perusahaan dapat merencanakan dan menyesuaikan kuantitas inventory yang cukup untuk memenuhi permintaan dimana akan meminimalisir biaya penyimpanan dan mengurangi risiko kekurangan stok. tujuannya agar operasi bisnis dapat berjalan dengan lanca r dan efisien s erta mengoptimalkan ketersediaan produk. 3.8 inventory management proses inventory management menggunakan metode safety stock merupakan proses untuk menjaga ketersediaan persediaan dalam platform secara efektif. tahapan pertama penelitian ini memerlukan analisis data historis 42 permintaan kentang fluktuasi pasokan dan waktu panen sehingga dapat mengidentifikasi kebutuhan pasokan dan resiko keterlambatan. penerapan metode safety stock pada penelitian ini akan menentukan tingkat persediaan tambahan yang diperlukan untuk mengatasi ketidakpastian dalam permintaan atau keterlambatan pasokan. hal ini bertujuan untuk memberikan keandalan dan menghindari kekurangan persediaan yang dapat mengha mbat operasional koperasi. metode safety stock dalam pengembangan platform koperasi petani kentang pada penelitian ini untuk meningkatkan efisiensi manajemen persediaan. 3.9 integrasi tahapan integrasi merupakan proses menggabungkan sistem aplikasi atau teknologi yang berbeda menjadi satu kesatuan yang berfungsi secara harmonis. pada proses ini berbagai komponen yang sebelumnya beroperasi secara terpisah agar dapat berinteraksi satu sama lain dalam mencapai tujuan bersama. integrasi bertugas menyatukan aspek desain sistem dengan uml prediksi permintaan dan inventory management . proses integrasi menjamin bahwa data yang diolah sebelumnya dapat digunakan dengan baik untuk mendukung pe ngambilan keputusan. selain itu integrasi ini melibatkan penggunaan artificial neural network ann dan metode safety stock yang terintegrasi dalam blockchain untuk rantai pasok. data dari prediksi permintaan dan pengelolaan stok dicatat secara transparan dan aman dalam blockchain. website koperasi akan terintegrasi dengan blockchain untuk memastikan efisiensi dan transparansi dalam seluruh proses manajemen rantai pasok. 3.10 pengujian sistem tahapan pengujian sistem dalam penelitian merupakan langkah untuk mengevaluasi kinerja atau fungsionalitas sistem yang dikembangkan atau diuji pada penelitian. proses pengujian sistem mencakup implementasi prototipe atau model sistem hingga serangkaian u ji coba. tujuan dari tahapan pengujian sistem adalah mengidentifikasi adanya kegagalan mengukur efektivitas sebuah sistem serta 43 memastikan sistem berjalan sesuai dengan tujuan dan persyaratan yang telah ditetapkan sebelumnya. pada penelitian ini sistem platform koperasi petani diharapkan dapat berjalan sesuai dengan tujuan dan persyaratan perkoperasian serta sesuai dengan model platform economic sharing . platform koperasi petani kentang pada penelitian ini akan berbasis website dan dilengkapi dengan kecerdasan buatan yang dikombinasikan dengan blockchain. 3.11 evaluasi tahapan selanjutnya adalah evaluasi. evaluasi dilakukan untuk memastikan bahwa semua komponen sistem berfungsi sesuai rencana. evaluasi melibatkan penilaian kinerja pada sistem secara keseluruhan dan memeriksa apakah integrasi berjalan tanpa hambatan. ta hapan evaluasi juga dapat mengidentifikasi apakah hasil pengujian sistem sesuai dengan tujuan awal dan menentukan area yang mungkin memerlukan peningkatan. hasil dari tahap evaluasi menjadi petunjuk penting untuk membuat perubahan dan peningkatan sehingga sistem dapat bekerja lebih baik lagi. 3.12 analisis hasil analisis merupakan tahapan penelitian dimana menyimpulkan serta menguraikan informasi dari hasil data yang telah diolah dan diuji sebelumnya. tahapan analisis dapat memberikan makna dari temuan temuan tersebut. tahapan ini memberikan identifikasi faktor faktor yang dapat mempengaruhi kinerja sistem dan memberikan rekomendasi untuk peningkatan di masa yang akan datang.,3.1 alur penelitian alur penelitian menggambarkan alur dari awal hingga akhir penelitian dilaksanakan. alur penelitian ini diuraikan pada gambar 3.1 di bawah ini. gambar 3.1 alur penelitian 27 3.2 identifikasi masalah identifikasi masalah adalah salah satu langkah pertama yang dilakukan sebelum melakukan penelitian. identifikasi masalah merupakan suatu proses mencari dan mengetahui masalah yang ingin diselesaikan. identifikasi masalah ini membantu penelitian untuk memah ami tantangan yang dihadapi oleh petani kentang skala nasional dan merancang solusi yang tepat sesuai dengan kebutuhan mereka. identifikasi masalah pada penelitian ini berfokus pada mengidentifikasi proses perancangan model koperasi petani mengidentifikas i metode prediksi permintaan dengan ann di dalam blockchain yang digunakan untuk mengoptimalkan permintaan pelanggan di masa depan selama periode tertentu dan mengidentifikasi metode safety stock di dalam blockchain yang digunakan agar dapat mengoptimalkan stok dan permintaan. identifikasi masalah pada penelitian ini peneliti dapat lebih memahami kendala dan kebutuhan petani kentang skala nasional. perancangan model platform koperasi untuk meningkatkan efisiensi dan kerjasama antarpetani dengan koperasi sebagai mitranya. sement ara itu metode prediksi permintaan dengan menggunakan artificial neural network ann diharapkan dapat membantu petani mengelola produksi secara lebih tepat sesuai dengan kebutuhan pasar dan koperasi dapat menyesuaikan persediaan stok dan permintaan secar a dinamis dari hasil prediksi permintaan. selain itu identifikasi masalah juga mencakup penerapan metode safety stock untuk mengoptimalkan manajemen stok memastikan ketersediaan barang dan meningkatkan responsibilitas terhadap fluktuasi permintaan pasar. dengan penerapan ann dan metode safety stock di dalam blockchain semua prediksi dan manajemen stok dapat dicatat di dlaam buku besar yang tidak dapat diubah sehingga meningkatkan transparansi dan keamanan data dalam rantai pasok. sehingga koperasi ini dapat melakukan perencanaan yang lebih akurat meminimalkan pemborosan dan meningkatkan ketersediaan kentang sesuai dengan kebutuhan pelanggan. dengan demikian platform koperasi menjadi responsif terhadap perubahan permintaan pasar mendukung pertumbuhan ekonomi para petani memperkuat kolaborasi antar anggota koperasi serta memiliki transparansi dan keamanan pada rantai pasok. 28 3.3 studi literatur studi literatur yang dilakukan pada penelitian engembangan platform koperasi petani ini dimulai dari pencarian dan review literatur literatur terbaru dan relevan yang telah diterbitkan. studi literatur juga membantu dalam mengetahui tantangan dan peluang yang mungkin dihadapi dalam pengembangan platform koperasi petani kentang. sehingga penelitian ini akan menghasilkan data yang sesuai dengan tujuan penelitian. proses ini memungkinkan peneliti untuk memahami konteks yang telah ada sebelumnya dan memanfaatkan pengetahua n serta data yang telah dihasilkan sebelumnya. beberapa wilayah indonesia berhasil dalam produksi kentang dan beberapa wilayah indonesia yang tidak dapat mempro duksi kentang. data tersebut memberikan gambaran lengkap mengenai kegiatan pertanian kentang di berbagai wilayah indonesia pada tahun 2022. berikut data bps tahun 2022 produksi kentang di berbagai wilayah indonesia. data primer yang akan digunakan pada penelitian ini adalah kebutuhan pengguna aliran data dari petani dengan koperasi sebagai mitranya data musim data historis penjualan data produksi kentang dan data harga kentang. berdasarkan informasi yang didapatkan dari salah satu petani di wonosobo jawa tengah disana terdapat banyak petani kentang dan sayuran lainnya. 30 gambar 3.2 pola distribusi kentang 3.5 blockchain pada penelitian ini untuk meningkatkan keamanan dan transparansi maka menggunakan teknologi blockchain untuk rantai pasok kentang. berikut flowchart kecerdasan buatan safety stock yang dikombinasikan di dalam blockchain. data rantai pasok yang telah dikumpulkan kemudian 31 dimasukkan ke dalam database. data tersebut diverifikasi dalam blockchain dengan proses pembuatan blok baru yang melibatkan perhitungan hash blok sebelumnya menyusun blok baru menghitung hash blok baru dan mencapai konsensus untuk menambahkan blok ke ra ntai. hasil prediksi permintaan disimpan dalam blockchain dengan proses pembuatan blok baru yang sama seperti langkah sebelumnya. hasil perhitungan safety stock disimpan dalam database dan dicatat dalam blockchain dengan pembuatan blok baru. 3.6 design sistem dengan uml pengembangan platform koperasi petani kentang menggunakan metode unified modeling language uml untuk menggambarkan struktur fungsi dan interaksi komponen sistem secara visual. analisis kebutuhan sistem dapat menentukan arah dan ruang lingkup proyek pengembang sistem serta memastikan bahwa produk akhir akan memenuhi harapan dan memecahkan masalah yang dihadapi oleh pengguna. pengguna platform ini terdiri dari consumers yang dapat mengakses produk pertanian secara langsung farmers yang memanfaatkan platform untuk memasarkan hasil panen companies yang terlibat dalam dukungan pengembangan teknologi dan partner cooperatives yang menjadi bagian dari 33 kolaborasi kerjasama antar koperasi untuk meningkatkan kesejahteraan bersama. keterlibatan seluruh pihak ini diharapkan platform koperasi petani menciptakan lingkungan yang saling mendukung dan berkelanjutan memperkuat konektivitas antar anggota untuk me ncapai tujuan bersama dalam dunia pertanian. proses verifi kasi produk kentang yang dihasilkan oleh petani. website koperasi akan mengirimkan informasi kepada konsumen bahwa transaksi telah berhasil dicatat. website koperasi mengubah status transaksi berdasarkan hasil verifikasi. input data yang akan digunakan a dalah data kuantitatif dan kualitatif yang dapat mempengaruhi permintaan di masa depan sehingga agar hasil prediksi permintaan dapat lebih akurat. dengan menerapkan metode ann pada prediksi permintaan ini penelitian dapat memberikan prediksi yang lebih tepat terkait kebutuhan pasar di masa mendatang sehingga dapat meningkatnya efektivitas rantai pasok. kemudian dilakukan data cleaning di normalisasi dan di transformasi untuk memastikan bahwa ann yang akan dibangun dapat bekerja dengan efektif dan menghasilkan prediksi yang akurat. berdasarkan hasil evaluasi model prediksi yang akurat dari model ann ini berguna untuk perusahan dalam membuat keputusan strategis seperti inventory management . proses prediksi permintaan dengan ann akan menghasilkan data permintaan yang diharapkan informasi tersebut digunakan untuk proses inventory management . 3.8 inventory management proses inventory management menggunakan metode safety stock merupakan proses untuk menjaga ketersediaan persediaan dalam platform secara efektif. tahapan pertama penelitian ini memerlukan analisis data historis 42 permintaan kentang fluktuasi pasokan dan waktu panen sehingga dapat mengidentifikasi kebutuhan pasokan dan resiko keterlambatan. penerapan metode safety stock pada penelitian ini akan menentukan tingkat persediaan tambahan yang diperlukan untuk mengatasi ketidakpastian dalam permintaan atau keterlambatan pasokan. hal ini bertujuan untuk memberikan keandalan dan menghindari kekurangan persediaan yang dapat mengha mbat operasional koperasi. metode safety stock dalam pengembangan platform koperasi petani kentang pada penelitian ini untuk meningkatkan efisiensi manajemen persediaan. selain itu integrasi ini melibatkan penggunaan artificial neural network ann dan metode safety stock yang terintegrasi dalam blockchain untuk rantai pasok. website koperasi akan terintegrasi dengan blockchain untuk memastikan efisiensi dan transparansi dalam seluruh proses manajemen rantai pasok. 3.10 pengujian sistem tahapan pengujian sistem dalam penelitian merupakan langkah untuk mengevaluasi kinerja atau fungsionalitas sistem yang dikembangkan atau diuji pada penelitian. proses pengujian sistem mencakup implementasi prototipe atau model sistem hingga serangkaian u ji coba. tujuan dari tahapan pengujian sistem adalah mengidentifikasi adanya kegagalan mengukur efektivitas sebuah sistem serta 43 memastikan sistem berjalan sesuai dengan tujuan dan persyaratan yang telah ditetapkan sebelumnya. pada penelitian ini sistem platform koperasi petani diharapkan dapat berjalan sesuai dengan tujuan dan persyaratan perkoperasian serta sesuai dengan model platform economic sharing . platform koperasi petani kentang pada penelitian ini akan berbasis website dan dilengkapi dengan kecerdasan buatan yang dikombinasikan dengan blockchain. 3.11 evaluasi tahapan selanjutnya adalah evaluasi. evaluasi dilakukan untuk memastikan bahwa semua komponen sistem berfungsi sesuai rencana. evaluasi melibatkan penilaian kinerja pada sistem secara keseluruhan dan memeriksa apakah integrasi berjalan tanpa hambatan. ta hapan evaluasi juga dapat mengidentifikasi apakah hasil pengujian sistem sesuai dengan tujuan awal dan menentukan area yang mungkin memerlukan peningkatan. hasil dari tahap evaluasi menjadi petunjuk penting untuk membuat perubahan dan peningkatan sehingga sistem dapat bekerja lebih baik lagi. 3.12 analisis hasil analisis merupakan tahapan penelitian dimana menyimpulkan serta menguraikan informasi dari hasil data yang telah diolah dan diuji sebelumnya. tahapan analisis dapat memberikan makna dari temuan temuan tersebut. tahapan ini memberikan identifikasi faktor faktor yang dapat mempengaruhi kinerja sistem dan memberikan rekomendasi untuk peningkatan di masa yang akan datang.
KUALIFIKASI_Riya Widayanti.txt,"Bab ini menyajikan desain yang digunakan dalam penelitian ini. Desain penelitian adalah rencana umum bagaimana penelitian akan dilakukan untuk menjawab pertanyaan dan pernyataan dalam penelitian. Hal ini menentukan sumber dari mana data akan dikumpulkan dan bagaimana mengumpulkan dan menganalisis data ini. Selanjutnya membahas masalah etika dan beberapa kendala yang dapat ditemui peneliti. Ini menunjukkan bahwa peneliti telah memikirkan elemen-elemen desain penelitian tertentu (Saunders, Lewis, & Thornhill, 2011). Pada bab ini akan dibahas mengenai filosofi keilmuan dari data governance, konsep teknolgi BLockchain dan penerapan data governance dalam teknologi blockchain di bidang pendidikan, yang akan memberikan pandangan utama saat melakukan penelitian. Selanjutnya akan dijelaskan pendekatan yang digunakan penelitian dalam pengumpulan data, menganalisis data yang digunakan serta etika lain yang akan dipatuhi terutama terkait kerahasiaan data yang digunakan. Jadi metodologi penelitian memberikan gambaran jelas mengenai strategi penelitian, pengambilan data, pengumpulan, pengolahan dan analisis dan serta keterbatasan penelitian. 3.1 Filosofi Keilmuan
Pengkajian ilmiah (penelitian) menurut aliran positivistik banyak dianut peneliti ilmu komputer merupakan upaya sistematis, investigatif, objektif, logis, hati-hati dan terencana dengan selalu berusaha mencari kebenaran. Penelitian dengan pendekatan positivistik adalah memiliki karakteristik: analitik, nomotetik, dedikatif, laboratorik, pembuktian dengan logika , kebenaran universal, dan bersifat bebas nilainya. (Jazi Eko Istiyanto, 2009). 3.2. Skema Penelitian
Untuk menyelesaikan penelitian dirancang kerangka pikir yang menggambarkan langkah-langkah yang harus ditempuh, dapat dilihat penjelasan dan urutannya sebagai berikut:

3.2.1 Mendefinisikan Tata Kelola Data untuk Organisasi
Upaya Tata Kelola Data harus mendukung strategi dan tujuan bisnis. Strategi dan sasaran bisnis organisasi menginformasikan strategi data perusahaan dan bagaimana tata kelola data dan aktivitas manajemen data perlu dioperasionalkan dalam organisasi. Tata kelola data memungkinkan tanggung jawab bersama untuk keputusan terkait data. Kegiatan tata kelola data melintasi batasbatas organisasi dan sistem untuk mendukung tampilan data yang terintegrasi. Tata kelola data membutuhkan pemahaman yang jelas tentang apa yang diatur dan siapa yang diatur, serta siapa yang mengatur
Uraian lebih detil tentang proses transformasi warna, ruang warna yang digunakan, algoritma trasnformasinya. Setiap univeritas merupakan node, dimana masing-masing node mengajukan beberapa kesepatan yang diturunkan dalam fungsional requirement yang nantinya akan dituangkan dalam consensus yang terdalam di dalam smartcard

3.2.2 Mengidentikasi fungsional Requirement
Berdasarkan kesepakatan fungsional requirement akan diusulkan smart contract
Penilaian yang menggambarkan keadaan saat ini dari kemampuan manajemen informasi organisasi, kematangan, dan efektivitas sangat penting untuk merencanakan program unit bisnis. Karena dapat digunakan untuk mengukur efektivitas program, penilaian juga berharga dalam mengelola dan mempertahankan program unit binis. Penilaian khas meliputi:
Kematangan pengelolaan data: Memahami apa yang dilakukan organisasi dengan data; mengukur kemampuan dan kapasitas manajemen datanya saat ini. Fokusnya adalah pada kesan yang dimiliki personel bisnis tentang seberapa baik perusahaan mengelola data dan menggunakan data untuk keuntungannya, serta pada kriteria objektif, seperti penggunaan alat, tingkat pelaporan, dll. Kesiapan kolaboratif: Penilaian ini mencirikan kemampuan organisasi untuk berkolaborasi dalam pengelolaan dan penggunaan data. Karena penatalayanan menurut definisi melintasi area fungsional, itu bersifat kolaboratif. Jika sebuah organisasi tidak tahu bagaimana berkolaborasi, budaya akan menjadi hambatan bagi penatalayanan. Jangan pernah berasumsi bahwa sebuah organisasi tahu bagaimana berkolaborasi. Ketika
dilakukan bersama dengan kapasitas perubahan, penilaian ini menawarkan wawasan tentang kapasitas budaya untuk melaksanakan Ditjen. Penyelarasan bisnis: Terkadang disertakan dengan kapasitas perubahan, penilaian keselarasan bisnis memeriksa seberapa baik organisasi menyelaraskan penggunaan data dengan strategi bisnis. Seringkali mengejutkan untuk mengetahui bagaimana aktivitas terkait data ad hoc dapat terjadi. 3.2.3 Membuat kerangka lapisan data logis
Dalam tahap ini setelah setiap node mnyepakati proses bisnis yang akan dipakai bersama dalam aplikasi blockchain, menetapkan skema pada lapisan data logis. Hasilnya ada bagaimana kerangka komunikasi dijelaskan dalam gambar 3.3. Pedoman Membuat kerangka logis:
1. Mekanismen Identitas Mangement, otoritas dan Autentifikasi
Identitas Mangement, otorisasi, dan mekanisme otentikasi sangat penting dalam sistem manajemen data karena hal tersebut terkait langsung dengan keamanan dan privasi sistem. Dalam konsep desain, entitas dalam jaringan Blockchain harus diidentifikasi secara unik menggunakan kunci publik (atau hash kunci publik) dalam pasangan kunci kriptografi asimetris; proses otentikasi dan otorisasi harus diterapkan dengan memanfaatkan teknik kriptografi kunci publik (misalnya, tanda tangan digital dan enkripsi). Dalam hal izin BC, lapisan kontrol akses tambahan dikonsolidasikan dengan menggunakan Otoritas Sertifikat (CA) dan Penyedia Layanan Keanggotaan (MSP). 2. Desain Buku Besar Terdistribusi:
Konten terdistribusi buku besar mencerminkan keadaan historis dan informasi terkini yang dicatat dalam buku besar yang dikelola oleh jaringan blockchain. Platform manajemen data pribadi harus mengklarifikasi informasi apa dan model data terkait yang akan disimpan dalam buku besar. (i) Informasi yang diperlukan agar tahan terhadap kerusakan, transparan dan dapat dilacak harus dicatat dalam buku besar yang didistribusikan. Setiap kumpulan data pribadi harus ditentukan oleh data subjek dan data controller menggunakan tanda tangan digital dalam buku besar yang didistribusikan;
Kebijakan Penggunaan Data harus ditetapkan dengan jelas dan dicatat dalam buku besar yang didistribusikan;
Aktivitas data harus dicatat dalam buku besar yang didistribusikan. Log harus berisi informasi tentang 'siapa', 'mengapa', 'kapan', 'apa' dan 'bagaimana' data pribadi diproses;
Hash data pribadi dapat dicatat dalam buku besar terdistribusi untuk pemeriksaan integritas data. (ii) Desain buku besar yang didistribusikan harus memastikan:
Node yang ditunjuk dalam jaringan blockchain dapat memverifikasi apakah suatu entitas adalah data subjeck atau data controller dari kumpulan data;
Node yang ditunjuk dalam jaringan blockchain harus dapat memverifikasi apakah aktivitas entitas memenuhi kebijakan penggunaan data seperti yang dicatat dalam buku besar terdistribusi
3. Kebijakan Penggunaan Data: Kebijakan tersebut menentukan tindakan tata kelola data termasuk hak, izin, dan kondisi. Kebijakan penggunaan harus didefinisikan secara halus dan ekspresif menggunakan bahasa kebijakan seperti eXtensible Access Control Markup Language (XACML) dan Model-based Security Toolkit (SecKit) yang ditujukan untuk domain IoT. Secara alami, manajemen data pribadi berbasis blockchain mengikuti konsep desain yang diusulkan memberikan kemampuan kontrol akses yang halus karena pengguna individu dapat menyesuaikan kebijakannya sendiri pada setiap kumpulan data dengan memaksakan preferensi kontrol akses yang dicatat ke buku besar. 4. Penyimpanan Data Off-chain: Data pribadi harus disimpan off-chain untuk skalabilitas yang lebih baik dan efisiensi yang lebih tinggi. Selain itu, menyimpan data pribadi langsung ke clockchain, bahkan dalam bentuk terenkripsi, dapat menimbulkan potensi kebocoran privasi dan mengakibatkan ketidakpatuhan terhadap GDPR. Tergantung pada skenario tertentu, DBMS konvensional (misalnya, Oracle atau MongoDB), layanan penyimpanan awan (misalnya, S3, AWS atau Azure), atau sistem penyimpanan dapat digunakan untuk penyimpanan data. Hanya referensi ke data yang disimpan secara on-chain (yaitu, disimpan dalam buku besar terdistribusi). Referensi disebut penunjuk data itu bisa menjadi hash, string koneksi, jalur absolut, atau pengidentifikasi yang merujuk ke kumpulan data; tergantung pada sistem penyimpanan off-chain tertentu yang digunakan dalam platform.","Bab ini menyajikan desain yang digunakan dalam penelitian ini. Hal ini menentukan sumber dari mana data akan dikumpulkan dan bagaimana mengumpulkan dan menganalisis data ini. Pada bab ini akan dibahas mengenai filosofi keilmuan dari data governance, konsep teknolgi BLockchain dan penerapan data governance dalam teknologi blockchain di bidang pendidikan, yang akan memberikan pandangan utama saat melakukan penelitian. Skema Penelitian
Untuk menyelesaikan penelitian dirancang kerangka pikir yang menggambarkan langkah-langkah yang harus ditempuh, dapat dilihat penjelasan dan urutannya sebagai berikut:

3.2.1 Mendefinisikan Tata Kelola Data untuk Organisasi
Upaya Tata Kelola Data harus mendukung strategi dan tujuan bisnis. Strategi dan sasaran bisnis organisasi menginformasikan strategi data perusahaan dan bagaimana tata kelola data dan aktivitas manajemen data perlu dioperasionalkan dalam organisasi. Tata kelola data memungkinkan tanggung jawab bersama untuk keputusan terkait data. Kegiatan tata kelola data melintasi batasbatas organisasi dan sistem untuk mendukung tampilan data yang terintegrasi. Fokusnya adalah pada kesan yang dimiliki personel bisnis tentang seberapa baik perusahaan mengelola data dan menggunakan data untuk keuntungannya, serta pada kriteria objektif, seperti penggunaan alat, tingkat pelaporan, dll. Hasilnya ada bagaimana kerangka komunikasi dijelaskan dalam gambar 3.3. Kebijakan Penggunaan Data: Kebijakan tersebut menentukan tindakan tata kelola data termasuk hak, izin, dan kondisi. Penyimpanan Data Off-chain: Data pribadi harus disimpan off-chain untuk skalabilitas yang lebih baik dan efisiensi yang lebih tinggi. Selain itu, menyimpan data pribadi langsung ke clockchain, bahkan dalam bentuk terenkripsi, dapat menimbulkan potensi kebocoran privasi dan mengakibatkan ketidakpatuhan terhadap GDPR. Tergantung pada skenario tertentu, DBMS konvensional (misalnya, Oracle atau MongoDB), layanan penyimpanan awan (misalnya, S3, AWS atau Azure), atau sistem penyimpanan dapat digunakan untuk penyimpanan data. Hanya referensi ke data yang disimpan secara on-chain (yaitu, disimpan dalam buku besar terdistribusi). Referensi disebut penunjuk data itu bisa menjadi hash, string koneksi, jalur absolut, atau pengidentifikasi yang merujuk ke kumpulan data; tergantung pada sistem penyimpanan off-chain tertentu yang digunakan dalam platform."
Kualifikasi Witta Listiya Ningrum.txt,"Bab ini akan menjelaskan tentang metodologi penelitian yang digunakan sebagai gambaran dari langkah-langkah yang akan dilakukan untuk menyelesaikan penelitian ini. 3.1 Tahapan Penelitian
Penelitian ini melakukan pengembangan model klasifikasi toksisitas pada platform sosial media. Tahapan penelitian yang digunakan dapat dilihat pada gambar 3.1. Tahapan metode penelitian pada gambar 3.1 terdiri dari beberapa langkah, yaitu :
1. Tahap Literature Review
Pada tahap ini dimulai dengan melakukan kajian dari berbagai sumber tertulis dalam bentuk buku, artikel dan jurnal serta penelitian-peneltiian terkait guna memahami dan mengidentifikasi kesenjangan dalam topik penelitian serta menemukan kelemahan dan kelebihan dalam penelitian. Selain itu juga untuk menentukan dan membandingkan metode serta algoritma yang sudah digunakan pada penelitian sebelumnya, yang nantinya akan mengembangkan atau menciptakan suatu metode atau algoritma terbaru. 2. Tahap Pengumpulan Data
Pada tahap ini dilakukan pengumpulan data yang akan digunakan untuk melatih dan menguji model. Data ini dapat berupa konten-konten pada sosial media yang akan dikategorikan ke dalam 3 kategori toksisitas yaitu toxic, non-toxic, dan netral. Data tersebut harus mencakup berbagai jenis media, seperti teks, gambar dan video, untuk memungkinkan model mengenali toksisitas dari berbagai jenis konten yang ada pada platform sosial media. 3. Large Language Model (LLM)
Large Language Model merupakan jenis model kecerdasan buatan (Artificial Intelligence) yang dilatih untuk memahami, menghasilkan dan memproses bahasa alami (Natural Languange) dalam skala besar. Large Language Model dilatih menggunakan dataset yang besar, terdiri dari teks yang diambil dari berbagai sumber seperti artikel, buku, situs web dan lainnya. 4. Large Language Model (LLM) Multimodal
Large Language Model pada penelitian yang dilakukan untuk memproses tidak hanya dalam bentuk jenis media teks, melainkan gambar dan juga video. Large Language Model (LLM) untuk klasifikasi multimodal melibatkan teks, gambar dan juga video. Meskipun LLM berfokus pada teks, model ini dapat diadaptasi atau dikombinasikan dengan model lain yang mendukung modalitas non-teks seperti gambar dan video, melalui pendekatan yang disebut dengan model multimodal. Pada tahap ini dilakukan pre-processing dari masing-masing jenis media yang digunakan. * Untuk representasi teks menggunakan teknik-teknik pemrosesan bahasa alami seperti tokenisasi, vektorisasi data (word embedding) dan penggunaan model bahasa pre-trained seperti BERT untuk mewakili teks dalam bentuk vektor numerik yang dapat dimengerti oleh model. * Untuk representasi gambar dan video menggunakan teknik-teknik pemrosesan gambar seperti ekstraksi fitur dengan convolutional neural networks (CNN) atau menggunakan model pre-trained seperti ResNet atau VGG untuk mewakili gambar dalam bentuk vektor numerik. 5. Generate Caption
Pada tahapan ini menggunakan model LLM, seperti BLIP atau Flamingo untuk menggabungkan kemampuan visual dan bahasa dalam menghasilkan teks/captioning dari representasi gambar dan video. 6. Model Klasifikasi Toksisitas
Pada tahapan ini dilakukan pengembangan model dari hasil penggabungan ketiga representasi tersebut, dengan menggunakan teknik fusion, seperti concatenation atau attention mechanism untuk menghasilkan hasil klasifikasi akhir. Model klasifikasi yang digunakan adalah Convolutional Neural Network (CNN). 7. Evaluasi Model
Pada tahapan ini dilakukan evaluasi untuk mengetahui kinerja terhadap model yang dikembangkan dengan menggunakan pengukuran akurasi, seperti precision, recall dan juga F1-score untuk klasifikasi teks, dan mengukur akurasi dengan confusion matrix untuk gambar dan video. 8. Hasil
Tahapan ini menghasilkan klasifikasi sesuai dengan label yang sudah dikategorikan ke dalam 3 kategori toksisitas yaitu toxic, non-toxic, dan netral.","3.1 Tahapan Penelitian
Penelitian ini melakukan pengembangan model klasifikasi toksisitas pada platform sosial media. Selain itu juga untuk menentukan dan membandingkan metode serta algoritma yang sudah digunakan pada penelitian sebelumnya, yang nantinya akan mengembangkan atau menciptakan suatu metode atau algoritma terbaru. Data tersebut harus mencakup berbagai jenis media, seperti teks, gambar dan video, untuk memungkinkan model mengenali toksisitas dari berbagai jenis konten yang ada pada platform sosial media. Large Language Model (LLM)
Large Language Model merupakan jenis model kecerdasan buatan (Artificial Intelligence) yang dilatih untuk memahami, menghasilkan dan memproses bahasa alami (Natural Languange) dalam skala besar. Generate Caption
Pada tahapan ini menggunakan model LLM, seperti BLIP atau Flamingo untuk menggabungkan kemampuan visual dan bahasa dalam menghasilkan teks/captioning dari representasi gambar dan video. Model Klasifikasi Toksisitas
Pada tahapan ini dilakukan pengembangan model dari hasil penggabungan ketiga representasi tersebut, dengan menggunakan teknik fusion, seperti concatenation atau attention mechanism untuk menghasilkan hasil klasifikasi akhir. Evaluasi Model
Pada tahapan ini dilakukan evaluasi untuk mengetahui kinerja terhadap model yang dikembangkan dengan menggunakan pengukuran akurasi, seperti precision, recall dan juga F1-score untuk klasifikasi teks, dan mengukur akurasi dengan confusion matrix untuk gambar dan video. Hasil
Tahapan ini menghasilkan klasifikasi sesuai dengan label yang sudah dikategorikan ke dalam 3 kategori toksisitas yaitu toxic, non-toxic, dan netral."
Kualifikasi_Aris Gunaryati.txt,"3.1 Gambaran Umum Penelitian
     Motivasi dari Metodologi yang diusulkan adalah membuat suatu metode peramalan yang sesuai dengan data runtun waktu yang ada serta meningkatkan akurasinya dengan tetap memperhatikan efisiensi waktu komputasinya. Langkah-langkah yang dilakukan dalam penelitian ini adalah menganalisis data jumlah kasus harian Covid 19 di Jakarta berdasarkan dataset dari situs https://corona.jakarta.go.id tanggal 6 Maret 2020 sampai 30 Juni 2021 sebagai data training dan nanti akan diprediksi untuk tanggal 1 Juli 2021 sampai dengan 31 Juli 2021 sebagai data uji dengan tahapan sebagai berikut :
1. Mempersiapkan data runtun waktu yang akan dianalisis
2. Menganalisis data runtun waktu yang ada menggunakan metode statistika ARIMA
3. Menganalisis data runtun waktu yang ada menggunakan metode Quantum Neural Network
4. Mengembangkan model Hybrid ARIMA-Quantum Neural Network
5. Menentukan model yang cocok untuk setiap variabel
6. Menguji kecocokan masing-masing model
7. Melakukan peramalan dengan menggunakan model yang cocok
8. Melakukan perbandingan tingkat akurasi hasil peramalan dengan tiap model

Untuk mendapatkan model peramalan yang diharapkan sesuai dengan data runtun waktu yang ada, maka perlu dilakukan pendekatan ilmiah yaitu dengan melihat pola data runtun waktu yang ada terlebih dahulu. Dengan melihat pola data awal yang dimiliki maka akan memudahkan dalam memilih model yang sesuai untuk data tersebut. Pendekatan lainnya adalah menggunakan tools untuk menentukan secara otomatis Bentuk model statistik ARIMA yang sesuai dengan runtun waktu yang ada, lalu model tersebut dilatih menggunakan quantum neural network agar diketahui pola-pola data yang	sudah	ada	dan	dapat	diuji	akurasinya. 3.2 Model ARIMA
2. Tahapan Analisis Time Series (ARIMA)
a. Membuat Plot Time Series
Identifikasi asumsi stasioneritas data runtun waktu. Suatu deret pengamatan dikatakan stasioner apabila proses tidak berubah seiring dengan perubahan waktu
Tidak stasioner dalam mean : jika trend tidak datar (tidak sejajar smbu waktu)
Tidak stasioner dalam varian : jika trend datar atau hampir datar, tetapi data tersebar membangun pola melebar atau menyempit (pola terompet)
Tidak stasioner dalam mean & varians : jika trend tidak datar dan data membentuk pola terompet. Augmented Dickey- Fuller (Uji Formal untuk Stasioneritas) Hipotesis :
H0 : Terdapat akar unit dan data tidak stasioner (=0)
H1 : Tidak terdapat akar unit dan data stasioner (<0 span="""">) Taraf Signifikansi : a = ... %
3.3 Model Neural Network
Dalam buku Jaringan Syaraf Tiruan dan Pemrogramannya Menggunakan MATLAB, Drs. Jong Jek Siang, M.Sc menyebutkan bahwa Jaringan Syaraf Tiruan adalah system pemroses informasi yang memiliki karakteristik mirip dengan jaringan syaraf biologi. Jaringan Syaraf Tiruan dibentuk sebagai generalisasi model matematika dari jaringan syaraf biologi, dengan asumsi bahwa Pemrosesan informasi terjadi pada banyak elemen sederhana (neuron)
a. Sinyal dikirimkan di antara neuron-neuron melalui penghubung-penghubung
b. Penghubung antar neuron memiliki bobot yang akan memperkuat atau memperlemah sinyal
c. Untuk menentukan output, setiap neuron menggunakan fungsi aktivasi (biasanya bukan fungsi linier) yang dikenakan pada jumlahan input yang diterima. Besarnya output ini selanjutnya dibandingkan dengan suatu batas ambang (treshhold)

Jaringan Syaraf Tiruan ditentukan oleh tiga hal :
a. Pola hubungan antar neuron (disebut arsitektur jaringan)
b. Metode untuk menentukan bobot penghubung (disebut metode training/learning algoritma)
c. Fungsi Aktivasi
Pemrosesan informasi dalam Jaringan Syaraf Tiruan dapat disingkat sebagai berikut : Sinyal (baik berupa aksi ataupun potensial) muncul sebagai masukan unit (sinapsis); efek dari tiap sinyal ini dinyatakan sebagai bentuk perkalian dengan sebuah nilai bobot untuk mengindikasikan kekuatan dari sinapsis. Semua sinyal yang diberi pengali bobot ini kemudian dijumlahkan satu sama lain untuk menghasilkan unit aktivasi. Jika aktivasi ini melampaui sebuah batas ambang tertentu maka unit tersebut akan memberikan keluaran dalam bentuk respon terhadap masukan. Unit aktivasi ini kemudian dibandingkan dengan sebuah nilai ambang, dan hasilnya dimasukkan kedalam fungsi transfer (fungsi non-linier) yang akan menghasilkan sebuah keluaran. Secara ringkas proses tersebut dapat digambarkan dalam gambar 3
Aktivasi dari unit masukan diatur dan diteruskan melalui jaring hingga nilai dari keluaran dapat ditentukan. Jaring berperan sebagai fungsi vektor yang mengambil satu vektor pada masukan dan mengeluarkan satu vektor lain pada keluaran. Model Jaringan Syaraf Tiruan dapat memiliki sebuah lapisan bobot, dimana masukan dihubungkan langsung dengan keluaran, atau beberapa lapisan yang didalamnya terdapat beberapa lapisan tersembunyi, karena berada tersembunyi diantara neuron masukan dan keluaran. Jaring syaraf menggunakan unit tersembunyi untuk menghasilkan representasi pola masukan secara internal didalam jaring syaraf. Fungsi transfer (non-linier) yang digunakan dalam tiap neuron (baik dilapisan masukan, keluaran, atau lapisan tersembunyi) dapat berupa fungsi nilai ambang, fungsi linier, fungsi sigmoid, ataupun fungsi gaussian, tergantung dari karakter neuron sesuai keinginan kita. Hal ini dapat dilihat pada gambar 4


3.3.1 Komponen Jaringan Syaraf
      Terdapat beberapa tipe jaringan syaraf, hampir semuanya memiliki komponen-komponen yang sama. Seperti halnya otak manusia, jaringan syaraf juga terdiri atas beberapa neuron dan ada hubungan antar neuron tersebut. Neuron-neuron tersebut akan mentransformasikan informasi yang diterima melalui sambungan keluarnya menuju ke neuron-neuron yang lain. Pada jaringan syaraf, hubungan ini dikenal dengan nama bobot. Informasi tersebut disimpan pada suatu nilai tertentu pada bobot tersebut. Neuron ini sebenarnya mirip dengan sel neuron biologis. Neuron-neuron buatan tersebut bekerja dengan cara yang sama pula dengan neuron biologis. Informasi (disebut dengan: input) akan dikirim ke neuron dengan bobot kedatangan tertentu. Input ini akan diproses oleh suatu fungsi perambatan yang akan menjumlahkan nilai-nilai semua bobot yang datang. Hasil penjumlahan ini kemudian akan dibandingkan dengan suatu nilai ambang (threshold) tertentu melalui fungsi aktivasi setiap neuron. Apabila input tersebut melewati suatu nilai ambang tertentu, maka neuron tersebut akan diaktifkan, tapi kalau tidak, maka neuron tersebut tidak akan diaktifkan. Apabila neuron tersebut diaktifkan, maka neuron tersebut akan mengirimkan output melalui bobot-bobot outputnya kesemua neuron yang berhubungan dengannnya. Pada Jaringan syaraf, neuron-neuron akan dikumpulkan dalam lapisan (layer) yang disebut dengan lapisan neuron (neuron layer). Neuron-neuron pada satu lapisan akan dihubungkan dengan lapisan-lapisan sebelum dan sesudahnya (kecuali lapisan input dan lapisan output). Informasi yang diberikan pada jaringan syaraf akan dirambatkan lapisan ke lapisan. Mulai dari lapisan input sampai ke lapisan output melalui lapisan lainnya, yang sering disebut sebagai lapisan tersembunyi (hidden layer). 3.3.2 Arsitektur Jaringan Syaraf
3.3.2.1 Jaringan dengan lapisan tunggal (single layer net)
      Jaringan dengan lapisan tunggal hanya memiliki satu lapisan dengan bobot- bobot terhubung. Jaringan ini hanya menerima input kemudian secara langsung akan mengolahnya menjadi output tanpa harus melalui lapisan tersembunyi. 3.3.3.3  Jaringan dengan banyak lapisan (multilayer net)
      Jaringan dengan banyak lapisan memiliki 1 atau lebih lapisan yang terletak diantara lapisan input dan lapisan output (memiliki 1 atau lebih lapisan tersembunyi). Umumnya, ada lapisan bobot-bobot yang terletak antara 2 lapisan yang bersebelahan. Jaringan dengan banyak lapisan ini dapat menyelesaikan permasalahan yang lebih sulit daripada jaringan dengan lapisan tunggal, tentu saja dengan pembelajaran yang lebih rumit. Namun demikian, pada banyak kasus, pembelajaran pada jaringan dengan banyak lapisan ini lebih sukses dalam menyelesaikan masalah. 3.3.3. Fungsi Aktivasi
      Ada beberapa fungsi aktivasi yang sering digunakan dalam jaringan syaraf tiruan, antara lain :
a. Fungsi Undak Biner (Hard Limit)
      Jaringan dengan lapisan tunggal sering menggunakan fungsi undak (step function) untuk mengkonversikan input dari suatu variabel yang bernilai kontinu ke suatu output biner (0 atau 1)
b. Fungsi undak biner (Threshold)
      Fungsi undak biner dengan menggunakan nilai ambang sering juga disebut dengan fungsi nilai ambang (Threshold) atau fungsi Heaviside. c. Fungsi Bipolar (Symetric Hard Limit)
      Fungsi bipolar sebenarnya hampir sama dengan fungsi undak biner, hanya saja output yang dihasilkan berupa 1, 0 atau -1
d. Fungsi Bipolar (dengan threshold)
      Fungsi bipolar sebenarnya hampir sama dengan fungsi undak biner dengan threshold. Hanya saja keluaran yang dihaslkan berupa 1, 0, atau -1
e. Fungsi Linear (Identitas)
Fungsi linear memiliki nilai output yang sama dengan nilai inputnya. f. Fungsi Saturating Linear
      Fungsi ini akan bernilai 0 jika inputnya kurang dari - 1/2, dan akan bernilai 1 jika inputnya lebih dari 1/2. Sedangkan jika nilai input terletak antara -1/2 dan 1/2, maka outputnya akan bernilai sama dengan nilai input ditambah 1/2
g. Fungsi Symetric Saturating Linear
      Fungsi ini akan bernilai -1 jika inputnya kurang dari -1, dan akan bernilai 1 jika inputnya lebih dari 1. Sedangkan jika nilai input terletak antara -1 dan 1, maka outputnya akan bernilai sama dengan nilai inputnya. h. Fungsi Sigmoid Biner
      Fungsi ini digunakan untuk jaringan syaraf yang dilatih dengan menggunakan metode backpropagation. Fungsi sigmoid biner memiliki nilai pada range 0 sampai 1. Oleh karena itu, fungsi ini sering digunakan untuk jaringan syaraf yang membutuhkan nilai output yang terletak pada interval 0 sampai 1. Namun, fungsi ini bisa juga digunakan oleh jaringan syaraf yang nilai outputnya 0 atau 1.
i. Fungsi Sigmoid Bipolar
      Fungsi sigmoid bipolar hampir sama dengan fungsi sigmoid biner, hanya saja output dari fungsi ini memiliki range antara 1 sampai -1
      Fungsi ni sangat dekat dengan fungsi hyperbolic tangent. Keduanya memiliki range antara -1 sampai 1. Untuk fungsi hyperbolic tangent,

3.4 MODEL HYBRID ARIMA NEURAL NETWORK
      Berdasarkan hasil peramalan model ARIMA, akan dilakukan proses analisis runtun waktu menggunakan metode jaringan syaraf tiruan. Dengan kata lain, output dari peramalan model ARIMA akan menjadi input pada proses pengolahan data menggunakan metode jaringan syaraf tiruan. Kemudian akan ditentukan model jaringan syaraf tiruan yang sesuai dan cocok untuk data runtun waktu tersebut. Secara matematis, hasil ramalan secara keseluruhan yang diperoleh adalah sebagai berikut :
Zt merupakan hasil peramalan yang merupakan gabungan nilai ramalan dari model ARIMA atau Exponential Smoothing dan nilai ramalan dari model JST. 3.5 MODEL QUANTUM HYBRID ARIMA NEURAL NETWORK
      Ada banyak pendekatan untuk pengembangan model Quantum Arima NN. Model-model ini fokus pada yang berbeda aspek komputasi kuantum dan pemrosesan saraf. Dalam komputasi kuantum, Sebagai unit informasi terkecil, bit kuantum atau qubit adalah sistem kuantum yang menyatakan terletak di ruang Hilbert dua dimensi. Seperti bit dalam klasik komputer, qubit berlabel dan mengekspresikan satu bit informasi: sesuai dengan bit 0 komputer klasik, dan bit 1. Keadaan qubit menyatakan superposisi keadaan yang kohere :
      Di mana dan menentukan probabilitas yang sesuai. Gerbang kuantum yang mencakup karakteristik komputasi kuantum merupakan dasar untuk implementasi fisik dari komputasi kuantum. Himpunan logika universal termasuk dalam logika kuantum. Mirip dengan bit klasik, gerbang dasar dapat membentuk gerbang kuantum bemacam-macam dan menyelesaikan keadaan kuantum dari beberapa logika transformasi. berbasis elemen pada gerbang pergeseran fasa 1 bit dan gerbang kontrol-Tidak 2 bit dalam dinamika kuantum diambil sebagai fungsi aktivasi dalam Jaringan saraf. Untuk memudahkan aplikasi, formulir berikut:
Fungsi kompleks diberikan untuk menyatakan keadaan kuantum:
adalah bilangan imaginer adalah kuantum fase

3.6 Pengukuran Kinerja
3.6.1 Mean Squared Error
      Dalam statistik, Mean Squared Error (MSE) sebuah estimator adalah nilai yang diharapkan dari kuadrat error. Error yang ada menunjukkan seberapa besar perbedaan hasil estimasi dengan nilai yang akan diestimasi. Perbedaan itu terjadi karena adanya keacakan pada data atau karena estimator tidak mengandung informasi yang dapat menghasilkan estimasi yang lebih akurat
3.6.2 Komparasi Hasil Peramalan
      Setelah nilai Mean Squared Error dari kedua metode didapatkan, maka akan dilakukan komparasi terhadap nilai MSE yang didapatkan pada periode testing (out- sample)
Jika nilai MSESTATISTIKA < MSEANN maka metode Statistika memiliki performa lebih baik dibandingkan metode ANN karena memiliki tingkat kesalahan relatif lebih kecil. Sebaliknya, jika MSESTATISTIKA > MSEANN maka metode Statistika memilki performa lebih buruk dibandingkan metode ANN karena tingkat kesalahan yang dihasilkan relatif lebih besar.","3.1 Gambaran Umum Penelitian
     Motivasi dari Metodologi yang diusulkan adalah membuat suatu metode peramalan yang sesuai dengan data runtun waktu yang ada serta meningkatkan akurasinya dengan tetap memperhatikan efisiensi waktu komputasinya. Langkah-langkah yang dilakukan dalam penelitian ini adalah menganalisis data jumlah kasus harian Covid 19 di Jakarta berdasarkan dataset dari situs https://corona.jakarta.go.id tanggal 6 Maret 2020 sampai 30 Juni 2021 sebagai data training dan nanti akan diprediksi untuk tanggal 1 Juli 2021 sampai dengan 31 Juli 2021 sebagai data uji dengan tahapan sebagai berikut :
1. Mempersiapkan data runtun waktu yang akan dianalisis
2. Menganalisis data runtun waktu yang ada menggunakan metode statistika ARIMA
3. Menganalisis data runtun waktu yang ada menggunakan metode Quantum Neural Network
4. Mengembangkan model Hybrid ARIMA-Quantum Neural Network
5. Melakukan perbandingan tingkat akurasi hasil peramalan dengan tiap model

Untuk mendapatkan model peramalan yang diharapkan sesuai dengan data runtun waktu yang ada, maka perlu dilakukan pendekatan ilmiah yaitu dengan melihat pola data runtun waktu yang ada terlebih dahulu. Pendekatan lainnya adalah menggunakan tools untuk menentukan secara otomatis Bentuk model statistik ARIMA yang sesuai dengan runtun waktu yang ada, lalu model tersebut dilatih menggunakan quantum neural network agar diketahui pola-pola data yang	sudah	ada	dan	dapat	diuji	akurasinya. Tahapan Analisis Time Series (ARIMA)
a. Membuat Plot Time Series
Identifikasi asumsi stasioneritas data runtun waktu. Suatu deret pengamatan dikatakan stasioner apabila proses tidak berubah seiring dengan perubahan waktu
Tidak stasioner dalam mean : jika trend tidak datar (tidak sejajar smbu waktu)
Tidak stasioner dalam varian : jika trend datar atau hampir datar, tetapi data tersebar membangun pola melebar atau menyempit (pola terompet)
Tidak stasioner dalam mean & varians : jika trend tidak datar dan data membentuk pola terompet. Semua sinyal yang diberi pengali bobot ini kemudian dijumlahkan satu sama lain untuk menghasilkan unit aktivasi. Unit aktivasi ini kemudian dibandingkan dengan sebuah nilai ambang, dan hasilnya dimasukkan kedalam fungsi transfer (fungsi non-linier) yang akan menghasilkan sebuah keluaran. Untuk fungsi hyperbolic tangent,

3.4 MODEL HYBRID ARIMA NEURAL NETWORK
      Berdasarkan hasil peramalan model ARIMA, akan dilakukan proses analisis runtun waktu menggunakan metode jaringan syaraf tiruan. Dengan kata lain, output dari peramalan model ARIMA akan menjadi input pada proses pengolahan data menggunakan metode jaringan syaraf tiruan. Kemudian akan ditentukan model jaringan syaraf tiruan yang sesuai dan cocok untuk data runtun waktu tersebut. Secara matematis, hasil ramalan secara keseluruhan yang diperoleh adalah sebagai berikut :
Zt merupakan hasil peramalan yang merupakan gabungan nilai ramalan dari model ARIMA atau Exponential Smoothing dan nilai ramalan dari model JST. 3.5 MODEL QUANTUM HYBRID ARIMA NEURAL NETWORK
      Ada banyak pendekatan untuk pengembangan model Quantum Arima NN. Mirip dengan bit klasik, gerbang dasar dapat membentuk gerbang kuantum bemacam-macam dan menyelesaikan keadaan kuantum dari beberapa logika transformasi. berbasis elemen pada gerbang pergeseran fasa 1 bit dan gerbang kontrol-Tidak 2 bit dalam dinamika kuantum diambil sebagai fungsi aktivasi dalam Jaringan saraf. Untuk memudahkan aplikasi, formulir berikut:
Fungsi kompleks diberikan untuk menyatakan keadaan kuantum:
adalah bilangan imaginer adalah kuantum fase

3.6 Pengukuran Kinerja
3.6.1 Mean Squared Error
      Dalam statistik, Mean Squared Error (MSE) sebuah estimator adalah nilai yang diharapkan dari kuadrat error. Error yang ada menunjukkan seberapa besar perbedaan hasil estimasi dengan nilai yang akan diestimasi. Perbedaan itu terjadi karena adanya keacakan pada data atau karena estimator tidak mengandung informasi yang dapat menghasilkan estimasi yang lebih akurat
3.6.2 Komparasi Hasil Peramalan
      Setelah nilai Mean Squared Error dari kedua metode didapatkan, maka akan dilakukan komparasi terhadap nilai MSE yang didapatkan pada periode testing (out- sample)
Jika nilai MSESTATISTIKA < MSEANN maka metode Statistika memiliki performa lebih baik dibandingkan metode ANN karena memiliki tingkat kesalahan relatif lebih kecil. Sebaliknya, jika MSESTATISTIKA > MSEANN maka metode Statistika memilki performa lebih buruk dibandingkan metode ANN karena tingkat kesalahan yang dihasilkan relatif lebih besar."
Kualifikasi_Rama Dian Syah.txt,"3.1   Tahapan Penelitian
      Tahapan penelitian dibagi atas beberapa tahapan yang dilakukan dari awal sampai akhir. Tahapan dimulai dari studi literatur sampai analisis yang membentuk alur secara sistematis. Tahapan penelitian ini terpada pada Gambar 3.1


3.2   Desain Algoritma
      Penelitian yang terdahulu menggunakan metode yang memiliki keamanan tinggi yang dibuktikan dengan beberapa parameter pengujian. Pada penelitian ini mengajukan pengembangan algoritma kriptografi citra digital dengan mengkombinasi teknik konfusi dengan algoritma Cat Map dan Henon Map serta teknik difusi dengan algoritma Logistic Map. Pengembangan pada algoritma ini diharapkan dapat memiliki keamanan yang lebih tinggi dengan melalui beberapa parameter pengujian. Diagram alur proses enkripsi dapat dilihat pada 3.2. 3.3   Pengujian
      Tahapan pengujian dilakukan untuk mengetahui hasil pada proses enkripsi dan dekripsi beberapa pengujian yang dilakukan yaitu:
1. Histogram
Histogram merupakan analisis statistik yang menunjukkan penyebaran atau distribusi piksel pada citra. Histogram sering digunakan untuk pada pengolahan citra untuk melihat kualitas citra. Kriptografi pada citra digital yang ideal memiliki distribusi nilai piksel yang beragam (Benlashram et al., 2020). 2. PSNR (Peak Signal Noise to Ratio)
PSNR digunakan untuk pengukuran kualitas citra antara citra asli dan noise yang terjadi pada citra terenkripsi. Nilai PSNR = 30 dB membuktikan kualitas yang baik pada citra asli atau citra terdekripsi (Lone et al., 2021). Berikut persamaan PSNR terdapat pada persamaan 3.1.","3.1   Tahapan Penelitian
      Tahapan penelitian dibagi atas beberapa tahapan yang dilakukan dari awal sampai akhir. Pada penelitian ini mengajukan pengembangan algoritma kriptografi citra digital dengan mengkombinasi teknik konfusi dengan algoritma Cat Map dan Henon Map serta teknik difusi dengan algoritma Logistic Map. Pengembangan pada algoritma ini diharapkan dapat memiliki keamanan yang lebih tinggi dengan melalui beberapa parameter pengujian. Nilai PSNR = 30 dB membuktikan kualitas yang baik pada citra asli atau citra terdekripsi (Lone et al., 2021)."
Kualifikasi_Remigius.txt,"3.1 Motivasi Penelitian
     Penelitian ini dilakukan dengan motivasi mengenalkan pentingnya sistem pembelajaran bidang arsitektur menggunakan teknologi metaverse. Dalam proses pengembangan sistem pembelajaran arsitektur berbasis metaverse ini, peneliti juga ingin menunjukkan perlunya keterlibatan komunitas dan persepsi pengguna bidang arsitektur agar sistem pembelajaran yang dihasilkan sesuai dengan kebutuhan dan harapan mereka dalam meningkatkan efektivitas pembelajaran arsitektur itu sendiri. Diharapkan, sistem pembelajaran arsitektur berbasis metaverse ini dapat memberi kemudahan kepada komunitas dosen dan mahasiswa dalam mempelajari berbagai sisi arsitektur dengan memasuki dunia virtual dan mereka dapat memahami materi yang diajarkan serta memecahkan permasalahan arsitektur yang dihadapi secara interaktif, kolaboratif, dan imersif dengan solusi tepat tanpa harus mencari berbagai referensi wujud nyata arsitektur di dunia fisik atau dunia nyata. Studi literatur Yose Indarta, Ambiyar, Agariadne Dwinggo Samala, Ronal Watrianthos (2022) berjudul ""Metaverse: Tantangan dan Peluang dalam Pendidikan"" menyimpulkan bahwa implementasi Metaverse di dunia pendidikan memiliki peluang besar dalam menunjang proses pelaksanaan pendidikan menjadi lebih baik. Pendidikan berbasis audiovisual merupakan aplikasi Metaverse paling popular dan banyak digunakan dalam pembelajaran. Berdasar penelitian, pendidikan berbasis pengalaman menjadi lebih baik, apakah melalui belajar secara langsung maupun simulasi didukung teknologi. Dengan konsep pembelajaran matakuliah Perkembangan Arsitektur 1 yang dilakukan dengan metode metaverse, pembelajaran secara online ini dapat dilakukan dengan lebih interaktif. Dalam proses metaverse menyediakan banyak dukungan-dukungan pada proses pembelajaran online dengan tidak menghilangkan pengalaman belajar di kampus. Metode belajar di mana saja dan kapan saja menjadi konsep menarik yang disenangi banyak pihak. Seharusnya
     waktu, ruang dan biaya dan lainnya dapat dipangkas dengan kehadiran teknologi metaverse. 3.2 Kerangka Penelitian
      Penelitian ini dilakukan dalam mencapai tujuan utama, yaitu pengembangan sistem pembelajaran arsitektur berbasis metaverse, terutama terkait perkembangan arsitektur. Penelitian ini dilakukan melalui beberapa tahap, antara lain:
      Tahapan Pengembangan Sistem Pembelajaran Perkembangan Arsitektur 1 Berbasis Metaverse

4. Efektivitas Pembelajaran Kolaboratif
- Presensi
- Imersi
- Kehadiran dalam realitas yang disimulasi
- Kapabilitas metaverse dalam membentuk lingkungan pengguna untuk memahami
realitas


- Pemahaman materi pembelajaran
- Kemampuan memahami materi pembelajaran Perkembangan Arsitektur berbasis
metaverse
Penelitian mengenai pengembangan sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse ini dilakukan dengan melibatkan komunitas, yang terdiri dari dosen dan mahasiswa, di Program Studi S1 Arsitektur, Jurusan Teknik Arsitektur. Peneliti melibatkan dosen matakuliah Perkembangan Arsitektur 1 dan 200 mahasiswa semester 3 yang mengambil matakuliah Perkembangan Arsitektur, yaitu sekitar 200 mahasiswa, yang terdiri dari 40 mahasiswa per kelas dari empat kelas. 1. Identifikasi Topik Pembelajaran
      Peneliti melakukan identifikasi topik pembelajaran tentang arsitektur dalam matakuliah Perkembangan Arsitektur. Pembelajaran ini dilakukan pada mahasiswa dari semester III dalam Program Studi S1 Arsitektur, Jurusan Teknik Arsitektur, Fakultas Teknik Sipil dan Perencanaan, Universitas Gunadarma. Pada tahap ini, peneliti menghimpun materi pembelajaran Perkembangan Arsitektur 1 selama satu semester, sehingga terbentuk suatu himpunan materi pembelajaran Perkembangan Arsitektur selama satu semester yang siap dimasukkan dalam
sistem pembelajaran Perkembangan Arsitektur berbasis metaverse. Dalam penelitian ini, beberapa topik pembelajaran ditemukan dalam matakuliah Perkembangan Arsitektur 1.
a. Perkembangan arsitektur di Indonesia dan latar belakang pembentukannya. b. Pengaruh kebudayaan Hindu, Budha, Islam, serta kolonial belanda dan arsitek-arsitek yang terlibat aktif di dalamnya. c. Arsitektur tradisional di Daerah Sumatera yang diwakili rumah tradisional di Aceh, Riau, Minangkabau, Batak. d. Arsitektur tradisional di Kalimantan yang diwakili rumah Panjang/ Lamin. e. Arsitektur tradisional di Daerah Sulawesi yang diwakili rumah Makasar/ Minahasa/ Toraja. f. Arsitektur tradisional di Daerah Jawa dan Bali yang diwakili rumah Jawa/ Sunda dan Bali. g. Arsitektur tradisional di Daerah Bali yang diwakili rumah tradisional Bali. h. Perkembangan arsitektur di Indonesia setelah masa kemerdekaan sampai sekarang termasuk gejala-gejala yang mendasarinya. 3. Arsitektur tradisional di Daerah Sumatera yang diwakili rumah tradisional di Aceh, Riau, Minangkabau, Batak
- Arsitektur tradisional Aceh, Riau, Minangkabau, Batak
Mahasiswa dapat menguraikan secara umum
- Pengaruh sistem kekerabatan dan kepercayaan masyarakat pada arsitektur tradisional
- Pola-pola kampung tradisional secara konseptual
beserta konsep ruang dalam arsitektur tradisional
4. Arsitektur tradisional di Daerah Kalimantan yang diwakili rumah Panjang/ Lamin. - Arsitektur tradisional dayak
- Pengenalan sistem kekerabatan dan kepercayaan masyarakat tradisional daerah setempat
- Pengenalan pola kampung tradisional, letak dan orientasinya
- Pengenalan rumah tradisional Rumah
Panjang/Lamin sebagai rumah tradisional dayak
Mahasiswa dapat menguraikan secara umum
- Pengaruh sistem kekerabatan dan kepercayaan masyarakat pada arsitektur tradisional
- Pola-pola kampung tradisional secara konseptual beserta konsep ruang dalam arsitektur tradisional
5. Arsitektur tradisional di Daerah Sulawesi yang diwakili rumah Makasar/ Minahasa/ Toraja
- Arsitektur tradisional Makasar, Minahasa, Toraja
- Pengenalan sistem kekerabatan dan kepercayaan masyarakat tradisional daerah setempat
- Pengenalan pola kampung tradisional, letak dan orientasinya
- Pengenalan rumah tradisional
Mahasiswa dapat menguraikan secara umum
- Pengaruh sistem kekerabatan dan kepercayaan masyarakat pada arsitektur tradisional


6. Arsitektur tradisional di Daerah Jawa dan Bali yang diwakili rumah Jawa/Sunda dan Bali
- Arsitektur tradisional Jawa, Sunda. - Pola orientasi alam pada arsitektur tradisional
- Fungsi ruang dalam rumah tradisional Jawa, Sunda
- Keterkaitan antara sistem kekerabatan dan sistem religi setempat
Mahasiswa dapat menguraikan
- Pola desa dan bentuk arsitektur yang lahir (umum) Mahasiswa dapat menerapkan
- Konsep dan pola orientasi arsitektur tradisional terhadap fungsi-fungsi baru yang ada sekarang
7. Arsitektur tradisional di Daerah Bali yang diwakili rumah tradisonal Bali
- Arsitektur tradisional Bali
- Fungsi Undagi dalam arsitektur tradisional Bali
- Konsep Nawa Sangah
- Pola orientasi alam pada arsitektur tradisional
- Fungsi ruang dalam rumah tradisional Bali
- Keterkaitan antara sistem kekerabatan dan sistem religi setempat
Mahasiswa dapat menguraikan
- Pola desa dan bentuk arsitektur yang lahir (umum) Mahasiswa dapat menerapkan
- Konsep dan pola orientasi arsitektur tradisional terhadap fungsi-fungsi baru yang ada sekarang
8. Perkembangan arsitektur di Indonesia setelah masa kemerdekaan sampai sekarang termasuk gejala-gejala yang
mendasarinya
- Arsitektur di Indonesia setelah masa kemerdekaan sampai sekarang
Mahasiswa dapat menjelaskan
- Perkembangan arsitektur di Indonesia setelah masa kemerdekaan sampai sekarang termasuk gejala- gejala yang mendasarinya

2. Konstruksi Dunia Visual
      Pada tahap ini, peneliti melakukan konstruksi dunia visual atas dasar materi pembelajaran Perkembangan Arsitektur, baik konstruksi fisik maupun konstruksi desain. Tujuannya adalah membentuk dunia fisik dan desain visual sebagai dasar terbentuknya dunia digital menuju dunia virtual terkait Perkembangan Arsitektur. Setelah itu, peneliti menyiapkan latar arsitektur dan tata-letak sesuai dengan materi pembelajaran yang sudah dipersiapkan terkait Perkembangan Arsitektur. Dalam mewujudkan sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse ini, peneliti juga melakukan konstruksi avatar dan konten pembelajaran Perkembangan Arsitektur 1 di dalam dunia digital, sehingga terbentuk dunia virtual berbasis konten pembelajaran Perkembangan Arsitektur. 3. Penggunaan Dunia Nyata
      Pada tahap ini, peneliti menerapkan sistem pembelajaran Perkembangan Arsitektur berbasis metaverse yang sudah dikembangkan untuk digunakan dalam dunia nyata. Penerapan sistem pembelajaran berbasis metaverse ini dilakukan pada komunitas dosen dan mahasiswa semester 3 Jurusan Teknik Arsitektur, Program Studi S1 Fakultas Teknik Sipil dan Perencanaan Universitas Gunadarma. Dalam hal ini, komunitas dosen dan mahasiswa ini melakukan koneksi ke dunia virtual berupa sistem pembelajaran Perkembangan Arsitektur berbasis metaverse dan semua jenis kegiatan yang dilakukan dalam menyelesaikan masalah dan menyediakan solusi yang diperlukan dapat tersimpan dalam basis data server, sehingga dapat diambil kembali setiap kali mereka masuk dan terlibat kembali dalam sistem pembelajaran virtual kolaboratif ini. Dalam sistem pembelajaran virtual kolaboratif ini, komunitas dosen dan mahasiswa dapat berinteraksi satu sama lain dalam penyelesaian masalah yang ada dan mencari solusi yang diperlukan, sehingga mereka benar-benar dapat hadir dan terlibat di dalam sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse secara intensif, interaktif, dan imersif. 4. Efektivitas Pembelajaran Kolaboratif
      Pada tahap ini, peneliti menguji efektivitas pembelajaran kolaboratif yang didasarkan pada persepsi pengguna mengenai sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse. Persepsi pengguna ini berkaitan dengan aspek input, proses, output, dan outcome dari sistem pembelajaran berbasis metaverse tersebut. Pertama, pada aspek input, persepsi pengguna dieksplorasi dan dievaluasi tentang kelengkapan dan kememadaian dari materi pembelajaran Perkembangan Arsitektur selama satu semester, baik narasi, grafik, interaksi maupun teknik visualisasinya. Kedua, pada aspek proses, persepsi pengguna dieksplorasi dan dievaluasi tentang ketepatan konstruksi fisik dan konstruksi desain, ketepatan latar arsitektur dan tata letak dunia digital, serta ketepatan karakter avatar yang merepresentasi pengguna dalam dunia virtual beserta konten pembelajaran yang dimasukkan ke dalam sistem pembelajaran Perkembangan Arsitektur berbasis metaverse. Ketiga, pada aspek output, persepsi pengguna dieksplorasi dan dievaluasi mengenai konektivitas dan persistensi pengguna ke dunia virtual, interaktivitas dan keterlibatan pengguna di dunia virtual, dan presensi serta imersi pengguna dalam komunitas dunia virtual. Keempat, pada aspek outcome, persepsi pengguna juga dieksplorasi dan dievaluasi tentang ketercapaian tujuan dari pembelajaran Perkembangan Arsitektur 1 berbasis metaverse sesuai dengan kriteria dan indikator yang ditetapkan dosen pengampu. 3.3 Pendekatan Penelitian
      Penelitian ini dilakukan menggunakan pendekatan kuantitatif eksperimental terhadap sistem pembelajaran Perkembangan Arsitektur berbasis metaverse yang dikembangkan dalam komunitas dosen dan mahasiswa Jurusan Teknik Arsitektur, Program Studi S1 Arsitektur Universitas Gunadarma. Sistem pembelajaran berbasis metaverse ini dikembangkan sesuai dengan materi pembelajaran Perkembangan Arsitektur 1 di kalangan mahasiswa Jurusan Teknik Arsitektur semester 3. Apabila pengembangan sistem pembelajaran ini sudah selesai, model pembelajaran berbasis metaverse tersebut diuji validitas dan reliabilitasnya dengan melibatkan penilaian objektif dan otoritatif dari para ahli,
baik ahli materi maupun media pembelajaran. Jika model sistem pembelajaran berbasis metaverse ini sudah dinyatakan valid dan reliabel, model tersebut diujicobakan kepada komunitas pengguna yang terdiri dari dosen dan mahasiswa dari Jurusan Teknik Arsitektur semester 3, sehingga akhirnya dapat diketahui efektivitas pembelajaran kolaboratif berbasis metaverse tersebut sesuai dengan kriteria dan indikator yang ditetapkan oleh dosen pengampu. Efektivitas pembelajaran kolaboratif berbasis metaverse dalam penelitian ini dievaluasi dengan melihat peningkatan pemahaman mahasiswa mengenai materi pembelajaran Perkembangan Arsitektur sesuai dengan kriteria dan indikator yang ditetapkan oleh dosen pengampu. Dari hasil uji efektivitas sistem pembelajaran ini, diharapkan dapat diketahui sejumlah kelebihan dan kekurangannya, sehingga dapat dijadikan sebagai bahan pertimbangan rekomendasi dalam meningkatkan kualitas sistem pembelajaran Perkembangan Arsitektur berbasis metaverse tersebut.","Dalam proses pengembangan sistem pembelajaran arsitektur berbasis metaverse ini, peneliti juga ingin menunjukkan perlunya keterlibatan komunitas dan persepsi pengguna bidang arsitektur agar sistem pembelajaran yang dihasilkan sesuai dengan kebutuhan dan harapan mereka dalam meningkatkan efektivitas pembelajaran arsitektur itu sendiri. Diharapkan, sistem pembelajaran arsitektur berbasis metaverse ini dapat memberi kemudahan kepada komunitas dosen dan mahasiswa dalam mempelajari berbagai sisi arsitektur dengan memasuki dunia virtual dan mereka dapat memahami materi yang diajarkan serta memecahkan permasalahan arsitektur yang dihadapi secara interaktif, kolaboratif, dan imersif dengan solusi tepat tanpa harus mencari berbagai referensi wujud nyata arsitektur di dunia fisik atau dunia nyata. Dengan konsep pembelajaran matakuliah Perkembangan Arsitektur 1 yang dilakukan dengan metode metaverse, pembelajaran secara online ini dapat dilakukan dengan lebih interaktif. 3.2 Kerangka Penelitian
      Penelitian ini dilakukan dalam mencapai tujuan utama, yaitu pengembangan sistem pembelajaran arsitektur berbasis metaverse, terutama terkait perkembangan arsitektur. Penelitian ini dilakukan melalui beberapa tahap, antara lain:
      Tahapan Pengembangan Sistem Pembelajaran Perkembangan Arsitektur 1 Berbasis Metaverse

4. Efektivitas Pembelajaran Kolaboratif
- Presensi
- Imersi
- Kehadiran dalam realitas yang disimulasi
- Kapabilitas metaverse dalam membentuk lingkungan pengguna untuk memahami
realitas


- Pemahaman materi pembelajaran
- Kemampuan memahami materi pembelajaran Perkembangan Arsitektur berbasis
metaverse
Penelitian mengenai pengembangan sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse ini dilakukan dengan melibatkan komunitas, yang terdiri dari dosen dan mahasiswa, di Program Studi S1 Arsitektur, Jurusan Teknik Arsitektur. Dalam mewujudkan sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse ini, peneliti juga melakukan konstruksi avatar dan konten pembelajaran Perkembangan Arsitektur 1 di dalam dunia digital, sehingga terbentuk dunia virtual berbasis konten pembelajaran Perkembangan Arsitektur. Dalam hal ini, komunitas dosen dan mahasiswa ini melakukan koneksi ke dunia virtual berupa sistem pembelajaran Perkembangan Arsitektur berbasis metaverse dan semua jenis kegiatan yang dilakukan dalam menyelesaikan masalah dan menyediakan solusi yang diperlukan dapat tersimpan dalam basis data server, sehingga dapat diambil kembali setiap kali mereka masuk dan terlibat kembali dalam sistem pembelajaran virtual kolaboratif ini. Dalam sistem pembelajaran virtual kolaboratif ini, komunitas dosen dan mahasiswa dapat berinteraksi satu sama lain dalam penyelesaian masalah yang ada dan mencari solusi yang diperlukan, sehingga mereka benar-benar dapat hadir dan terlibat di dalam sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse secara intensif, interaktif, dan imersif. Efektivitas Pembelajaran Kolaboratif
      Pada tahap ini, peneliti menguji efektivitas pembelajaran kolaboratif yang didasarkan pada persepsi pengguna mengenai sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse. Keempat, pada aspek outcome, persepsi pengguna juga dieksplorasi dan dievaluasi tentang ketercapaian tujuan dari pembelajaran Perkembangan Arsitektur 1 berbasis metaverse sesuai dengan kriteria dan indikator yang ditetapkan dosen pengampu. 3.3 Pendekatan Penelitian
      Penelitian ini dilakukan menggunakan pendekatan kuantitatif eksperimental terhadap sistem pembelajaran Perkembangan Arsitektur berbasis metaverse yang dikembangkan dalam komunitas dosen dan mahasiswa Jurusan Teknik Arsitektur, Program Studi S1 Arsitektur Universitas Gunadarma. Sistem pembelajaran berbasis metaverse ini dikembangkan sesuai dengan materi pembelajaran Perkembangan Arsitektur 1 di kalangan mahasiswa Jurusan Teknik Arsitektur semester 3. Apabila pengembangan sistem pembelajaran ini sudah selesai, model pembelajaran berbasis metaverse tersebut diuji validitas dan reliabilitasnya dengan melibatkan penilaian objektif dan otoritatif dari para ahli,
baik ahli materi maupun media pembelajaran. Jika model sistem pembelajaran berbasis metaverse ini sudah dinyatakan valid dan reliabel, model tersebut diujicobakan kepada komunitas pengguna yang terdiri dari dosen dan mahasiswa dari Jurusan Teknik Arsitektur semester 3, sehingga akhirnya dapat diketahui efektivitas pembelajaran kolaboratif berbasis metaverse tersebut sesuai dengan kriteria dan indikator yang ditetapkan oleh dosen pengampu. Efektivitas pembelajaran kolaboratif berbasis metaverse dalam penelitian ini dievaluasi dengan melihat peningkatan pemahaman mahasiswa mengenai materi pembelajaran Perkembangan Arsitektur sesuai dengan kriteria dan indikator yang ditetapkan oleh dosen pengampu. Dari hasil uji efektivitas sistem pembelajaran ini, diharapkan dapat diketahui sejumlah kelebihan dan kekurangannya, sehingga dapat dijadikan sebagai bahan pertimbangan rekomendasi dalam meningkatkan kualitas sistem pembelajaran Perkembangan Arsitektur berbasis metaverse tersebut."
Miftakhul Zaen_KUALIFIKASI.txt,3.1 tahapan penel itian dalam penelitian mengenai pengembangan algoritma dbscan dengan kuantum terdapat langkahlangkah yang dilakukan seperti pada gambar 3.1. langkah langkah yang dilaukan d iantaranya yaitu pengumpulan data definisi qubits kriteria inisialis asi sistem kuantum hingga evaluasi klaster. data definisi qubits kriteria inisialisasi sistem kuantum penentuan eps dan minpts kuantum identifikasi core supplier dengan kuantum sirkuitidentifikasi noise supplier dengan kuantum sirkuit penanganan noise dengan kuantum stateformasi klaster supplier dengan kuantum measurementimplementasi quantum distance measure identifikasi core supplier dengan kuantum sirkuit evaluasi klaster1 2 3 4 5 6 9 10 117 8 gamb ar 3.1 tahapan penel itian 1. data tahap awal dalam penelitian di awali dengan pembuatan data dimana data yang digunakan pada penel itian ini adalah data s intetik. data sintetik digunakan untuk mendapatkan jumlah data yang besar sela in itu data sintetik juga b ersifat fleksibel kar ena ju mlah data yang digunakan dapat ditentukan sesuai dengan kebutuhan pengujian algo ritma yang dikembang kan. data sintetik yang dibuat berisikan nama supplier harga kualitas dan waktu pengiriman. 2. definis i qubits kriteria pada taha p ini kriteria yang digunak an untuk pengelompokan supplier diubah menjadi representasi kuantum menggunakan qubits. setia p kriteria mungkin diwakili ol eh satu atau lebih qubits tergantung pada kompleksitas yang diperlukan. kriteri a yang digunakan dalam peng elompokan supplier yaitu harga kualitas dan waktu pengiri man. 3. inisialisasi sistem kuantum pada tahapan ini melakukan p ersiapan awal dari komputer k uantum yaitu mengatur qubits ke state awal dan memas tikan semua qubits berada dalam keadaan awal sebelum operasi kuantum dijalankan. pada tahapan ini juga menentuk kan jumlah qubits yan g digunakan. 4. implementasi quantum distance measure pada tahapan ini melakukan p enerapan metode untuk mengukur jarak antar supplier dalam ruang kuantum dengan menggunakan prins ipprinsip mekanika kuantum . tahapan ini digunakan dalam proses pengelom pokkan data menggunakan quantum dbscan karena jarak antar supplier akan digunakan untuk menentukan klaster 5. penentuan eps dan minpts kuan tum pada tahap ini men entukan nilai nilai epsilo n atau eps dan minimum poi nts minpts dalam konteks kuantum untuk menentukan batas batas klaster . epsilon atau eps digunakan u ntuk menen tukan radius yang menentukan lingkungan di sekitar setiap titik data. dua titik dianggap ber tetangga jika jarak antara mereka kurang dari nilai e ps. minimum points atau min pts untuk menentukan jumlah minimum titik yang diperlukan untuk membentuk sebuah klaster . 6. identi fikasi core supplier dengan kuantum sirkuit pada tahapan ini m enggunakan rangka ian kuantum untuk mengident ifikasi supplier ini core suppl ier. supplier inti adalah supplier yang memiliki cukup banyak tetangga yang s esuai dengan minpts dalam radius epsilon yang telah ditentukan. 7. identifikasi noise supplier dengan kuantum sirkuit pada tahapan ini mengi dentifikasi supplier noise atau outlier yang mem iliki jarak tidak cukup dek at atau memiliki jarak yang jauh dengan supplier lain untuk dianggap bagian dari klaster . 8. penanganan noise dengan quantum state pada tahapan ini menge lola supplier noise yang telah diidentifikasi menggunakan teknik kuantu m untuk memisahkan atau mengelompok kan noise secara terpisah. dalam dbscan klasik noise adalah titik data yang tidak termasuk dalam klaster apa pun. titik titik ini tidak memiliki cuku p tetangga dalam radius epsilon eps atau tidak terhubung ke core poin t. 9. identifikasi core supplier dengan quantum circuit pada tahapan ini mengidentifikasi titik titik data yang berada dalam jarak epsilon atau e ps dari titik inti tetapi tidak memiliki cukup tetanga untuk masuk ke dalam klaster dengan menggunakan kuantum sirkuit . 10. formasi kluster supplier dengan quantum measurement pada tahapan ini m embentukan klaster supplier dengan mengukur state kuantum yang telah diubah melalui interaksi antar qubits yang mewakili supplier . 11. evaluasi kluster tahap terakhir di mana kualit as dan k eefektifan kluster yang ter bentuk dievaluasi. tahapan ini bertujuan untuk menilai seberapa baik kluster yang terbentuk mengguna kan. 3.2 rangkuman langkah langk ah penelitian setelah mengembangkan algoritma kuantum dbscan selanjutnya membandingk annya dengan algo ritma dbscan untuk mengetahui seberapa baik algoritma dbscan jika dibandingkan dengan algorit ma klasiknya . langkah langka h tersebut dapat dilihat pada gambar 3.2 rangkuman langkah langka h prosedur peneli tian. data definisi qubits kriteria inisialisasi sistem kuantum penentuan eps dan minpts kuantum identifikasi core supplier dengan kuantum sirkuitidentifikasi noise supplier dengan kuantum sirkuit penanganan noise dengan kuantum stateformasi klaster supplier dengan kuantum measurementimplementasi quantum distance measure identifikasi core supplier dengan kuantum sirkuitnormalisasi data penentuan epsilon dan minpts hitung jarak antar supplier identifikasi core supplier identifikasi core supplieridentifikasi noise supplier supplier tidak termasuk dalam klasterformasi klaster supplier evaluasi klasterusulan algoritma gambar 3.2 rangkuman langkah langkah prosedur penelitian,3.1 tahapan penel itian dalam penelitian mengenai pengembangan algoritma dbscan dengan kuantum terdapat langkahlangkah yang dilakukan seperti pada gambar 3.1. langkah langkah yang dilaukan d iantaranya yaitu pengumpulan data definisi qubits kriteria inisialis asi sistem kuantum hingga evaluasi klaster. data definisi qubits kriteria inisialisasi sistem kuantum penentuan eps dan minpts kuantum identifikasi core supplier dengan kuantum sirkuitidentifikasi noise supplier dengan kuantum sirkuit penanganan noise dengan kuantum stateformasi klaster supplier dengan kuantum measurementimplementasi quantum distance measure identifikasi core supplier dengan kuantum sirkuit evaluasi klaster1 2 3 4 5 6 9 10 117 8 gamb ar 3.1 tahapan penel itian 1. data tahap awal dalam penelitian di awali dengan pembuatan data dimana data yang digunakan pada penel itian ini adalah data s intetik. data sintetik digunakan untuk mendapatkan jumlah data yang besar sela in itu data sintetik juga b ersifat fleksibel kar ena ju mlah data yang digunakan dapat ditentukan sesuai dengan kebutuhan pengujian algo ritma yang dikembang kan. data sintetik yang dibuat berisikan nama supplier harga kualitas dan waktu pengiriman. 2. definis i qubits kriteria pada taha p ini kriteria yang digunak an untuk pengelompokan supplier diubah menjadi representasi kuantum menggunakan qubits. 10. formasi kluster supplier dengan quantum measurement pada tahapan ini m embentukan klaster supplier dengan mengukur state kuantum yang telah diubah melalui interaksi antar qubits yang mewakili supplier . tahapan ini bertujuan untuk menilai seberapa baik kluster yang terbentuk mengguna kan. 3.2 rangkuman langkah langk ah penelitian setelah mengembangkan algoritma kuantum dbscan selanjutnya membandingk annya dengan algo ritma dbscan untuk mengetahui seberapa baik algoritma dbscan jika dibandingkan dengan algorit ma klasiknya . data definisi qubits kriteria inisialisasi sistem kuantum penentuan eps dan minpts kuantum identifikasi core supplier dengan kuantum sirkuitidentifikasi noise supplier dengan kuantum sirkuit penanganan noise dengan kuantum stateformasi klaster supplier dengan kuantum measurementimplementasi quantum distance measure identifikasi core supplier dengan kuantum sirkuitnormalisasi data penentuan epsilon dan minpts hitung jarak antar supplier identifikasi core supplier identifikasi core supplieridentifikasi noise supplier supplier tidak termasuk dalam klasterformasi klaster supplier evaluasi klasterusulan algoritma gambar 3.2 rangkuman langkah langkah prosedur penelitian
Ragmar Faikar Eka_Kualifikasi.txt,bab metode penelitian menjelaskan mengenai tahapan yang dilakukan dalam penelitian beserta menjelaskan mengenai jadwal dan estimasi waktu tahapan yang dilakukan pada penelitian ini serta menjelaskan mengenai kegiatan yang dilakukan selama penelitian . tahapan penelitian dijelaskan dalam bentuk flowchart sehingga dapat menjelaskan proses yang dilakukan mulai dari studi literatur sampai dengan kesimpulan jadwal dan estimasi penelitian digambarkan dalam bentuk time table untuk menjadwalkan dan melakukan estimasi waktu dari tiap tahap yang dilakuk an. 3.1 tahapan penelitian terdapat beberapa tahapan yang dilakukan untuk melakukan penelitian ini beberapa tahapan yang dilakukan dapat dilihat pada gambar 3.1 . gambar 3.1 tahapan penelitian 31 3.1.1 studi literatur tahap pertama yang dilakukan yaitu studi literature yang bertujuan untuk mencari informasi atau pengetahuan dari paper atau buku sebagai teori pendukung untuk melakukan penelitian dan mencari novelty atau gap peneltian yang sudah dilakukan. paper dan buku yang digunakan dalam penelitian ini merupakan paper atau buku 5 tah un terakhir. 3.1.2 pengumpulan data tahap kedua yaitu pengumpulan data data yang digunakan pada penelitian ini adalah data citra digital kelapa sawit dengan tingkat kematangan belum matang setengah matang matang terlalu matang dan tandan buah yang kosong . data diambil dari beberapa sumber melalui website kaggle dan roboflow lalu dilakukan pemilihan gambar yang sesuai untuk dijadikan sebagai dataset. gambar 3.2 contoh data kelapa sawit dari masing masing kelas 3.1.3 preprocessing data sebelum data digunakan pada model machine learning yang dibuat data tersebut akan dilakukan preprocessing data agar data yang akan dilatih sesuai dengan keperluan yang dibutuhkan. preprocessing yang dilakukan adalah augmentasi data resize dan segmentasi data . gambar asli akan dilakukan resize menjadi ukuran 224x224 piksel lalu data tersebut akan di augmentasi untuk memperbanyak dan m emvariasi data agar dan hasil augmentasi akan dijadikan sebagai data latih untuk model yang dibuat. augmentasi yang dilakuk an adalah rotation_range width_shift_range height_shift_range brightness_range shear_range zoom_range horizontal_flip vertical_flip . segmentasi dilakukan untuk memisahkan objek gambar dengan latar belakang sehingga proses klasifikasi akan lebih terfokus pada objek yang akan diproses. 32 3.1.4 pembuatan model tahap keempat yaitu pembuatan model machine learning menggunakan mobilenetv3 smalllarge dan menggabungkannya dengan attention module cbam convolutional block attention module. 3.1.5 melatih model tahap kelima yaitu melatih model yang sudah dibuat menggunakan dataset yang sudah dikumpulkan pada tahap pengumpulan data . 3.1.6 evaluasi model tahap keenam yaitu evaluasi model dari hasil pelatihan model akan dievaluasi atay dinilai apakah model tersebut sudah baik atau belum . jika hasil dari evaluasi atau penilaian kinerja model kurang memuaskan maka tahap kelima akan dilakukan kembali. 3.1.7 deploy model tahap ketujuh yaitu deploy model model yang sudah dievaluasi dan dikatakan baik akan di deploy menjadi tflite sehingga dapat diintegrasikan ke dalam mobile device . 3.1.8 pembuatan aplikasi tahap kedelapan yaitu pembuatan aplikasi android tahap ini dilakukan menggunakan android studio untuk membuat aplikasi android . proses pembuatannya meliputi pembuatan tampilan user memasukan tflite ke dalam aplikasi sehingga aplikasi dapat menggunakan model machine learning untuk mengklasifikasi kematangan kelapa sawit menggunakan kamera smartphone. 3.1.9 pengujian aplikasi tahap kesembilan yaitu pengujian aplikasi yang sudah dibuat aplikasi akan diuji fungsi utamanya yaitu klasifikasi kematangan kelapa sawit. 3.1.10 kesimpulan penelitian tahap kesepuluh yaitu menulis kesimpulan mengenai penelitian yang sudah dilakukan kesimpulan ini mencakup kinerja model dan aplikasi yang sudah dibuat kelebihan dan kekurangan penelitian dan penelitian selanjutnya yang akan dilakukan. 33 3.2 jadwal estimasi penelitian jadwal estimasi penelitian menjelaskan mengenai rancangan kegiatan yang dilakukan selama penelitian beserta estimasi waktu tiap kegiatan yang dilakukan. gambar jadwal estimasi penelitian dapat dilihat pada gambar 3.3. gambar 3.3 jadwal estimasi penelitian gambar diatas merupakan jadwal atau estimasi penelitian kegiatan studi literatur dilakukan mulai dari bulan pertama pada tahun pertama studi literatur dilakukan untuk mencari pengetahuan dan informasi untuk mendapatkan novelty dan gap dari penelitian yan g akan dilakukan pengumpulan data dilakukan mulai dari bulan kedua tahun pertama beriringan dengan studi literature. pada bulan keenam ditahun pertama dilakukan kegiatan preprocessing data preprocessing data yang dilakukan adalah pemilihan data yang akan digunakan pada penelitian dari proses pengumpulan data melakukan resize augmentasi dan segmentasi data. pembuatan model model dilakukan pada bulan keenam beriringan dengan kegiatan pelatihan model serta melakukan evaluasi kinerja model yang sudah dilatih. bulan kesebelas melakukan kegiatan pembuatan aplikasi android kegiatan yang dilakukan yaitu membuat aplikasi android menggunakan android studio mendeploy model yang sudah dievaluasi menjadi format tflite lalu mengimplementasikannya ke dalam aplikasi android yang sudah dibuat. pada bulan ketiga ditahun kedua akan dilakukan pengujian kinerja aplikasi dengan data baru lalu dilakukan evaluasi atau penilaian terhadap fungsi fungsi aplikasi tersebut. bulan keempat tahun kedua merupakan kegiatan terakhir yaitu m enulis kesimpulan penelitian. 34 3.3 kegiatan penelitian kegiatan yang dilakukan untuk melakukan penelitian dari tahun pertama sampai tahun ketiga dapat dilihat pada tabel 2 berikut. tahun pertama tahun kedua tahun ketiga studi literatur evaluasi model submit jurnal pertama pembuatan proposal bab 1 sampai bab 3 deploy dimplementasi model pembuatan jurnal kedua pengumpulan dataset pembua tan aplikasi submit jurnal kedua preprocessing data menulis hasil penelitian bab 4 pembuatan model pengujian dan evaluasi aplikasi melatih model menulis hasil penelitian bab 4 dan bab 5 pembuatan jurnal pertama tabel 2. kegiatan penelitian kegiatan yang dilakukan pada tahun pertama yaitu melakukan studi literaur untuk pembuatan proposal penelitian bab 1 sam pai bab 3 lalu dilanjutkan dengan pengumpulan dan preprocessing data setelah mendapatkan data kegiatan pembuatan dan pelatihan m odel dapat dilakukan. pada tahun kedua dilakukan evaluasi model dan saat hasil evaluasi model sudah cukup baik model akan di deploy untuk dapat diimp lementasi ke dalam aplikasi yang sudah dibuat. aplikasi akan dievaluasi dan diuji kinerjanya sehingga mendapatkan kesimpulan dari peneli tian untuk ditulis da lam b ab 4 sampai bab 5 . pada akhir tahun kedu a setelah mendapatkan kesimpulan penelitian dilakukan pembuatan jurnal pertama dan dilanjutkan pada tahun ketiga untuk pembuatan jurnal kedua.,tahapan penelitian dijelaskan dalam bentuk flowchart sehingga dapat menjelaskan proses yang dilakukan mulai dari studi literatur sampai dengan kesimpulan jadwal dan estimasi penelitian digambarkan dalam bentuk time table untuk menjadwalkan dan melakukan estimasi waktu dari tiap tahap yang dilakuk an. 3.1.2 pengumpulan data tahap kedua yaitu pengumpulan data data yang digunakan pada penelitian ini adalah data citra digital kelapa sawit dengan tingkat kematangan belum matang setengah matang matang terlalu matang dan tandan buah yang kosong . gambar asli akan dilakukan resize menjadi ukuran 224x224 piksel lalu data tersebut akan di augmentasi untuk memperbanyak dan m emvariasi data agar dan hasil augmentasi akan dijadikan sebagai data latih untuk model yang dibuat. 32 3.1.4 pembuatan model tahap keempat yaitu pembuatan model machine learning menggunakan mobilenetv3 smalllarge dan menggabungkannya dengan attention module cbam convolutional block attention module. proses pembuatannya meliputi pembuatan tampilan user memasukan tflite ke dalam aplikasi sehingga aplikasi dapat menggunakan model machine learning untuk mengklasifikasi kematangan kelapa sawit menggunakan kamera smartphone. tahun pertama tahun kedua tahun ketiga studi literatur evaluasi model submit jurnal pertama pembuatan proposal bab 1 sampai bab 3 deploy dimplementasi model pembuatan jurnal kedua pengumpulan dataset pembua tan aplikasi submit jurnal kedua preprocessing data menulis hasil penelitian bab 4 pembuatan model pengujian dan evaluasi aplikasi melatih model menulis hasil penelitian bab 4 dan bab 5 pembuatan jurnal pertama tabel 2. kegiatan penelitian kegiatan yang dilakukan pada tahun pertama yaitu melakukan studi literaur untuk pembuatan proposal penelitian bab 1 sam pai bab 3 lalu dilanjutkan dengan pengumpulan dan preprocessing data setelah mendapatkan data kegiatan pembuatan dan pelatihan m odel dapat dilakukan. pada tahun kedua dilakukan evaluasi model dan saat hasil evaluasi model sudah cukup baik model akan di deploy untuk dapat diimp lementasi ke dalam aplikasi yang sudah dibuat. aplikasi akan dievaluasi dan diuji kinerjanya sehingga mendapatkan kesimpulan dari peneli tian untuk ditulis da lam b ab 4 sampai bab 5 . pada akhir tahun kedu a setelah mendapatkan kesimpulan penelitian dilakukan pembuatan jurnal pertama dan dilanjutkan pada tahun ketiga untuk pembuatan jurnal kedua.
Reviana Siti Mardiah_Kualifikasi.txt,3.1 tahapan penelitian tahapan penelitian merupakan gambaran dari langkah langkah atau proses yang akan dilakukan dalam suatu penelitian. penelitian ini terdiri dari lima tahap an. tahap pertama adalah mengidentifikasi permasalahan pihak terkait interaksi tujuan dan tinjauan pustaka . tahap an kedua adalah membuat model manajemen persediaan beras perum bulog berdasarkan hasil wawancara awal dengan pihak terkait. tahap ketiga adalah melakukan analisis terhadap model manajemen persediaan beras perum bulog untuk mengidentifikasi area yang perlu ditingkatkan . hasil analisis ini akan digunakan untuk merumuskan solusi terhadap permasalahan yang ada . tahap keempat adalah pengembangan solusi berbasis teknologi yang terdiri dari pengembangan be rbagai model dan prototype sistem yang akan diuji . usulan yang pertama adalah model generative ai untuk menghasilkan data sintetis yang realistis yang dapat digunakan sebagai data pelatihan untuk model prediksi . model ini kemudian diintegrasikan ke dalam model ml prediksi produksi hasil panen . usulan yang kedua adalah model prediksi permintaan beras yang merupakan model yang mirip dengan model prediksi produksi hasil panen beras dengan beberapa penyesuaian agar sesuai dengan karakteristik data untuk prediksi permintaan beras. usulan yang ketiga adalah pengembangan prototype decision support system yang mengintegrasikan model prediksi dan optimasi untuk mendukung kebijakan terkait pengadaan cadangan beras . tahap kelima adalah uji coba terhadap prototype decision support system . gambar 3. 1 adalah tahapan pada penelitian ini. 59 gambar 3. 1 tahapan penelitian 3.2 pemodelan manajemen persediaan beras perum bulog manajemen persediaan cadangan beras nasional telah menjadi perhatian penting dalam beberapa tahun terakhir karena meningkatnya permintaan pangan global perubahan iklim dan ketidakstabilan ekonomi. cadangan ini merupakan stok strategis yang diawasi oleh pemerintah untuk menstabilkan persediaan dan harga beras memberikan bantuan saat terjadi kekurangan pangan dan mendukung tujuan ketahanan pangan nasional . manajemen persed iaan cadangan beras yang efektif sangat penting untuk memitigasi risiko yang terkait dengan gangguan persediaan dan fluktuasi harga beras yulianis rachman 2021 yang pada akhirnya akan menjamin ketahanan pangan dan stabilitas ekonomi octania 2021 usdianto setiyowati 2023 . 60 para pemangku kepentingan yang terlibat dalam manajemen cadangan beras ini termasuk kementerian pertanian kementerian perdagangan kem enteri an keuangan dan kem enteri badan usaha milik negara sebagai regulator serta perum bulog yang bertanggung jawab untuk mengelola persediaan beras pemerintah dan stabilisasi harga di tingkat produsen dan konsumen octania 2021 . perum bulog bertanggung jawab untuk menjaga stabilitas harga dengan membeli gabah dan beras dari petani dengan harga yang ditentukan pemerintah ketika harga beli gabah turun sehingga melindungi petani dari kerugian dan menjual beras dengan harga yang lebih rendah daripada harga pasar ketika terjadi kenaikan harga beras untuk memastikan keterjangkauan harga beras bagi masyarakat octania 2021 . lembaga ini bertanggung jawab atas manajemen salah satu komponen cadangan beras nasional yaitu cadangan beras pemerintah cbp termasuk pada pengadaan dalam negeri dan impor penyimpanan dan penyaluran beras untuk kebutuhan stabilisasi harga bantuan pangan dan keadaan darurat fang chen zhang pei gao wang 2020 octania 2021 . beberapa penelitian telah menekankan peran penting perum bulog dalam manajemen persediaan cadangan beras di indonesia . melalui manajemen cbp perum bulog memainkan peran penting dalam menjaga ketahanan pangan nasional terutama saat terjadi fluktuasi harga atau gangguan p ersediaan . keberadaan cbp yang dikelola perum bulog tidak hanya menstabilkan harga beras di pasar tetapi juga menjamin ketersediaan beras bagi masyarakat sehingga berkontribusi terhadap stabilitas ekonomi nasional octania 2021 putro purwaningsih sensuse suryono 2022 silalahi et al. 2019 . mengingat peran penting ini penerapan teknologi ai dapat membantu perum bulog dalam mengoptimalkan berbagai aspek dalam manajemen cadangan beras pemerintah seperti prediksi permintaan dan produksi hasil panen optimasi cadangan beras dan pengambilan keputusan yang lebih baik. ai dapat digunakan untuk menganalisis data historis dan realtime guna menghasilkan prediksi yang akurat mengenai permintaan dan produksi hasil panen beras mehmood et al. 2023 rai et al. 2021 sehingga memungkinkan 61 perum bulog untuk mengoptimalkan cadangan beras menghindari kelebihan atau kekurangan cadangan beras dan mengambil keputusan yang lebih baik dalam manajemen cadangan beras pemerintah h. qin 2023 . berdasarkan kajian model manajemen persediaan beras perum bulog maka penelitian ini berfokus pada pemanfaatan teknologi ai untuk efektivitas manajemen cadangan beras pemerintah terutama pada proses pengadaan . gambar 3.1 menggambarkan model manajemen persediaan beras perum bulog . gambar 3. 2 model manajemen persediaan perum bulog 3.3. analisis analisis ini bertujuan untuk mengungkap kelemahan dan proses yang kompleks dalam manajemen persediaan cadangan beras pemerintah di perum bulog . penelitian ini menggunakan metode analisis swot untuk mengidentifikasi titik titik lemah yang krusial dalam manajemen persediaan cadangan beras pemerintah dan mengembangkan strategi untuk meningkatkan efisiensi dan efektivitas pengelolaan persediaan cadangan beras di indonesia analisis swot bertujuan untuk mengetahui kekuatan kelemahan peluang dan ancaman bisnis. analisis lingkungan internal di fokuskan untuk mengetahui kekuatan dan kelemahan sedangkan analisis lingkungan eksternal difokuskan untuk mengetahui peluang dan ancaman putra pujangkoro situmorang 2022 . 62 1. analisis swot 1. strengths kekuatan s1 dukungan pemerintah perum bulog didukung oleh berbagai kebijakan pemerintah yang bertujuan menjaga stabilitas harga dan ketersediaan beras seperti yang diatur dalam uu no. 18 tahun 2012 tentang pangan dan perpres no. 48 tahun 2016 tentang penugasan kepada perum bulog . hal ini memberikan akses terhadap dukun gan kebijakan dan finansial yang kuat termasuk alokasi anggaran khusus untuk pengadaan cbp anggraini faqih sangadji kadarisman revany 2021 octania 2021 utomo 2020 . s2 infrastruktur logistik yang memadai perum bulog memiliki infrastruktur logistik yang cukup baik termasuk gudang penyimpanan yang tersebar di berbagai daerah yang berperan penting dalam menjaga kecukupan persediaan cadangan beras octania 2021 utomo 2020 s3 pengalaman dan keahlian perum bulog memiliki pengalaman puluhan tahun dalam manajemen persediaan beras mulai dari pengadaan hingga distribusi yang menjadi keunggulan dalam menjaga stabilitas harga dan persediaan beras anggraini et al. 2021 utomo 2020 . 2. weaknesses kelemahan w1 ketidakmampuan untuk bersaing dengan sektor swasta perum bulog sering menghadapi tantangan dalam bersaing dengan sektor swasta yang mampu menawarkan harga lebih tinggi kepada petan i octania 2021 . gambar 3.3 menggambarkan perbedaan yang signifikan antara harga gabah di tingkat petani dengan harga beli yang ditetapkan pemerintah. kon disi ini menyebabkan petani lebih memilih menjual hasil panennya ke sektor swasta. 63 gambar 3. 3 perbandingan harga gabah bps 2023 w2 ketergantungan pada impor meskipun perum bulog memprioritaskan pengadaan dalam negeri untuk memenuhi cbp tetapi masih terdapat ketergantungan pada impor beras terutama selama periode penurunan produksi dalam negeri yang membuat persediaan cbp rentan terhadap fluktuasi harga dan kebijakan perdagangan internasional octania 2021 utomo 2020 . tabel 3.1 dan gambar 3.4 menunjukkan banyaknya jumlah impor beras yang dilakukan perum bulog setiap tahunnya. tabel 3. 1 jumlah impor beras bps 2024 negara asal 2017 2018 2019 2020 2021 2022 2023 berat bersih ton india 32209.7 337999 7973.3 10594.4 215386.46 178533.57 69715.7 thailand 108944.8 795600.1 53278 88593.1 69360.037 80182.506 1381921.2 vietnam 16599.9 767180.9 33133.1 88716.4 65692.874 81828.039 1147705.3 pakistan 87500 310990 182564.9 110516.5 52479.011 84407 309309.7 myanmar 57475 41820 166700.6 57841.4 3790 3830 141204 jepang 72.1 0.2 90 0.3 230.291 56.087 61.5 tiongkok 2419 227.7 24.3 23.8 42.601 6 7 lainnya 54.3 6.5ssss 744.6 0.3 760.146 364.065 12933.3 total 305274.8 2253824.4 444508.8 356286.2 407741.42 429207.27 3062857.6 02000400060008000 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024perbandingan harga rata rata gkp di tingkat petani dan harga pembelian pemerintah tingkat petani kelompok kualitas gkp gabah kering panen tingkat petani hpp harga pembelian pemerintah gkp gkp gabah kering panen64 gambar 3. 4 jumlah impor beras bps 2024 w3 manajemen persediaan yang tidak efisien terdapat ketidakmampuan dalam memprediksi permintaan dan persediaan secara akurat dan realtime yang menyebabkan kelebihan dan kekurangan persediaan anggraini et al. 2021 utomo 2020 . w4 keterbatasan teknologi dan transparansi sistem yang ada saat ini tidak memiliki kapasitas untuk mencatat transaksi secara transparan yang mengakibatkan meningkatnya risiko kebocoran dan penipuan di sepanjang rantai pasok anggraini et al. 2021 utomo 2020 . w5 koordinasi antar lembaga koordinasi antara perum bulog dan lembaga pemerintah lainnya seperti kementerian pertanian dan kementerian perdagangan terkadang menghadapi tantangan karena adanya perbedaan data sehingga berdampak pada kelancaran pengambilan keputusan kebijakan impor siahaan 2023 . 3. opportunities peluang o1 memanfaatkan teknologi canggih integrasi teknologi canggih seperti blockchain dan ai berpotensi meningkatkan presisi transparansi dan efisiensi dalam mengelola persediaan cbp anggraini et al. 2021 putro et al. 2022 utomo 2020 . 0500000100000015000002000000250000030000003500000 2017 2018 2019 2020 2021 2022 2023berat bersih tonimpor beras tahun 2017 2023 impor beras65 o2 pengembangan sistem yang terintegrasi terdapat peluang untuk mengembangkan sistem yang lebih terintegrasi dan canggih yang dapat memfasilitasi pengelolaan cbp yang lebih baik anggraini et al. 2021 putro et al. 2022 utomo 2020 . 4. threats ancaman t1 ketidakstabilan harga dan persediaan global ketidakstabilan harga dan persediaan beras di pasar global dapat mempengaruhi kemampuan perum bulog untuk mengimpor beras dalam jumlah yang cukup dan dengan harga yang stabil octania 2021 . t2 dampak perubahan iklim perubahan iklim yang ekstrim dapat mengganggu produksi beras dalam negeri sehingga meningkatkan risiko ketidakcukupan cbp dan fluktuasi harga beras di tingkat konsumen octania 2021 saud wang fahad alharby bamagoos mjrashi alabdallah alzahrani abdelgawad adnan 2022 . 2. strategi setelah dilakukan analisis swot maka dapat dirumuskan strategi yang dapat digunakan perum bulog untuk meningkatkan kekuatan mengatasi kelemahan memanfaatkan peluang dan meminimalkan potensi ancaman . berikut adalah rincian strategi strategi tersebut . a. strategi s o leveraging strengths to optimize opportunities so1 pemanfaatan teknologi blockchain untuk transparansi dan efisiensi memanfaatkan dukungan pemerintah untuk mengadopsi teknologi blockchain dalam manajemen pangan yang dapat meningkatkan transparansi efisiensi dan keamanan dalam transaksi dan pencatatan. teknologi ini membantu dalam pencatatan mengotomatisasi kontrak memverifikasi transaksi dan melacak keaslian produk dari produsen ke konsumen s1 o1. 66 so2 pengembangan sistem terintegrasi meningkatkan infrastruktur yang sudah ada dengan mengembangkan sistem manajemen data yang terintegrasi dan canggih untuk memperkuat pengelolaan cbp secara lebih efektif s2 o2. so3 pengembangan model prediksi permintaan dan produksi hasil panen memanfaatkan ai untuk mengembangkan model prediksi permintaan dan produksi hasil panen beras yang akurat sehingga memungkinkan pengambilan keputusan pengadaan yang lebih tepat untuk menghindari kelebihan atau kekurangan persediaan s3 o1 o2. so4 optimalisasi program peningkatan produksi pangan memanfaatkan tenaga ahli dan infrastruktur yang ada untuk mendukung program pemerintah dalam meningkatkan produksi pangan lokal mengurangi ketergantungan impor serta memperkuat stabilitas harga dan persediaan beras s1 s3 o3. b. strategi s t using strengths to counter threats st1 pengembangan alat pendukung keputusan untuk pengadaan impor mengembangkan decision support tool dst dengan menggunakan input dari model prediksi untuk membantu menentukan kebijakan impo r beras. ds t akan membantu mengidentifikasi jumlah impor yang optimal berdasarkan analisis kebutuhan dan persediaan beras dalam negeri sehingga mengurangi risiko ketidakstabilan persediaan dan harga beras s2 t1. st2 optimalisasi manajemen krisis dengan prediksi produksi hasil panen menerapkan model prediksi hasil panen untuk mempersiapkan dan merespons secara efektif dampak perubahan iklim pada produksi beras . model prediksi ini dimanfaatkan untuk meningkatkan ketahanan pangan dan kesiapan dalam menghadapi fluktuasi hasil panen yang tidak terduga s3 t2. 67 c. strategi wo minimizing weaknesses by seizing opportunities wo1 mengadopsi teknologi blockchain untuk meningkatkan kepercayaan dan efisiensi mengadopsi teknologi blockchain untuk mengatasi keterbatasan teknologi saat ini seperti sistem yang kurang transparan . blockchain akan meningkatkan kepercayaan dan kredibilitas dalam operasi perum bulog memfasilitasi transaksi yang lebih aman dan audit yang dapat diverifikasi w1 w4 o1. wo2 peningkatan koordinasi antar lembaga melalui sistem terintegrasi membangun sistem terintegrasi yang melibatkan semua lembaga terkait untuk mengatasi masalah kurangnya koordinasi dan perbedaan data serta memfasilitasi pengambilan keputusan yang lebih cepat dan akurat w5 o2. d. strategi wt minimizing weaknesses and avoiding threats wt1 mengoptimalkan kebijakan impor dengan model prediksi menggunakan model prediksi untuk mengurangi ketergantungan pada impor dengan mengidentifikasi jumlah produksi hasil panen dalam negeri yang dapat memenuhi permintaan sekaligus mengetahui perlu atau tidaknya dilakukan impor w2 w3 t1. wt2 peningkatan manajemen persediaan melalui analisis tingkat lanjut meningkatkan sistem manajemen persediaan dengan model prediksi agar lebih responsif terhadap perubahan permintaan dan kondisi darurat serta mengurangi risiko kekurangan persediaan dan mengatasi fluktuasi harga w3 t2. setelah melakukan analisis swot da n merumuskan strategi maka hasilnya akan dirangkum dalam bentuk matriks swot yang terlampir pada gambar 3.5. 68 gambar 3. 5 matriks swot berdasarkan analisis swot yang dilakukan terhadap perum bulog telah diidentifikasi beberapa strategi penting yang menjadi fokus penelitian ini . strategi tersebut meliputi pengembangan model prediksi permintaan dan produksi optimasi manajemen krisis dengan prediksi produksi pembuatan alat pendukung keputusan untuk pengadaan optimasi kebijakan impor dengan model prediksi dan peningkatan manajemen persediaan melalui analisis lanjutan. strategi strategi ini bertujuan untuk meningkatkan kinerja perum bulog dalam manajemen cadangan beras pemerintah cbp dan dapat memberikan rekomendasi berbasis data untuk mendukung kebijakan pengadaan cadangan beras . 3.3.1 regulasi terkait manajemen cadangan beras pemerintah pemerintah telah menetapkan regulasi dalam manajemen cadangan beras untuk menjamin ketahanan pangan dan menangani potensi keadaan darurat. 69 regulasi regulasi ini bertujuan untuk menjamin ketersediaan beras selama keadaan darurat dan memanfaatkan cadangan beras yang di kelola pemerintah . berikut adalah regulasi regulasi utama yang berkaitan dengan cadangan beras pemerinta h. 1. peraturan presiden nomor 125 tahun 2022 tentang penyelenggaraan cadangan pangan pemerintah pasal 2 ayat 1 pemerintah menetapkan jenis dan jumlah cadangan pangan pemerintah cpp untuk menjamin ketersediaan pangan di seluruh wilayah indonesia. pasal 4 ayat 1 penugasan kepada badan usaha milik negara untuk mengelola cpp termasuk perum bulog danatau bumn pangan. pasal 8 ayat 1 pendanaan untuk penyelenggaraan cpp bersumber dari apbn danatau sumber pendanaan lain yang sah dan tidak mengikat . 2. peraturan badan pangan nasional nomor 12 tahun 2022 tentang penyelenggaraan cadangan beras pemerintah pasal 2 penetapan jumlah cbp dilakukan dengan mempertimbangkan a. produksi beras danatau gabah secara nasional b. penanggulangan keadaan darurat dan kerawanan pangan c. pengendalian dan stabilisasi harga dan pasokan beras danatau gabah pada tingkat produsen dan konsumen d. pelaksanaan perjanjian internasional dan bantuan pangan kerja sama internasional e. angka kecukupan gizi yang dianjurkan pasal 3 ayat 1 penetapan jumlah cbp sebagaimana dimaksud dalam pasal 2 ditetapkan oleh kepala badan . pasal 3 ayat 4 penetapan jumlah cbp sebagaimana dimaksud pada ayat 1 dilakukan paling sedikit satu kali dalam 1 satu tahaun . pasal 4 ayat 3 target pengadaan cbp terdiri atas volume pengadaa dalam negeri danatau pengadaan luar negeri. pasal 5 ayat 1 penyelenggaraan cbp dilakukan melalui pengadaan pengelolaan dan penyaluran . 70 pasal 5 ayat 2 penyelenggaraan cbp sebagaimana dimaksud pada ayat 1 dilaksanakan oleh badan pangan nasional melalui penugasan kepada perum bulog . pasal 10 ayat 1 dalam hal pengadaan cbp melalui produksi dalam negeri tidak emncukupi untuk pemenuhan cadangan menjaga stabilitas harga dalam negeri danatau memenuhi kebutuhan pemerintah dapat dilakukan pengadaan cbp dari luar negeri dengan tetap menjaga kepentingan produsen dan konsumen dalam negeri. 3. peraturan menteri perdagangan nomor 127 tahun 2018 tentang pengelolaan cadangan beras pemerintah pasal 2 ayat 1 pengelolaan cadangan beras pemerintah untuk menjamin ketersediaan pasokan dan stabilisasi harga. pasal 5 ayat 1 penugasan kepada perum bulog untuk melakukan pengadaan beras dari produksi dalam negeri berdasarkan harga pembelian pemerintah hpp. 4. peraturan menteri koordinator bidang perekonomian republik indonesia nomor 5 tahun 2018 tentang koordinasi pengelolaan cadangan beras pemerintah untuk stabilisasi harga pasal 1 ayat 3 rapat koordinasi adalah rapat yang dipimpin oleh menteri koordinator bidang perekonomian. pasal 2 ayat 2 rapat koordinasi sebagaimana dimaksud pada ayat 1 paling sedikit melibatkan menteri pertanian menteri perdagangan menteri keuangan menteri badan usaha milik negara dan direktur utama perum bulog. 5. keputusan kepala badan pangan nasional nomor 379.1ts.03.03k112023 menetapkan jumlah persediaan minimal berbagai komoditas pangan yang harus dimiliki pemerintah hingga akhir tahun 2024. untuk beras jumlah minimal yang harus dimiliki adalah 24 juta ton dengan persediaan akhir tahun minimal 12 juta ton. 71 3.3.2 parameter yang dipertimbangkan dalam menentukan jumlah cbp berdasarkan pasal 2 dari peraturan badan pangan nasional nomor 12 tahun 2022 terdapat beberapa parameter kunci yang dipertimbangkan dalam penetapan jumlah cbp . berikut adalah parameter parameter tersebut 1. produksi beras danatau gabah secara nasional merupakan total hasil produksi beras atau gabah di seluruh negeri. 2. penanggulangan keadaan darurat dan kerawanan pangan merupakan potensi dan realitas situasi darurat atau krisis pangan yang memerlukan penggunaan cadangan. 3. pengendalian dan stabilisasi harga dan pasokan beras danatau gabah merupakan kemampuan cadangan untuk mengontrol fluktuasi harga dan pasokan beras di pasar. 4. pelaksanaan perjanjian internasional dan bantuan pangan kerja sama internasional merupakan kewajiban dan komitmen internasional yang mempengaruhi jumlah beras yang perlu disimpan. 5. angka kecukupan gizi yang dianjurkan merupakan kebutuhan gizi yang harus dipenuhi melalui konsumsi beras oleh populasi. 3.4. pengembangan model prediksi produksi hasil panen beras gambar 3. 6 menggambarkan delapan tahapan yang dijalani dalam pengembangan model prediksi untuk produksi hasil panen beras pada penelitian ini. setiap tahap dirancang untuk memastikan keakuratan dan efektivitas model dalam memprediksi hasil panen . tahap pertama adalah pengumpulan data yang meliputi data historis produksi padi dan data explanatory variable . tahap kedua adalah explorasi data eda yang meliputi penanganan data duplikat penanganan nilai yang hilang missing value analisis statistik deskriptif analisis univariat dan analisis multivariat . 72 gambar 3. 6 tahapan penelitian prediksi permintaan yang diusulkan 73 tahap ketiga adalah preprocessing data prapemprosesan yang meliputi penanganan outlier dan skewness pengkodean encoding fitur kategorikal serta normalisasi normalize . tahap ke empat adalah feature engineering rekayasa fitur yang melibatkan kontruksi fitur dan seleksi fitur . tahap kelima adalah membagi data split data menjadi data pelatihan training dan data pengujian testing . tahap ke enam adalah augmentasi data. tahap ketujuh adalah pemodelan menggunakan algoritma xgboost yang dilatih menggunakan dataset pelatihan. tahap kedelapan adalah evaluasi hasil yang meliputi pengukuran performa model yang dapat dilihat dari nilai r2 root mean squared error rmse dan mean absolute error mae. jika performa model perlu ditingkatkan hyperparameter dapat disesuaikan kembali di tahap k etujuh . setelah performa terbaik tercapai model dapat diintegrasikan ke dalam model decision support system . 3.4.1 pengumpulan data pengembangan model prediksi produksi hasil panen ini akan menggunakan data yang dikumpulkan dari website badan pusat statistik bps dan bmkg . dataset ini berisi variabel provinsi tahun produksi luas panen curah hujan kelembapan dan suhu rata rata untuk periode 1993 2020. penjelasan mengenai masing masing variabel dapat dilihat di tabel 3 .2 dan sampel dari dataset ini dapat dilihat di lampiran 1. tabel 3. 2 deskripsi variabel variabel deskripsi tipe data provinsi informasi mengenai nama provinsi yang menghasilkan padiberas categorical tahun tahun produksi panen date produksi jumlah hasil produksi hasil panen dalam satu tahun numerical luas panen ukuran luas lahan yang dipanen di provinsi tersebut numerical curah hujan curah hujan yang diterima provinsi tersebut dalam satu tahun numerical kelembapan tingkat kelembapan udara rata rata di provinsi tersebut dalam satu tahun numerical suhu rata rata suhu udara rata rata di provinsi tersebut dalam satu tahun numerical dataset yang digunakan dalam penelitian ini mencakup berbagai informasi yang berkaitan dengan hasil panen dari berbagai provinsi . setiap baris mencakup beberapa variabel yang penting untuk analisis prediksi produksi hasil panen . 74 variabel provinsi menunjukkan nama provinsi di indonesia di mana produksi padi beras diukur sementara variabel tahun menunjukkan tahun pengumpulan data produksi padi beras. variabel produksi menunjukkan jumlah total produksi panen beras dalam ton yang diproduksi di provinsi tersebut pada tahun tertentu. variabel luas panen menunjukkan luas lahan pertanian di provinsi tersebut yang ditanami padi dan dipanen untuk menghasilkan beras dalam satuan hektar. variabel curah hujan menunjukkan rata rata curah hujan dalam milimeter yang diterima oleh provinsi tersebut pada tahun tersebut sedangkan variabel kelembaban menampilkan tingkat kelembaban udara rata rata di provinsi tersebut dalam persentase. terakhir variabel suhu rata rata menunjukkan suhu udara rata rata di provinsi tersebut dalam derajat celcius yang diukur selama satu tahun. 3.4.2 exploratory data analysis eda exploratory data analysis eda adalah tahapan penting dalam pemodelan predi ksi. hal ini melibatkan analisis data historis secara menyeluruh menggunakan statistik deskriptif visualisasi data univariat dan multivariat seperti plot untuk mengidentifikasi pola tren dan korelasi. eda memainkan peran penting dalam mengelola kualitas data. eda juga memandu seleksi dan rekayasa fitur yang sangat penting untuk mengembangkan model prediksi yang akurat santhoshkumar vanila 2024 . eda memungkinkan prediksi produksi hasil panen yang tepat dengan membangun fondasi yang kuat untuk pemodelan tingkat lanjut . penelitian ini menggunakan python untuk eda . a. deteksi data duplikat sangat penting untuk mendeteksi dan menghapus data duplikat untuk menjaga keakuratan analisis dan pengembangan model prediktif. data duplikat dapat menyebabkan hasil yang bias dan tidak dapat diandalkan koumarelas jiang naumann 2020 . pada penelitian ini tidak ditemukan adanya data duplikat dalam dataset yang digunakan. 75 b. deteksi nilai yang hilang missing value proses deteksi dan penanganan data hilang missing value dilakukan untuk memastikan keberlanjutan dalam data deret waktu time series . hal ini penting untuk prediksi produksi hasil panen karena memungkinkan model untuk menangkap pola dan trend dengan akurat atau dengan kata lain tanpa melakukan pengisian nilai nol ini analisis yang dilakukan kemungkinan besar akan menghasilkan hasil yang tidak akurat atau bias vallés pérez et al. 2022 . pada penelitian ini tidak dilakukan proses replace missing value karena dataset yang digunakan tidak memuat missing value seperti yang terlihat pada tabel 3. 3. tabel 3. 3 missing value variabel missing value provinsi 0 tahun 0 produksi 0 luas panen 0 curah hujan 0 kelembapan 0 suhu ratarata 0 c. analisis statistik deskriptif analisis statistik deskriptif dilakukan untuk mendapatkan wawasan awal mengenai properti statistik dari dataset . hal ini termasuk penghitungan rata rata median standar deviasi dan lainnya yang memberikan gambaran umum tentang bentuk dan distribusi data. tabel 3.4 adalah ringkasan analisis statistik deskriptif dataset hasil panen. tabel 3. 4 ringkasan statistik tahun produksi luas panen curah hujan kelembapan suhu ratarata count 224 224 224 224 224 224 mean 2006 5 1679701e06 374349 96692 2452 .490759 80948705 26801964 std 8095838 1161387e06 232751 161987 1031 972625 487868 1197041 min 1993 4293800e04 63142 04 2225 542 2219 25 1999 75 5488570e05 146919 5 1703 525 78975 261775 50 2006 5 1667773e06 373551 5 2315 7 82375 2673 75 2013 25 2436851e06 514570 25 3039 7 84 272 max 2020 4881089e06 872737 5522 906 2985 berdasarkan tabel 3.4 diketahui bahwa d istribusi data cenderung normal karena perbedaan antara nilai mean dan median tidak terlalu besar . produksi dan luas panen menunjukkan variasi yang besar dengan adanya outlier yang 76 ditunjukkan oleh perbedaan antara mean dan median yang cukup besar. curah hujan menunjukkan rentang nilai yang luas namun perbedaan antara mean dan median relatif kecil. kelembapan menunjukkan variasi yang cukup stabil dengan perbedaan antara mean dan median yang tidak besar . suhu rata rata menunjukkan data yang konsisten dengan perbedaan minimal antara mean dan median . d. analisis univariat analisis univariat dilakukan untuk setiap variabel dalam dataset menggunakan teknik box plots density plot violin plots dan count plot . hal ini membantu dalam memvisualisasikan distribusi mendeteksi outliers dan memahami variasi dalam data shabdin yaacob sjarif 2020 . gambar 3. 7 a box plots b density plot c violin plots dan d count plot box plot memberikan visualisasi distribusi berbasis waktu dari berbagai variabel seperti produksi area panen curah hujan kelembaban dan suhu ratarata. produksi dan luas panen menunjukkan tren yang meningkat dari waktu ke waktu sementara curah hujan kelembaban dan suhu rata rata 77 relatif stabil dengan outlier yang terlihat pada variabel curah hujan kelembaban dan suhu rata rata. density plot menggambarkan distribusi probabilitas dari setiap variabel. distribusi produksi dan luas panen menunjukkan pola miring ke kanan right skewed yang mengindikasikan adanya konsentrasi nilai di batas bawah kisaran dan beberapa nilai yang sangat tinggi. distribusi curah hujan dan suhu terlihat lebih simetris sementara distribusi kelembaban menunjukkan bukti bimodalitas dengan dua puncak yang berbeda. hal serupa juga terjadi pada violin plot yang memberikan wawasan tentang distribusi probabilitas data. variabel produksi dan luas panen menunjukkan distribusi yang condong ke kanan. sebaliknya variabel curah hujan dan suhu menunjukkan distribusi yang lebih simetris sedangkan variabel kelembaban menunjukkan pola bimodal. terakhir count plot menunjukkan jumlah pengamatan terlihat bahwa semua provinsi memiliki jumlah observasi yang sama. e. analisis multivariat analisis multivariat melibatkan visualisasi data multivariat menggunakan pair plot dan heatmap korelasi untuk mengeksplorasi hubungan antar variabel. visualisasi ini membantu mengidentifikasi pola dan hubungan yang mungkin tidak terlihat ketika menganalisis variabel secara terpisah ratilal reddy 2023 . gambar 3.8 menunjukkan pair plot dan gambar 3.9 menunjukkan heatmap korelasi. pair plot menggambarkan distribusi dan hubungan antara variabel dengan fokus pada korelasi yang kuat antara produksi dan luas panen yang mengindikasikan bahwa luas panen secara signifikan mempengaruhi tingkat produksi. hubungan ini diperkuat oleh heatmap korelasi yang menunjukkan koefisien korelasi sebesar 091 yang menandakan adanya korelasi yang kuat antara kedua variabel tersebut. 78 gambar 3. 8 pair plot gambar 3. 9 heatmap korelasi 79 3.4.3 preprocessing data tahap preprocessing dilakukan untuk mempersiapkan dataset untuk keperluan analisis machine learning . berdasarkan hasil eda yang telah dilakukan maka tahapan preprocessing yang dilakukan meliputi penanganan outlier dan skewness pengkodean encoding fitur kategorikal serta normalisasi normalize . tahap ini sangat penting untuk menghasilkan model prediksi berbasis machine learning yang akurat dan efektif. a. penanganan outliers dan skewness berdasarkan analisis box plots dan violin plots terlihat bahwa dataset yang digunakan mengandung banyak outlier sehingga perlu menerapkan metode untuk mengatasinya. di sisi lain analisis density plot menunjukkan bahwa terdapat data yang skew atau miring pada dataset yang dapat mempengaruhi kinerja model. untuk menangani keduanya transformasi log akan digunakan dalam penelitian ini choi et al. 2022 . b. pengkodean encoding fitur kategorikal untuk memproses variabel input kategorikal dengan cara yang dapat dipahami dan digunakan untuk analisis maka digunakan representasi label encoding . representasi ini mengubah variabel kategorikal menjadi representasi numerik berdasarkan penomoran kategori secara berurutan di mana setiap kategori diberi nilai numerik yang unik dahouda joe 2021 . pada penelitian ini label encoding diterapkan pada variabel provinsi. c. normalisasi normalize pada penelitian ini proses normalisasi data dilakukan dengan menggunakan metode interquartile range iqr atau robust normalization yang melibatkan penghitungan rentang interkuartil data dan menggunakannya untuk menormalkan data. iqr adalah rentang antara kuartil pertama q1 dan kuartil ketiga q3 yang mewakili 50 bagian tengah data. dengan menormalkan data metode iqr dapat membantu meningkatkan akurasi prediksi vaitheeshwari sathieshkumar 2019 . 80 3.4.4 features engineering feature engineering dilakukan untuk meningkatkan interpretabilitas model dengan membuat fitur fitur yang lebih mudah dimengerti. fitur fitur yang terstruktur dengan baik dapat membantu dalam menjelaskan alasan dibalik prediksi yang dibuat oleh model. proses feature engineering terdiri dari lima teknik utama yaitu feature improvement feature construction feature selection feature extraction dan feature learning ozdemir 2022 . pada penelitian ini dilakukan dua perlakuan dimana model dibangun dengan dan tanpa proses feature engineering. hal ini dilakukan untuk membuktikan hasil penelitian swaminathan and venkitasubramony 2023 yang menyatakan bahwa penggunaan feature engineering sanga t diperlukan untuk pemodelan prediksi untuk menghasilkan prediksi yang lebih akurat dan dapat diandalkan. selain itu penggunaan features engineering keterlibatan explanatory variabel dan data yang lebih banyak akan menjadi salah satu fokus utama pada penelitian in. penelitian ini melibatkan feature construction dan feature selection seperti yang terlihat pada gambar 3. 10. gambar 3. 10 flowchart feature engineering feature construction melibatkan proses pembuatan fitur baru dari data yang sudah ada. hal i ni melibatkan penggabungan data untuk membuat fitur yang lebih informatif dan relevan dengan masalah yang sedang dihadapi ozdemir 2022 . pada penelitian ini dibuat fitur efisiensi produksi dari produksi dan luas panen . seleksi fitur feature selection berperan penting dalam meningkatkan akurasi dalam model prediksi . teknik ini melibatkan identifikasi fitur yang paling berpengaruh terhadap hasil panen . pemilihan fitur yang tepat dapat memfasilitasi prediksi hasil panen padiberas masa depan. proses seleksi ini esensial untuk menyesuaikan model prediksi dengan dinamika hasil panen yang berubah ubah bedi 2023 . 81 3.4.5 pembagian data split data pada p roses pembuatan model machine learning data dapat dibagi menjadi dua set yaitu data pelatihan training dan data pengujian testing . mesin dilatih dengan data pelatihan dan diuji dengan data pengujian untuk mengevaluasi keandalan dan performanya dalam mencapai tujuan penelitian . untuk menentukan rasio pembagian data yang optimal penelitian ini menggunakan tiga rasio yang umum digunakan yaitu 7030 8020 dan 9010 untuk data pelatihan dan pengujian grigorev 2021 nguyen ly ho al ansari le tran prakash pham 2021 . gambar 3. 11 membagi data menjadi pelatihan dan pengujian grigorev 2021 3.4.6 augmentasi data augmentasi data data augmentation da adalah teknik yang digunakan dalam machine learning dan deep learning untuk meningkatkan ukuran dataset pelatihan dengan membuat versi modifikasi dari data yang ada. proses ini sangat penting untuk meningkatkan performa model terutama dalam skenario di mana jumlah data pelatihan yang tersedia terbatas bayer et al. 2022 moreno barea et al. 2020 onishi meguro 2023 . generative ai genai akan dimanfaatkan untuk menambah data training dalam penelitian ini khususnya dengan menggunakan generative adversarial networks gan. 3.4.7 pemodelan dengan gridsearchcv xgboost merupakan model machine learning canggih yang membangun ansambel pohon keputusan secara berurutan mengoptimalkan loss function dan menangani ketidakseimbangan data missing value serta overfitting . prediksi produksi beras menggunakan algoritma xgboost membutuhkan penyetingan 82 beberapa hyperparameter sebagai penunjang pemodelan . proses untuk memilih hyperparameter terbaik dalam model machine learning untuk meningkatkan kinerja dikenal sebagai penyetelan hyperparameter tuning soleymani mohammadzadeh 2023 . model prediksi pada penelitian ini menggunakan parameter n_estimator max_depth dan learning_rate yang diadopsi dari penelitian ørebæk and geitle 2021 . range parameter yang dipertimbangkan pada penelitian ini dapat dilihat di tabel 3. 5. tabel 3. 5 parameter yang dipilih dan nilainya ørebæk geitle 2021 hyperparameter v alue range learning _rate 011 n_estimator 100250 max_depth 1 14 uji coba satu persatu kombinasi yang telah diatur dan ditambah dengan gridsearchcv akan mampu menentukan kombinasi mana yang menghasilkan performa terbaiknya. pada konteks machine learning 10fold cross validation seringkali lebih disukai karena memberikan estimasi yang lebih dapat diandalkan untuk kinerja model terutama ketika ukuran dataset terbatas marcot hanea 2021 . untuk itu pada penelitian ini nilai cv disetting 10 yang mana setiap kombinasi model dan parameter divalidasi sebanyak 10 kali dan data akan dibagi menjadi 10 bagian sama besar secara acak yang mana 9 bagian akan digunakan untuk training dan 1 bagian untuk validasi . dari hasil uji coba secara acak didapatkan kombinasi terbaik yaitu n_estimator 115 learning_rate 028 serta max_depth 1. kombinasi ini yang dimasukan ke dalam algoritma xgboost seperti yang terlihat di gambar 3. 12. gambar 3. 12 flowchart pemodelan jika ingin meningkatkan performa model maka dapat dilakukan penyetelan ulang atau tuning model. proses ini melibatkan penyesuaian hyperparameter dan konfigurasi model untuk mencapai hasil yang lebih baik. dengan penyetelan ulang dapat mengeksplorasi kombinasi hyperparameter yang berbeda atau menggunakan teknik optimasi lanjut seperti gridserchcv 83 liyanage basnayake gamage prabhashi kasthuriarachchi abeywardhana 2023 . meskipun memerlukan upaya yang cukup besar penyetelan ulang dapat menghasilkan peningkatan signifikan dalam performa prediksi. untuk itu dalam konteks penelitian ini akan terus dilakukan juga evaluasi dan penelitian pada tahap pemodelan ak an terus dilakukan untuk memastikan bahwa model sesuai dengan tujuan penelitian. 3.4.8 evaluasi model setelah model dilatih model tersebut menjalani tahap evaluasi di mana tingkat kesalahan diuji untuk memastikan model berfungsi dengan baik. penelitian ini menggunakan rsquared r2 root mean squared error rmse dan mean absolute error mae untuk menguji tingkat kesalahan. r2 mengukur proporsi varians dalam variabel dan berkisar antara 0 1 dimana semakin mendekati 1 maka semakin layak suatu model untuk digunakan . rmse mengukur besarnya tingkat kesalahan prediksi dimana semakin rendah nilainya mendekati nol maka hasil prediksi akan semakin akurat. di sisi lain mae menghitung rata rata kesalahan kuadrat antara nilai aktual dan nilai yang diramalkan dengan nilai yang rendah atau mendekati nol menunjukkan kecocokan yang lebih baik antara hasil permintaan dan data aktual chicco et al. 2021 . gambar 3. 13 adalah skema evaluasi model pada penelitian ini. gambar 3. 13 flowchart evaluasi model pada tahap evaluasi model juga diuji dengan dua rancangan yang berbeda yaitu model tanpa features engineering serta pada model dengan features engineering. tabel 3. 6 adalah hasil evaluasi model pada penelitian ini. tabel 3. 6 hasil evaluasi model dataset kondisi tanpa feature engineering kondisi dengan feature engineering r2 rmse mae r2 rmse mae 7030 0773 0441 0176 0933 0209 0115 8020 0795 0418 0155 0956 0163 0099 9010 0976 0111 0079 0976 0109 0076 84 hasil evaluasi menunjukkan pengaruh positif dari penggunaan feature engineering fe terhadap kinerja model machine learning dalam skema pembagian dataset yang berbeda 7030 8020 dan 9010. khususnya penggunaan feature engineering membantu meningkatkan nilai r² yang menandakan peningkatan kemampuan model dalam menjelaskan variabilitas data yang diamati. selanjutnya nilai rmse dan mae menurun dengan implementasi fe mengindikasikan bahwa kesalahan pada model prediksi menjadi lebih kecil dan prediksi menjadi lebih akurat secara umum. analisis lebih lanjut pada perbedaan skema pembagian data menunjukkan bahwa model dengan proporsi data pelatihan yang lebih besar 9010 menunjukkan hasil yang paling stabil dan akurat. hal i ni menyoroti pentingnya fe dalam meningkatkan efektivitas model dan menunjukkan keuntungan dari alokasi yang lebih besar pada data pelatihan dalam pengembangan model prediksi berbasis machine learning . nilai rmse terendah yang diperoleh adalah 0109 yang me nunjukkan potensi model untuk menghasilkan prediksi produksi yang sangat akurat jika terus dikembangkan. nosratabadi et al. 2021 mendapatkan nilai rmse 33.575.59574 untuk model prediksi nya hal ini menunjukkan bahwa model prediksi yang diusulkan memiliki potensi untuk menyaingi model yang ada dalam hal keakuratan hasil prediksi . meskipun hal ini tidak mutlak karena model ini dibangun dengan algoritma dan data yang berbeda. namun potensi model ini menghasilkan prediksi yang sangat akurat masih terlihat. berdasarkan hasil ini penelitian ini akan fokus pada penambahan data pelatihan menggunakan generative adversarial network gan dan pengembangan lebih lanjut pada teknik feature engineering untuk meningkatkan kinerja prediksi. 3.5 pengembangan model prediksi permintaan model prediksi yang digunakan untuk prediksi permintaan adalah model yang sama dengan yang digunakan untuk memprediksi produksi atau hasil panen . namun beberapa penyesuaian dilakukan untuk memastikan bahwa model sesuai dengan karakteristik data permintaan beras terutama pada tahapan feature s engineering augmentasi data dan pemodelan dengan gridsearchcv . gambar 85 3.14 adalah tahapan pengembangan model prediksi permintaan pada penelitian ini. gambar 3. 14 tahapan penelitian prediksi permintaan yang diusulkan 86 pengembangan model prediksi permintaan beras ini terdiri dari delapan tahap. tahap pertama adalah pengumpulan data yang meliputi data historis konsumsi beras bashir yuliana 2019 dan data explanatory variable seperti harga beras iqbal 2019 sossou igue 2019 pendapatan per kapita bashir yuliana 2019 yusuf et al. 2020 populasi bashir yuliana 2019 rasio kemiskinan qian ito zhao 2020 dan peristiwa khusus seperti bulan ramadhan dan hari raya hasanah 2020 . tahap kedua adalah explorasi data eda yang meliputi penanganan data duplikat penanganan nilai yang hilang missing value analisis statistik deskriptif analisis univariat dan analisis multivariat. tahap ketiga adalah preprocessing data prapemprosesan yang meliputi penanganan outlier dan skewness pengkodean encoding fitur kategorikal dan normalisasi normalize . tahap ke empat adalah feature engineering rekayasa fitur yang melibatkan konstruksi fitur dan seleksi fitur . tahap kelima adalah membagi data split data menjadi data pelatihan training dan data pengujian testing . tahap ke enam adalah augmentasi data. tahap ketujuh adalah pemodelan menggunakan algoritma xgboost yang dilatih menggunakan dataset pelatihan. tahap kedelapan adalah evaluasi hasil yang meliputi pengukuran performa model yang dapat dilihat dari nilai r2 root mean squared error rmse dan mean absolute error mae. jika performa model perlu ditingkatkan hyperparameter dapat disesuaikan kembali di tahap k etujuh . setelah performa terbaik tercapai model dapat diintegrasikan ke dalam model decision support system . 3.6 prototype decision support system dss untuk pengadaan cadangan beras pemerintah decision support system dss pada penelitian ini akan digunakan untuk mendukung proses pengambilan keputusan internal terkait kebijakan pengadaan cadangan beras pemerintah . dss ini akan mengintegrasikan data hasil prediksi produksi hasil panen permintaan dan persediaan aktual untuk menentukan variabel keputusan pengadaan yang optimal. alat ini memungkinkan perum 87 bulog untuk merencanakan dan mengelola cadangan beras pemerintah cbp secara efisien sesuai dengan kebutuhan masyarakat . gambar 3.1 5 adalah ilustrasi dari proses pemenuhan cbp. gambar 3. 15 ilustrasi pemenuhan cbp gambar 3.1 5 menggambarkan alur proses pemenuhan cbp yang dilakukan oleh perum bulog dan lembaga terkait . proses ini dimulai dengan perum bulog memeriksa tingkat persediaan cbp. jika ditemukan adanya kekurangan persediaan maka langkah selanjutnya adalah menyusun strategi untuk mengatasi kekurangan tersebut. terdapat beberapa opsi untuk mengatasi kekurangan tersebut meliputi pengadaan dalam negeri kombinasi antara pengadaan dalam negeri dan impor atau impor penuh jika pengadaan dalam negeri tidak mencukupi. gambar 3.1 6 menguraikan proses pengambilan keputusan untuk mengelola persediaan cbp. proses ini dimulai dengan penerimaan data tentang prediksi produksi hasil panen dalam negeri dan kebutuhan beras mendatang. selanjutnya evaluasi kondisi pasar beras internasional kebijakan pemerintah dan perubahan iklim global . lalu dilakukan perhitungan terhadap jumlah cadangan beras pemerintah cbp yang optimal yang menentukan jumlah beras yang harus disimpan untuk kebutuhan darurat. untuk mencapai optimasi ini penelitian ini akan menggunakan metode deep reinforcement learning drl. berbeda dengan metode heuristik drl lebih kuat dengan hasil konvergensi yang stabil dan lebih cocok untuk masalah pengambilan keputusan z. zhang zhang qiu 2019 . tahap berikutnya adalah memeriksa jumlah persediaan aktual . 88 gambar 3. 16 diagram alir skenario dasar pengambilan keputusan 89 berdasarkan data persediaan aktual dilakukan evaluasi apakah jumlah tersebut sudah mencukupi untuk memenuhi prediksi permintaan atau kebutuhan beras mendatang . jika persediaan cukup maka tidak diperlukan tindakan pengadaan tambahan . sebaliknya jika persediaan dinilai belum mencukupi kebutuhan maka akan dilakukan penyerapan atau pengadaan dari produksi hasil panen dalam negeri . pada kondisi di mana produksi hasil panen dalam negeri yang ada pada petani atau mitra kerja perum bulog sangat minim maka akan dilakukan pengadaan dari sumber dalam negeri dan impor untuk mengisi kekurangan tersebut. jika kondisi di mana produksi hasil panen dalam negeri kosong. penentuan jumlah pengadaan didasarkan pada penghitungan jumlah cbp yang diperlukan untuk memenuhi kekurangan antara kebutuhan dan persediaan yang tersedia dengan turut mempertimbangkan ketersediaan persediaan global kebijakan perdagangan dan kejadian luar biasa seperti bencana alam atau gangguan politik yang bisa mempengaruhi pengadaan. output dari dss ini adalah sebuah rekomendasi untuk strategi dan jumlah pengadaan beras. rekomendasi ini dapat digunakan oleh perum bulog untuk membuat kebijakan pengadaan beras khususnya dalam menentukan kebutuhan akan impor beras. keseluruhan proses ini penting untuk memastikan ketersediaan cbp dan menghindari potensi krisis pangan. dss yang diusulkan adalah sebuah platform yang mengintegrasikan dua model prediksi dan proses optimasi untuk cadangan persediaan beras pemerintah . sistem ini membantu perum bulog dan lembaga terkait dalam me ngambil kebij akan strategis terkait pengadaan cadangan beras pemerintah . hal ini untuk memastikan ketersediaan cadangan beras pemerintah cbp sehingga mendukung stabilitas harga dan ketahanan pangan n asional. gambar 3. 17 adalah proto type dss yang diusulkan . 90 gambar 3. 17 prototype dss yang diusulkan 3.7 uji coba uji coba sistem bertujuan untuk memvalidasi apakah sistem yang dikembangkan sesuai dengan tujuan awal dan layak untuk digunakan. pada tahap ini prototype sistem akan menjalani evaluasi secara menyeluruh untuk memastikan bahwa fungsionalitasnya telah memenuhi standar yang diharapkan dan untuk mengidentifikasi potensi error sutiah supriyono 2021 . tahap uji coba pada penelitian ini akan menggunakan metode black box testing yang dapat menemukan error secara cepat dan efisien di berbagai kategori . hal ini termasuk fungsi yang salah atau hilang kesalahan interface masalah dengan struktur data atau akses database eksternal kesalahan kinerja serta kesalahan yang terkait dengan operasi dan shutdown sistem corso moss koren lee kochenderfer 2021 .,3.1 tahapan penelitian tahapan penelitian merupakan gambaran dari langkah langkah atau proses yang akan dilakukan dalam suatu penelitian. tahap an kedua adalah membuat model manajemen persediaan beras perum bulog berdasarkan hasil wawancara awal dengan pihak terkait. tahap ketiga adalah melakukan analisis terhadap model manajemen persediaan beras perum bulog untuk mengidentifikasi area yang perlu ditingkatkan . hasil analisis ini akan digunakan untuk merumuskan solusi terhadap permasalahan yang ada . tahap keempat adalah pengembangan solusi berbasis teknologi yang terdiri dari pengembangan be rbagai model dan prototype sistem yang akan diuji . usulan yang pertama adalah model generative ai untuk menghasilkan data sintetis yang realistis yang dapat digunakan sebagai data pelatihan untuk model prediksi . model ini kemudian diintegrasikan ke dalam model ml prediksi produksi hasil panen . usulan yang kedua adalah model prediksi permintaan beras yang merupakan model yang mirip dengan model prediksi produksi hasil panen beras dengan beberapa penyesuaian agar sesuai dengan karakteristik data untuk prediksi permintaan beras. usulan yang ketiga adalah pengembangan prototype decision support system yang mengintegrasikan model prediksi dan optimasi untuk mendukung kebijakan terkait pengadaan cadangan beras . tahap kelima adalah uji coba terhadap prototype decision support system . 1 tahapan penelitian 3.2 pemodelan manajemen persediaan beras perum bulog manajemen persediaan cadangan beras nasional telah menjadi perhatian penting dalam beberapa tahun terakhir karena meningkatnya permintaan pangan global perubahan iklim dan ketidakstabilan ekonomi. cadangan ini merupakan stok strategis yang diawasi oleh pemerintah untuk menstabilkan persediaan dan harga beras memberikan bantuan saat terjadi kekurangan pangan dan mendukung tujuan ketahanan pangan nasional . manajemen persed iaan cadangan beras yang efektif sangat penting untuk memitigasi risiko yang terkait dengan gangguan persediaan dan fluktuasi harga beras yulianis rachman 2021 yang pada akhirnya akan menjamin ketahanan pangan dan stabilitas ekonomi octania 2021 usdianto setiyowati 2023 . 60 para pemangku kepentingan yang terlibat dalam manajemen cadangan beras ini termasuk kementerian pertanian kementerian perdagangan kem enteri an keuangan dan kem enteri badan usaha milik negara sebagai regulator serta perum bulog yang bertanggung jawab untuk mengelola persediaan beras pemerintah dan stabilisasi harga di tingkat produsen dan konsumen octania 2021 . perum bulog bertanggung jawab untuk menjaga stabilitas harga dengan membeli gabah dan beras dari petani dengan harga yang ditentukan pemerintah ketika harga beli gabah turun sehingga melindungi petani dari kerugian dan menjual beras dengan harga yang lebih rendah daripada harga pasar ketika terjadi kenaikan harga beras untuk memastikan keterjangkauan harga beras bagi masyarakat octania 2021 . lembaga ini bertanggung jawab atas manajemen salah satu komponen cadangan beras nasional yaitu cadangan beras pemerintah cbp termasuk pada pengadaan dalam negeri dan impor penyimpanan dan penyaluran beras untuk kebutuhan stabilisasi harga bantuan pangan dan keadaan darurat fang chen zhang pei gao wang 2020 octania 2021 . beberapa penelitian telah menekankan peran penting perum bulog dalam manajemen persediaan cadangan beras di indonesia . melalui manajemen cbp perum bulog memainkan peran penting dalam menjaga ketahanan pangan nasional terutama saat terjadi fluktuasi harga atau gangguan p ersediaan . keberadaan cbp yang dikelola perum bulog tidak hanya menstabilkan harga beras di pasar tetapi juga menjamin ketersediaan beras bagi masyarakat sehingga berkontribusi terhadap stabilitas ekonomi nasional octania 2021 putro purwaningsih sensuse suryono 2022 silalahi et al. mengingat peran penting ini penerapan teknologi ai dapat membantu perum bulog dalam mengoptimalkan berbagai aspek dalam manajemen cadangan beras pemerintah seperti prediksi permintaan dan produksi hasil panen optimasi cadangan beras dan pengambilan keputusan yang lebih baik. ai dapat digunakan untuk menganalisis data historis dan realtime guna menghasilkan prediksi yang akurat mengenai permintaan dan produksi hasil panen beras mehmood et al. 2021 sehingga memungkinkan 61 perum bulog untuk mengoptimalkan cadangan beras menghindari kelebihan atau kekurangan cadangan beras dan mengambil keputusan yang lebih baik dalam manajemen cadangan beras pemerintah h. qin 2023 . berdasarkan kajian model manajemen persediaan beras perum bulog maka penelitian ini berfokus pada pemanfaatan teknologi ai untuk efektivitas manajemen cadangan beras pemerintah terutama pada proses pengadaan . gambar 3.1 menggambarkan model manajemen persediaan beras perum bulog . 2 model manajemen persediaan perum bulog 3.3. analisis analisis ini bertujuan untuk mengungkap kelemahan dan proses yang kompleks dalam manajemen persediaan cadangan beras pemerintah di perum bulog . penelitian ini menggunakan metode analisis swot untuk mengidentifikasi titik titik lemah yang krusial dalam manajemen persediaan cadangan beras pemerintah dan mengembangkan strategi untuk meningkatkan efisiensi dan efektivitas pengelolaan persediaan cadangan beras di indonesia analisis swot bertujuan untuk mengetahui kekuatan kelemahan peluang dan ancaman bisnis. 62 1. analisis swot 1. strengths kekuatan s1 dukungan pemerintah perum bulog didukung oleh berbagai kebijakan pemerintah yang bertujuan menjaga stabilitas harga dan ketersediaan beras seperti yang diatur dalam uu no. s2 infrastruktur logistik yang memadai perum bulog memiliki infrastruktur logistik yang cukup baik termasuk gudang penyimpanan yang tersebar di berbagai daerah yang berperan penting dalam menjaga kecukupan persediaan cadangan beras octania 2021 utomo 2020 s3 pengalaman dan keahlian perum bulog memiliki pengalaman puluhan tahun dalam manajemen persediaan beras mulai dari pengadaan hingga distribusi yang menjadi keunggulan dalam menjaga stabilitas harga dan persediaan beras anggraini et al. gambar 3.3 menggambarkan perbedaan yang signifikan antara harga gabah di tingkat petani dengan harga beli yang ditetapkan pemerintah. kon disi ini menyebabkan petani lebih memilih menjual hasil panennya ke sektor swasta. 1 jumlah impor beras bps 2024 negara asal 2017 2018 2019 2020 2021 2022 2023 berat bersih ton india 32209.7 337999 7973.3 10594.4 215386.46 178533.57 69715.7 thailand 108944.8 795600.1 53278 88593.1 69360.037 80182.506 1381921.2 vietnam 16599.9 767180.9 33133.1 88716.4 65692.874 81828.039 1147705.3 pakistan 87500 310990 182564.9 110516.5 52479.011 84407 309309.7 myanmar 57475 41820 166700.6 57841.4 3790 3830 141204 jepang 72.1 0.2 90 0.3 230.291 56.087 61.5 tiongkok 2419 227.7 24.3 23.8 42.601 6 7 lainnya 54.3 6.5ssss 744.6 0.3 760.146 364.065 12933.3 total 305274.8 2253824.4 444508.8 356286.2 407741.42 429207.27 3062857.6 02000400060008000 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024perbandingan harga rata rata gkp di tingkat petani dan harga pembelian pemerintah tingkat petani kelompok kualitas gkp gabah kering panen tingkat petani hpp harga pembelian pemerintah gkp gkp gabah kering panen64 gambar 3. 3. opportunities peluang o1 memanfaatkan teknologi canggih integrasi teknologi canggih seperti blockchain dan ai berpotensi meningkatkan presisi transparansi dan efisiensi dalam mengelola persediaan cbp anggraini et al. 0500000100000015000002000000250000030000003500000 2017 2018 2019 2020 2021 2022 2023berat bersih tonimpor beras tahun 2017 2023 impor beras65 o2 pengembangan sistem yang terintegrasi terdapat peluang untuk mengembangkan sistem yang lebih terintegrasi dan canggih yang dapat memfasilitasi pengelolaan cbp yang lebih baik anggraini et al. 2. strategi setelah dilakukan analisis swot maka dapat dirumuskan strategi yang dapat digunakan perum bulog untuk meningkatkan kekuatan mengatasi kelemahan memanfaatkan peluang dan meminimalkan potensi ancaman . a. strategi s o leveraging strengths to optimize opportunities so1 pemanfaatan teknologi blockchain untuk transparansi dan efisiensi memanfaatkan dukungan pemerintah untuk mengadopsi teknologi blockchain dalam manajemen pangan yang dapat meningkatkan transparansi efisiensi dan keamanan dalam transaksi dan pencatatan. 66 so2 pengembangan sistem terintegrasi meningkatkan infrastruktur yang sudah ada dengan mengembangkan sistem manajemen data yang terintegrasi dan canggih untuk memperkuat pengelolaan cbp secara lebih efektif s2 o2. so3 pengembangan model prediksi permintaan dan produksi hasil panen memanfaatkan ai untuk mengembangkan model prediksi permintaan dan produksi hasil panen beras yang akurat sehingga memungkinkan pengambilan keputusan pengadaan yang lebih tepat untuk menghindari kelebihan atau kekurangan persediaan s3 o1 o2. so4 optimalisasi program peningkatan produksi pangan memanfaatkan tenaga ahli dan infrastruktur yang ada untuk mendukung program pemerintah dalam meningkatkan produksi pangan lokal mengurangi ketergantungan impor serta memperkuat stabilitas harga dan persediaan beras s1 s3 o3. b. strategi s t using strengths to counter threats st1 pengembangan alat pendukung keputusan untuk pengadaan impor mengembangkan decision support tool dst dengan menggunakan input dari model prediksi untuk membantu menentukan kebijakan impo r beras. st2 optimalisasi manajemen krisis dengan prediksi produksi hasil panen menerapkan model prediksi hasil panen untuk mempersiapkan dan merespons secara efektif dampak perubahan iklim pada produksi beras . model prediksi ini dimanfaatkan untuk meningkatkan ketahanan pangan dan kesiapan dalam menghadapi fluktuasi hasil panen yang tidak terduga s3 t2. wo2 peningkatan koordinasi antar lembaga melalui sistem terintegrasi membangun sistem terintegrasi yang melibatkan semua lembaga terkait untuk mengatasi masalah kurangnya koordinasi dan perbedaan data serta memfasilitasi pengambilan keputusan yang lebih cepat dan akurat w5 o2. wt2 peningkatan manajemen persediaan melalui analisis tingkat lanjut meningkatkan sistem manajemen persediaan dengan model prediksi agar lebih responsif terhadap perubahan permintaan dan kondisi darurat serta mengurangi risiko kekurangan persediaan dan mengatasi fluktuasi harga w3 t2. strategi tersebut meliputi pengembangan model prediksi permintaan dan produksi optimasi manajemen krisis dengan prediksi produksi pembuatan alat pendukung keputusan untuk pengadaan optimasi kebijakan impor dengan model prediksi dan peningkatan manajemen persediaan melalui analisis lanjutan. strategi strategi ini bertujuan untuk meningkatkan kinerja perum bulog dalam manajemen cadangan beras pemerintah cbp dan dapat memberikan rekomendasi berbasis data untuk mendukung kebijakan pengadaan cadangan beras . 3.4. pengembangan model prediksi produksi hasil panen beras gambar 3. 6 menggambarkan delapan tahapan yang dijalani dalam pengembangan model prediksi untuk produksi hasil panen beras pada penelitian ini. setiap tahap dirancang untuk memastikan keakuratan dan efektivitas model dalam memprediksi hasil panen . 3.4.1 pengumpulan data pengembangan model prediksi produksi hasil panen ini akan menggunakan data yang dikumpulkan dari website badan pusat statistik bps dan bmkg . eda memungkinkan prediksi produksi hasil panen yang tepat dengan membangun fondasi yang kuat untuk pemodelan tingkat lanjut . tahap ini sangat penting untuk menghasilkan model prediksi berbasis machine learning yang akurat dan efektif. proses untuk memilih hyperparameter terbaik dalam model machine learning untuk meningkatkan kinerja dikenal sebagai penyetelan hyperparameter tuning soleymani mohammadzadeh 2023 . meskipun memerlukan upaya yang cukup besar penyetelan ulang dapat menghasilkan peningkatan signifikan dalam performa prediksi. rmse mengukur besarnya tingkat kesalahan prediksi dimana semakin rendah nilainya mendekati nol maka hasil prediksi akan semakin akurat. 13 flowchart evaluasi model pada tahap evaluasi model juga diuji dengan dua rancangan yang berbeda yaitu model tanpa features engineering serta pada model dengan features engineering. 6 adalah hasil evaluasi model pada penelitian ini. 6 hasil evaluasi model dataset kondisi tanpa feature engineering kondisi dengan feature engineering r2 rmse mae r2 rmse mae 7030 0773 0441 0176 0933 0209 0115 8020 0795 0418 0155 0956 0163 0099 9010 0976 0111 0079 0976 0109 0076 84 hasil evaluasi menunjukkan pengaruh positif dari penggunaan feature engineering fe terhadap kinerja model machine learning dalam skema pembagian dataset yang berbeda 7030 8020 dan 9010. khususnya penggunaan feature engineering membantu meningkatkan nilai r² yang menandakan peningkatan kemampuan model dalam menjelaskan variabilitas data yang diamati. analisis lebih lanjut pada perbedaan skema pembagian data menunjukkan bahwa model dengan proporsi data pelatihan yang lebih besar 9010 menunjukkan hasil yang paling stabil dan akurat. hal i ni menyoroti pentingnya fe dalam meningkatkan efektivitas model dan menunjukkan keuntungan dari alokasi yang lebih besar pada data pelatihan dalam pengembangan model prediksi berbasis machine learning . nilai rmse terendah yang diperoleh adalah 0109 yang me nunjukkan potensi model untuk menghasilkan prediksi produksi yang sangat akurat jika terus dikembangkan. 2021 mendapatkan nilai rmse 33.575.59574 untuk model prediksi nya hal ini menunjukkan bahwa model prediksi yang diusulkan memiliki potensi untuk menyaingi model yang ada dalam hal keakuratan hasil prediksi . meskipun hal ini tidak mutlak karena model ini dibangun dengan algoritma dan data yang berbeda. namun potensi model ini menghasilkan prediksi yang sangat akurat masih terlihat. berdasarkan hasil ini penelitian ini akan fokus pada penambahan data pelatihan menggunakan generative adversarial network gan dan pengembangan lebih lanjut pada teknik feature engineering untuk meningkatkan kinerja prediksi. 3.5 pengembangan model prediksi permintaan model prediksi yang digunakan untuk prediksi permintaan adalah model yang sama dengan yang digunakan untuk memprediksi produksi atau hasil panen . gambar 85 3.14 adalah tahapan pengembangan model prediksi permintaan pada penelitian ini. 14 tahapan penelitian prediksi permintaan yang diusulkan 86 pengembangan model prediksi permintaan beras ini terdiri dari delapan tahap. tahap kedelapan adalah evaluasi hasil yang meliputi pengukuran performa model yang dapat dilihat dari nilai r2 root mean squared error rmse dan mean absolute error mae. jika performa model perlu ditingkatkan hyperparameter dapat disesuaikan kembali di tahap k etujuh . setelah performa terbaik tercapai model dapat diintegrasikan ke dalam model decision support system . 3.6 prototype decision support system dss untuk pengadaan cadangan beras pemerintah decision support system dss pada penelitian ini akan digunakan untuk mendukung proses pengambilan keputusan internal terkait kebijakan pengadaan cadangan beras pemerintah . dss ini akan mengintegrasikan data hasil prediksi produksi hasil panen permintaan dan persediaan aktual untuk menentukan variabel keputusan pengadaan yang optimal. alat ini memungkinkan perum 87 bulog untuk merencanakan dan mengelola cadangan beras pemerintah cbp secara efisien sesuai dengan kebutuhan masyarakat . proses ini dimulai dengan perum bulog memeriksa tingkat persediaan cbp. proses ini dimulai dengan penerimaan data tentang prediksi produksi hasil panen dalam negeri dan kebutuhan beras mendatang. selanjutnya evaluasi kondisi pasar beras internasional kebijakan pemerintah dan perubahan iklim global . lalu dilakukan perhitungan terhadap jumlah cadangan beras pemerintah cbp yang optimal yang menentukan jumlah beras yang harus disimpan untuk kebutuhan darurat. berbeda dengan metode heuristik drl lebih kuat dengan hasil konvergensi yang stabil dan lebih cocok untuk masalah pengambilan keputusan z. zhang zhang qiu 2019 . 16 diagram alir skenario dasar pengambilan keputusan 89 berdasarkan data persediaan aktual dilakukan evaluasi apakah jumlah tersebut sudah mencukupi untuk memenuhi prediksi permintaan atau kebutuhan beras mendatang . sebaliknya jika persediaan dinilai belum mencukupi kebutuhan maka akan dilakukan penyerapan atau pengadaan dari produksi hasil panen dalam negeri . pada kondisi di mana produksi hasil panen dalam negeri yang ada pada petani atau mitra kerja perum bulog sangat minim maka akan dilakukan pengadaan dari sumber dalam negeri dan impor untuk mengisi kekurangan tersebut. jika kondisi di mana produksi hasil panen dalam negeri kosong. output dari dss ini adalah sebuah rekomendasi untuk strategi dan jumlah pengadaan beras. rekomendasi ini dapat digunakan oleh perum bulog untuk membuat kebijakan pengadaan beras khususnya dalam menentukan kebutuhan akan impor beras. dss yang diusulkan adalah sebuah platform yang mengintegrasikan dua model prediksi dan proses optimasi untuk cadangan persediaan beras pemerintah . sistem ini membantu perum bulog dan lembaga terkait dalam me ngambil kebij akan strategis terkait pengadaan cadangan beras pemerintah . hal ini untuk memastikan ketersediaan cadangan beras pemerintah cbp sehingga mendukung stabilitas harga dan ketahanan pangan n asional. 17 prototype dss yang diusulkan 3.7 uji coba uji coba sistem bertujuan untuk memvalidasi apakah sistem yang dikembangkan sesuai dengan tujuan awal dan layak untuk digunakan. hal ini termasuk fungsi yang salah atau hilang kesalahan interface masalah dengan struktur data atau akses database eksternal kesalahan kinerja serta kesalahan yang terkait dengan operasi dan shutdown sistem corso moss koren lee kochenderfer 2021 .
Reza Al Husna_Kualifikasi.txt,3.1 tahapan penelitian secara garis besar penelitian ini terdiri dari beberapa tahapan yaitu akuisisi data preprocessing data pengembangan dan pelatihan model pengujian dan evaluasi model serta pengembangan system deteksi penyakit daun kakao ditunjukkan pada gambar 3.1. gambar 3.1 tahapan penelitian 3.2 akuisisi data penyakit daun tanaman kakao pengumpulan citra penyakit daun tanaman kakao dikumpulkan secara langsung oleh peneliti data primer dan juga menggunakan data yang dikumpulkan oleh peneliti lain data sekunder. terdapat 4 kelas penyakit dan satu kelas daun sehat yang akan digunakan dalam penelitian ini yaitu daun sehat penyakit antraknosa colletotrichum gloeosporioides penyakit vascular streak dieback vsd penyakit leaf blotch dan penyakit cocoa swollen shoot virus disease cssvd. 46 gambar 3.2 contoh 4 jenis penyakit daun tanaman kakao dataset primer akan dilakukan pengambilan foto penyakit daun tanaman kakao yang terdapat pada kebun kakao di daerah kabupaten solok provinsi sumatra barat. pengambilan akan dilakukan dari jarak 20cm dari kamera yang bertujuan menangkap detail kecil seperti bercak kecil atau lesi pada daun perubahan warna serta tekstur permukaan daun. dataset sekunder meng gunakan dataset yang telah digunakan u mum oleh para peneliti lain terkait penyakit daun tanaman kakao. 3.3 preprocessing 3.3.1 resiz e dataset perubahan ukuran citra dilakukan menggunakan metode nearest neighbor interpolation . cara kerja dari metode ini dengan cara mengambil nilai piksel terdekat dari citra asli untuk menentukan nilai piksel baru dalam citra yang akan diubah ukurannya. citra diubah ukurannya menjadi seragam 224x224 piksel . faktor skala dihitung dengan membandingkan dimensi citra asli dimana 𝑊𝐻 dengan dimensi baru 𝑊𝐻 yang akan diubah. untuk setiap piksel dalam citra baru dengan koordinat 𝑖𝑗 hitung koordinat terdekat di citra asli 𝑖𝑗. selanjutnya map nilai piksel yaitu mengambil nilai piksel dari citra asli pada koordinat 𝑖𝑗 dan menetapkan nilai ke piksel baru di koordinat 𝑖𝑗 dalam citra yang diubah ukurannya. 3.3.2 grayscale pada tahap ini citra rgb dikonversi ke grayscale untuk membantu menyederhanakan dan.memfokuskan informasi intensitas cahaya yang lebih relevan. gejala penyakit pada daun tanaman kakao seperti perubahan warna bi ntikbintik atau 47 nekrosis dapat lebih mudah diindetifikasi melalui variasi intensitas cahaya . grayscale dapat mempertahankan informasi penting dengan lebih sederhana. 3.3.3 augmentasi data set augmentasi data dilakukan untuk meningkatkan variasi pada data set yang akan digunakan serta untuk mencegah terjadinya overfitting. teknik augmentasi yang diterapkan penelitian ini seperti rotasi flipping zooming dan cropping . 3.3.4 ekstraksi fitur 3.3.4.1 histogram oriented of gradients hog ekstraksi fitur hog digunakan dalam penelitian ini untuk menangkap bentuk dan tekstur . hog berfokus pada gradien intensit as lokal dan arah tepi yang menggambarkan struktur dan tekstur dari daun yang terkena penyakit pada tanaman kakao hog dapat menangani perubahan dalam rotasi dan skala yang memungkinkan pendeteksian penyakit y ang konsisten pada pengambilan gambar dari sudut atau jarak yang berbeda. 3.3.4.2 local binary pattern lbp penerapan e kstraksi fitur lbp pada penelitian ini untuk menangkap tekstur lokal dalam citra. lbp membantu dalam menangkap informasi tekstur seperti berca k bercak lubang kecil yang terdapat pada daun perubahan wa rna daun yang tidak merata dan perubahan permukaan lainnya. 3.3.5 splitting data 3.3.5.1 data training data training digunakan untu melatih model untuk mengenali pola maupun karakteristik visual yang membedakan daun sehat dengan daun yang terinfeksi 48 penyakit. melalui proses pelatihan ini model mengoptimalkan parameter untuk memini malkan kesalahan dalam memprediksi. 3.3.5.2 data testing data testing digunakan untuk melakukan pengujian pada model yang telah dilatih sebelumnya untuk mengevaluasi kinerja model . 3.4 pengembangan dan pelatihan model data citra daun kakao yang telah melalui preprocessing dan ekstraksi fitur kemudian digunakan untuk pelatihan dan pembuatan model deep learning menggunakan pendekatan feature fusion berbasis attention yaitu fitur ekstraksi hog dan lbp digabungkan ke dalam vision transformer yang menggunakan attention mechanism . attention mechanism dalam vision transformer memberikan fokus yang berbeda pada fitur hog dan lbp . penggunaan attention mechanism dapat meningkatkan akurasi model dengan mengurangi pengaruh noise atau informasi yang tidak relevan dalam gambar. gambar 3.3 pengembangan dan pelatihan model 3.5 pengujian dan evaluasi model pengujian dan evaluasi model dilakukan untuk me lihat akurasi model saat mengidentifikasi penyakit daun tanaman kakao. proses evaluasi dimulai dengan pengujian yang terdiri dari data yang belum pernah dilihat oleh model selama melakukan fase pelatihan . matrik evaluasi digunakan untuk mengukur kinerja model 49 secara menyeluruh . matrik evaluasi yang digunakan seperti akurasi presisi recall dan f1score. gambar 3. 4 pengujian dan evaluasi model 3.6 pengembangan sistem deteksi penyakit daun kakao setelah melakukan pelatihan dan pengembangan model serta tahap pengujian dan evaluasi model system deteksi untuk penyakit daun kakao diimplemetasikan dengan melibatkan pengintegrasian model ke dalam aplikasi atau perang kat keras. pengembangan system menciptakan solusi yang efektif dan efisien dalam mengidentifikasi penyakit daun kakao. 50 gambar 3. 5 alur identifikasi penyakit tanaman kakao,3.1 tahapan penelitian secara garis besar penelitian ini terdiri dari beberapa tahapan yaitu akuisisi data preprocessing data pengembangan dan pelatihan model pengujian dan evaluasi model serta pengembangan system deteksi penyakit daun kakao ditunjukkan pada gambar 3.1. gambar 3.1 tahapan penelitian 3.2 akuisisi data penyakit daun tanaman kakao pengumpulan citra penyakit daun tanaman kakao dikumpulkan secara langsung oleh peneliti data primer dan juga menggunakan data yang dikumpulkan oleh peneliti lain data sekunder. terdapat 4 kelas penyakit dan satu kelas daun sehat yang akan digunakan dalam penelitian ini yaitu daun sehat penyakit antraknosa colletotrichum gloeosporioides penyakit vascular streak dieback vsd penyakit leaf blotch dan penyakit cocoa swollen shoot virus disease cssvd. 46 gambar 3.2 contoh 4 jenis penyakit daun tanaman kakao dataset primer akan dilakukan pengambilan foto penyakit daun tanaman kakao yang terdapat pada kebun kakao di daerah kabupaten solok provinsi sumatra barat. dataset sekunder meng gunakan dataset yang telah digunakan u mum oleh para peneliti lain terkait penyakit daun tanaman kakao. 3.4 pengembangan dan pelatihan model data citra daun kakao yang telah melalui preprocessing dan ekstraksi fitur kemudian digunakan untuk pelatihan dan pembuatan model deep learning menggunakan pendekatan feature fusion berbasis attention yaitu fitur ekstraksi hog dan lbp digabungkan ke dalam vision transformer yang menggunakan attention mechanism . penggunaan attention mechanism dapat meningkatkan akurasi model dengan mengurangi pengaruh noise atau informasi yang tidak relevan dalam gambar. gambar 3.3 pengembangan dan pelatihan model 3.5 pengujian dan evaluasi model pengujian dan evaluasi model dilakukan untuk me lihat akurasi model saat mengidentifikasi penyakit daun tanaman kakao. 4 pengujian dan evaluasi model 3.6 pengembangan sistem deteksi penyakit daun kakao setelah melakukan pelatihan dan pengembangan model serta tahap pengujian dan evaluasi model system deteksi untuk penyakit daun kakao diimplemetasikan dengan melibatkan pengintegrasian model ke dalam aplikasi atau perang kat keras. pengembangan system menciptakan solusi yang efektif dan efisien dalam mengidentifikasi penyakit daun kakao. 5 alur identifikasi penyakit tanaman kakao
Robert_Kualifikasi.txt,"3.1 Alur Penelitian
     Gambar 3.1 menunjukkan metode penelitian. Terdapat 5 tahap utama yang akan dilakukan, yang pertama adalah studi literatur untuk menyusun bab 1 dan bab
2. Tahap kedua adalah pengumpulan citra ekspresi wajah (data citra berupa data primer dan data sekunder). Tahap ketiga adalah pembentukan dataset untuk tiap model (SVM, CNN, dan MNN), skenario pembentukan dataset dilakukan berdasarkan pada penelitian (Robert, 2023). Pada tahap keempat dilakukan pembentukan model, khusus untuk SVM dan CNN menggunakan model pada penelitian (Robert, 2023), sedangkan MNN menggunakan usulan pada penelitian ini. Tahap terakhir adalah pelatihan dan pengujian untuk semua model (SVM, CNN, MNN), terdapat tahap parameter tuning untuk tiap model. Kemudian semua performa dari tiap model akan dibandingkan satu sama lain, dan juga dianalisis pada bab 4. Gambar 3.1. Metode penelitian
3.2 Pengumpulan Citra Ekspresi Wajah
Citra ekspresi wajah dikumpulkan secara langsung oleh peneliti (data primer) dan juga menggunakan data yang dikumpulkan oleh peneliti lain (data sekunder). Terdapat 7 ekspresi wajah yang akan digunakan dalam penelitian ini, yaitu: marah, jijik, menghina, senang, sedih, kaget, dan netral (tanpa ekspresi). Gambar 3.2 menunjukkan contoh 7 ekspresi wajah manusia yang digunakan penelitian ini. Gambar 3.2. Contoh 7 jenis ekspresi wajah
Dataset primer akan dilakukan pengambilan citra ekspresi wajah mahasiswa Universitas Gunadarma baik pria maupun wanita. Pengambilan akan dilakukan dari beberapa sudut pandang guna menambah variasi dataset. Gambar 3.3 menunjukan contoh dataset primer dari berbagai sudut pandang. Gambar 3.3. Contoh dataset primer
Dataset sekunder digunakan dataset yang telah digunakan umum oleh peneliti lain terkait pengenalan ekspresi wajah. Terdapat beberapa dataset yang umum digunakan dalam penelitian ekspresi wajah. Pertama, Extended Cohn- Kanade (CK+) yang berisi citra ekspresi wajah pria dan wanita dari berbagai etnis dengan resolusi tinggi (Kanade, Cohn, & Tian, 2000; Lucey et al., 2010). Kedua, Taiwanese Facial Expression Image Dataset (TFEID) yang berisi citra ekspresi wajah pria dan wanita dari etnis Taiwan (Chen & Yen, 2007). Ketiga, Japanese Female Facial Expression (JAFFE) yang terdiri dari citra ekspresi wajah wanita etnis Jepang (Lyons, 2021; Lyons, Kamachi, & Gyoba, 2020). Gambar 3.4 menunjukan contoh citra dataset CK+ (a), JAFFE (b), dan TFEID (c). Gambar 3.4. Contoh dataset sekunder
Tabel 3.1 menunjukkan detail dari tiap dataset, mulai dari jumlah citra dari tiap kelas serta ruang warna dan ukuran citra. CK+ memiliki jumlah yang tidak seimbang pada kelas neutral dan memiliki ruang warna campur antara RGB dan Gray, dengan ukuran citra dikisaran 640 490. JAFFE dataset memiliki jumlah citra pada tiap kelas yang seimbang dengan perbedaan diantara 0 hingga 2 citra, ukuran citra 256 256, dan ruang warna grayscale. TFEID juga memiliki jumlah citra yang seimbang ditiap kelas, ukuran citra dikisaran 481 600, ruang warna RGB. 3.3 Pembentukan Dataset
Secara garis besar, dalam pembuatan model AI (khususnya ML dan DL) terdapat proses yang berperan penting, yaitu preprocessing dataset seperti ekstrasi fitur, penyesuaian ukuran citra, dan augmentasi (Deshmukh et al., 2016; Franchi et al., 2020; Mohammad & Ali, 2011; Ravi et al., 2020; Sawardekar & Naik, 2018; Shan et al., 2009). Pada penelitian (Robert, 2023), dilakukan sebuah skenario pembentukan dataset menggunakan beberapa metode pengolahan citra (seperti konversi warna ke grayscale, deteksi wajah, dan ekstrasi fitur), di mana preprocessing mempengaruhi performa dari model ML dan DL. Selain itu, pada penelitian (Alam & Yao, 2019) juga dilakukan penelitian yang serupa, di mana preprocessing mempengaruhi performa model machine learning. Pada tahap ketiga, dilakukan pembentukan dataset. Gambar 3.5 menunjukkan alur pembentukan dataset untuk SVM. Pertama, dilakukan pendeteksian wajah menggunakan VJA, proses ini berguna untuk mengurangi noise pada citra. Hasil VJA membuat ukuran citra bervariasi, oleh karena itu dilakukan resizing citra untuk menyamakan semua ukuran citra dan juga menyesuaikan dengan dimensi input model. Selanjutnya dilakukan konversi warna citra dari RGB ke Grayscale dikarena fitur warna tidak dibutuhkan dan agar dapat diekstrasi fiturnya menggunakan LMP. Terakhir, terdapat dua proses ekstrasi fitur berbeda. Proses ekstrasi fitur pertama menggunakan LMP (Robert, 2023). Proses ekstrasi fitur kedua adalah usulan dari penelitian ini, di mana pertama diaplikasikan Gabor Filter terlebih dahulu kemudian diekstrasi menggunakan LMP. Gambar 3.5. Pembentukan dataset untuk SVM
Gambar 3.6 menunjukkan alur pembentukan dataset untuk CNN dan MNN. Terdapat 3 proses yang akan dilakukan. Pertama, dilakukan deteksi wajah menggunakan VJA guna mengurangi noise. Kemudian dilakukan konversi warna dari RGB ke Grayscale karena fitur warna tidak dibutuhkan untuk mengenali ekspresi wajah. Terakhir, mengubah ukuran citra untuk menyamakan semua ukuran citra dan sesuai dengan dimensi input model. Skema pembentukan dataset ini berdasarkan performa terbaik dari penelitian (Robert, 2023). Gambar 3.6. Pembentukan dataset untuk CNN dan MNN
Dataset yang sudah melalui pembentukan dataset, dilakukan augmentasi dari sisi geometris seperti membalikan flipping secara horizontal dan vertikal, dan rotasi dari 0  hingga 45 . Tujuan dari augmentasi dataset adalah memperbanyak dataset dan juga variasi dataset. Dataset yang telah dibentuk kemudian dibagi menjadi dua jenis dataset. Jenis pertama adalah training dataset dengan jumlah 90% dari total semua dataset. Kedua adalah testing dataset yang terdiri dari 10% dari total semua dataset. Gambar 3.7 menunjukkan visualisasi pembagian dataset. SVM training dataset digunakan untuk melatih model, sedangkan testing dataset digunakan untuk menguji dataset. CNN dan MNN terdapat pembagian lagi pada training, di mana 90% dari training dataset digunakan untuk melatih model dan 10% dari training dataset digunakan untuk validasi, dan testing dataset digunakan untuk menguji model. Proses pembentukan dataset untuk SVM, CNN, dan MNN menggunakan Algoritma 3.1 hingga Algoritma 3.6. Gambar 3.7. Visualisasi pembagian dataset
3.3.1 Deteksi wajah
Proses deteksi wajah menggunakan VJA, VJA memanfaatkan dua komponen yaitu integral image dan Haar Basis Function. Algoritma 3.1 menunjukkan cara menghitung dari integral image. Input merupakan citra dengan ruang warna grayscale (dengan nama variabel img) dengan ukruan ? ? h dan output berupa integral image (yang disimpan pada nama variabel itg_img). Algoritma integral image cukup sederhana, baris 1 dan 2 dilakukan perulangan terhadap baris dan kolom citra yang digunakan untuk menentukan koordinat integral image yang sedang dihitung. Baris ketiga dilakukan perhitungan integral image untuk koordinat (? ?, ? ?) yang menghitung total nilai piksel citra asli mulai dari koordinat (0,0) hingga (? ?, ? ?) menggunakan Persamaan (2.18). Algoritma 3.1. Integral image
Algoritma 3.2 menunjukkan cara kerja dari VJA. Input dari Algoritma 3.2 adalah citra integral dengan ukuran ? ? h yang diproses menggunakan Algoritma
3.1. Terdapat beberapa parameter yang digunakan pada Algoritma 3.2. Parameter pertama adalah detection window dengan ukuran ? ?2   h2 yang digunakan untuk perhitungan Haar. Parameter kedua adalah Haar yang menggunakan Gambar 2.14. Parameter ketiga adalah nilai threshold untuk masing-masing Haar yang digunakan untuk menentukan apakah Haar tersebut merupakan fitur wajah atau bukan. Output dari algoritma ini adalah berupa koordinat wajah yang dimulai pada koordinat [? ?1, ? ?1] dengan panjang dan lebar yaitu [? ?2, h2]. Baris pertama dan kedua dilakukan perulangan terhadap baris dan kolom citra yang digunakan untuk menentukan koordinat detction window, agar lebih mudah memahami dapat melihat Gambar 3.8. Setiap perulangan pada baris pertama dan kedua, dilakukan komputasi tiap area Haar (mulai dari jenis Haar (a) hingga (d)). Pada baris 4,9,13,17 terdapat tiga perhitungan. Perhitungan pertama adalah area putih, perhitungan kedua adalah area hitam, perhitungan ketiga adalah fitur (area putih - area hitam), untuk lebih mudah memahami perhitungan area putih, area hitam, dan fitur dapat melihat Gambar 3.9. Setiap perhitungan nilai fitur (a) hingga fitur (d), dilakukan pengecekan apakah nilai fitur yang dihitung merupakan fitur atau bukan dengan cara membandingkan nilai fitur dengan threshold masing- masing (TH_a hingga TH_d) seperti baris 6,10,14,18. Jika salah satu dari keempat nilai fitur lebih kecil dari threshold yang ditentukan maka Haar tersebut bukan merupakan fitur wajah dan algoritma akan melakukan pergeseran detection window ke koordinat selanjutnya. Selain itu, jika semua nilai fitur lebih besar dari threshold maka detection window tersebut merupakan wajah, dan algoritma akan memberikan empat nilai yaitu ? ?, ? ?, ? ?? ?, ? ?h. ? ?, ? ? merupakan koordinat dari deteksi terakhir. ? ?? ?, ? ?h merupakan luas dari detection window itu sendiri. Algoritma 3.2. Algoritma Viola-Jones
Input	: citra integral w h ==> itg_gry
Parameter : detection window dengan ukuran w2 h2 ==> dw
Gambar 3.8. Pergeseran Detection Window
Gambar 3.9. Contoh perhitungan Haar
Gambar 3.9 menunjukkan contoh perhitungan untuk fitur a dan b pada Algoritma 3.2 baris 4 dan 9. Pertama, dilakukan perhitungan pada area hitam dan putih. Untuk mendapatkan hanya luas ? ? dilakukan pengurangan dengan luas ? ? dan ? ? kemudian dilakukan pertambahan luas ? ?. Perhitungan dilakukan secara demikian dikarenakan pada integral image luas ? ? = ? ? + ? ? + ? ? + ? ?, luas ? ? = ? ? + ? ?, dan luas ? ? = ? ? + ? ?. Secara teknis, pengurangan dengan luas ? ? dilakukan dua kali
dikarenakan luas ? ? dan ? ?, oleh karena itu dilakukan pertambahan luas ? ? setelah pengurangan dengan ? ? dan ? ?. Perhitungan luas ? ? memiliki pola yang sama dengan perhitungan luas ? ?. Kemudian dilakukan pengurangan antara area putih dengan hitam untuk mendapatkan nilai fitur. Fitur kemudian dibandingkan dengan nilai thershold untuk menentukan apakah Haartersebut fitur wajah atau bukan. Pola untuk perhitungan area putih, area hitam, dan fitur untuk Haar (a), (b), (c), (d) memiliki pola yang sama. Gambar 3.10 menunjukkan hasil dari implementasi algoritma deteksi wajah pada sebuah citra. 3.3.2 Image Resize
Proses perubahan ukuran citra menggunakan metode bicubic interpolation. Algoritma 3.3 menunjukkan cara kerja dari bicubic interpolation. Input berupa citra (yang ingin diubah ukurannya) dan rasio (skala ukuran yang diinginkan). Parameter berupa koefisien a yang dapat mempengaruhi koordinat tetangga (umumnya nilai dikisaran -0.75 hingga -0.5). Hasil dari algoritma ini adalah citra dengan ukuran [(?? ? ? ), (?? ? ? ), ? ?]. Pada baris 1 dilakukan perhitungan ? ?? ?, ? ??? yang digunakan untuk menentukan ukuran citra setelah diubah ukurannya dan kemudian dilakukan perhitungan h yang digunakan untuk konstanta yang akan mempengaruhi koordinat tetangga pada piksel yang akan dihitung. Baris 2 dilakukan pembuatan matriks yang digunakan untuk menyimpan hasil. Baris 4,5,6 dilakukan perulangan pada channel,
? ?? ?, ? ??? (matriks yang dibuat pada baris 2), perulangan ini digunakan untuk menentukan koordinat matriks hasil yang akan dihitung. Baris 6 dan 7 dilakukan perhitungan ? ? dan ? ?, yang merupakan konstanta yang mempengaruhi koordinat tetangga (nilai berubah setiap perulangan berjalan pada baris 3, 4, 5). Baris 8 hingga 15 dilakukan perhitungan ? ?1 hingga ? ?4 dan ? ?1 hingga ? ?4, variabel tersebut digunakan untuk menentukan bobot (weight) tiap tetangga dan koordinat tetangga pada piksel yang sedang dihitung. Baris 16 dan 17 dilakukan perhitungan menggunakan persamaan (2.19), di mana baris 16 perhitungan untuk bobot horizontal, baris 17 untuk bobot vertikal yang masing-masing disimpan pada matrix_l dan matrix_r. Baris 19 dilakukan pembuatan matriks berukuran 4 4 yang digunakan untuk menyimpan nilai piksel tetangga. Baris 20 hingga 35 merupakan pengambilan nilai tetangga berdasarkan dari perhitungan sebelumnya. Baris 36 merupakan perkalian antara matriks tetangga 4 4 dengan bobot horizontal, kemudian dikalikan dengan bobot vertikal. Hasil yang didapatkan kemudian disimpan pada variabel output. Algoritma 3.3. Image Resize (menggunakan Bicubic Interpolation)
Gambar 3.11 menunjukkan hasil perubahan ukuran citra wajah menggunakan Algoritma 3.3. Berdasarkan Gambar 3.11 ukuran dari input citra adalah 600 480 dan ratio = 0,5. Ukuran menjadi 300 240 setelah melalui proses algoritma perubahan ukuran citra. 3.3.3 Color conversion
      Proses konversi warna dari RGB ke Grayscale menggunakan Persamaan (2.2). Algoritma 3.4 menujukan proses konversi ruang warna citra dari RGB ke Grayscale dengan cara mengambil nilai Y dari YCbCr. Input dari algoritma ini adalah citra RGB yang memiliki dimensi ? ? ? ? ? ? yang diberi nama variabel img_rgb. Output dari algoritma ini adalah citra grayscale yang disimpan pada variable img_gry. Baris 1 dan 2 dilakukan perulangan baris dan kolom citra yang digunakan untuk menentukan koordinat citra grayscale yang akan dihitung. Baris 3 dilakukan konversi citra grayscale menggunakan Persamaan (2.2), di mana dilakukan penjumlahan nilai dari ketiga channel citra RGB. Tiap channel memiliki bobot masing-masing, untuk channel R memiliki bobot 0.299, channel G memiliki bobot 0.587, dan channel B memiliki bobot 0.114. Gambar 3.12 menunjukkan contoh hasil implementasi algoritma konversi warna dari RGB ke Grayscale (dengan menggunakan nilai Y dari YCbCr). 3.3.4 Gabor Filter
Pada proses SVM terdapat proses Gabor filter sebelum diekstraksi menggunakan LMP. Pertama dilakukan pembuatan filter terlebih dahulu. Algoritma
3.5 menunjukkan cara pembuatan Gabor filter. Input dari algoritma ini adalah sebuah citra grayscale yang memiliki ukuran [? ?, ? ?]. Kemudian jumlah filter, luas kernel, lambda (?? ), psi (?? ), sigma ? ?, dan gamma ? ? yang akan digunakan untuk pembuatan Gabor filter. Terakhir adalah threshold bawah dan threshold atas yang digunakan untuk deteksi tepi. Pada baris 1 dilakukan perulangan untuk menentukan rotasi (nilai theta) Gabor filter berdasarkan jumlah filter yang digunakan. Baris 2 dilakukan pembuatan Gabor filter berdasarkan Persamaan (2.23) dan parameter yang di-input. Gambar 3.13 menunjukkan contoh Gabor filter dengan total 16 filter. Pada baris 4 dilakukan perulangan terhadap filter-filter, di mana filter-filter tersebut digunakan untuk konvolusi citra pada baris 5. Pada baris 7 dilakukan deteksi tepi menggunakan Canny edge detection dengan parameter lower threshold dan upper threshold. Algoritma 3.5. Gabor Filter
3.3.5 Ekstrasi fitur LMP
Proses ekstrasi fitur menggunakan metode LMP. Algoritma 3.6 menunjukkan proses dari ekstrasi fitur LMP. Input dari algoritma LMP adalah citra grayscale dengan ukuran ? ? h. Parameter yang digunakan adalah pattern yang memiliki dimensi 8   2 yang digunakan untuk menentukan arah perhitungan tetangga. Hasil dari Algoritma 3.6 adalah citra LMP dengan ukuran (?? - 4)   (h - 4), ukuran citra berkurang untuk menghindari perhitungan di luar dari ukuran citra. Baris 2 dan 3 dilakukan perulangan baris dan kolom citra yang digunakan untuk menentukan koordinat citra LMP yang akan dihitung. Baris 4 menyimpan nilai piksel berdasarkan koordinat baris 2 dan 3. Baris 5 dilakukan perulangan untuk mengambil arah dari ptn secara berurutan. Baris 6 dan 7 digunakan untuk megambil nilai piksel tetangga radius pertama (disimpan pada variable cur_val1) dan nilai piksel tetangga radius kedua (disimpan pada variable cur_val2). Baris 9 dilakukan perbandingan nilai tengah (ctr_val) dengan nilai tetangga radius pertama (cur_val1) berdasarkan (2.22). Baris 14 dilakukan perbandingan nilai tetangga radius pertama dengan nilai tetangga radius kedua (cur_val2) berdasarkan persamaan (2.21). Kemudian dilakukan perhitungan nilai LMP menggunakan persamaan (2.20). Gambar 3.15 menunjukkan contoh hasil implementasi algoritma LMP pada citra wajah. Algoritma 3.6. Local Monotonic Pattern. 3.4 Pembentukan Model
3.4.1 Pembentukan Model SVM
Dalam penelitian sebelumnya (Robert, 2023), telah dilakukan pengenalan ekspresi wajah menggunakan SVM dengan menggunakan 4 kernel yaitu: Linear, Polynomial, Sigmoid, dan RBF. Pada penelitian tersebut dataset TFEID yang digunakan untuk melatih dan menguji model. Dataset TFEID memiliki keterbatasan dalam jumlah dan variasi etnis, di mana hanya terdapat 1 etnis saja yaitu orang Taiwan. Berdasarkan dari penelitian (Robert, 2023), didapatkan model terbaik untuk mengenal ekspresi wajah adalah menggunakan kernel Sigmoid. Pada penelitian ini akan dilakukan pengujian ulang SVM dengan dataset gabungan berdasarkan usulan. Terdapat 4 kernel yang akan digunakan untuk model SVM dalam penelitian ini yaitu: Linear, Polynomial, Sigmoid, dan RBF. Persamaan (2.29), (2.30), (2.31), (2.32)menunjukkan persamaan dari keempat kernel yang digunakan secara berurutan. Selain kernel terdapat parameter yang juga akan dikonfigurasi yaitu C. Terdapat beberapa nilai C berada diantara 0 hingga 100. 3.4.2 Pembentukan Model CNN
Dalam penelitian sebelumnya (Robert, 2023), model CNN yang digunakan adalah MobileNetV2, yang merupakan pre-trained model. Penelitian tersebut menggunakan dataset TFEID yang memiliki keterbatasan baik dalam jumlah maupun variasi dari etnis ekspresi wajah manusia. Oleh karena itu, dalam penelitian ini akan dilakukan pelatihan dan pengujian ulang menggunakan dataset gabungan sesuai dengan usulan yang telah diuraikan sebelumnya. Tabel 3.2 menunjukkan arsitektur MobileNetV2 dari konvolusi layer pertama hingga output layer. Kolom pertama menunjukkan tipe dari layer dan juga stride yang digunakan. Kolom kedua menunjukan ukuran filter dan jumlah filter. Kolom terakhir menunjukkan ukuran masukan citra dari layer sebelumnya. Fungsi aktivasi (activation function) pada output layer adalah Softmax, selain itu model juga dilakukan parameter tuning pada Learning rate, batch size, dan fungsi aktivasi pada FC-Layer saat dilakukan proses pelatihan. Tabel 3.2. Arsitektur MobileNetV2
3.4.3 Pembentukan Model MNN
Gambar 3.16 menunjukan alur dari pembentukan MNN. Pertama dilakukan penentuan SE yang digunakan. Dalam penelitian ini akan digunakan bentuk SE berdasarkan penelitian lain (Shen et al., 2019), dan SE yang diusulkan pada penelitian ini. Selanjutnya dilakukan penentuan operasi morfologi, pertama akan digunakan operasi morfologi yang telah dilakukan peneliti lain dan juga operasi morfologi usulan pada penelitian ini. Kemudian dilakukan uji coba, analisis, dan dibandingkan pada semua operasi morfologi dan SE yang diusulkan pada penelitian ini. Terakhir dilakukan perancangan arsitektur MNN berdasarkan dari analisis dari tahap ketiga. Gambar 3.16. Proses pembentukan model
3.4.3.1 Ekstrasi Fitur
3.4.3.1.1 Penentuan Structure Element
Dalam penelitian ini digunakan dua SE dan tiga operasi morfologi yang dibandingkan satu dengan yang lain. Gambar 3.17 menunjukkan SE bentuk beserta ukuran yang digunakan untuk operasi morfologi. Terdapat dua bentuk SE yang digunakan, yaitu disk dan kotak. Terdapat empat ukuran SE yang digunakan, yaitu 3 3, 5 5, 9 9, dan 15 15, ukuran digunakan untuk kedua bentuk. Di mana kotak berwarna putih bernilai 1 dan hitam adalah 0. Gambar 3.17. Struktur element yang digunakan
3.4.3.1.2 Penentuan Operasi Morfologi
Tahap setelah penentuan bentuk dan juga ukuran SE yang akan digunakan adalah penentuan operasi morfologi yang digunakan. Terdapat tiga operasi yang digunakan dalam penelitian ini. Operasi pertama adalah operasi morgologi opening pada citra original, kemudian dilakukan pengurangan nilai piksel antara citra original dengan hasil opening, operasi opening itu sendiri adalah operasi erosi yang dilanjutkan operasi dilasi. Operasi kedua adalah erosi pada citra original, kemudian dilakukan pengurangan citra original terhadap hasil erosi. Ketiga adalah dilasi pada citra original, kemudian dilakukan pengurangan hasil dilasi terhadap citra original. Ketiga operasi tersebut menggunakan Algoritma 3.7, Algoritma 3.8, dan Algoritma 3.9 secara berurutan. Algoritma 3.7. Gradien Morfologi Opening
Pada Algoritma 3.7. terdapat dua input yang digunakan, pertama adalah citra original dengan ukuran ? ? ? ?. Kedua adalah structure element dengan ukuran ? ? ? ?. Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? ? ?). Pada baris pertama dilakukan perhitungan space yang digunakan untuk menentukan koordinat mulai operasi opening. Baris 2 dan 3 dilakukan perulangan baris dan kolom citra yang digunakan untuk menentukan koordinat operasi opening. Baris 4 digunakan untuk menyimpan kandidat nilai piksel pada baris 5 hingga 11. Baris 12 dilakukan pengambilan nilai terkecil yang digunakan untuk sebagai hasil fitur citra berdasarkan persamaan. Kemudian baris 15 dan 16 dilakukan perulangan baris dan kolom citra untuk menentukan koordinat operasi opening. Baris 17 digunakan untuk menyimpan kandidat nilai piksel pada baris 18 hingga 24. Baris 25 dilakukan pengambilan nilai terbesar yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.14). Kemudian pada baris 28 dan 29 dilakukan perulangan kembali pada baris dan kolom citra untuk menentukan koordinat perhitungan. Baris 30 dilakukan pengurangan antara citra original dengan citra opening. Algoritma 3.8. Gradien Morfologi Erosi
Pada Algoritma 3.8. terdapat dua input yang digunakan, pertama adalah citra original dengan ukuran ? ? ? ?. Kedua adalah structure element dengan ukuran ? ? ? ?. Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? ? ?). Pada baris pertama dilakukan perhitungan space yang digunakan untuk menentukan koordinat mulai operasi opening. Baris 1 dan 2 dilakukan perulangan baris dan kolom citra yang digunakan untuk menentukan koordinat operasi opening. Baris 3 digunakan untuk menyimpan kandidat nilai piksel pada baris 4 hingga 10. Baris 11 dilakukan pengambilan nilai terkecil yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.10). Kemudian pada baris 14 dan 15 dilakukan perulangan kembali pada baris dan kolom citra untuk menentukan koordinat perhitungan. Baris 16 dilakukan pengurangan antara citra original dengan citra opening. Algoritma 3.9. Gradien Morfologi Dilasi
Pada Algoritma 3.9. terdapat dua input yang digunakan, pertama adalah citra original dengan ukuran ? ? ? ?. Kedua adalah structure element dengan ukuran ? ? ? ?. Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? ? ?). Baris 1 dan 2 dilakukan perulangan baris dan kolom citra yang digunakan untuk menentukan koordinat operasi dilasi. Baris 3 digunakan untuk menyimpan kandidat nilai piksel pada baris 4 hingga 10. Baris 11 dilakukan pengambilan nilai terbesar yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.8). Kemudian pada baris 14 dan 15 dilakukan perulangan kembali pada baris dan kolom citra untuk menentukan koordinat perhitungan. Baris 16 dilakukan pengurangan antara citra dilasi dengan citra original. 3.4.3.1.3 Percobaan dan Perbandingan Hasil Operasi Morfologi
Tabel 3.2 menunjukkan hasil operasi morfologi menggunakan SE berdasarkan 3.4.3.1 dan 3.4.3.2. Pada operasi morfologi Tabel 3.2. digunakan citra yang memiliki objek persegi panjang berwarna putih. Di mana putih pada citra bernilai 1 sedangkan hitam adalah 0. Tabel 3.3. Operasi Morfologi dan Structure Element pada Persegi Panjang
Berdasarkan dari Tabel 3.3 dapat dilihat operasi GMO dengan SE disk 3 3 menghasilkan corner pada persegi panjang. Operasi GMD dengan SE disk 3 3 memiliki hasil garis berbentuk persegi panjang dengan nilai tiap corner terdapat hilang. Operasi GME dengan disk 3 3 mirip dengan GMD dengan SE disk 3 3 namun corner dari GME utuh. Operasi GMO menggunakan SE kotak 3 3 tidak dapat mengekstrasi fitur dari persegi. Operasi GMD dengan SE kotak 3 3 menghasilkan garis yang berbentuk persegi panjang. GME dengan SE kotak 3 3 memiliki hasil yang mirip dengan GMD, namun memiliki luas yang berbeda. Pada GMD garis terlelak pada luar objek sedangkan GME berada pada dalam objek. Kemudian dilakukan operasi menggunakan 3 ukuran SE lain yaitu 5 5, 7 7, 9 9, dan 15 15. Dari hasil yang didapatkan, tiap ukuran memiliki hasil yang mirip dengan operasi yang digunakan. GMO mengekstrasi tiap sudut pada persegi dengan perbedaan pada ukuran sudut, di mana semakin besar SE semakin besar sudut yang didapatkan. GMD dapat mengkestrasi tepi dari persegi, di mana semakin besar ukuran SE semakin tebal garis yang didapatkan. GME dapat mengekstrasi tepi dari persegi, di mana semakin besar ukuran SE semakin tebal garis yang didapatkan. Operasi GMD dan GME memiliki hasil yang mirip dengan perbedaan tepi pada GMD menebal kearah luar persegi, sedangkan tepi pada GME menebal kearah dalam. Tabel 3.4 merupakan hasil morfologi menggunakan citra wajah dengan warna grayscale. Nilai intensitas piksel pada citra berkisaran antara 0 hingga 255. Operasi morfologi dan SE yang digunakan berdasarkan usulan pada subbab 3.4.3.1 dan 3.4.3.2. Tabel 3.4. Structure Element dan Operasi Morfologi pada Wajah
Berdasarkan Tabel 3.4, dapat dilihat hasil ekstrasi fitur menggunakan berbagai kombinasi dua bentuk dan lima ukuran. Pertama dilakukan ekstrasi fitur menggunakan SE ukuran 3 3 terhadap semua bentuk SE dan operasi. SE disk 3 3 dengan operasi GMO, fitur tidak terlihat. SE disk 3 3 dengan operasi GME atau GMD, fitur terlihat namun tidak jelas. SE kotak 3 3 dengan operasi GMO, fitur juga tidak terlihat. SE kotak 3 3 dengan operasi GME atau GMD fitur sedikit terlihat dan tidak memiliki perbedaan secara kasat mata. Kemudian dilakukan menggunakan SE ukuran 5 5. SE disk 5 5 dengan operasi GMO terdapat sangat sedikit fitur yang didapatkan yaitu pada bagian mata. SE disk 5 5 dengan operasi GME atau GMD fitur lebih jelas terlihat dan terdapat perbedaan, di mana operasi GME tepi pada bagian mata dengan bola mata tergabung sedangkan GMD tidak. Selain itu luas lubang hidung dan mulut juga lebih besar pada operasi GME dibandingkan operasi GMD. SE kotak 5 5 juga memiliki hasil yang sama dengan disk 5 5 dengan perbedaan yang tidak dapat dilihat kasat mata. Kemudian dilakukan operasi menggunakan SE ukuran 7 7. SE disk 7 7 dengan operasi GMO terdapat sangat sedikit fitur yang didapatkan yaitu pada bagian mata. SE disk 7 7 dengan operasi GME atau GMD fitur lebih jelas terlihat dan terdapat perbedaan, di mana operasi GME tepi pada bagian mata dengan bola mata tergabung sedangkan GMD tidak. Selain itu luas lubang hidung dan mulut juga lebih besar pada operasi GME dibandingkan operasi GMD. SE kotak 7 7 juga memiliki hasil yang sama dengan disk 7 7 dengan perbedaan yang tidak dapat dilihat kasat mata. Pada operasi menggunakan SE ukuran 9 9. SE disk 9 9 dengan operasi GMO fitur yang didapatkan tidak akurat karena tidak mengekstrasi tepi (bentuk) dari ekspresi wajah. SE disk 9 9 dengan operasi GME dan GMD memiliki fitur yang mirip dengan SE 5 5, di mana operasi GME tepi pada bagian mata tergabung sedangkan GMD tidak. Selain itu luas pada lubang hidung dan mulut juga lebih luas dibandingkan dengan SE 5 5, dan memiliki garis tepi yang lebih tebal. SE kotak 9 9 dengan operasi GMO, fitur yang didapatkan mirip dengan disk 9 9 dengan operasi GMO namun memiliki fitur yang lebih jelas terlihat. SE kotak 9 9 dengan operasi 9 9 GME fitur yang didapatkan terlihat dengan jelas dan mirip dengan hasil yang menggunakan SE disk 9 9 dengan operasi GME. Namun terdapat perbedaan fitur yang signifikan pada bagian hidung, di mana bentuk dari hidung sedikit berubah menjadi sedikit kotak. SE kotak 9 9 dengan operasi GMD, hasil yang didapatkan hampir sama dengan hasil yang menggunakan SE disk 9 9 dengan operasi GMD, hanya terdapat perbedaan pada tebal garis pada hidung. Operasi menggunakan SE 15 15, hasil yang didapatkan memiliki pola yang sama dengan operasi yang menggunakan SE 9 9. SE disk atau kotak 15 15 dengan operasi GMO fitur lebih terlihat jelas dibandingkan dengan ukuran 9 9. SE disk atau kotak 15 15 dengan operasi GME fitur lebih tebal, namun untuk SE kotak bagian hidung menjadi lebih kotak dari ukuran sebelumnya. Berdasarkan dari hasil yang didapatkan dan analisis. SE disk 5 5 atau kotak 5 5 dengan operasi GMD memiliki hasil terbaik. Tepi pada bagian mata tergabung, bagian mulut tidak menjadi lebih luas. Namun pada bagian hidung menjadi lebih kecil dibandingkan citra original. Selain itu operasi menggunakan GME dengan SE disk 5 5 atau kotak 5 5 juga memberikan hasil yang baik. Tepi dari tiap komponen wajah terlihat namun tepi pada mata dan bola mata tergabung, namun memberikan hasil ekstrasi fitur bagian hidung lebih baik dibandingkan GMD. Karena luas hidung lebih sesuai pada GME dibandingkan GMD. 3.4.3.2 Pembentukan dan Pelatihan Model
Gambar 3.18 menunjukkan arsitektur dari MNN secara garis besar. Di mana terdapat 5 layer utama yang memiliki tugas masing-masing. Di mana terdapat beberapa variasi arsitektur Variasi pertama terdapat pada morphology layer, di mana akan digunakan dua jenis ekstrasi fitur berdasarkan pada hasil subbab 3.4.3.3. Variasi kedua terdaoat pada hidden layer, di mana akan digunakan beberapa kombinasi fully-connected layer. Gambar 3.18. Model MNN yang diusulkan
Input Layer adalah layer pertama dari model MNN yang bertugas untuk menerima input berupa citra. Di mana dimensi dari intput layer itu sendiri sesuai dengan ukuran citra pada dataset yaitu 160 160. Kedua adalah morphology layer. Menerima masukan dari input layer kemudian dilakukan proses morfologi. Terdapat dua jenis morfologi layer yang akan digunakan. Morfology layer pertama adalah dilation layer kemudian subtraction layer. Di mana lapisan morfologi jenis pertama menggunakan hasil terbaik dari operasi morfologi dilasi pada subbab 3.4.3.3. Morphology layer jenis kedua adalah erosion layer dilanjutkan subtraction layer. Di mana lapisan morfologi jenis kedua ini menggunakan hasil terbaik dari operasi morfologi erosi pada subab 3.4.3.3. Lapisan ketiga adalah flatten layer, lapisan yang bertugas mengubah citra menajadi feature vector. Masukan dari lapisan ini adalah hasil dari subtraction layer pada lapisan morfologi. Di mana hasil dari subtraction layer adalah citra dengan ukuran 160 160 yang kemudian diubah mnejadi satu dimensi yaitu 25600. Lapisan keempat adalah Fully Connected Layer (FC Layer) yang bertugas untuk mempelajari dan menganalisa nilai feature vector dari flatten layer. Pada lapisan ini dilakukan beberapa konfigurasi FC Layer mulai dari jumlah FC Layer, dan jumlah Neuron pada FC Layer. Konfigurasi pertama akan diuji coba 2 FC Layer dengan masing-masing neuron adalah 512, dan 256. Konfigurasi kedua akan dicoba 1024, dan 512. Kemudian dari situ akan dicoba analisis mana yang lebih baik sehingga dapat dikonfigurasi lebih lanjut. Jika konfigurasi pertama memiliki hasil lebih baik artinya memungkinkan FC Layer untuk dibuat lebih sederhana dengan mengurangi jumlah neuron. Jika konfigurasi kedua lebih baik artinya terdapat kemungkinan untuk meninkatkan performa dari model karena dataset memiliki kompleksitas tinggi. Beberapa lapisan akan dilakukan tuning paramter. Tuning pertama terdapat pada bentuk SE, ukuran SE, jenis operasi pada morphological layer. Tuning kedua terdapat pada FC-Layer yaitu fungsi aktivasi (ReLu/Sigmoid/Tanh), dan jumlah neuron pada tiap hidden layer. Selain arsitektur, pada proses pelatihan juga dilakukan tuning pada learning rate, jumlah epoch, dan batch size. Terakhir adalah Output Layer, merupakan penentuan dari ekspresi berdasarkan dari bobot hidden layer. Di mana fungsi aktivasi yang digunakan untuk output layer adalah softmax yang artinya output berupa probabilitas dari tiap ekspresi. Kemudian untuk loss function yang akan digunakan adalah categorical crossentropy, dimana fungsi loss ini digunakan jika model memprediksi multi- kelas (multi-class prediction).","3.1 Alur Penelitian
     Gambar 3.1 menunjukkan metode penelitian. Terdapat 5 tahap utama yang akan dilakukan, yang pertama adalah studi literatur untuk menyusun bab 1 dan bab
2. Tahap kedua adalah pengumpulan citra ekspresi wajah (data citra berupa data primer dan data sekunder). Tahap ketiga adalah pembentukan dataset untuk tiap model (SVM, CNN, dan MNN), skenario pembentukan dataset dilakukan berdasarkan pada penelitian (Robert, 2023). Pada tahap keempat dilakukan pembentukan model, khusus untuk SVM dan CNN menggunakan model pada penelitian (Robert, 2023), sedangkan MNN menggunakan usulan pada penelitian ini. Tahap terakhir adalah pelatihan dan pengujian untuk semua model (SVM, CNN, MNN), terdapat tahap parameter tuning untuk tiap model. Kemudian semua performa dari tiap model akan dibandingkan satu sama lain, dan juga dianalisis pada bab 4. Gambar 3.1. Metode penelitian
3.2 Pengumpulan Citra Ekspresi Wajah
Citra ekspresi wajah dikumpulkan secara langsung oleh peneliti (data primer) dan juga menggunakan data yang dikumpulkan oleh peneliti lain (data sekunder). Terdapat 7 ekspresi wajah yang akan digunakan dalam penelitian ini, yaitu: marah, jijik, menghina, senang, sedih, kaget, dan netral (tanpa ekspresi). Gambar 3.2 menunjukkan contoh 7 ekspresi wajah manusia yang digunakan penelitian ini. Gambar 3.2. Contoh 7 jenis ekspresi wajah
Dataset primer akan dilakukan pengambilan citra ekspresi wajah mahasiswa Universitas Gunadarma baik pria maupun wanita. Pengambilan akan dilakukan dari beberapa sudut pandang guna menambah variasi dataset. Gambar 3.3 menunjukan contoh dataset primer dari berbagai sudut pandang. Gambar 3.3. Contoh dataset primer
Dataset sekunder digunakan dataset yang telah digunakan umum oleh peneliti lain terkait pengenalan ekspresi wajah. Terdapat beberapa dataset yang umum digunakan dalam penelitian ekspresi wajah. Pertama, Extended Cohn- Kanade (CK+) yang berisi citra ekspresi wajah pria dan wanita dari berbagai etnis dengan resolusi tinggi (Kanade, Cohn, & Tian, 2000; Lucey et al., 2010). Kedua, Taiwanese Facial Expression Image Dataset (TFEID) yang berisi citra ekspresi wajah pria dan wanita dari etnis Taiwan (Chen & Yen, 2007). Ketiga, Japanese Female Facial Expression (JAFFE) yang terdiri dari citra ekspresi wajah wanita etnis Jepang (Lyons, 2021; Lyons, Kamachi, & Gyoba, 2020). Gambar 3.4 menunjukan contoh citra dataset CK+ (a), JAFFE (b), dan TFEID (c). Gambar 3.4. Contoh dataset sekunder
Tabel 3.1 menunjukkan detail dari tiap dataset, mulai dari jumlah citra dari tiap kelas serta ruang warna dan ukuran citra. CK+ memiliki jumlah yang tidak seimbang pada kelas neutral dan memiliki ruang warna campur antara RGB dan Gray, dengan ukuran citra dikisaran 640 490. JAFFE dataset memiliki jumlah citra pada tiap kelas yang seimbang dengan perbedaan diantara 0 hingga 2 citra, ukuran citra 256 256, dan ruang warna grayscale. TFEID juga memiliki jumlah citra yang seimbang ditiap kelas, ukuran citra dikisaran 481 600, ruang warna RGB. 3.3 Pembentukan Dataset
Secara garis besar, dalam pembuatan model AI (khususnya ML dan DL) terdapat proses yang berperan penting, yaitu preprocessing dataset seperti ekstrasi fitur, penyesuaian ukuran citra, dan augmentasi (Deshmukh et al., 2016; Franchi et al., 2020; Mohammad & Ali, 2011; Ravi et al., 2020; Sawardekar & Naik, 2018; Shan et al., 2009). Pada penelitian (Robert, 2023), dilakukan sebuah skenario pembentukan dataset menggunakan beberapa metode pengolahan citra (seperti konversi warna ke grayscale, deteksi wajah, dan ekstrasi fitur), di mana preprocessing mempengaruhi performa dari model ML dan DL. Selain itu, pada penelitian (Alam & Yao, 2019) juga dilakukan penelitian yang serupa, di mana preprocessing mempengaruhi performa model machine learning. Pada tahap ketiga, dilakukan pembentukan dataset. Gambar 3.5 menunjukkan alur pembentukan dataset untuk SVM. Pertama, dilakukan pendeteksian wajah menggunakan VJA, proses ini berguna untuk mengurangi noise pada citra. Hasil VJA membuat ukuran citra bervariasi, oleh karena itu dilakukan resizing citra untuk menyamakan semua ukuran citra dan juga menyesuaikan dengan dimensi input model. Selanjutnya dilakukan konversi warna citra dari RGB ke Grayscale dikarena fitur warna tidak dibutuhkan dan agar dapat diekstrasi fiturnya menggunakan LMP. Terakhir, terdapat dua proses ekstrasi fitur berbeda. Proses ekstrasi fitur pertama menggunakan LMP (Robert, 2023). Proses ekstrasi fitur kedua adalah usulan dari penelitian ini, di mana pertama diaplikasikan Gabor Filter terlebih dahulu kemudian diekstrasi menggunakan LMP. Gambar 3.5. Pembentukan dataset untuk SVM
Gambar 3.6 menunjukkan alur pembentukan dataset untuk CNN dan MNN. Terdapat 3 proses yang akan dilakukan. Pertama, dilakukan deteksi wajah menggunakan VJA guna mengurangi noise. Kemudian dilakukan konversi warna dari RGB ke Grayscale karena fitur warna tidak dibutuhkan untuk mengenali ekspresi wajah. Terakhir, mengubah ukuran citra untuk menyamakan semua ukuran citra dan sesuai dengan dimensi input model. Skema pembentukan dataset ini berdasarkan performa terbaik dari penelitian (Robert, 2023). Gambar 3.6. Pembentukan dataset untuk CNN dan MNN
Dataset yang sudah melalui pembentukan dataset, dilakukan augmentasi dari sisi geometris seperti membalikan flipping secara horizontal dan vertikal, dan rotasi dari 0  hingga 45 . SVM training dataset digunakan untuk melatih model, sedangkan testing dataset digunakan untuk menguji dataset. CNN dan MNN terdapat pembagian lagi pada training, di mana 90% dari training dataset digunakan untuk melatih model dan 10% dari training dataset digunakan untuk validasi, dan testing dataset digunakan untuk menguji model. Visualisasi pembagian dataset
3.3.1 Deteksi wajah
Proses deteksi wajah menggunakan VJA, VJA memanfaatkan dua komponen yaitu integral image dan Haar Basis Function. Parameter ketiga adalah nilai threshold untuk masing-masing Haar yang digunakan untuk menentukan apakah Haar tersebut merupakan fitur wajah atau bukan. Output dari algoritma ini adalah berupa koordinat wajah yang dimulai pada koordinat [? Selain itu, jika semua nilai fitur lebih besar dari threshold maka detection window tersebut merupakan wajah, dan algoritma akan memberikan empat nilai yaitu ? Gambar 3.10 menunjukkan hasil dari implementasi algoritma deteksi wajah pada sebuah citra. Baris 2 dilakukan pembuatan matriks yang digunakan untuk menyimpan hasil. Image Resize (menggunakan Bicubic Interpolation)
Gambar 3.11 menunjukkan hasil perubahan ukuran citra wajah menggunakan Algoritma 3.3. Gambar 3.15 menunjukkan contoh hasil implementasi algoritma LMP pada citra wajah. 3.4 Pembentukan Model
3.4.1 Pembentukan Model SVM
Dalam penelitian sebelumnya (Robert, 2023), telah dilakukan pengenalan ekspresi wajah menggunakan SVM dengan menggunakan 4 kernel yaitu: Linear, Polynomial, Sigmoid, dan RBF. Berdasarkan dari penelitian (Robert, 2023), didapatkan model terbaik untuk mengenal ekspresi wajah adalah menggunakan kernel Sigmoid. Operasi pertama adalah operasi morgologi opening pada citra original, kemudian dilakukan pengurangan nilai piksel antara citra original dengan hasil opening, operasi opening itu sendiri adalah operasi erosi yang dilanjutkan operasi dilasi. Operasi kedua adalah erosi pada citra original, kemudian dilakukan pengurangan citra original terhadap hasil erosi. Ketiga adalah dilasi pada citra original, kemudian dilakukan pengurangan hasil dilasi terhadap citra original. Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? Baris 12 dilakukan pengambilan nilai terkecil yang digunakan untuk sebagai hasil fitur citra berdasarkan persamaan. Baris 25 dilakukan pengambilan nilai terbesar yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.14). Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? Baris 11 dilakukan pengambilan nilai terkecil yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.10). Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? Baris 11 dilakukan pengambilan nilai terbesar yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.8). 3.4.3.1.3 Percobaan dan Perbandingan Hasil Operasi Morfologi
Tabel 3.2 menunjukkan hasil operasi morfologi menggunakan SE berdasarkan 3.4.3.1 dan 3.4.3.2. Operasi Morfologi dan Structure Element pada Persegi Panjang
Berdasarkan dari Tabel 3.3 dapat dilihat operasi GMO dengan SE disk 3 3 menghasilkan corner pada persegi panjang. Operasi GMD dengan SE disk 3 3 memiliki hasil garis berbentuk persegi panjang dengan nilai tiap corner terdapat hilang. Operasi GMD dengan SE kotak 3 3 menghasilkan garis yang berbentuk persegi panjang. GME dengan SE kotak 3 3 memiliki hasil yang mirip dengan GMD, namun memiliki luas yang berbeda. Dari hasil yang didapatkan, tiap ukuran memiliki hasil yang mirip dengan operasi yang digunakan. Operasi GMD dan GME memiliki hasil yang mirip dengan perbedaan tepi pada GMD menebal kearah luar persegi, sedangkan tepi pada GME menebal kearah dalam. Tabel 3.4 merupakan hasil morfologi menggunakan citra wajah dengan warna grayscale. Structure Element dan Operasi Morfologi pada Wajah
Berdasarkan Tabel 3.4, dapat dilihat hasil ekstrasi fitur menggunakan berbagai kombinasi dua bentuk dan lima ukuran. SE kotak 5 5 juga memiliki hasil yang sama dengan disk 5 5 dengan perbedaan yang tidak dapat dilihat kasat mata. SE kotak 7 7 juga memiliki hasil yang sama dengan disk 7 7 dengan perbedaan yang tidak dapat dilihat kasat mata. SE disk 9 9 dengan operasi GMO fitur yang didapatkan tidak akurat karena tidak mengekstrasi tepi (bentuk) dari ekspresi wajah. Selain itu luas pada lubang hidung dan mulut juga lebih luas dibandingkan dengan SE 5 5, dan memiliki garis tepi yang lebih tebal. SE kotak 9 9 dengan operasi GMO, fitur yang didapatkan mirip dengan disk 9 9 dengan operasi GMO namun memiliki fitur yang lebih jelas terlihat. SE kotak 9 9 dengan operasi 9 9 GME fitur yang didapatkan terlihat dengan jelas dan mirip dengan hasil yang menggunakan SE disk 9 9 dengan operasi GME. Namun terdapat perbedaan fitur yang signifikan pada bagian hidung, di mana bentuk dari hidung sedikit berubah menjadi sedikit kotak. SE kotak 9 9 dengan operasi GMD, hasil yang didapatkan hampir sama dengan hasil yang menggunakan SE disk 9 9 dengan operasi GMD, hanya terdapat perbedaan pada tebal garis pada hidung. Operasi menggunakan SE 15 15, hasil yang didapatkan memiliki pola yang sama dengan operasi yang menggunakan SE 9 9. SE disk atau kotak 15 15 dengan operasi GMO fitur lebih terlihat jelas dibandingkan dengan ukuran 9 9. SE disk atau kotak 15 15 dengan operasi GME fitur lebih tebal, namun untuk SE kotak bagian hidung menjadi lebih kotak dari ukuran sebelumnya. Berdasarkan dari hasil yang didapatkan dan analisis. SE disk 5 5 atau kotak 5 5 dengan operasi GMD memiliki hasil terbaik. Tepi pada bagian mata tergabung, bagian mulut tidak menjadi lebih luas. Namun pada bagian hidung menjadi lebih kecil dibandingkan citra original. Selain itu operasi menggunakan GME dengan SE disk 5 5 atau kotak 5 5 juga memberikan hasil yang baik. Tepi dari tiap komponen wajah terlihat namun tepi pada mata dan bola mata tergabung, namun memberikan hasil ekstrasi fitur bagian hidung lebih baik dibandingkan GMD. Karena luas hidung lebih sesuai pada GME dibandingkan GMD. 3.4.3.2 Pembentukan dan Pelatihan Model
Gambar 3.18 menunjukkan arsitektur dari MNN secara garis besar. Di mana terdapat 5 layer utama yang memiliki tugas masing-masing. Di mana terdapat beberapa variasi arsitektur Variasi pertama terdapat pada morphology layer, di mana akan digunakan dua jenis ekstrasi fitur berdasarkan pada hasil subbab 3.4.3.3. Variasi kedua terdaoat pada hidden layer, di mana akan digunakan beberapa kombinasi fully-connected layer. Gambar 3.18. Model MNN yang diusulkan
Input Layer adalah layer pertama dari model MNN yang bertugas untuk menerima input berupa citra. Di mana dimensi dari intput layer itu sendiri sesuai dengan ukuran citra pada dataset yaitu 160 160. Kedua adalah morphology layer. Menerima masukan dari input layer kemudian dilakukan proses morfologi. Terdapat dua jenis morfologi layer yang akan digunakan. Morfology layer pertama adalah dilation layer kemudian subtraction layer. Di mana lapisan morfologi jenis pertama menggunakan hasil terbaik dari operasi morfologi dilasi pada subbab 3.4.3.3. Morphology layer jenis kedua adalah erosion layer dilanjutkan subtraction layer. Di mana lapisan morfologi jenis kedua ini menggunakan hasil terbaik dari operasi morfologi erosi pada subab 3.4.3.3. Lapisan ketiga adalah flatten layer, lapisan yang bertugas mengubah citra menajadi feature vector. Masukan dari lapisan ini adalah hasil dari subtraction layer pada lapisan morfologi. Di mana hasil dari subtraction layer adalah citra dengan ukuran 160 160 yang kemudian diubah mnejadi satu dimensi yaitu 25600. Lapisan keempat adalah Fully Connected Layer (FC Layer) yang bertugas untuk mempelajari dan menganalisa nilai feature vector dari flatten layer. Pada lapisan ini dilakukan beberapa konfigurasi FC Layer mulai dari jumlah FC Layer, dan jumlah Neuron pada FC Layer. Konfigurasi pertama akan diuji coba 2 FC Layer dengan masing-masing neuron adalah 512, dan 256. Konfigurasi kedua akan dicoba 1024, dan 512. Kemudian dari situ akan dicoba analisis mana yang lebih baik sehingga dapat dikonfigurasi lebih lanjut. Jika konfigurasi pertama memiliki hasil lebih baik artinya memungkinkan FC Layer untuk dibuat lebih sederhana dengan mengurangi jumlah neuron. Jika konfigurasi kedua lebih baik artinya terdapat kemungkinan untuk meninkatkan performa dari model karena dataset memiliki kompleksitas tinggi. Beberapa lapisan akan dilakukan tuning paramter. Tuning pertama terdapat pada bentuk SE, ukuran SE, jenis operasi pada morphological layer. Tuning kedua terdapat pada FC-Layer yaitu fungsi aktivasi (ReLu/Sigmoid/Tanh), dan jumlah neuron pada tiap hidden layer. Selain arsitektur, pada proses pelatihan juga dilakukan tuning pada learning rate, jumlah epoch, dan batch size. Terakhir adalah Output Layer, merupakan penentuan dari ekspresi berdasarkan dari bobot hidden layer. Di mana fungsi aktivasi yang digunakan untuk output layer adalah softmax yang artinya output berupa probabilitas dari tiap ekspresi. Kemudian untuk loss function yang akan digunakan adalah categorical crossentropy, dimana fungsi loss ini digunakan jika model memprediksi multi- kelas (multi-class prediction)."
Tatya Atyanti Paramastri_Kualifikasi.txt,3.1. alur penelitian 1. identifikasi masalah gambar 3.1 alur penelitian identifikasi masalah dilakukan supaya permasalahan yang diangkat jelas. identifikasi masalah dilakukan dengan cara melihat permasalahan nyata melalui literatur seperti jurnal penelitian wawancara dengan ahli dan keresahan yang dirasakan oleh peneliti secara pribadi. permasalahan yang diangkat pada penelitian ini adalah motif batik indonesia sangat beragam dan memiliki maknanya masingmasing. namun tidak banyak masyarakat yang masih mengetahui nama makna dan pemakaian dari masingmasing motif batik. menurut dewan ahli ppbi paguyuban pecinta batik indonesia sekar jagad ibu mari s. condronegoro saat ini sering kali ditemukan kesalahan dalam pemakaian motif batik seperti mengenakan kain yang seharusnya digunakan pada upacara kematian ketika menghadiri acara pernikahan. solusi yang diusulkan adalah melakukan klasifikasi motif batik. metode klasifikas yang umum digunakan adalah cnn. namun cnn klasik memiliki kelemahan dalam memahami makna menyeluruh dari gambar terutama yang berkaitan dengan hubungan antar bagian gambar yang berbeda. selain itu cnn klasik juga rentan terhadap overfitting di mana model terlalu terlatih pada data pelatihan dan tidak dapat menggeneralisasi dengan baik ke data baru. 2. studi literatur studi literatur dilakukan supaya penelitian memiliki landasan yang jelas. studi literatur dilakukan dengan sumber jurnal penelitian terdahulu serta buku yang berisikan metode yang sesuai dengan penelitian. fokus studi literatur terbagi menjadi tiga topik yaitu klasifikasi motif batik komputasi kuantum dan deteksi tepi. 3. pengumpulan dataset pengumpulan data dilakukan berdasarkan keperluan penelitian. data yang dikumpulkan merupakan data primer yang akan dikumpulkan degan bantuan ahli yaitu dewan ahli ppbi sekar jagad ibu mari s. condronegoro. ppbi sekar jagad merupakan perkumpulan pecinta batik yang diawasi langsung penasehat utama oleh permaisuri gusti kanjeng ratu hemas istri dari sri sultan hamengku buwono x sehingga informasi yang didapatkan bisa dijamin kebenarannya. pengumpulan dataset primer ini dilakukan dengan diskusi wawancara serta bimbingan dewan ahli ppbi sekar jagad supaya dataset yang digunakan sesuai dengan kebenaran dan kebutuhan penelitian yang dilakukan. sehingga hasil yang didapatkan memuaskan dan akurat. dataset yang akan dikumpulkan merupakan citra motif batik daur hidup yogyakarta dari kain batik tradisional yang merupakan batik cap maupun batik tulis bukan printing. motif yang akan digunakan dalam penelitian akan didiskusikan terlebih dahulu dengan narasumber supaya dapat mewakili batik daur hidup yogyakarta yang sangat penting untuk diketahui dalam bersosialdi masyarakat. misalnya seperti motif batik yang memiliki makna khusus dan tak pantas untuk dikenakan pada berapa acara motif batik yang memiliki larangan dan lainnya. hal ini dilakukan karena batik daur hidup yogyakarta tercatat memiliki ratusan motif hingga tahun 2006 sekar jagad 2015. gambar 3.2 gawangan kain citra batik akan diambil dengan menggunakan kamera dimana kain akan dibentangkan pada gawangan untuk difoto di dalam ruangan dengan pecahayaan yang sama dan di luar ruangan dengan cuaca yang sama. citra batik akan diambil dari berbagai posisi supaya citra lebih beragam. kemudian motif batik yang akan diambil beragam namun akan dipisahkan terlebih dahulu berdasarkan jenisnya. hal ini dikarenakan untuk satu kelompok motif yang sama terkadang terdapat bentuk yang terlihat berbeda. sehingga dibutuhkan pengujian bertahap untuk melihat apakah model dapat mendeteksi motif dengan benar. a b c gambar 3.3. a motif parang gondosuli b motif batik parang barong c motif batik parang kusuma data primer digunakan karena terdapat kekurangan dari data sekunder yang dapat ditemukan. seperti motif batik yang terpotong sehingga tidak terlihat serta motif yang salah pada beberapa kelas. beberapa motif juga memiliki bentuk atau komponen serupa sehingga butuh dikonsultasikan lebih lanjut kepada ahli. a b gambar . a motif parang yang terpotong b motif batik ceplok namun memiliki konponen parang 4. prapengolahan data proses prapengolahan data dilakukan untuk menyiapkan data sebelum diimplementasikan dalam model klasifikasi citra. prapengolahan data meliputi resize mengubah ruang warna menjadi grayscale augmentasi dan split data. resize dilakukan untuk memperkecil ukuran gambar aslinya. hal ini dilakukan untuk memastikan semua citra memiliki ukuran yang sama sehingga algoritma dapat bekerja secara konsisten dan efisien. selain itu ukuran citra yang lebih kecil dapat mempercepat proses segmentasi dan klasifikasi tanpa kehilangan informasi penting. proses selanjutnya adalah mengubah ruang warna menjadi grayscale perubahan warna ini dilakukan karena dapat meningkatkan kontras meningkatkan efisiensi komputasi dan meningkatkan ketahanan terhadap variasi pencahayaan. citra grayscale memiliki rentang intensitas yang lebih kecil dibandingkan citra rgb sehingga kontras tepi lebih jelas. selain itu citra grayscale membutuhkan lebih sedikit memori dan sumber daya komputasi dibandingkan citra rgb. kemudian citra grayscale tidak terpengaruh oleh variasi pencahayaan sehingga tepi dapat dideteksi dengan lebih akurat. selanjutnya proses augmentasi digunakan untuk meningkatkan ukuran dataset dengan menambahkan data baru tanpa perlu melakukan pengumpulan data baru. hal ini bermanfaat untuk mengatasi masalah keterbatasan data. selain itudengan melakukan augmentasi data akan menjadi lebih bervariasi sehingga dapat mencegah terjadinya overfitting dan lebih stabil terhadap perubahan data. proses terakhir adalah melakukan pembagian data. data akan dibagi menjadi tiga dataset yaitu pelatihan pengujian dan validasi. 5. komputasi kuantum model komputasi kuantum merupakan usulan dalam penelitian ini. adapun kombinasi model pertama yang akan dilakukan meliputi segmentasi tepi berbasis kuantum dengan menggunakan metode canny dan klasifikasi dengan metode quantum convolutional neural network qcnn. komputasi kuantum diterapkan mulai dari proses segmentasi karena berdasarkan penelitian terdahulu deteksi tepi berbasis kuantum dapat mendeteksi lebih banyak tepi dibandingkan deteksi tepi klasik berdasarkan jumlah tepi yang dihasilkan sundani dkk. 2019. hal serupa juga berlaku pada metode qcnn yang memiliki hasil lebih baik dan akurat dibandingkan dengan metode cnn klasik. sehingga hipotesisnya hasil dari ekstraksi fitur dan klasifikasi akan menjadi lebih optimal. gambar 3.2 perbandingan hasil deteksi tepi berbasis kuantum dan klasik sundani dkk. 2019 6. komputasi kuatum dan klasik kombinasi model berikutnya adalah melakukan deteksi tepi berbasis kuatum dengan model canny. kemudian klasifikasi dilakukan dengan menggunakan cnn klasik. kombinasi ini dilakukan untuk mengetahui seberapa jauh pengaruh penggunaan komputasi kuantum pada deteksi tepi dengan model canny. 7. komputasi klasik dan kuantum komputasi klasik dan kuantum disini adalah kombinasi antara deteksi tepi klasik dengan model canny yang kemudian dilanjutkan dengan melakukan klasifikasi dengan menggunakan metode quantum convolutional neural network qcnn. hal ini dilakukan untuk melihat seberapa jauh pengaruh dari penerapan model qcnn yang menggunakan komputasi kuantum. 8. komputasi klasik pengolahan data dengan komputasi klasik dilakukan dengan menggunakan deteksi tepi canny kasik yang dikombinasikan dengan klasifikasi cnn klasik. pengolahan data ini dilakukan sebagai pembanding performa model segmentasi dan klasifikasi berbasis kuantum. pengolahan data kedua model komputasi kuantum dan komputasi klasik akan dilakukan dengan menggunakan komputer yang sama yaitu komputer klasik. 9. evaluasi performa tahap terakhir adalah melakukan evaluasi performa. performa akan dibandingkan dari akurasi yang dihasilkan. adapun akurasi akan dihitung menggunakan confusion matrix pada kedua model. evaluasi ini akan dilakukan pada keempat kombinasi model untuk mengetahui seberapa jauh perbedaan dan fungsi penerapan komputasi kuantup pada setiap model.,permasalahan yang diangkat pada penelitian ini adalah motif batik indonesia sangat beragam dan memiliki maknanya masingmasing. namun tidak banyak masyarakat yang masih mengetahui nama makna dan pemakaian dari masingmasing motif batik. menurut dewan ahli ppbi paguyuban pecinta batik indonesia sekar jagad ibu mari s. condronegoro saat ini sering kali ditemukan kesalahan dalam pemakaian motif batik seperti mengenakan kain yang seharusnya digunakan pada upacara kematian ketika menghadiri acara pernikahan. solusi yang diusulkan adalah melakukan klasifikasi motif batik. fokus studi literatur terbagi menjadi tiga topik yaitu klasifikasi motif batik komputasi kuantum dan deteksi tepi. sehingga hasil yang didapatkan memuaskan dan akurat. dataset yang akan dikumpulkan merupakan citra motif batik daur hidup yogyakarta dari kain batik tradisional yang merupakan batik cap maupun batik tulis bukan printing. motif yang akan digunakan dalam penelitian akan didiskusikan terlebih dahulu dengan narasumber supaya dapat mewakili batik daur hidup yogyakarta yang sangat penting untuk diketahui dalam bersosialdi masyarakat. selain itu ukuran citra yang lebih kecil dapat mempercepat proses segmentasi dan klasifikasi tanpa kehilangan informasi penting. proses selanjutnya adalah mengubah ruang warna menjadi grayscale perubahan warna ini dilakukan karena dapat meningkatkan kontras meningkatkan efisiensi komputasi dan meningkatkan ketahanan terhadap variasi pencahayaan. selanjutnya proses augmentasi digunakan untuk meningkatkan ukuran dataset dengan menambahkan data baru tanpa perlu melakukan pengumpulan data baru. komputasi kuantum diterapkan mulai dari proses segmentasi karena berdasarkan penelitian terdahulu deteksi tepi berbasis kuantum dapat mendeteksi lebih banyak tepi dibandingkan deteksi tepi klasik berdasarkan jumlah tepi yang dihasilkan sundani dkk. 2019. hal serupa juga berlaku pada metode qcnn yang memiliki hasil lebih baik dan akurat dibandingkan dengan metode cnn klasik. sehingga hipotesisnya hasil dari ekstraksi fitur dan klasifikasi akan menjadi lebih optimal. gambar 3.2 perbandingan hasil deteksi tepi berbasis kuantum dan klasik sundani dkk. pengolahan data ini dilakukan sebagai pembanding performa model segmentasi dan klasifikasi berbasis kuantum. pengolahan data kedua model komputasi kuantum dan komputasi klasik akan dilakukan dengan menggunakan komputer yang sama yaitu komputer klasik. performa akan dibandingkan dari akurasi yang dihasilkan.
Tia Haryanti_Kualifikasi.txt,3.1 kerangka umum penelitian ini bertujuan untuk mengembangkan sistem deteksi dini kantuk sebelum berkendara dengan menggunakan kombinasi data visual berupa data citra wajah dan data fisiologis. kondisi predriving mengacu pada kondisi sebelum pengemudi memulai perjalanan sehingga sistem ini sangat penting untuk mencegah risiko kecelakaan di jalan . sistem ini mengintegrasikan teknologi pengenalan wajah dan analisis data fisiologis untuk memberikan deteksi yang lebih akurat. blok d iagram secara umum yang digunakan pada penelitian ini dapat dilihat pada gambar 3.1 blok diagram. objek preprocessing data fisiologis ekstrasi fiturpenggabungan fitur klasifikasi data image data visual kantuk ya tidak gambar 3.1 blok diagram model ini terdiri dari tiga tahapan yaitu input proses dan output . penelitian deteksi dini kantuk untuk kondisi predriving menggabungkan data visual yaitu pengumpulan data citra wajah pengemudi yang diambil menggunakan kamera serta data fisiologis yang diukur berupa data ekg menggunakan perangkat wearable yaitu smartwatch dan pulse oximeter untuk mengukur saturasi oksigen spo2 . tahapan preprocessing dan ekstraksi fitur dilakukan pada kedua jenis data yaitu data citra gambar dan data fisiologis. model convolutional neural network cnn digunakan untuk mengekst raksi fitur dari data citra wajah yang merupakan data visual sementara long short term memory lstm digunakan untuk memproses data fisiologis yang bersifat timeseries. fitur fitur yang diekstraksi dari kedua model ini digabungkan untuk menghasilkan vect or fitur gabungan. vektor fitur ini kemudian digunakan sebagai input untuk model support vector machine 43 svm yang melakukan klasifikasi akhir untuk mendeteksi kantuk. hasil deteksi kemudian digunakan untuk memberikan peringatan kepada pengemudi layak tidak nya pengemudi untuk berkendara. 3.2 tahapan peneletian tahapan penelitian merupakan urutan atau langkah langkah yang dilakukan secara terstruktur dan sistematis pada penelitian ini secara garis besar terbagi menjadi empat tahapan. berikut adalah gambar 3.2 tahapan penelitian yang dilakukan pada penelitian ini. pengumpulan data data visualdata fisiologis pemilihan dan persiapan dataset preprocessing data pembuatan modelekstraksi fitur penggabungan fitur evaluasi pemisahan dataset pembangunan model pelatihan model evaluasi model implementasi gambar 3. 2 tahapan penelitian 44 3.3. pemilihan dan persiapan dataset tahapan ini merupakan tahapan identifikasi awal dari penelitian meliputi identifikasi masalah penelitian yang berfokus pada masalah utama yaitu mendeteksi kantuk pada pengemudi menggunakan pemrosesan citra dan fisiologis . tahapan ini dilakukan untuk memasikan bahwa hanya data yang relevan berkualitas tinggi dan siap untuk dip roses lebih lanjut yang digunakan. pemilihan dataset memastikan bahwa dataset yang dikumpulkan relevan dengan tujuan penelitian yaitu hanya menggunakan data yang berkaitan dengan kondisi predriving serta memastikan bahwa data visual dan data fisiologis diambil pada waktu yang sama. tahapan pengumpulan data dan preprocessing data merupakan tahap awal untuk mempersiapkan dataset yang akan digunakan. 3.3.1 pengumpulan data data dibagi menjadi dua kategori utama yaitu data primer dan data sekunder . data primer diperoleh berdasarkan pengumpulan dan pengamatan langsung oleh peneliti berdasarkan kondisi subjek penelitian dan rekaman aktivitas fisik atau ekspresi wajah menggunakan kamera serta pengukuran fisiologis yang menggunakan perangkat wearable . data primer ini berupa data objektif dengan mengumpulkan data citra wajah dan pengukuran fisiologis . berikut merupakan gambar 3. 3 pengumpulan data. matapengumpulan data data visual citra wajahdata fisiologis ekgsaturasi oksigen spo2kamera smartwatch pulse oximetry gambar 3 .3 pengumpulan data dataset visual berupa citra wajah yang berfokus pada wajah pengemudi yang diambil menggunakan kamera dengan spesifikasi 12 mp. data visual dan fisiologis berupa data yang diambil dari partisipan dalam kondisi terjaga dan mengantuk . data fisiologis mencakup pengukuran langsung dari respons tubuh berupa sinyal 45 ekg elektrokardiogram yang merekam detak jantung hr variabilitas detak jantung atau heart rate variability hrv menggunakan perangkat wearable dan pengukuran saturasi oksigen dalam darah spo2 yang diukur menggunakan pulse oximeter. 3.3.2 preprocessing data melakukan analisis eksploratif data untuk memahami karakteristik dataset sehingga meningkatkan kualitas deteksi . preprocessing yang dilakukan yaitu pre processing citra dan preprocessing data fisiologis. preprocessing citra yaitu dengan mendeteksi wajah dan mata normalisasi pencahayaan pemotongan area wajah yang relevan. ektraksi frame dari video menggunakan opencv . pre processing data fisiologis yaitu dengan normalisasi data dan segmentasi. berikut merupakan gambar 3. 4 preprocessing data. data acquisitionfacial landmark detectionroi extraction gambar dari kameraeye detection data fisiologis ekg spo2 noise removal normalizationsave processed datasegmentationresize imagesnormalize pixel value data augmentation gambar 3. 4 preprocessing data data set yang dikumpulkan kemudian diolah yang meliputi normalisasi penghilangan noise dan teknik pra pemrosesan lainnya untuk membuat data siap digunakan dalam ekstraksi fitur. langkah ini melibatkan pembersihan dan penyiapan data untuk analisis. proses preprocessing untuk data visual atau data gambar yaitu 1. pengumpulan data visual dengan mengambil gambar wajah pengemudi menggunakan kamera berfo kus pada mata. 2. deteksi wajah dan deteksi mata menggunakan algoritma deteksi wajah seperti haar cascades atau dlib untuk mendetekasi dan melokalisasi wajah dalam gambar. 3. deteksi mata yaitu mendeteksi mata daam area wajah yang terdeteksi. 46 4. ekstraksi roi region of interest dengan mengambil area mata dari gambar. 5. teknik normalisasi untuk mengubah ukuran gambar mata menjadi dimensi yang konsisten missal nya 64x64 pixel serta menormalisasi nilai pixel gambar dalam rentang 0 1 atau 1 1. 6. augmentasi gambar dilakukan untuk meningkatkan variasi data seperti rotasi flipping horizontal atau vertikal zooming dan perubahan cahaya 7. penyimpanan data yang diproses dengan menyimpan gambar yang telah diproses dan fitur yang diekstraski dalam format terstruktur csv atau database . proses preprocessing untuk data fisiologis yaitu 1. pengumpulan data fisiologis menggunakan wearable untuk merekam detak jantung hr variabilitas detak jantung hrv dan saturasi oksigen spo2. 2. pembersihan data dengan menghilangkan noise dengan menggunakan teknik filtering dan imputasi data hilang dengan mengisi data yang hilang menggunakan metode seperti mean median atau interpolasi. 3. normalisasi data dengan min max sehingga menyesuaikan denga n skala data ke rentang yang konsisten 0 1. 4. segmentasi data dilakukan dengan membagi data menjadi segmen dengan ukuran waktu tetap yaitu 30 detik. 5. normalisasi data untuk memastikan konsistensi skala antar subjek dan pengukuran. 6. penyimpanan data yang d iproses yaitu data fisiologis dalam format terstruktur. langkah selanjutnya yaitu sinkronisasi data dengan menggabungkan data visual serta data fisiologis berdasarkan timestamp. selanjutnya memastikan bahwa data visual dan fisiologis yang telah disinkronk an mencerminkan kondisi yang sama pada waktu yang sama. selanjutnya yaitu menyimpan data yang telah disinkronkan dalam format yang mudah diakses untuk dianalisis lebih lanjut. 3.4. pembuatan model pembuatan model merupakan proses implementasi dari desain arsitektur yang telah direncanakan . langkah dari pembuatan model yaitu penulisan kode untuk membangun model sesuai dengan desain arsitektur yaitu cnn lstm dan svm. 47 selanjutnya mengonfigurasi model dengan optimizer fungsi loss dan metrik evaluasi. kemu dian melakukan pelatihan model menggunakan dataset yang telah dibagi menjadi training set dan validation set pada tahapan preprocessing . selanjutnya dilakukan validasi serta tuning hyperparameters untuk mengoptimalkan kinerja model. 3.4.1 ekstraksi fitur ekstraksi fitur dilakukan untuk menangkap karakteristik penting dari data yang telah diproses. fitur ini akan digunakan sebagai input untuk model pembelajaran mesin. ektraksi fitur dilakukan pada data visual berupa data gambar dan data fisiologis. 1. data v isual a. eye aspect ratio ear digunakan untuk mendeteksi apakah mata terbuka atau tertutup. di mana pi adalah titik titik landmark mata. b. pupil dilation digunakan untuk mengukur perubahan ukuran pupil. c. redness of eyes mengukur tingkat kemerahan pada ma ta. d. eye openess mengukur bukaan mata berdasarkan jarak vertikal antara kelopak mata atas dan bawah. 2. data fisiologis a. heart rate hr mengukur detak jantung per menit. b. heart rate variability hrv mengukur variabilitas detak jantung. c. respiratory rate rr mengukur laju pernapasan. d. spo2 saturasi oksigen dalam darah ekstraksi fitur dengan convolutional neural network cnn adalah proses yang menggunakan lapisan konvolusi dan pooling untuk menangkap fitur penting dari data gambar. langkah langkah ekstraksi fitur dengan cnn 1. convolutional layer menggunakan filter untuk menangkap fitur spasial dari gambar. 48 2. pooling layer mengurangi dimensi peta fitur sambil mempertahankan fitur penting. 3. fully connected layer menghubungkan peta fitur yang telah diratakan untuk melakukan klasifikasi atau ekstraksi fitur. 4. pelatihan model menyesuaikan bobot filter melalui backpropagation dengan data latih. 5. ekstraksi fitur menggunakan model yang telah dilatih untuk mengekstraksi fitur dari gambar baru. ekstraksi fitur dengan long short term memory lstm adalah proses yang menggunakan jaringan lstm untuk menangkap pola temporal dan hubungan jangka panjang dalam data sekuensial seperti data fisiologis ekg hr hrv rr dan spo2. lstm sangat efektif dal am menangani data yang memiliki ketergantungan waktu. ekstraksi fitur dengan lstm melibatkan beberapa langkah penting 1. menyiapkan data menyiapkan data sekuensial dalam bentuk yang sesuai untuk input ke lstm. 2. membangun model lstm membangun model lstm dengan lapisan lstm dan dense untuk ekstraksi fitur. 3. melatih model lstm melatih model menggunakan data sekuensial untuk menyesuaikan bobot jaringan. 4. ekstraksi fitur menggunakan model yang telah dilatih untuk mengekstraksi fitur dari data seku ensial baru. 3.4.2 penggabungan fitur fitur fitur yang telah diekstraksi dari ekstraksi fitur dengan model cnn yaitu dari gambar visul dengan mengekstraksi bagian mata dan ektraksi fitur dari data sekuensial dengan menggunakan lstm berupa data fisiologis . selanju tnya penggabungan fitur visual dan fitur sekuensial menggunakan metode penggabungan concatenation digabungkan membentuk satu set fitur komprehensif yang akan digunakan untuk pelatihan model yaitu klasifikasi akhir menggunakan model sv m. 49 3.4.3 pemisahan dataset pembagian dataset merupakan langkah penting dalam proses pelatihan dan evaluasi model. merujuk pada penelitian li k. gong y. ren z. 2020 untuk pembagian dataset dibagi menjadi tiga bagian yaitu training set 40 validation set 10 dan test set 50 namun pada penelitian ini pembagian dataset yang terdiri dari data gambar dan data fisiologis dibagi menjadi berikut 1. training set 75 data yang digunakan untuk melatih melatih model . 2. validation set 15 digu nakan untuk tuning hyperparameters dan memilih model terbaik . 3. test set 15 digunakan untuk mengevaluasi kinerja akhir model . 3.4.4 desain arsitektur desain a rsitektur merupakan proses menentukan struktur dan komponen model yang akan dibangun yang terdiri dari jenis model jumlah dan jenis layer fungsi aktivasi teknik regularisasi dan konfigurasi model. jenis model penelitian ini melibatkan dua model utama yaitu convolutional neural network cnn untuk data visual dan long short term memory lstm untuk data fisiologis. hasil dari kedua model digabungkan dan diklasifikasikan menggunakan support vector machine svm. model ini terdiri dari tiga tahapan yaitu akuisisi data pre processing data ekstraksi fitur penggabungan fitur dan klasifikasi dengan svm dan output sistem . berikut merupakan gambar 3. 5 arsitektur model. ear pupil dilation redness of eyes eye openesspreprocessing alignmentmata akuisisi databehavioral fisiologis ekstraksi fiturklasifikasidriver mengantuk alarm atau notifikasi tidak layak alarm atau notifikasi layak yatidak perangkat sensor wearablemembaca hrv hr dan spo2object cnn lstmpenggabungan fitur ekstraksi gambar 3. 5 arsitektur model tahap ini mencakup perancangan arsitektur cnn yang akan digunakan termasuk pemilihan jumlah dan jenis layer fungsi aktivasi dan teknik regularisasi. 50 digunakan untuk mengolah data visual seperti mengenali mata tertutup atau mulut menguap sebagai indikator kantuk. lstm digunakan untuk menga nalisis data fisiologis yang berurutan seperti pola detak jantung yang m enunjukkan kelelahan atau penurunan kewaspadaan. menggabungkan fitur yang diekstrak dari cnn dan lstm untuk mendapatkan representasi data yang komprehensif memastikan bahwa model dapat mengidentifikasi kantuk berdasarkan kombinasi indikator visual dan fis iologis. selanjutnya yaitu menggunakan support vector machines svm untuk mengklasifikasikan data sebagai kantuk atau tidak kantuk. svm dipilih karena kemampuannya dalam mengklasifikasikan data yang kompleks dan memberikan batas keputusan yang jelas layak atau tidak layak pengemudi untuk berkendara. jika pengklasifikasi mendeteksi keadaan mengantuk maka pengklasifikasi menghasilkan alarm atau notifikasi pemberitahuan untuk memberi tahu bahwa pengemudi tidak layak untuk berkendara atau kembali ke f ase pertama dan memulai ulang prosedur. 3.4.5 pelatihan model dengan dataset pelatihan m odel dilakukan dengan menggunakan training set dengan tuning hyperparamaters berdasarkan kinerja pada validation set. pelatihan model dilakukan dengan model svm menggunakan training set. 3.5 evaluasi model gabungan ini dievaluasi menggunakan metrik seperti akurasi presisi recall dan f1score untuk memastikan performa dan keandalannya. implementasi sistem ini diharapkan dapat memberikan notifikasi atau peringatan kepada pengemudi jika tanda tanda kantuk terdeteksi selama kondisi predriving sehingga dapat meningkatkan keselamatan berkendara secara signifikan. berdasarkan hasil validasi model dapat ditune atau dioptimalkan untuk meningkatkan performa misalnya dengan mengubah arsitektur parameter atau teknik training . 3.6 implementasi setelah penyempurnaan model dianggap siap untuk digunakan. model ini harus dapat secara akurat mendeteksi kantuk pengemudi dalam berbagai kondisi dengan minimal kesalahan. langkah selanjutnya yaitu penerapan model dalam sistem nyata dan pemantauan efektivitasnya dalam kondisi pengemudi pada 51 lingkungan predriving. model yang telah dioptimalkan diintegrasikan ke dalam sistem deteksi dini kantuk untuk pengujian awal. selanjutn ya yaitu m elakukan uji coba lapangan untuk mengevaluasi efektivitas sistem dalam kondisi nyata memungkinkan pengumpulan feedback untuk perbaikan lebih lanjut. 3.7 rencana kegiatan tabel 3.1 rencana kegiatan no nama kegiatan bulan 1 2 3 4 5 6 7 8 9 10 11 12 1 kajian literatur 2 perencanaan penelitian 3. pengumpulan data 4. prapemrosesan data 5. pembuatan model 6. pelatihan dan evaluasi model 7. penyusunan laporan akhir 8. presentasi laporan akhir 9. publikasi jurnal ilmiah internasional 10. pengajuan hki,3.1 kerangka umum penelitian ini bertujuan untuk mengembangkan sistem deteksi dini kantuk sebelum berkendara dengan menggunakan kombinasi data visual berupa data citra wajah dan data fisiologis. kondisi predriving mengacu pada kondisi sebelum pengemudi memulai perjalanan sehingga sistem ini sangat penting untuk mencegah risiko kecelakaan di jalan . sistem ini mengintegrasikan teknologi pengenalan wajah dan analisis data fisiologis untuk memberikan deteksi yang lebih akurat. blok d iagram secara umum yang digunakan pada penelitian ini dapat dilihat pada gambar 3.1 blok diagram. objek preprocessing data fisiologis ekstrasi fiturpenggabungan fitur klasifikasi data image data visual kantuk ya tidak gambar 3.1 blok diagram model ini terdiri dari tiga tahapan yaitu input proses dan output . penelitian deteksi dini kantuk untuk kondisi predriving menggabungkan data visual yaitu pengumpulan data citra wajah pengemudi yang diambil menggunakan kamera serta data fisiologis yang diukur berupa data ekg menggunakan perangkat wearable yaitu smartwatch dan pulse oximeter untuk mengukur saturasi oksigen spo2 . tahapan preprocessing dan ekstraksi fitur dilakukan pada kedua jenis data yaitu data citra gambar dan data fisiologis. model convolutional neural network cnn digunakan untuk mengekst raksi fitur dari data citra wajah yang merupakan data visual sementara long short term memory lstm digunakan untuk memproses data fisiologis yang bersifat timeseries. fitur fitur yang diekstraksi dari kedua model ini digabungkan untuk menghasilkan vect or fitur gabungan. vektor fitur ini kemudian digunakan sebagai input untuk model support vector machine 43 svm yang melakukan klasifikasi akhir untuk mendeteksi kantuk. hasil deteksi kemudian digunakan untuk memberikan peringatan kepada pengemudi layak tidak nya pengemudi untuk berkendara. pengumpulan data data visualdata fisiologis pemilihan dan persiapan dataset preprocessing data pembuatan modelekstraksi fitur penggabungan fitur evaluasi pemisahan dataset pembangunan model pelatihan model evaluasi model implementasi gambar 3. 2 tahapan penelitian 44 3.3. pemilihan dan persiapan dataset tahapan ini merupakan tahapan identifikasi awal dari penelitian meliputi identifikasi masalah penelitian yang berfokus pada masalah utama yaitu mendeteksi kantuk pada pengemudi menggunakan pemrosesan citra dan fisiologis . pemilihan dataset memastikan bahwa dataset yang dikumpulkan relevan dengan tujuan penelitian yaitu hanya menggunakan data yang berkaitan dengan kondisi predriving serta memastikan bahwa data visual dan data fisiologis diambil pada waktu yang sama. 3.3.2 preprocessing data melakukan analisis eksploratif data untuk memahami karakteristik dataset sehingga meningkatkan kualitas deteksi . langkah dari pembuatan model yaitu penulisan kode untuk membangun model sesuai dengan desain arsitektur yaitu cnn lstm dan svm. 2. membangun model lstm membangun model lstm dengan lapisan lstm dan dense untuk ekstraksi fitur. 3.4.4 desain arsitektur desain a rsitektur merupakan proses menentukan struktur dan komponen model yang akan dibangun yang terdiri dari jenis model jumlah dan jenis layer fungsi aktivasi teknik regularisasi dan konfigurasi model. hasil dari kedua model digabungkan dan diklasifikasikan menggunakan support vector machine svm. 50 digunakan untuk mengolah data visual seperti mengenali mata tertutup atau mulut menguap sebagai indikator kantuk. menggabungkan fitur yang diekstrak dari cnn dan lstm untuk mendapatkan representasi data yang komprehensif memastikan bahwa model dapat mengidentifikasi kantuk berdasarkan kombinasi indikator visual dan fis iologis. selanjutnya yaitu menggunakan support vector machines svm untuk mengklasifikasikan data sebagai kantuk atau tidak kantuk. svm dipilih karena kemampuannya dalam mengklasifikasikan data yang kompleks dan memberikan batas keputusan yang jelas layak atau tidak layak pengemudi untuk berkendara. jika pengklasifikasi mendeteksi keadaan mengantuk maka pengklasifikasi menghasilkan alarm atau notifikasi pemberitahuan untuk memberi tahu bahwa pengemudi tidak layak untuk berkendara atau kembali ke f ase pertama dan memulai ulang prosedur. 3.5 evaluasi model gabungan ini dievaluasi menggunakan metrik seperti akurasi presisi recall dan f1score untuk memastikan performa dan keandalannya. implementasi sistem ini diharapkan dapat memberikan notifikasi atau peringatan kepada pengemudi jika tanda tanda kantuk terdeteksi selama kondisi predriving sehingga dapat meningkatkan keselamatan berkendara secara signifikan. berdasarkan hasil validasi model dapat ditune atau dioptimalkan untuk meningkatkan performa misalnya dengan mengubah arsitektur parameter atau teknik training . 3.6 implementasi setelah penyempurnaan model dianggap siap untuk digunakan. model ini harus dapat secara akurat mendeteksi kantuk pengemudi dalam berbagai kondisi dengan minimal kesalahan. langkah selanjutnya yaitu penerapan model dalam sistem nyata dan pemantauan efektivitasnya dalam kondisi pengemudi pada 51 lingkungan predriving. model yang telah dioptimalkan diintegrasikan ke dalam sistem deteksi dini kantuk untuk pengujian awal. selanjutn ya yaitu m elakukan uji coba lapangan untuk mengevaluasi efektivitas sistem dalam kondisi nyata memungkinkan pengumpulan feedback untuk perbaikan lebih lanjut. 3.7 rencana kegiatan tabel 3.1 rencana kegiatan no nama kegiatan bulan 1 2 3 4 5 6 7 8 9 10 11 12 1 kajian literatur 2 perencanaan penelitian 3. pengumpulan data 4. prapemrosesan data 5. pembuatan model 6. pelatihan dan evaluasi model 7. penyusunan laporan akhir 8. presentasi laporan akhir 9. publikasi jurnal ilmiah internasional 10. pengajuan hki
Utami Lestari_Kualifikasi.txt,"Penelitian ini bertujuan untuk mengembangkan aplikasi berbasis Large Language Model (LLM) dengan arsitektur GPT-4 yang mampu melakukan telaah sejawat(peer review) secara otomatis pada artikel ilmiah dari jurnal komputer. Data utama yang digunakan adalah artikel ilmiah berbahasa Indonesia dalam bidang ilmu komputer dari berbagai jurnal akademik. Sebelum digunakan, data akan diperiksa untuk menghilangkan informasi pribadi yang dapat mengidentifikasi penulis atau reviewer. Aplikasi ini diharapkan dapat membantu para peneliti dan editor jurnal dalam menganalisis dan memperoleh wawasan dari artikel yang seringkali bersifat kompleks dan teknis. Untuk melakukan penelitian ini perlu dilakukan beberapa tahapan hingga penelitian selesai, tahapan yang dilaukan mulai dari pengumpulan data, preprocessing data, melakukan pemodelan untuk telaah sejawat, mengevaluasi model dan validasi ahli. Untuk tahapan penelitian dapat dilihat pada gambar 3.1. 3.1.1 Pengumpulan data
Proses pengumpulan data dilakukan dengan cara mengumpulkan artikel ilmiah dari berbagai sumber terbuka dengan topik artikel ilmu computer. Pengumpulan data menggunakan teknik webscraping, artikel yang telah dikumpulkan akan diproses melalui tahap preprocessing. 3.1.2 Preprocessing data
Proses preprocessing data merupakan langkah yang sangat penting dalam persiapan data untuk pemodelan LLM. Proses ini melibatkan beberapa tahap penting yang bertujuan untuk membersihkan dan menyiapkan data teks agar sesuai dengan kebutuhan model serta meningkatkan kualitas dan konsistensi representasi teks. Proses preprocessing dilakukan melalui beberapa tahap seperti tokenisasi, pembersihan teks ,normalisasi, token encoding, penghapusan stopword, stemming, segmentasi kalimat dan pemisahan dataset. Gambar 3. 2 Tahapan Preprocessing
Tahap pertama adalah tokenisasi, di mana teks dipecah menjadi unit-unit yang lebih kecil yang dikenal sebagai token, memungkinkan model untuk menganalisis teks pada tingkat yang lebih granular. Selanjutnya, dilakukan pembersihan teks untuk menghilangkan karakter atau simbol yang tidak diinginkan seperti tanda baca, angka, dan karakter khusus lainnya, serta penghapusan spasi berlebih dan karakter yang tidak relevan. Normalisasi juga dilakukan untuk mengubah teks menjadi bentuk standar, termasuk mengubah semua huruf menjadi huruf kecil, menghapus aksen dari huruf, dan menangani variasi penulisan yang berbeda untuk kata yang sama. Setelah itu, token yang dihasilkan dari tokenisasi perlu diubah menjadi representasi numerik melalui token encoding, menggunakan teknik embeddings dari model transformer. Penghapusan stopword, yaitu kata-kata umum yang sering muncul dalam teks tetapi tidak memiliki makna khusus yang penting untuk analisis, juga dilakukan untuk mengurangi dimensi data dan fokus pada kata- kata yang lebih bermakna. Proses selanjutnya adalah stemming dan lemmatisasi yang bertujuan untuk mengurangi kata-kata ke bentuk dasar atau akar katanya, dengan stemming memotong akhiran kata dan lemmatisasi menggunakan kamus bahasa untuk mengembalikan kata ke bentuk dasar yang benar secara gramatikal. Selanjutnya segmentasi kalimat dilakukan untuk memisahkan teks menjadi kalimat-kalimat individu yang bisa dianalisis lebih lanjut secara terpisah. Tahap terakhir dalam preprocessing adalah pemisahan dataset menjadi bagian-bagian yang berbeda, seperti data latih, data validasi, dan data uji, yang penting untuk mengevaluasi kinerja model secara adil dan menghindari overfitting. Melalui proses preprocessing yang cermat dan terstruktur, data teks menjadi lebih bersih, terorganisir, dan siap digunakan dalam pemodelan, sehingga tidak hanya meningkatkan efisiensi pemrosesan data tetapi juga memungkinkan model untuk belajar dan melakukan prediksi dengan lebih akurat. 3.1.3 Pembuatan Model LLM
Setelah dataset yang di kumpulkan dan melalui proses preprocessing maka dilanjutkan tahap pemodelan dengan menggunakan LLM. Pada tahap ini dilakukan pemodelan dengan arsitektur GPT-4 untuk platform tinjauan artikel ilmiah. Proses pemodelan dimulai dengan fine-tuning GPT-4 menggunakan dataset yang telah dipreproccesing sebelumnya. Fine-tuning dilakukan untuk menyesuaikan model dengan gaya penulisan dan terminologi spesifik yang digunakan dalam artikel ilmiah. Selama fase pelatihan, model dievaluasi secara berkala untuk memastikan kinerjanya sesuai dengan harapan, dan parameter model dioptimalkan untuk meningkatkan kualitas output. Penggunaan GPT-4 untuk platform tinjauan artikel ilmiah dapat menyediakan analisis yang mendalam dan komprehensif, membantu reviewer untuk lebih cepat dan efisien dalam menilai kualitas dan kontribusi sebuah artikel. Hal ini tidak hanya meningkatkan produktivitas tetapi juga memastikan bahwa artikel yang dipublikasikan memenuhi standar ilmiah yang tinggi. 3.1.4 Evaluasi Model LLM
Evaluasi model merupakan langkah yang penting dalam pengembangan sistem kecerdasan buatan, karena memungkinkan untuk menilai kinerja dan efektivitas model dalam menyelesaikan tugas tertentu. Proses evaluasi membantu mengidentifikasi kelemahan dan kekuatan model, serta memberikan wawasan tentang seberapa baik model dapat digunakan. Tanpa evaluasi yang tepat, model yang dikembangkan dapat menghasilkan prediksi yang tidak akurat atau tidak dapat diandalkan, yang berpotensi menyebabkan kinerja sistem yang buruk secara keseluruhan. Pada penelitian ini dilakukan evaluasi model dengan melihat nilai akurasi, presisi, recall dan F1-Score. 1. Akurasi memberikan gambaran umum tentang seberapa baik model klasifikasi melakukan prediksi secara keseluruhan. 2. Presisi memberikan informasi tentang seberapa banyak prediksi positif yang sebenarnya benar dari semua prediksi positif yang dilakukan oleh model. 3. Recall memberikan informasi tentang seberapa banyak instance positif yang berhasil diidentifikasi oleh model dari semua instance positif yang
4. sebenarnya dalam dataset. 5. F1-Score berguna ketika kelas target tidak seimbang dalam dataset, karena mencakup baik presisi maupun recall dalam perhitungannya. 3.1.5 Validasi Ahli
Proses validasi ahli ini memastikan bahwa model GPT-4 yang digunakan untuk telaah sejawat mampu memberikan evaluasi yang akurat, relevan, dan sesuai dengan standar akademik, dengan masukan berharga dari para ahli di bidangnya. 3.2 Jadwal Penelitian
Jadwal penelitian bertujuan untuk mengatasi target waktu penelitian, memastikan bahwa penelitian ini dapat diselesaikan sesuai dengan batas waktu yang telah ditetapkan. Adanya jadwal penelitian, diharapkan penelitian dapat berjalan secara efisien dan sesuai rencana, sehingga memberikan kepastian bahwa semua tahapan penelitian dapat diselesaikan tepat pada waktunya. Table jadwal penelitian dapat dilihat pada table 3.1","Penelitian ini bertujuan untuk mengembangkan aplikasi berbasis Large Language Model (LLM) dengan arsitektur GPT-4 yang mampu melakukan telaah sejawat(peer review) secara otomatis pada artikel ilmiah dari jurnal komputer. Data utama yang digunakan adalah artikel ilmiah berbahasa Indonesia dalam bidang ilmu komputer dari berbagai jurnal akademik. Sebelum digunakan, data akan diperiksa untuk menghilangkan informasi pribadi yang dapat mengidentifikasi penulis atau reviewer. Aplikasi ini diharapkan dapat membantu para peneliti dan editor jurnal dalam menganalisis dan memperoleh wawasan dari artikel yang seringkali bersifat kompleks dan teknis. Proses ini melibatkan beberapa tahap penting yang bertujuan untuk membersihkan dan menyiapkan data teks agar sesuai dengan kebutuhan model serta meningkatkan kualitas dan konsistensi representasi teks. 2 Tahapan Preprocessing
Tahap pertama adalah tokenisasi, di mana teks dipecah menjadi unit-unit yang lebih kecil yang dikenal sebagai token, memungkinkan model untuk menganalisis teks pada tingkat yang lebih granular. 3.1.4 Evaluasi Model LLM
Evaluasi model merupakan langkah yang penting dalam pengembangan sistem kecerdasan buatan, karena memungkinkan untuk menilai kinerja dan efektivitas model dalam menyelesaikan tugas tertentu. Tanpa evaluasi yang tepat, model yang dikembangkan dapat menghasilkan prediksi yang tidak akurat atau tidak dapat diandalkan, yang berpotensi menyebabkan kinerja sistem yang buruk secara keseluruhan. Recall memberikan informasi tentang seberapa banyak instance positif yang berhasil diidentifikasi oleh model dari semua instance positif yang
4. sebenarnya dalam dataset. 3.1.5 Validasi Ahli
Proses validasi ahli ini memastikan bahwa model GPT-4 yang digunakan untuk telaah sejawat mampu memberikan evaluasi yang akurat, relevan, dan sesuai dengan standar akademik, dengan masukan berharga dari para ahli di bidangnya. 3.2 Jadwal Penelitian
Jadwal penelitian bertujuan untuk mengatasi target waktu penelitian, memastikan bahwa penelitian ini dapat diselesaikan sesuai dengan batas waktu yang telah ditetapkan. Adanya jadwal penelitian, diharapkan penelitian dapat berjalan secara efisien dan sesuai rencana, sehingga memberikan kepastian bahwa semua tahapan penelitian dapat diselesaikan tepat pada waktunya. Table jadwal penelitian dapat dilihat pada table 3.1"
Yoga Panji Perdana Nugraha_Kualifikasi.txt,3.1 motivasi industri manufaktur memiliki berbagai macam produk yang ada di dalamnya. dalam upaya pemenuhan kualitas yang tinggi serta menjaga kepuasan pelanggan dan reputasi perusahaan maka mendeteksi produk yang cacat sedini mungkin merupakan aspek yang penting. sehingga motivasi dari disertasi ini adalah sebagai berikut. 1. pengembangan aplikasi pendeteksi cacat pada produk ini didasari keinginan peneliti untuk meningkatkan kinerja pengendalian kualitas pada industri manufaktur sehingga dapat membantu menjaga kualitas produk serta efisien si dalam kegiatan pengendalian kualitas. 2. untuk meminimalisir pemborosan waktu bahan baku biaya dan sumber daya lainnya karena deteksi cacat pada produk dilakukan sedini dan secepat mungkin. 3. meningkatkan efisiensi pada kegiatan inspeksi produk d engan mene rapkan otomatisasi mel alui aplikasi yang dikembangkan. 4. mengintegrasikan teknologi yang sedang berkembang seperti artificial intelligence dengan industri manufaktur sehingga tercipta manufaktur cerdas yang akan berakibat pendapatan profit perusahaan yang op timal. 5. memberikan kontribusi pemahaman dan pengembangan teknologi baru dalam deteksi objek sehingga bisa menjadi referensi untuk pembaca serta penelitian selanjutnya. 3.2 alur kerja riset alur kerja riset digambarkan melalui diagram alir. tujuannya agar penel itian dapat terstruktur sehingga tidak ada tah apan penelitian yang terlewat. secara umum b erikut ini merupakan diagram alir penelitian ini. tahap awal tahap pengembanganperancangan dan pembuatan prototype alat deteksi cacatpengumpulan data cacat objek uji coba prototype alat deketsi cacat objekperancangan model deteksi cacat objek menggunakan deep learning implementasi dan pelatihan model deteksi cacat objek evaluasi dan penyempurnaan model deteksi cacat objek pengujian model deteksi objek menggunakan deep learning pembuatan aplikasi pendeteksi objek cacattahap optimasi pengajuan hki dan jurnal internasional q 1 gambar 3. 1 diagram alir penelitian diagram alir penelitian di atas menggambarkan alur penelitian yang akan dilakukan. berikut ini adalah penjelasan dari diagram alir penelitian di atas. 1. tahap awal kegiatan yang dilakukan pada tahap awal ini adalah merancang dan membuat prototype alat deteksi cacat dan pengumpulan data cacat objek. prototype alat ini menggunakan ban berjalan dengan motor listrik sebagai penggeraknya dengan alat pencahayaan yang cukup. alat ini nantinya digunakan untuk mengumpulkan data yang akan digunakan untuk melakukan perancangan dan pelatihan m odel deteksi cacat objek. pengumpulan data dilakukan untuk memperoleh data yang dibutuhkan pada penelitian ini. data bisa berupa data primer dan data sekunder ataupun keduanya bergantung pada kebutuhan penelitian yang akan dilakukan. data primer dikumpulan dengan memotret objek pada ban berjalan menjadi citra baik citra bergerak maupun citra tak bergerak yang akan menjadi satu kesatuan yaitu dataset . data primer dikumpulkan menggunakan alat yang dirancang seperti di bawah ini. gambar 3. 2 rancangan prototipe alat gambar 3.2 di atas menggambarkan rancangan alat yang akan dikembangkan. alat tersebut pertama digunakan sebagai media untuk pengambilan data primer yaitu data citra dari objek yang akan dideteksi. objek berupa sekrup akan berjalan melalui ban berjalan conveyor yang nantinya akan ditangkap gambarnya oleh webcam atau kamera yang terhubung dengan komputer untuk menyimpan gambar tersebut untuk kebutuhan pelatihan model. sedangkan data sekunder dikumpulkan melalui website kaggle maupun website atau jurnal lain yang sejenis. hasil dari akuisisi citra ini akan digunakan untuk pelatihan dan pengujian data. sampel yang diambil adalah objek berupa sekrup yang terdapat kecacatan. data tersebut kemudian dikumpulkan menjadi sebuah da taset yang akan digunakan untuk melatih model. data data yang diambil kemudian dikelompokkan menjadi beberapa kelas sesuai dengan jenis cacat yang ada pada sekrup tersebut. luaran pada tahap ini adalah pengajuan hki untuk prototype alat pendeteksi cacat ob jek yang dirancang. 2. tahap pengembangan tahap ini terdapat beberapa kegiatan yang dilakukan. pertama adalah melakukan uji coba prototype alat deteksi cacat objek yang digambarkan pada gambar 3.2 di atas . uji coba dilakukan dengan menyesuaikan tinggi kamera tingkat pencahayaan kecepatan ban berjalan serta pengaturan tempat ban berjalan untuk menjaga efektivitas dan efisiensi dalam mendeteksi objek. kedua adalah merancang model untuk mendeteksi cacat objek dengan menggunakan deep learning . sebelum melatih da ta dilakukan preprocessing terlebih dahulu. kegiatan ini dilakukan dengan menggunakan website roboflow. preprocessing dilakukan mengoptimalkan pelatihan dengan menganotasi citra untuk menandai bagian penting dari citra region of interest menyamakan orie ntasi citra mengubah ukuran citra agar sama memperbanyak variasi data dengan augmentasi dan generalisir data sehingga menjadi satu kesatuan dataset yang lebih siap untuk dilatih. setelah preprocessing dilakukan maka diharapkan pelatihan data yang dilaku kan lebih optimal. pelatihan data dilakukan untuk melatih model mengenali citra yang akan dideteksi sehingga pada penerapannya mendapatkan hasil deteksi yang akurat dan optimal. pelatihan data dilakukan dengan menggunakan salah satu algoritma dari teknolog i kecerdasan artifisial yaitu deep learning dengan bahasa pemrograman yang digunakan adalah python. pada pelatihan data ini juga akan mendapatkan nilai pengukuran evaluasi measurment evaluation berupa accuracy recall and precision dan mean average prec ision map. pada umumnya pelatihan harus memiliki jumlah data dalam hal ini adalah citra yang lebih banyak dibandingkan pengujian. 3. tahap optimasi tahap pengembangan telah dilakukan kemudian masuk ke tahap optimasi. tahap ini terdapat kegiatan yaitu eval uasi dan penyempurnaan model deteksi cacat objek. evaluasi dan penyempurnaan dilakukan agar fitur yang ada pada aplikasi yang akan dikembangkan dapat ditampilkan dengan maksimal. fitur yang akan ditambahkan pada model pendeteksi objek berupa kemampuan komp uter untuk secara otomatis menyimpan hasil deteksi menjadi sebuah basis data. sehingga nantinya data tersebut dapat menjadi acuan bagi departemen terkait untuk inovasi ke depannya. setelah pelatihan data dilakukan maka selanjutnya adalah pengujian data. p engujian data dilakukan untuk menguji model sejauh mana dapat mendeteksi cacat dari suatu produk. pada pengujian data dilakukan dengan mengunggah data secara acak selain data yang digunakan pada pelatihan. pada akhirnya akan menampilkan output model dalam mendeteksi cacat pada produk. setelah itu maka dibangun aplikasi yang mampu mendeteksi cacat produk pada industri secara real time. aplikasi ini nantinya akan menampilkan hasil deteksi dari produk yang bergerak. informasi yang disampaikan antara lain kondi si dari produk cacat atau tidak serta bagian mana yang cacat akan ditandai oleh bounding box . hal ini akan dengan cepat membantu operator mengetahui cacat jenis apa yang terjadi. sehingga dapat ditindaklanjuti sesegera mungkin yang secara tidak langsung ju ga membantu dalam pengambilan keputusan. target penelitian ini adalah pengajuan hki serta publikasi artikeljurnal ilmiah internasional bereputasi q1 ieee access . 3.3 pendekatan pendekatan yang dilakukan adalah dengan menggunakan teknologi artificial int elligence dalam mengadopsi kemampuan manusia dalam mendeteksi objek. pendekatan ini menggabungkan antara pengolahan citra dan deep learning dengan memanfaatkan salah satu arsitektur yang dimilikinya. selain itu diterapkan juga pengukuran evaluasi seperti precision recall dan mean average precision map untuk memastikan model yang dikembangkan dapat digunakan dengan optimal. nantinya akan dikembangkan sebuah aplikasi yang kemungkinan berbasis web untuk mempermudah pengguna untuk mengambil gambar bergerak maupun tak bergerak yang kemudian mengirimnya ke sistem pendeteksi cacat dan menerima hasil deteksi secara real time. hasil deteksi secara real time dikehendaki agar produk dapat diperiksa selama proses produksi berlangsung sehingga cacat dapat dideteksi secepat dan seakurat mungkin. hal ini akan membantu operator untuk melakukan kegiatan inspeksi produk dengan efisien.,3.1 motivasi industri manufaktur memiliki berbagai macam produk yang ada di dalamnya. dalam upaya pemenuhan kualitas yang tinggi serta menjaga kepuasan pelanggan dan reputasi perusahaan maka mendeteksi produk yang cacat sedini mungkin merupakan aspek yang penting. 1. pengembangan aplikasi pendeteksi cacat pada produk ini didasari keinginan peneliti untuk meningkatkan kinerja pengendalian kualitas pada industri manufaktur sehingga dapat membantu menjaga kualitas produk serta efisien si dalam kegiatan pengendalian kualitas. 2. untuk meminimalisir pemborosan waktu bahan baku biaya dan sumber daya lainnya karena deteksi cacat pada produk dilakukan sedini dan secepat mungkin. 3. meningkatkan efisiensi pada kegiatan inspeksi produk d engan mene rapkan otomatisasi mel alui aplikasi yang dikembangkan. 4. mengintegrasikan teknologi yang sedang berkembang seperti artificial intelligence dengan industri manufaktur sehingga tercipta manufaktur cerdas yang akan berakibat pendapatan profit perusahaan yang op timal. 5. memberikan kontribusi pemahaman dan pengembangan teknologi baru dalam deteksi objek sehingga bisa menjadi referensi untuk pembaca serta penelitian selanjutnya. tahap awal tahap pengembanganperancangan dan pembuatan prototype alat deteksi cacatpengumpulan data cacat objek uji coba prototype alat deketsi cacat objekperancangan model deteksi cacat objek menggunakan deep learning implementasi dan pelatihan model deteksi cacat objek evaluasi dan penyempurnaan model deteksi cacat objek pengujian model deteksi objek menggunakan deep learning pembuatan aplikasi pendeteksi objek cacattahap optimasi pengajuan hki dan jurnal internasional q 1 gambar 3. 1. tahap awal kegiatan yang dilakukan pada tahap awal ini adalah merancang dan membuat prototype alat deteksi cacat dan pengumpulan data cacat objek. 2 rancangan prototipe alat gambar 3.2 di atas menggambarkan rancangan alat yang akan dikembangkan. tahap ini terdapat kegiatan yaitu eval uasi dan penyempurnaan model deteksi cacat objek. evaluasi dan penyempurnaan dilakukan agar fitur yang ada pada aplikasi yang akan dikembangkan dapat ditampilkan dengan maksimal. p engujian data dilakukan untuk menguji model sejauh mana dapat mendeteksi cacat dari suatu produk. pada akhirnya akan menampilkan output model dalam mendeteksi cacat pada produk. setelah itu maka dibangun aplikasi yang mampu mendeteksi cacat produk pada industri secara real time. aplikasi ini nantinya akan menampilkan hasil deteksi dari produk yang bergerak. selain itu diterapkan juga pengukuran evaluasi seperti precision recall dan mean average precision map untuk memastikan model yang dikembangkan dapat digunakan dengan optimal. nantinya akan dikembangkan sebuah aplikasi yang kemungkinan berbasis web untuk mempermudah pengguna untuk mengambil gambar bergerak maupun tak bergerak yang kemudian mengirimnya ke sistem pendeteksi cacat dan menerima hasil deteksi secara real time. hasil deteksi secara real time dikehendaki agar produk dapat diperiksa selama proses produksi berlangsung sehingga cacat dapat dideteksi secepat dan seakurat mungkin.
