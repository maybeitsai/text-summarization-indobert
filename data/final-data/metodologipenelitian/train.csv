nama_dokumen,kalimat,summary
Alifurrohman_Kualifikasi.txt,3.2 pengumpulan data langkah awal adalah mengumpulkan dataset yang akurat dan relevan. dataset didapatkan dari data sekunder dataset ini merupakan hal yang penting dari simulasi dan eksperimen mencakup koordinat lokasi yang mungkin meliputi lokasi depot dan titik pengiriman jendela waktu untuk setiap pengiriman yang menentukan batas awal dan akhir kapan pengiriman harus dilakukan serta jumlah kendaraan. data ini harus mencerminkan situasi dunia nyata untuk memastikan bahwa model yang dikembangkan dapat diaplikasikan secara praktis. 3.3 persiapan data langkah berikutnya adalah persiapan data. pada persiapan data dilakukan normalisasi data. n ormalisasi merupakan proses penting untuk menyamakan skala data memastikan bahwa model dapat memprosesnya dengan efisien. normalisasi minmax digunakan pada penelitian ini. minmax adalah teknik yang mengubah skala nilai data ke dalam rentang baru seperti 0 hingga 1 atau 1 hingga 1. teknik ini memastikan bahwa setiap fitur atau kolom data memberikan kontribusi yang seimbang dalam analisis tanpa membiarkan fitur dengan skala besar m endominasi. pengecekan matriks korelasi dilakukan untuk memahami hubungan antara variabel variabel dalam dataset. korelasi membantu mengidentifikasi fitur fitur yang saling terkait dan memberikan wawasan tentang bagaimana setiap fitur dapat mempengaruhi model prediksi rute. koefisien korelasi pearson digunakan untuk mengukur hubungan linear antara fitur. 3.4 desain model implementasi deep q network dqn dengan mekanisme attention untuk dynamic vehicle routing problem with time windows dvrptw melibatkan beberapa langkah utama mulai dari pemilihan kerangka kerja hingga pembuatan lingkungan simulasi. 1. pemilihan kerangka kerja 28 kerangka kerja yang digunakan yaitu tensorflow dimana kerangka kerja ini menawarkan lingkungan yang komprehensif dengan tensorboard untuk visualisasi serta dukungan terhadap tpu untuk akselerasi komputasi. tensorflow mungkin lebih cocok untuk produksi dan skala besar. 2. desain model dqn dan multi head erattention dqn adalah algoritma pembelajaran penguatan yang menggunakan jaringan saraf tiruan untuk memperkirakan fungsi nilai q yang merepresentasikan nilai maksimum hadiah kumulatif yang diharapkan diberikan sebuah state dan semua strategi yang mungkin diambil. i mplementasi dqn melibatkan beberapa komponen utama jaringan q jaringan ini memperkirakan nilai q untuk setiap aksi dari state tertentu. dalam kasus dvrptw input bisa berupa representasi dari state saat ini misalnya lokasi kendaraan status pengiriman dan output adalah nilai q untuk setiap kemungkinan aksi misalnya memilih lokasi pengiriman berikutnya. memory replay untuk meningkatkan stabilitas dan efisiensi pembelajaran dqn menggunakan teknik memory replay di mana transisi state aksi reward state baru disimpan dalam sebuah buffer . batch transisi ini kemudian digunakan untuk melatih jaringan q memungkinkan pengalaman dari masa lalu digunakan kembali. strategi eksplorasi seperti εgreedy di mana aksi acak dipilih dengan probabilitas ε untuk mendorong eksplorasi lingkungan. mekanis me attention terdiri dari tiga matriks utama query q key k dan value v untuk setiap head i. adapun langkah langkahnya implementasinya sebagai berikut a. definisikan ukuran input dan parameter mentukan jumlah fitur input dimensi embedding jumlah heads untuk mekanisme attention dan jumlah unit dalam lapisan tersembunyi dqn. serta jumlah tindakan yang mungkin dilakukan oleh agen. b. definisikan multi head er attention menerapkan mekanisme multi head er attention pada representasi vektor dari embedding layer . multi head er attention menggunakan query q key k 29 dan value v untuk menangkap hubungan kontekstual dalam data. proses ini membantu model untuk fokus pada aspek aspek penting dari data input. attention score dihitung dengan mengalikan query dengan key kemudian membaginya dengan skala biasanya akar dari dimensi key dan menerapkan fungsi softmax untuk mendapatkan bobot attention . output attention diperoleh dengan mengalikan bobot perhatian dengan value . multi head er attention melakukan proses ini beberapa kali secara paralel dengan beberapa heads dan hasilnya digabungkan untuk data i nput. c. definisikan dqn dengan lapisan tersembunyi memb uat beberapa lapisan tersembunyi hidden layers menggunakan fungsi aktivasi relu. lapisan tersembunyi ini memungkinkan jaringan untuk belajar representasi yang kompleks dari data input. output dari mekanisme attention diberikan sebagai input ke dqn. dqn memperkirakan q value s untuk setiap tindakan yang mungkin dilakukan oleh agen berdasarkan representasi state yang telah diperkaya. d. output layer untuk q value s lapisan output menghasilkan q value s untuk setiap tindakan yang mungkin dilakukan oleh agen. q value s ini menunjukkan seberapa baik setiap tindakan dalam memaksimalkan reward di masa depan. misalnya jika agen memiliki 5 kemungkinan tindakan lapisan output akan menghasilkan 5 q value s satu untuk setiap tindakan. e. memilih tindakan menggunakan strategi εgreedy implementasikan strategi εgreedy untuk memastikan agen mengeksplorasi lingkungan sekaligus mengeksploitasi pengetahuan yang ada. dengan probabilitas ε agen memilih tindakan secara acak untuk eksplorasi dan dengan probabilitas 1 ε agen memilih tindakan dengan q value tertinggi untuk eksploitasi. f. memperbarui model dengan pengalaman dari replay buffer mengg unakan replay buffer untuk menyimpan transisi state action reward next state dan menggunakannya untuk melatih model. batch transisi 30 diambil secara acak dari replay buffer untuk mengurangi korelasi antara sampel pelatihan dan meningkatkan stabilitas pelatihan. 3.5 pelatihan model selama fase pelatihan model secara berulang kali dihadapkan pada berbagai skenario dari masalah r ute kendaraan. untuk setiap episode model mengambil serangkaian aksi berdasarkan policy atau kebijakan saat ini yang awalnya adalah kebijakan acak dengan tujuan meminimalkan jarak total dan memenuhi jendela waktu pengiriman. setelah mengambil aksi model menerima feedback dari lingkungan berupa reward yang merupakan ukuran dari performa aksi tersebut dan state baru yang mencerminkan kondisi terkini dari lingk ungan setelah aksi diambil. informasi ini digunakan untuk memperbarui kebijakan model dengan cara mengoptimalkan parameter jaringan sehingga meningkatkan estimasi nilai q yang merepresentasikan hadiah kumulatif yang diharapkan. untuk meningkatkan stabilitas dan efisiensi pelatihan teknik seperti experience replay dan target networks digunakan. experience replay memungkinkan model untuk belajar dari pengalaman masa lalu yang disimpan dalam memory replay sedangkan target networks membantu mengurangi pergeseran target yang bergerak selama proses pembelajaran. melalui interaksi yang berulang dan proses optimisasi ini model secara bertahap belajar untuk memprediksi nilai q yang lebih akurat untuk setiap kombinasi state dan aksi yang mengarah pada pembentukan kebijakan rute yang lebih optimal. selain itu pada tahap pelatihan model ini juga dilakukan penyetelan hyperparameter untuk menemukan nilai optimal hyperparameter guna meningkatkan kinerja model. ini merupakan langkah penting dalam machine learning karena dapat menghasilkan peningkatan akurasi efisiensi dan generalizability model. penyetelan hyperparameter menggunakan teknik random search yaitu teknik yang digunakan untuk penyetelan hyperparameter yang melibatkan pemilihan acak dari ruang yang ditentukan untuk menemukan kombinasi terbaik yang mengoptimalkan kinerja model. teknik ini lebih efektif dan 31 efisien untuk penyetelan hyperparameter terutama pada kasus dengan ruang pencarian yang besar serta dapat menemukan solusi yang baik dalam waktu yang lebih singkat. 3.6 evaluasi model setelah fase pelatihan model deep q network dqn dengan multi header attention untuk dynamic vehicle routing problem with time windows dvrptw selesai langkah evaluasi menjadi penting untuk memahami seberapa efektif model dalam menyelesaikan masalah yang ditargetkan. evaluasi dilakukan dengan menguji model terhadap kumpulan data pengujian yang tidak terlibat selama proses pelatihan me mberikan masukan penting tentang kemampuan generalisasi model terhadap skenario baru dan belum pernah dilihat. dalam kont eks dvrptw metrik yang relevan seperti total jarak tempuh oleh semua kendaraan dan kepatuhan terhadap jendela waktu pengiriman menjadi fokus utama. total jarak tempuh mencerminkan efisiensi rute yang dihasilkan sementara kepatuhan terhadap jendela waktu mencerminkan kualitas layanan yang dapat dijamin oleh model. 3.7 analisis dan penyempurnaan langkah terakhir yaitu analisis secara mendalam kinerja model pada dataset pengujian. penyempurnaan dilakukan untuk mengatasi kelemahan yang telah dianalisis sebelumnya seperti penyempurnaan pada tuning hyperparameter untuk peningkatan kinerja modifikasi arsitektur dan pelatihan ulang model. 3.8 jadwal penelitian jadwal penelitian digunakan untuk meningkatkan efektivitas dalam proses penelitian. adanya jadwal penelitian ini setiap proses penelitian sudah terjadwal dalam tabel 3.1 sehingga penelitian lebih efektif dan optimal.,3.2 pengumpulan data langkah awal adalah mengumpulkan dataset yang akurat dan relevan. dataset didapatkan dari data sekunder dataset ini merupakan hal yang penting dari simulasi dan eksperimen mencakup koordinat lokasi yang mungkin meliputi lokasi depot dan titik pengiriman jendela waktu untuk setiap pengiriman yang menentukan batas awal dan akhir kapan pengiriman harus dilakukan serta jumlah kendaraan. data ini harus mencerminkan situasi dunia nyata untuk memastikan bahwa model yang dikembangkan dapat diaplikasikan secara praktis. n ormalisasi merupakan proses penting untuk menyamakan skala data memastikan bahwa model dapat memprosesnya dengan efisien. korelasi membantu mengidentifikasi fitur fitur yang saling terkait dan memberikan wawasan tentang bagaimana setiap fitur dapat mempengaruhi model prediksi rute. 3.4 desain model implementasi deep q network dqn dengan mekanisme attention untuk dynamic vehicle routing problem with time windows dvrptw melibatkan beberapa langkah utama mulai dari pemilihan kerangka kerja hingga pembuatan lingkungan simulasi. informasi ini digunakan untuk memperbarui kebijakan model dengan cara mengoptimalkan parameter jaringan sehingga meningkatkan estimasi nilai q yang merepresentasikan hadiah kumulatif yang diharapkan. untuk meningkatkan stabilitas dan efisiensi pelatihan teknik seperti experience replay dan target networks digunakan. selain itu pada tahap pelatihan model ini juga dilakukan penyetelan hyperparameter untuk menemukan nilai optimal hyperparameter guna meningkatkan kinerja model. ini merupakan langkah penting dalam machine learning karena dapat menghasilkan peningkatan akurasi efisiensi dan generalizability model. 3.6 evaluasi model setelah fase pelatihan model deep q network dqn dengan multi header attention untuk dynamic vehicle routing problem with time windows dvrptw selesai langkah evaluasi menjadi penting untuk memahami seberapa efektif model dalam menyelesaikan masalah yang ditargetkan. evaluasi dilakukan dengan menguji model terhadap kumpulan data pengujian yang tidak terlibat selama proses pelatihan me mberikan masukan penting tentang kemampuan generalisasi model terhadap skenario baru dan belum pernah dilihat. total jarak tempuh mencerminkan efisiensi rute yang dihasilkan sementara kepatuhan terhadap jendela waktu mencerminkan kualitas layanan yang dapat dijamin oleh model. 3.7 analisis dan penyempurnaan langkah terakhir yaitu analisis secara mendalam kinerja model pada dataset pengujian. penyempurnaan dilakukan untuk mengatasi kelemahan yang telah dianalisis sebelumnya seperti penyempurnaan pada tuning hyperparameter untuk peningkatan kinerja modifikasi arsitektur dan pelatihan ulang model. 3.8 jadwal penelitian jadwal penelitian digunakan untuk meningkatkan efektivitas dalam proses penelitian. adanya jadwal penelitian ini setiap proses penelitian sudah terjadwal dalam tabel 3.1 sehingga penelitian lebih efektif dan optimal.
Armando Tirta Dwilaga_Kualifikasi.txt,3.1 gambaran umum penelitian penelitian ini digunakan untuk mengatasi sensitivitas terhadap cacat pada gambar ban dengan melibatkan penggunaan jaringan syaraf menggunakan algoritma convolutional neural network cnn dan membangun model atau kerangka kerja menggunakan keras. berikut adalah gambar 3.1 blok diagram gambaran umum penelitian. data preparation data augmentasi data splitting model training forward pass model evaluation backward passfinetuning if neededinput unit processing unit output unit model deployment inference12 gambar 3.1 blok diagram gambaran umum penelitian berdasarkan gambar 3.1 blok diagram gambaran umum penelitian maka dapat dijelaskan di blok tersebut terbagi menjadi 3 bagian yaitu bagian pertama adalah unit masukan berisikan data preparation di mana gambar ban dimuat diubah menjadi format yang sesuai dipersiapkan untuk pelatihan model convolutional neural network cnn seperti pemrosesan gambar ban selanjutnya data augmentation di mana data dibuat lebih ber variasi dari training data yang ada sehingga dapat meningkatkan keberagaman training data tanpa h arus mengambil data baru mencakup rotasi pergeseran horizontalvertikal perbesar gambar perubahan kecerahan gambar sampai mengubah nilai pixel selanjutnya data di mana dataset yang telah di augmentasi dan disiapkan dibagi menjadi subset yang berbeda untuk training untuk melatih model validation untuk menyempurnakan model serta me mvalidasi performanya selama pelatihan dan testing untuk mengevaluasi kinerja model akhir . data set dibagi menjadi training data validation data dan testing data dalam proporsi tertentu. bagian kedua adalah unit pemrosesan yang bertindak adalah model training forward pass tahap di mana input diproses melalui model untuk menghasilkan prediksi tujuannya melatih model convolutional neural network cnn menggunakan dataset pelatihan di mana data dari unit masukan diteruskan melalui jaringan neural di lakukan transformasi linier konvulasi dan non linier fungsi aktivasi dilakukan pada data di setiap lapisan untuk menghasilkan output prediksi yang melibatkan komputasi di setiap neuron dan lapisan jaringan yang merupakan inti dari proses pembelajaran dalam jaringan saraf. selanjutnya unit pemrosesan finetuning tujuannya dilakukan untuk menyempurnakan model lebih lanjut setelah pelatihan awal dengan dataset yang lebih kecil atau lebih spesifik nantinya. proses di dalam finetuning menyesuaikan bobot menggunakan kumpulan data yang lebih kecil untuk menyesuaikan bobot model untuk performa yang lebih baik pelatihan khusus fokus pada fitur data yang lebih relevan dengan objek . bagian ketiga adalah unit keluaran yang bertindak ada proses model evaluatioan backward pass tahap di mana gradien memperbarui parameter model dalam arah yang akan mengurangi fungsi loss dari fungsi loss metrik yang mengukur seberapa baik atau buruk model melakukan prediksi dibandingkan nilai aktualnya dihitung dan digunakan untuk memperbarui parameter model selama pelatihan tujuannya mengevaluasi performa model yang dilatih dan model dievaluasi menggunakan metrik yang relevan accuracy precision recall dan f1 score berdasarkan prediksi yang dihasilkan dari model terhadap validasi atau uji data. output dari proses ini adalah tentang hasil evaluasi model yang memberikan informasi kinerja model. selanjutnya ada dua alur pilihan yang bisa dilakukan alur pertama jika hasil prediksi sudah sesuai dengan keinginan maka bisa langsung masuk ke model deployment inference dan alu r kedua jika hasil prediksi masih perlu diperbaiki pada bagian unit pemrosesan terlebih dahulu fine tuning untuk penggunaan data set lebih kecil jika menunjukan model belum mencapai performa yang diharapkan baru masuk ke model deployment inference tujuannya menerapkan model terlatih untuk membuat prediksi pada data baru yang belum terlihat. model deployment inference yang telah dilatih digunakan untuk membuat prediksi pada data baru atau dalam situasi dunia nyata tahap di mana model menerima input baru dan menghasilkan output berdasarkan pada pembelajaran yang dilakukan selama proses pelatihan dan merupakan output akhir dari keseluruhan proses di mana model mengambil keputusan atau membuat prediksi berdasarkan pada pengalaman yang telah diperoleh selama pelatihan. 3.2. tahapan penelitian penelitian ini di dalamnya terdapat tahapan tahapan yang dilakukan untuk membentuk satu kesatuan yang utuh dari awal sampai akhir dan membentuk kerangka penelitian mengenai klasifikasi pada produk ban menggunakan algoritma convolutional neural network cnn . berikut gambar 3.2 tahapan penelitian. study of literature data acquisition data augmentation data splitting model buildingdata preprocessing model evaluation testing gambar 3.2 tahapan penelitian berdasarkan gambar 3.2 tahapan penelitian maka dapat dijelaskan proses yang terlibat di dalamnya ada 8 yaitu studi literatur data aquisition data preprocessing data augmentation texture feature extraction data splitting model building dan model evaluation testing di mana tahap ke dua sampai lima merupakan tahap proses menyiapkan sebuah data sebelum dilakukan pemodelan. 3.2.1 studi literatur tahap pertama adalah studi literatur di mana studi yang dilakukan berasal dari artikel ilmiah dan buku yang menunjang dalam menganalisis terkait dengan metode pen gukuran kualitas mengenai klasifikasi produk ban meninjau penggunaan pembelajaran mesin algoritma convolutional neural network cnn dari beberapa tahun ke belakang dalam konteks peng ukuran kualitas untuk klasifikasi terhadap kondisi kondisi produk ban . sehingga dapat menemukan teknik terbaik yang dapat diaplikasikan pada masalah yang ada. berikut merupakan gambar 3. 3 tahapan study literature . study of literature load libraries initialize imagedatagenerator set image directory and parameters create test training and validation dataset gambar 3. 3 tahapan study literature 3.2.2 data aquisition tahap k edua adalah data aquisition dengan mengumpulkan kumpulan data sesuai tujuan penelitian dengan target untuk kumpulan data gambar ban untuk training data validation data dan testing data memastikan bahwa kumpulan data tersebut memiliki varian yang secara akurat memang mewakili kondisi produk ban dan diperoleh dari sumber sumber terpercaya . berikut merupakan gambar 3. 4 tahapan data aquisition . data acquisition normal defecttire dataset gambar 3. 4 tahapan data aquisition 3.2.3 data preprocessing tahap ketiga adalah data preprocessing melakukan pra pemrosesan data untuk menyiapkan gambar untuk model pelatihan dan pengujian proses ini meliputi normalisasi dan penskalaan dengan fitur dalam program image data generator . bermaksud merapikan menata dan menyiapkan data untuk pemeriksaan tambahan. normalisasi data pengkodean variabel mengatasi nilai yang hilang menghapus data yang tidak relevan atau hilang dan modifikasi data lainnya untuk memenuhi persyaratan analisis a dalah persiapan data. berikut merupakan gambar 3.5 tahapan data preprocessing . data preprocessing data normalization data scalinginitiation split data into training and validation sets rescale target_size gambar 3. 5 tahapan data preprocessing 3.2.4 data augmentation tahap keempat adalah data augmentation meningkatkan variasi dalam dataset dengan teknik augmentasi data menggunakan operasi seperti rotasi pergeserarn horizontalvertikal perbesar gambar perubahan kecerahan gambar sampai mengubah nilai pixel untuk memperkaya dataset dan mengurangi overfitting saat disajikan dengan data baru yang belum pernah dilihat sebelumnya performa model akan menurun drastis karena model tersebut dapat menyesuaikan diri dengan kumpulan training data dengan sangat efektif. augmentasi data dilakukan dengan dua cara secara statis dan dinamis yang artinya secara statis yaitu menambah data secara fisiknya dan dinamis tidak menambah secara fisik tetapi secara k uantitas dataset yang dapat diakses secara fisik di komputer tidak bertambah ketika image data generator digunakan pada dataset. sebaliknya pada saat runtime hanya menghasilkan variasi dari gambar yang sudah ada dibuat secara dinami s dan cukup bagi model untuk berlatih dari berbagai kondisi gambar ban yang ada pada kenyataaanya. secara lebih jelas nilai teknik augmentasi pertama dilakukan dengan manual menggunakan bantuan dari website roboflow dengan resize gambar menjadi 640 x 640 pada augmentasinya menggunakan model flip horizontal dan vertikal 90 pemutaran searah jarum jam berlawanan arah jarum jam dan terbalik rotasi 45 dan 45 shear 5 horizontal dan 5 vertikal brightness 20 sampai 20. data asli pada dataset berjumlah 1 .028 data gambar ban setelah dilakukan augmentasi secara fisik menggunakan website roboflow ada data yang tidak dapat diidentifikasi ada 3 gambar sehingga total gambar asli yang berhasil di upload dan dijadikan data asli yang tetap berjumlah 1.025 data ga mbar dan setelah di augmentasi bertambah menjadi 2 .050 data gambar ban. rinciannya pada data asli training adalah 560 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 1.121 gambar. rincian data asli pada validation data berjumlah 140 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 279 gambar. rincian data asli pada testing data berjumlah 328 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 6 50 gambar. testing data pada prosesnya sebenarnya tidak mengalami augmentasi karena pada proses pengujian atau evaluasi mode l ingin menggunakan data asli yang sebenarnya untuk melihat kinerja model pada kasus kasus yang belum pernah dilihat sebelumnya. augmentasi kedua yaitu dilakukan rotasi melakukan pemutaran gambar secara penuh dan secara acak dengan nilai 360 atau rentang nilai 0 360 derajat kedua width shift range yang menggeser gambar secara acak ke kiri atau kanan dengan nilai 0 .05 atau gambar dapat digeser sampai 5 dari lebar aslinya . ketiga height shift range gambar dapat digeser secara vertikal dengan nilai 0 .05 atau gambar dapat digeser sampai 5 dari tinggi aslinya . keempat shear range untuk menggeser gambar dengan sudut geser berlawanan arah jarum jam dengan nilai 0.05. kelima zoom range memperbesar gambar sebanyak 0.05 atau gambar dapat diperbesar sampai 5. keenam horizontal flip adalah memberikan variasi tambahan dengan mengubah orientasi gambar secara horizontal acak dengan keterangan nilai true. ketujuh vertikal flip adalah memberikan variasi tambahan dengan mengubah orientasi gambar secara vertikal acak dengan keterangan nilai true. kedelapan brightness range mengubah atau menentukan kecerahan pada gambar secara acak dengan nilai rentan 0.75 1.25 atau kecerahan dapat diubah mulai dari rentnag 75 sampai 125 dari kecerahan asli gambarnya. kesembilan resecale mengubah nilai skala piksel 0.1 dengan membaginya setiap nilai piksel pada nilai 255 sehingga dapat membant u untuk normalisasi data. kesepuluh validation split mengatur pembagian data untuk validasi dengan nilai 0.2 atau 20 data dari keseluruhan data untuk alokasi validation data dan 80 un tuk alokasi training data. merupakan pendekatan augmentasi awal di mana sebelum data masuk ke model untuk proses pelatihan dan akan diperbesar sebelum pembagian dataset menjadi batch untuk setiap epoch nya sehingga model akan dilatih menggunakan dataset yang telah diaugmentsi sejak awal dan seluruh augmentasi akan diterapkan pada setiap epoch nya dengan penggunaan ukuran batch 64 dengan jumlah batch training 36 dan validasi 10. rinciannya data asli pada training data berjumlah 1.121 gambar dan setelah dilakukan augmentasi bertambah sebanyak 1.152 gambar sehingga data pada training data berjumlah total menjadi 2 .273 gambar. rincian data asli pada validation data berjumlah 2 79 gambar dan setelah dilakukan augmentasi bertambah sebanyak 320 gambar sehingga data pada validation data berjumlah total menjadi 59 9 gambar. testing data tidak mengalami augmentasi karena pada proses pengujian atau evaluasi mode l ingin menggunakan data asli yang sebenarnya untuk melihat kinerja model pada kasus kasus yang belum pernah dilihat sebelumnya. berikut merupakan gambar 3. 6 tahapan data augmentation . data augmentation flip rotation shear brightness gambar 3. 6 tahapan data augmentation 3.2.5 data splitting tahap ke lima data splitting dengan membagi file dataset menjadi subset training data validation data dan testing data berisikan gambar ban normal dan gambar ban tidak normal sehingga subset training data digunakan untuk melatih model sedangkan subset validation data digunakan untuk menguji kinerja model. sebenarnya langkah langkah dalam proses pra pemrosesan data yang mempersiapkan data mentah untuk digunakan dalam pelatihan model adalah tahapan yang sudah disebutkan sebelumnya data aquisition data prerocessing data augmentation dan splitting data. prosedur yang disebutkan di atas berkonsentrasi pada pengumpulan sanitasi pengorganisasian dan penambahan jumlah data yang diperlukan untuk pelatihan model. rinciannya yaitu file yang tersimpan di dalam komputer total data gambar sebanyak 2.050 gambar yang dibagi menjadi dua pertama adalah file testing data dengan jumlah data tersimpan sebanyak 650 gambar yang dibagi menjadi sub file crack berjumlah 420 dan sub file normal berjumlah 230 data. kedua adalah file training data dengan jumlah data tersimpan sebanyak 1400 gambar yang dibagi menjadi sub file crack berjumlah 654 dan sub file normal berjumlah 746 data. maka ketika dilakukan data splitting pada program secara otomatis yang pada data augmentasi diatur menjadi pembagian 80 untuk training data dan 20 untuk validation data yaitu untuk train data sebanyak 1.121 gambar dengan 2 kelas validation data sebanyak 279 gambar dengan 2 kelas dan test data sebanyak 650 gambar dengan 2 kelas. testing data bernilai tetap h al ini bertujuan agar kuantitas data awal yang telah ditentukan sebelumnya tetap terjaga dan testing data tidak terpengaruh oleh prosedur pemisahan. setelah model dilatih dan divalidasi testing data digunakan untuk mengevaluasi performa akhir model. akibatnya testing data tidak terbagi dan rincian asli 648 foto masih berlaku. berikut merupakan gambar 3.7 tahapan splitting data . data splitting training data validation data testing data gambar 3. 7 tahapan splitting data 3.2.6 model building tahap ke enam adalah model building membangun model convolutional neural network cnn dengan keras membangun arsitektur model convolutional neural network cnn menggunakan keras mengatur lapisan lapisan seperti convolutional maxpooling2d flatten dan dense untuk membangun model. learning rate dalam penggunaan algoritma optimasi menggunakan adaptive momentum adam untuk menghasilkan pembelajaran yang adaptif pemilihan penggunaan adaptive momentum adam jika dibandingkan dengan learning rate lain seperti stochastic gradient descent sgd karena kecepatan pembelajaran adaptif untuk adaptive momentum adam bisa secara otomatis menyesuaikan learning rate untuk setiap parameter dalam model klasifikasi ban sedangkan stochastic gradient descent sgd memiliki learning rate tetap selama pelatihan model klasifikasi ban yang penentuannya dari user dan tidak bisa menyesuaikan learning rate secara otomatis berdasarkan kondisi a ktual dari setiap parameter. selanjutnya secara kestabilan dan konvergensi adaptive momentum adam menyambung dari awal dapat mengubah kecepatan pembelajaran secara adaptif sehingga membuatnya lebih stabil dan kecil kemungkinannya terjebak pada tingkat minimum lokal nilai yang dianggap sebagai titik terendah dari loss function dalam model sehingga adaptive momentum adam cenderung mencapai konvergensi tingkat kinerja yang diharapkan lebih cepat dan andal dalam berbagai keadaan sedangkan stochastic gradient descent sgd mungkin lebih stuck pada nilai minimum atau terjebak pada nilai minimum lokal ya ng disebabkan oleh kemungkinan bergantung pada seberapa tepat kecepatan pemelajaran dipilih kecepatan pemelajaran yang tetap dapat membuat model mencapai konvergensi terlalu cepat atau terlalu lambat. adapun melakukan pendekatan kedua penam bahan data keti ka masuk ke model building dan terjadi proses pemodelan setelah menggunakan epoch . augment asi data diterapkan setelah data melewati beberapa epoch selama proses pelatihan sehingga variasi data yang dihasilkan akan berbeda beda pada setiap epoch dan model dapat terus menerus terlatih dengan variasi data yang lebih besar . menggunakan 100 epoch sehingga total training data yang diproses menjadi 230.400 gambar dan validation data menjadi 6.400 gambar. sehingga jumlah data yang diproses selama pelatihan menjadi sangat besar dan pada akhirnya nanti akan menyiapkan m odel kompilasi dalam mengatur pengoptimal adam fungsi kerugian biner crossentropy dan metrik evaluasi akurasi. berikut merupakan gambar 3. 8 tahapan building model . model building define cnn model compile model gambar 3 .8 tahapan building model berdasarkan hasil analisis sebelumnya maka dapat diketahui untuk j umlah data asli training data adalah 1.121 jumlah data asli validation data adalah 279 jumlah data asli training data setelah augmentasi adalah 1152 jumlah data asli validation data setelah augmentasi adalah 320 jumlah epoch yang digunakan sebanyak 100 dan ukuran batch adalah 64. berikut merupakan perhitungan manualnya ketika masuk ke model building dan terjadi proses pemodelan setelah menggunakan epoch . 1. jumlah batch per epoch untuk training data . 𝑆𝑡𝑒𝑝𝑝𝑒𝑟𝑒𝑝𝑜𝑐ℎjumlahdatatrainjumlahdataaugmentasi train 𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒 11211152 64 2273 64 35515636𝑏𝑎𝑡𝑐ℎ 2. jumlah batch per epoch untuk validation data. 𝑆𝑡𝑒𝑝𝑝𝑒𝑟𝑒𝑝𝑜𝑐ℎjumlahdatavalidjumlahdataaugmentasi valid 𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒 279320 64 599 64 9359310𝑏𝑎𝑡𝑐ℎ 3. total jumlah data setelah augmentasi untuk semua epoch . a. training data total𝑇𝑟𝑎𝑖𝑛𝑖𝑛𝑔 datajumlah𝐵𝑎𝑡𝑐ℎx𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒xjumlah𝐸𝑝𝑜𝑐ℎ𝑠 36x64x100 230.400 b. validation data total𝑉𝑎𝑙𝑖𝑑𝑎𝑡𝑖𝑜𝑛 datajumlah𝐵𝑎𝑡𝑐ℎx𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒xjumlah𝐸𝑝𝑜𝑐ℎ𝑠 10x64x100 64.000 3.2.7 model evaluation testing tahap kedelapan adalah model evaluation testing digunakan sebagai bahan terusan pada model building yang dibuat untuk melakukan evaluasi performanya dengan menggunakan bagian pengujian dan parameter yang digunakan pada metrik evaluasi seperti akurasi presisi recall dan f1score . berikut merupakan gambar 3. 9 tahapan model evaluation testing . model evaluation testing accuracy precision recall f1score gambar 3. 9 tahapan model evaluation testing 3.3 arsitektur convolutional neural network cnn convolutional neural network cnn yang dibangun menggunakan model atau kerangka kerja yang pada dasarnya menggunakan keras dan juga tensorflow dengan menambahkan beberapa model lapisan lapisan seperti lapisan convolutional conv2d laposan pooling maxpooling 2d flatten dan lapisan fully connected dense . berikut merupakan gambar 3. 10 tahapan convolutional neural network cnn dengan model keras. conv2d filter 128 ukuran filter 3 atau 3x3 strides 2 input size 189x189 jumlah neuron 1280maxpooling2d pool size 2 strides 2 input size 94x94 jumlah neuron 1280conv2d 2nd filter 64 ukuran filter 3 atau 3x3 strides 2 input size 46x46 jumlah neuron 73792maxpooling2d 2nd pool size 2 strides 2 input size 23x23 jumlah neuron 73792 conv2d 3nd filter 32 ukuran filter 3 atau 3x3 strides 2 input size 11x11 jumlah neuron 18464maxpooling2d 2nd pool size 2 strides 2 input size 5x5 jumlah neuron 18464conv2d 4nd filter 16 ukuran filter 3 atau 3x3 strides 2 input size 2x2 jumlah neuron 4624maxpooling2d 4nd pool size 2 strides 2 input size 1x1 jumlah neuron 4624 flatten input size 1x1x16 jumlah neuron 16dense layer 1 dengan aktivasi relu jumlah neuron 128 jumlah parameter 2176dropout layer 1 rate 0.2 20dense layer 2 dengan aktivasi relu jumlah neuron 64 jumlah parameter 8256 dropout layer 2 rate 0.2 20dense layer 3 dengan aktivasi sigmoid jumlah neuron 1 biner sigmoid jumlah parameter 65 training fit callbacks optimizer adam evaluation confusion matrix report epochs 100 gambar 3. 10 tahapan convolutional neural network cnn dengan model keras berdasarkan gambar 3. 10 tahapan convolutional neural network cnn dengan model keras maka dapat dijelaskan mulai dari yang mencakup lapisan lapisan konvolusi yang telah dilatih pada dataset besar seperti imagenet untuk mengekstrak fitur dari gambar gambar penggunaan image size diatur dengan 379 379 batch size 64 kernel size 3 strides 2 untuk cov2d dan 2 untuk maxpooling2d dan pool size 2. selanjutnya conv2d yang merupakan convolutional layer pertama yang berfungsi untuk mengekstrak fitur fitur visual dari gambar. filter convolutional layer pertama yang berfungsi untuk mengekstrak fitur fitur visual diterapkan pada gambar untuk menghasilkan fitur fitur yang lebih abstrak formula untuk mengetahui jumlah training datanya dengan . selanjutnya max pooling 2d di mana tahap pooling digunakan untuk mengurangi dimensi spasial dari setiap feature map yang dihasilkan oleh layer sebelumnya. max pooling memilih nilai maksimum di dalam jendela pooling untuk m engurangi ukuran fitur dan mempertahankan informasi penting. selanjutnya conv2d dan max pooling 2d diulang sampai 4 layer karena untuk terus mengekstrak fitur fitur yang semakin kompleks dari gambar. selanjutnya flatten digunakan untuk mengubah tensor multi dimensi menjadi tensor satu dimensi di mana setelah serangkaian layer konvolusi dan pooling masukan dari layer terakhir perlu diubah menjadi vektor tunggal sebelum dimasukkan ke dalam layer dense . flatten layer melakukan hal ini dengan mengubah matriks output menjadi array satu dimensi. selanjutnya dense layers lapisan dense digunakan sebagai lapisan output dalam model klasifikasi di mana jumlah neuron dalam lapisan output sesuai dengan jumlah kelas yang harus diprediksi di mana ada tiga lapisan dense ditambahkan dengan fungsi pertama dan kedua menggunakan relu sebagai 0 f x max x yang artinya menunjukkan bahwa keluarannya nol jika masukannya negatif atau nol dan output x jika masukannya positif dengan 128 unit neuron dan pada dense kedua 64 unit neuron karena tugasnya mengurangi dimensi representasi pada lapisan dense pertama maka model dapat mempelajari pola yang lebih rumit dan mendalam dari data dengan menambahkan lapisan yang lebih padat yang dapat meningkatkan performa model dalam tugas klasifikasi gambar. lapisan dense ketiga dengan fungsi aktivasi sigmoid untuk output biner dengan menunjukan kelas prediksi dari gambar yaitu normal atau crack . di antara tig a lapisan dense di ikuti dengan lapisan dropout untuk mencega h overfitting di mana model pembelajaran mesin terlalu menghafal pola dari training data yang tersedia sehingga kinerjanya menurun secara signifikan saat diuji dengan data baru yang tidak dilihat sebelumnya juga dimasukkan setelah setiap lapisan dense untuk mencegah overfitting dengan secara acak menonaktifkan seba gian unit sebanyak 0.2 atau 20 dari neuron selama pelatihan. selanjutnya training di mana model diterapkan pada training data dengan menggunakan metode fit dan callback . model fit digunakan untuk melatih model dengan training data dan model callback menggunakan modelcheckpoint untuk menyimpan model terba ik selama pelatihan berkaitan dengan performa pada validation data mengontrol proses pelatihan. terakhir evaluation di mana performa model pada testing data dinilai menggunakan hasil klasifikasi dan confusion matrix untuk memahami kinerjanya testing data. banyaknya parameter atau bobot dan jumlah data yang harus dipelajari selama pelatihan bergantung pada jumlah neuron pada lapisan. jumlah data yang harus dipelajari model selama pelatihan tercermin dalam jumlah parameter ini. berikut merupakan perhitungan dalam mengetahui total neuron yang dikerjakan oleh setiap lapisan. 1. first conv2d totalneuronukuranfilterxjumlah𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝐼𝑛𝑝𝑢𝑡1xfilter 3x3x11x128 10x128 1280 kedalaman gambar yang diproses lapisan konvolusi sebenarnya ditunjukkan oleh jumlah saluran masukan. tiga saluran merah hijau dan biru membentuk sebuah gambar jika diwarnai artinya ada tiga saluran masukan. karena kata grayscale digunakan untuk mendeskripsikan gambar ini hanya ada satu saluran warna dan bernilai 1 . sehingga jumlah neuronnya 1280 yang berarti ada 1280 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan . selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 379x379 menjadi 189x189 sebagai berikut. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝐾𝑒𝑟𝑛𝑒𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 3793 11 376 21 189 2. first maxpooling2d tidak ada parameter baru yang ditambahkan dan jumlah neuron dalam contoh ini lapisan konvolusi pertama tetap sama. setiap filter diubah menjadi setengah dari ukuran inputnya 189x189 menjadi 94x94 dan jumlah neuronnya 1280 mengikuti lapisan konvolusi pertama . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 1892 21 187 21 94 3. second cov2d totalneuronukuranfilterxjumlah𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝐼𝑛𝑝𝑢𝑡1xfilter 3x3x1281x64 1153x128 73792 jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. sehingga jumlah neuronnya 73792 yang berarti ada 73792 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan . selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 94x94 menjadi 46x46 sebagai berikut. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝐾𝑒𝑟𝑛𝑒𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 943 21 91 21 46 4. second maxpooling2d tidak ada parameter baru yang ditambahkan dan jumlah neuron dalam contoh ini lapisan konvolusi kedua tetap sama. setiap filter diubah menjadi setengah dari ukuran inputnya 46x46 menjadi 23x23 dan jumlah neuronnya 73792 mengikuti lapisan konvolusi kedua . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 462 21 44 21 23 5. third cov2d totalneuronukuranfilterxjumlah𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝐼𝑛𝑝𝑢𝑡1xfilter 3x3x641x32 577x32 18464 jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. sehingga jumlah neuronnya 18464 yang berarti ada 18464 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan . selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 23x23 menjadi 11x11 sebagai berikut. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝐾𝑒𝑟𝑛𝑒𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 233 21 20 21 11 6. third maxpooling2d tidak ada parameter baru yang ditambahkan dan jumlah neuron dalam contoh ini lapisan konvolusi ketiga tetap sama. setiap filter diubah menjadi setengah dari ukuran inputnya 11x11 menjadi 5x5 dan jumlah neuronnya 18464 mengikuti lapisan konvolusi ketiga . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 112 21 9 21 5 7. fourth cov2d totalneuronukuranfilterxjumlah𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝐼𝑛𝑝𝑢𝑡1xfilter 3x3x321x16 289x16 4624 jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. sehingga jumlah neuronnya 4624 yang berarti ada 4624 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan . selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 5x5 menjadi 2x2 sebagai berikut. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝐾𝑒𝑟𝑛𝑒𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 53 21 1 21 1.52 8. fourth maxpooling2d tidak ada parameter baru yang ditambahkan dan jumlah neuron dalam contoh ini lapisan konvolusi keempat tetap sama. setiap filter diubah menjadi setengah dari ukuran inputnya 2x2 menjadi 1x1 dan jumlah neuronnya 4624 mengikuti lapisan konvolusi keempat . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 12 21 1 21 0.51 9. flatten tidak mengubah parameter yang ada karena fungsi flatten hanya mengubah matriks multidimensi menjadi vektor tunggal berdasarkan hasil dari jumlah filter pada lapisan lima cov2d yaitu 8 dan maxpooling2d dengan ukuran inputnya 1x1 sehingga menjadi matriks multidimensi 1 1 16 diubah menjadi nilai vektor tunggal dengan panjang 16 atau menjadi jumlah neuron sebanyak 16. 10. dense layer 1 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 161x128 17x128 2176 11. dropout layer 1 menggunakan 0.2 yang artinya sebanyak 20 dari neuron dalam dense layer 1 akan dinonaktifkan secara acak. 12. dense layer 2 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 1281x64 129x64 8256 13. dropout layer 2 menggunakan 0.2 yang artinya sebanyak 20 dari neuron dalam dense layer 2 akan dinonaktifkan secara acak. 14. dense layer 2 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 641x1 65x1 65 ketika dimensi spasial tinggi dan lebar dikurangi menggunakan operasi lapisan pooling seperti maxpooling jumlah neuron di setiap lapisan pooling akan menurun. misalnya dimensi spasial setiap filter tinggi dan lebar di lapisan maxpooling disesuaikan menjadi setengah dari dimensi masukannya. karena hanya separuh dari masukan yang diproses lebih lanjut hal ini juga menyebabkan berkurangnya jumlah neuron pada lapisan tersebut. sedangkan penurunan pada dense terjadi karena penentuan jumlah neuron.,3.1 gambaran umum penelitian penelitian ini digunakan untuk mengatasi sensitivitas terhadap cacat pada gambar ban dengan melibatkan penggunaan jaringan syaraf menggunakan algoritma convolutional neural network cnn dan membangun model atau kerangka kerja menggunakan keras. berikut adalah gambar 3.1 blok diagram gambaran umum penelitian. data preparation data augmentasi data splitting model training forward pass model evaluation backward passfinetuning if neededinput unit processing unit output unit model deployment inference12 gambar 3.1 blok diagram gambaran umum penelitian berdasarkan gambar 3.1 blok diagram gambaran umum penelitian maka dapat dijelaskan di blok tersebut terbagi menjadi 3 bagian yaitu bagian pertama adalah unit masukan berisikan data preparation di mana gambar ban dimuat diubah menjadi format yang sesuai dipersiapkan untuk pelatihan model convolutional neural network cnn seperti pemrosesan gambar ban selanjutnya data augmentation di mana data dibuat lebih ber variasi dari training data yang ada sehingga dapat meningkatkan keberagaman training data tanpa h arus mengambil data baru mencakup rotasi pergeseran horizontalvertikal perbesar gambar perubahan kecerahan gambar sampai mengubah nilai pixel selanjutnya data di mana dataset yang telah di augmentasi dan disiapkan dibagi menjadi subset yang berbeda untuk training untuk melatih model validation untuk menyempurnakan model serta me mvalidasi performanya selama pelatihan dan testing untuk mengevaluasi kinerja model akhir . data set dibagi menjadi training data validation data dan testing data dalam proporsi tertentu. bagian kedua adalah unit pemrosesan yang bertindak adalah model training forward pass tahap di mana input diproses melalui model untuk menghasilkan prediksi tujuannya melatih model convolutional neural network cnn menggunakan dataset pelatihan di mana data dari unit masukan diteruskan melalui jaringan neural di lakukan transformasi linier konvulasi dan non linier fungsi aktivasi dilakukan pada data di setiap lapisan untuk menghasilkan output prediksi yang melibatkan komputasi di setiap neuron dan lapisan jaringan yang merupakan inti dari proses pembelajaran dalam jaringan saraf. selanjutnya unit pemrosesan finetuning tujuannya dilakukan untuk menyempurnakan model lebih lanjut setelah pelatihan awal dengan dataset yang lebih kecil atau lebih spesifik nantinya. proses di dalam finetuning menyesuaikan bobot menggunakan kumpulan data yang lebih kecil untuk menyesuaikan bobot model untuk performa yang lebih baik pelatihan khusus fokus pada fitur data yang lebih relevan dengan objek . bagian ketiga adalah unit keluaran yang bertindak ada proses model evaluatioan backward pass tahap di mana gradien memperbarui parameter model dalam arah yang akan mengurangi fungsi loss dari fungsi loss metrik yang mengukur seberapa baik atau buruk model melakukan prediksi dibandingkan nilai aktualnya dihitung dan digunakan untuk memperbarui parameter model selama pelatihan tujuannya mengevaluasi performa model yang dilatih dan model dievaluasi menggunakan metrik yang relevan accuracy precision recall dan f1 score berdasarkan prediksi yang dihasilkan dari model terhadap validasi atau uji data. output dari proses ini adalah tentang hasil evaluasi model yang memberikan informasi kinerja model. selanjutnya ada dua alur pilihan yang bisa dilakukan alur pertama jika hasil prediksi sudah sesuai dengan keinginan maka bisa langsung masuk ke model deployment inference dan alu r kedua jika hasil prediksi masih perlu diperbaiki pada bagian unit pemrosesan terlebih dahulu fine tuning untuk penggunaan data set lebih kecil jika menunjukan model belum mencapai performa yang diharapkan baru masuk ke model deployment inference tujuannya menerapkan model terlatih untuk membuat prediksi pada data baru yang belum terlihat. model deployment inference yang telah dilatih digunakan untuk membuat prediksi pada data baru atau dalam situasi dunia nyata tahap di mana model menerima input baru dan menghasilkan output berdasarkan pada pembelajaran yang dilakukan selama proses pelatihan dan merupakan output akhir dari keseluruhan proses di mana model mengambil keputusan atau membuat prediksi berdasarkan pada pengalaman yang telah diperoleh selama pelatihan. 3.2. tahapan penelitian penelitian ini di dalamnya terdapat tahapan tahapan yang dilakukan untuk membentuk satu kesatuan yang utuh dari awal sampai akhir dan membentuk kerangka penelitian mengenai klasifikasi pada produk ban menggunakan algoritma convolutional neural network cnn . 3.2.1 studi literatur tahap pertama adalah studi literatur di mana studi yang dilakukan berasal dari artikel ilmiah dan buku yang menunjang dalam menganalisis terkait dengan metode pen gukuran kualitas mengenai klasifikasi produk ban meninjau penggunaan pembelajaran mesin algoritma convolutional neural network cnn dari beberapa tahun ke belakang dalam konteks peng ukuran kualitas untuk klasifikasi terhadap kondisi kondisi produk ban . 5 tahapan data preprocessing 3.2.4 data augmentation tahap keempat adalah data augmentation meningkatkan variasi dalam dataset dengan teknik augmentasi data menggunakan operasi seperti rotasi pergeserarn horizontalvertikal perbesar gambar perubahan kecerahan gambar sampai mengubah nilai pixel untuk memperkaya dataset dan mengurangi overfitting saat disajikan dengan data baru yang belum pernah dilihat sebelumnya performa model akan menurun drastis karena model tersebut dapat menyesuaikan diri dengan kumpulan training data dengan sangat efektif. sebaliknya pada saat runtime hanya menghasilkan variasi dari gambar yang sudah ada dibuat secara dinami s dan cukup bagi model untuk berlatih dari berbagai kondisi gambar ban yang ada pada kenyataaanya. secara lebih jelas nilai teknik augmentasi pertama dilakukan dengan manual menggunakan bantuan dari website roboflow dengan resize gambar menjadi 640 x 640 pada augmentasinya menggunakan model flip horizontal dan vertikal 90 pemutaran searah jarum jam berlawanan arah jarum jam dan terbalik rotasi 45 dan 45 shear 5 horizontal dan 5 vertikal brightness 20 sampai 20. data asli pada dataset berjumlah 1 .028 data gambar ban setelah dilakukan augmentasi secara fisik menggunakan website roboflow ada data yang tidak dapat diidentifikasi ada 3 gambar sehingga total gambar asli yang berhasil di upload dan dijadikan data asli yang tetap berjumlah 1.025 data ga mbar dan setelah di augmentasi bertambah menjadi 2 .050 data gambar ban. 7 tahapan splitting data 3.2.6 model building tahap ke enam adalah model building membangun model convolutional neural network cnn dengan keras membangun arsitektur model convolutional neural network cnn menggunakan keras mengatur lapisan lapisan seperti convolutional maxpooling2d flatten dan dense untuk membangun model. selanjutnya secara kestabilan dan konvergensi adaptive momentum adam menyambung dari awal dapat mengubah kecepatan pembelajaran secara adaptif sehingga membuatnya lebih stabil dan kecil kemungkinannya terjebak pada tingkat minimum lokal nilai yang dianggap sebagai titik terendah dari loss function dalam model sehingga adaptive momentum adam cenderung mencapai konvergensi tingkat kinerja yang diharapkan lebih cepat dan andal dalam berbagai keadaan sedangkan stochastic gradient descent sgd mungkin lebih stuck pada nilai minimum atau terjebak pada nilai minimum lokal ya ng disebabkan oleh kemungkinan bergantung pada seberapa tepat kecepatan pemelajaran dipilih kecepatan pemelajaran yang tetap dapat membuat model mencapai konvergensi terlalu cepat atau terlalu lambat. 9 tahapan model evaluation testing 3.3 arsitektur convolutional neural network cnn convolutional neural network cnn yang dibangun menggunakan model atau kerangka kerja yang pada dasarnya menggunakan keras dan juga tensorflow dengan menambahkan beberapa model lapisan lapisan seperti lapisan convolutional conv2d laposan pooling maxpooling 2d flatten dan lapisan fully connected dense . 10 tahapan convolutional neural network cnn dengan model keras. filter convolutional layer pertama yang berfungsi untuk mengekstrak fitur fitur visual diterapkan pada gambar untuk menghasilkan fitur fitur yang lebih abstrak formula untuk mengetahui jumlah training datanya dengan . max pooling memilih nilai maksimum di dalam jendela pooling untuk m engurangi ukuran fitur dan mempertahankan informasi penting. selanjutnya dense layers lapisan dense digunakan sebagai lapisan output dalam model klasifikasi di mana jumlah neuron dalam lapisan output sesuai dengan jumlah kelas yang harus diprediksi di mana ada tiga lapisan dense ditambahkan dengan fungsi pertama dan kedua menggunakan relu sebagai 0 f x max x yang artinya menunjukkan bahwa keluarannya nol jika masukannya negatif atau nol dan output x jika masukannya positif dengan 128 unit neuron dan pada dense kedua 64 unit neuron karena tugasnya mengurangi dimensi representasi pada lapisan dense pertama maka model dapat mempelajari pola yang lebih rumit dan mendalam dari data dengan menambahkan lapisan yang lebih padat yang dapat meningkatkan performa model dalam tugas klasifikasi gambar. terakhir evaluation di mana performa model pada testing data dinilai menggunakan hasil klasifikasi dan confusion matrix untuk memahami kinerjanya testing data. bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 23x23 menjadi 11x11 sebagai berikut. setiap filter diubah menjadi setengah dari ukuran inputnya 11x11 menjadi 5x5 dan jumlah neuronnya 18464 mengikuti lapisan konvolusi ketiga . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 112 21 9 21 5 7. fourth cov2d totalneuronukuranfilterxjumlah𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝐼𝑛𝑝𝑢𝑡1xfilter 3x3x321x16 289x16 4624 jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 5x5 menjadi 2x2 sebagai berikut. setiap filter diubah menjadi setengah dari ukuran inputnya 2x2 menjadi 1x1 dan jumlah neuronnya 4624 mengikuti lapisan konvolusi keempat . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 12 21 1 21 0.51 9. flatten tidak mengubah parameter yang ada karena fungsi flatten hanya mengubah matriks multidimensi menjadi vektor tunggal berdasarkan hasil dari jumlah filter pada lapisan lima cov2d yaitu 8 dan maxpooling2d dengan ukuran inputnya 1x1 sehingga menjadi matriks multidimensi 1 1 16 diubah menjadi nilai vektor tunggal dengan panjang 16 atau menjadi jumlah neuron sebanyak 16. 10. dense layer 1 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 161x128 17x128 2176 11. dropout layer 1 menggunakan 0.2 yang artinya sebanyak 20 dari neuron dalam dense layer 1 akan dinonaktifkan secara acak. 12. dense layer 2 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 1281x64 129x64 8256 13. dropout layer 2 menggunakan 0.2 yang artinya sebanyak 20 dari neuron dalam dense layer 2 akan dinonaktifkan secara acak. 14. dense layer 2 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 641x1 65x1 65 ketika dimensi spasial tinggi dan lebar dikurangi menggunakan operasi lapisan pooling seperti maxpooling jumlah neuron di setiap lapisan pooling akan menurun. misalnya dimensi spasial setiap filter tinggi dan lebar di lapisan maxpooling disesuaikan menjadi setengah dari dimensi masukannya. karena hanya separuh dari masukan yang diproses lebih lanjut hal ini juga menyebabkan berkurangnya jumlah neuron pada lapisan tersebut. sedangkan penurunan pada dense terjadi karena penentuan jumlah neuron.
Bayu Kumoro.txt,"3.1. Obyek penelitian
       Obyek penelitian ini adalah gambar RGB sebagai cover (diambil dari internet Google Image) atau gambar grayscale yang akan disisipkan pesan berupa teks. 3.2. Tahapan Penelitian
       Penelitian ini berusaha mengembangkan algoritma dan prototipe sebagai solusi dari masalah dan kekurangan dari teknik yang pernah dilakukan peneliti terdahulu yang dapat mengatasi optimasi hardware FPGA. Rencana penelitian mencoba menggabungkan mengembangkan algoritma dan mengembangkan metode yang dapat mengoptimalkan performa FPGA. Sehingga, metode untuk hasil yang diinginkan yaitu penggunaan komponen efisien, power yang digunakan lebih kecil dibandingkan penelitian sebelumnya. Rencana yang akan dilakukan menggunakan metode dan modifikasi pada Penelitian (Abdullah AlWatyan, 2017) dengan melakukan tahapan penyediaan pesan rahasia dan Cover Image. Dengan pesan rahasia akan dissipkan pada cover Image dengan key yang sudah dibuat pada hardware FPGA hingga menjadi Stego Image. Pada tahapan ekstraksi Stego Image akan mengambil pesa rahasia yang sudah disisipkan dengan key yang sudah dibuat. Kemudian, Stego Image akan menjadi pesan rahasia dan Cover Image. Dan untuk mengoptimalkan hardware FPGA akan menggunakan perbandingan metode yang dilakukan peneliti (E. A. Elshazly, 2018) dengan menggunakan metode algoritma steganografi gambar GEMD. Pada gambar 6 menggambarkan alur hardware dari proposal. Tahap pertama dimulai dengan simulasi metode pada MATLAB. Pada MATLAB, difokuskan pada metode algoritma yang diajukan sampai dengan pengecekan kualitas stego image dan cover image. Setelah tahap MATLAB sudah selesai, dilanjutkan pada hardware FPGA. Pada FPGA, diimplementasikan algoritma yang sudah dibuat dan pengecekan kualitas gambar dilanjutkan dengan analisa energi yang digunakan FPGA. Jika energi sudah optimal, dilakukan analisa performa algoritma dan sistem secara keseluruhan. 3.3 Proses Algoritma Pemasukan Dan Ekstraksi Pesan Tersembunyi
      Pseudo-random number generator (PRNG) dapat digunakan untuk memilih piksel secara acak dan menyematkan pesan. Ini akan membuat bit pesan lebih sulit ditemukan dan mudah-mudahan mengurangi realisasi pola dalam gambar. Data dapat disembunyikan dalam LSB bidang warna tertentu (bidang merah) dari piksel yang dipilih secara acak dalam ruang warna RGB. Jika pesan jauh lebih kecil dari kapasitas gambar, masalah dapat terjadi di mana informasi akan dikemas ke dalam satu bagian gambar misalnya setengah bagian atas. Ini diselesaikan dengan menggunakan PRNG yang akan menyebarkan pesan ke seluruh gambar. Karenanya noise juga akan didistribusikan secara acak. Generator angka acak pseudo menghitung dan memilih urutan piksel yang akan dipilih untuk penyematan data berdasarkan kunci. Pesan yang akan disembunyikan dikonversi menjadi byte yang masing-masing karakter dalam pesan dikonversi ke ASCII-nya. Sebagai contoh jika kita mengambil karakter ""A"" dalam pesan maka ""A"" = 1000001 disimpan dalam array byte. Karena nilai ASCII untuk ""A"" adalah 65 dan setara biner adalah 1000001. As gambar terdiri dari kontribusi piksel dari komponen merah, hijau dan biru dan setiap piksel memiliki angka dari komponen warna (untuk gambar bitmap 24-bit masing-masing merah, hijau dan pixel biru memiliki 8 bit). Pada 8 bit dari jumlah warna, jika kita mengubah bit yang paling tidak signifikan, sistem penglihatan kita dapat mendeteksi perubahan dalam pixel dan dengan demikian dimungkinkan untuk mengganti bit pesan dengan bit pixel gambar. Misalnya jika nilai bidang merah piksel adalah 10111011, dan penulis ingin menyimpan informasi dalam bit paling tidak signifikan, pada situasi terburuk nilai piksel berubah menjadi 10111010, pemeriksaan menunjukkan bahwa HVS tidak dapat membedakan perubahan ini. Jadi kami menyimpan informasi kami ke dalam bit terkecil dari bidang merah piksel. Jika kami mengubah LSB dalam satu byte gambar, kami dapat menambah atau mengurangi satu dari nilai yang diwakilinya. Ini berarti kita dapat menimpa bit terakhir dalam byte tanpa mempengaruhi warnanya. Untuk menyembunyikan pesan, data terlebih dahulu dikonversi ke dalam format byte dan disimpan dalam array byte. Pesan tertanam di setiap bit ke posisi LSB dari bidang merah setiap piksel. Ia menggunakan piksel pertama (pada titik 0) untuk menyembunyikan panjang pesan (jumlah karakter). Anggap piksel asli kami sebagai bit: (r7 r6 r5 r4 r3 r2 r1 r0, g7 g6 g5 g4 g3 g2 g1 g0, b7 b6 b5 b4 b3 b2 b1 b0 b0) Selain itu, karakter kami (byte) memiliki beberapa bit: (c7 c6 c5 c4 c3 c2 c1 c0). Menempatkan bit karakter dalam LSB piksel merah saja alih-alih menempatkannya dalam LSB merah, hijau, dan biru. Tujuannya adalah untuk membuat pesan jauh lebih aman. Pesan disimpan dalam byte merah. Jadi piksel yang dipilih tersebar dan keamanan pesan lebih tinggi. Kemudian kita dapat menempatkan bit karakter di piksel merah terendah, bit karakter berikutnya di piksel merah terendah berikutnya, dan seterusnya. (r7 r6 r5 r4 r3 r2 r1 c0, g7 g6 g5 g4 g3 g2 g1 g0, b7 b6 b5 b4 b3 b2 b2 b1 b0). Jika kita mengambil contoh piksel (225.100.100) diwakili dalam bentuk biner (11100001, 01100100, 01100100) di mana untuk menanamkan karakter pesan ""d"" memiliki nilai biner 1100100 (nilai ASCII 100) kemudian setelah menanamkan bit pertama dari ""d"" di bidang merah pixel kita dapat memperoleh Pixel baru sebagai (224, 100.100) diwakili dalam biner (11100000, 01100100, 01100100). Di sini kita dapat melihat bahwa nilai piksel (225, 100.100) diubah menjadi (224.100.100). Dari percobaan dapat diamati bahwa perubahan tersebut tidak akan memiliki perbedaan warna yang nyata pada gambar. Paling buruk, nilai desimal piksel dapat bertambah atau berkurang satu. Perubahan nilai piksel seperti itu tidak memengaruhi gambar dan tidak terdeteksi. Proses penyematan adalah sebagai berikut. Input: Gambar sampul, stego-key dan file teks 
Output: gambar stego
1. Ekstrak piksel gambar sampul. 2. Ekstrak karakter file teks. 3. Ekstrak karakter dari tombol Stego. 4. Pilih piksel pertama dan pilih karakter dari tombol Stego dan letakkan di komponen piksel pertama. 5. Tempatkan beberapa simbol terminating untuk menunjukkan ujung kunci. 0 telah digunakan sebagai simbol terminating dalam algoritma ini. 6. Masukkan karakter file teks di setiap komponen pertama piksel berikutnya dengan menggantinya. 7. Ulangi langkah 6 hingga semua karakter tertanam. 8. Sekali lagi letakkan beberapa simbol terminating untuk menunjukkan akhir data. 9. Memperoleh gambar stego. Proses ekstraksi adalah sebagai berikut. Input: File stego-image, stego-key 
Output: Pesan teks rahasia. 1. Ekstrak piksel gambar stego. 2. Sekarang, mulai dari piksel pertama dan ekstrak karakter kunci stego dari komponen pertama piksel. 3. Ikuti Langkah 3 hingga mengakhiri simbol, jika tidak ikuti langkah 4. 4. Jika kunci yang diekstraksi ini cocok dengan kunci yang dimasukkan oleh penerima, maka ikuti Langkah 5, jika tidak, matikan program. 5. Jika kunci sudah benar, maka pergi ke piksel berikutnya dan ekstrak karakter pesan rahasia dari komponen pertama piksel berikutnya. Ikuti Langkah 5 hingga hingga mengakhiri simbol, jika tidak ikuti langkah 6. 6. Ekstrak pesan rahasia
      Dalam proses metode pengkodean ini, kunci acak digunakan untuk mengacak gambar sampul dan kemudian menyembunyikan bit-bit pesan rahasia ke dalam bit paling tidak signifikan dari piksel dalam gambar sampul. Ujung pengirim dan penerima berbagi kunci stego dan kunci acak. Kunci acak biasanya digunakan untuk menabur generator angka acak untuk memilih lokasi piksel dalam gambar untuk menanamkan pesan rahasia. Input: Gambar sampul, kunci stego dan pesan 
Output: gambar stego
1. Baca karakter dari file teks yang akan disembunyikan dan konversikan nilai ASCII dari karakter menjadi nilai biner yang setara menjadi array integer 8 bit. 2. Baca gambar warna RGB (gambar sampul) ke mana pesan akan tertanam. 3. Baca bit merah terakhir. 4. Inisialisasi kunci acak dan permutasi acak piksel gambar sampul dan membentuk kembali menjadi matriks. 5. Inisialisasi stego-key dan XOR dengan file teks menjadi sembunyikan dan berikan pesan. 6. Masukkan bit pesan rahasia ke LSB dari Pixel bidang merah. 7. Tulis piksel di atas ke Stego Image File. Dalam proses ekstraksi ini, proses pertama-tama mengambil kunci dan kemudian kunci acak. Kunci-kunci ini mengambil poin dari LSB di mana pesan rahasia didistribusikan secara acak. Proses decoding mencari bit-bit tersembunyi dari sebuah pesan rahasia ke bit paling tidak signifikan dari piksel dalam gambar sampul menggunakan kunci acak. Dalam algoritma penguraian kata kunci acak harus cocok yaitu kunci acak yang digunakan dalam pengkodean harus cocok karena kunci acak menetapkan titik-titik penyembunyian pesan dalam kasus pengkodean. Kemudian penerima dapat mengekstraksi pesan yang tertanam persis menggunakan stego-key. Input: File stego-image, stego-key, kunci acak. Output: Pesan rahasia. 1. Buka file gambar Stego dalam mode baca dan dari file Gambar, baca warna RGB dari setiap piksel. 2. Ekstrak komponen merah dari gambar host. 3. Baca bit terakhir dari setiap piksel. 4. Inisialisasi kunci acak yang memberikan posisi tombol bit pesan dalam piksel merah yang disematkan secara acak. Untuk mendekode, pilih piksel dan Ekstrak nilai LSB piksel merah. 5. Baca masing-masing piksel kemudian isi array dikonversi menjadi nilai desimal yang sebenarnya adalah nilai ASCII dari karakter tersembunyi. 6. Nilai ASCII yang didapat dari atas adalah XOR dengan stego-key dan memberikan file pesan, yang disembunyikan di dalam gambar sampul. 3.4 Pengecekan Kualitas Cover Image Dengan Stego Image
       Structural Similarity Index Metrics (SSIM) dikenal sebagai kualitas metric yang digunakan untuk mengukur kemiripan diantara 2 buah citra dan dipercaya berkorelasi dengan kualitas persepsi Human Visual System (HVS). Nilai SSIM berada pada rentang -1 hingga 1. Semakin tinggi nilai SSIM, maka semakin tinggi tingkat kemiripan dari 2 buah citra.","3.1. Obyek penelitian
       Obyek penelitian ini adalah gambar RGB sebagai cover (diambil dari internet Google Image) atau gambar grayscale yang akan disisipkan pesan berupa teks. 3.2. Tahapan Penelitian
       Penelitian ini berusaha mengembangkan algoritma dan prototipe sebagai solusi dari masalah dan kekurangan dari teknik yang pernah dilakukan peneliti terdahulu yang dapat mengatasi optimasi hardware FPGA. Rencana penelitian mencoba menggabungkan mengembangkan algoritma dan mengembangkan metode yang dapat mengoptimalkan performa FPGA. Sehingga, metode untuk hasil yang diinginkan yaitu penggunaan komponen efisien, power yang digunakan lebih kecil dibandingkan penelitian sebelumnya. Rencana yang akan dilakukan menggunakan metode dan modifikasi pada Penelitian (Abdullah AlWatyan, 2017) dengan melakukan tahapan penyediaan pesan rahasia dan Cover Image. Dengan pesan rahasia akan dissipkan pada cover Image dengan key yang sudah dibuat pada hardware FPGA hingga menjadi Stego Image. Pada tahapan ekstraksi Stego Image akan mengambil pesa rahasia yang sudah disisipkan dengan key yang sudah dibuat. Kemudian, Stego Image akan menjadi pesan rahasia dan Cover Image. Dan untuk mengoptimalkan hardware FPGA akan menggunakan perbandingan metode yang dilakukan peneliti (E. A. Elshazly, 2018) dengan menggunakan metode algoritma steganografi gambar GEMD. Tahap pertama dimulai dengan simulasi metode pada MATLAB. Pada MATLAB, difokuskan pada metode algoritma yang diajukan sampai dengan pengecekan kualitas stego image dan cover image. Setelah tahap MATLAB sudah selesai, dilanjutkan pada hardware FPGA. Pada FPGA, diimplementasikan algoritma yang sudah dibuat dan pengecekan kualitas gambar dilanjutkan dengan analisa energi yang digunakan FPGA. 3.3 Proses Algoritma Pemasukan Dan Ekstraksi Pesan Tersembunyi
      Pseudo-random number generator (PRNG) dapat digunakan untuk memilih piksel secara acak dan menyematkan pesan. Data dapat disembunyikan dalam LSB bidang warna tertentu (bidang merah) dari piksel yang dipilih secara acak dalam ruang warna RGB. Generator angka acak pseudo menghitung dan memilih urutan piksel yang akan dipilih untuk penyematan data berdasarkan kunci. As gambar terdiri dari kontribusi piksel dari komponen merah, hijau dan biru dan setiap piksel memiliki angka dari komponen warna (untuk gambar bitmap 24-bit masing-masing merah, hijau dan pixel biru memiliki 8 bit). Pada 8 bit dari jumlah warna, jika kita mengubah bit yang paling tidak signifikan, sistem penglihatan kita dapat mendeteksi perubahan dalam pixel dan dengan demikian dimungkinkan untuk mengganti bit pesan dengan bit pixel gambar. Jika kami mengubah LSB dalam satu byte gambar, kami dapat menambah atau mengurangi satu dari nilai yang diwakilinya. Untuk menyembunyikan pesan, data terlebih dahulu dikonversi ke dalam format byte dan disimpan dalam array byte. Pesan tertanam di setiap bit ke posisi LSB dari bidang merah setiap piksel. Masukkan bit pesan rahasia ke LSB dari Pixel bidang merah. Dalam proses ekstraksi ini, proses pertama-tama mengambil kunci dan kemudian kunci acak. Kunci-kunci ini mengambil poin dari LSB di mana pesan rahasia didistribusikan secara acak. Kemudian penerima dapat mengekstraksi pesan yang tertanam persis menggunakan stego-key. Inisialisasi kunci acak yang memberikan posisi tombol bit pesan dalam piksel merah yang disematkan secara acak. Untuk mendekode, pilih piksel dan Ekstrak nilai LSB piksel merah. 5. Baca masing-masing piksel kemudian isi array dikonversi menjadi nilai desimal yang sebenarnya adalah nilai ASCII dari karakter tersembunyi. 6. Nilai ASCII yang didapat dari atas adalah XOR dengan stego-key dan memberikan file pesan, yang disembunyikan di dalam gambar sampul. 3.4 Pengecekan Kualitas Cover Image Dengan Stego Image
       Structural Similarity Index Metrics (SSIM) dikenal sebagai kualitas metric yang digunakan untuk mengukur kemiripan diantara 2 buah citra dan dipercaya berkorelasi dengan kualitas persepsi Human Visual System (HVS). Nilai SSIM berada pada rentang -1 hingga 1. Semakin tinggi nilai SSIM, maka semakin tinggi tingkat kemiripan dari 2 buah citra."
Devi Resviani_KUALIFIKASI.txt,3.1 tahapan penelitian tahapan penelitian merupakan serangkaian langkah langkah yang dilakukan dalam penelitian. gambaran mengenai tahapan penelitian ini dapat dilihat pada gambar 3.1. studi literatur pengumpulan dan analisis data preprocessing data pemilihan algoritma yang efektif pengembangan model machine learning sistem peringatan pemeliharaan prediktif evaluasi identifikasi permasalahan integrasi model dengan sistem peringatan pemeliharaan prediktif gambar 3.1 tahapan penelitian gambar 3.1 menunjukkan tahapan penelitian sebagai dasar untuk pengembangan sistem pemeliharaan prediktif menggunakan teknik machine 45 learning. tahapan penelitian ini terdiri dari studi literatur untuk memahami keadaan yang terfokus terhadap tentang mesin kompresor reciprocating metode prediksi pemeliharaan mesin predictive maintenance dan machine learning. identifikasi permasalahan secara spesifik yang akan diatasi oleh penelitian termasuk mendefinisikan ruang lingkup serta tujuan dari penelitian. pengumpulan data yang relevan dan melakukan analisis awal untuk memahami karakteristik dan pola dalam data. preprocessing data melibatkan pembersihan data normalisasi dan transformasi data agar siap digunakan dalam model machine learning. tahap ini bertujuan untuk mengatasi masalah data yang hilang outliers dan memastikan data berada dalam format yang sesuai. pemilihan algoritma machine learning yang efektif untuk membantu mencapai akurasi yang lebih tinggi dan efisiensi dalam prediksi. pengembangan dan melatih model machine learning menggunakan algoritma yang telah dipilih dengan data yang telah diperoses tahap ini melibatkan pembagian data menjadi set pelatihan dan set pengujian serta termasuk mengatur parameter model untuk mencapai kinerja terbaik. s istem peringatan pemeliharaan prediktif menggunakan model machine learning untuk memprediksi kegagalan mesin sistem ini bertujuan untuk memberikan peringatan dini sebelum terjadinya kerusakan atau kegagalan mesin. i ntegrasi model dengan sistem peringatan pemeliharaan prediktif tahap ini memastikan bahwa model dapat bekerja secara realtime dan memberikan peringatan yang akurat kepada pengguna. serta tahapan terakhir adalah evaluasi kinerja sistem secara keseluruhan untuk memastikan bahwa sistem peringatan pemeliharaan prediktif berfungsi dengan baik dan mencapai tujuan yang diinginkan. 3.2 pengumpulan dan analisis data observasi data dilakukan pada platform terpercaya yang menyediakan berbagai dataset publik yaitu kaggle . data yang digunakan yaitu dataset mesin kompresor reciprocating yang divisualisasikan oleh akun kaggle bernama ahmet okudan. dataset tersebut berisi data operasional dari mesin kompresor reciprocating dengan total 1000 sampel. setiap sampel dalam dataset tersebut mencakup 26 kolom yang berisi informasi dapat dilihat pada tabel 3.1. 46 tabel 3.1 deskripsi kolom dataset mesin kompresor reciprocating no nama kolom deskripsi 1. id identifikasi unik untuk setiap entri 2. rpm kecepatan putaran mesin dalam rotasi per menit 3. motor power daya motor dalam watt 4. torque torsi yang dihasilkan oleh mesin 5. outlet pressure bar tekanan keluaran dalam bar 6. air flow aliran udara yang diukur 7. noise db tingkat kebisingan dalam desibel 8. outlet temperature suhu keluaran 9. water pump outlet pressure tekanan keluaran pompa air 10. water inlet temperature suhu air masuk 11. water outlet temperature suhu air keluar 12. water pump power daya pompa air 13. water flow aliran air 14. oil pump power daya pompa oli 15. oil tank temperature suhu tangki oli 16. gacceleration x gaccx akselerasi gravitasi pada sumbu x 17. gacceleration y gaccy akselerasi gravitasi pada sumbu y 18. gacceleration z gaccz akselerasi gravitasi pada sumbu z 19. hacceleration x haccx akselerasi horizontal pada sumbu x 20. hacceleration y haccy akselerasi horizontal pada sumbu y 21. hacceleration z haccz akselerasi horizontal pada sumbu z 22. bearings kondisi bearing misalnya ok 23. water pump kondisi pompa air misalnya ok 24. radiator kondisi radiator misalnya clean 25. exvalve kondisi katup eksvalve misalnya clean 26. ac motor kondisi motor ac misalnya stable analisis data penelitian ini menggunakan metode exploratory data analysis eda bertujuan untuk memberikan gambaran umum tentang data dan mengidentifikasi pola atau anomali yang mungkin tidak terlihat dengan metode lain. penerapan metode eda dalam dataset mesin kompresor reciprocating menggunakan software python open access di notebook jupyter gambaran mengenai alur proses analisis data dapat dilihat pada gambar 3.2. 47 visualisasi data impor dataset identifikasi outlier analisis korelasi antar variabel statistik deskriptif impor library informasi dataset gambar 3.2 alur proses analisis data gambar 3.2 menggambarkan alur proses analisis data dengan tahap pertama adalah mengimpor library atau pustaka yang diperlukan untuk analisis data. tahap selanjutnya adalah mengimpor dataset yang akan dianalisis. tahap berikutnya adalah memeriksa informasi dasar tentang dataset untuk memahami struktur dan isi data. tahap ini termasuk melihat beberapa baris pertama dataset tipe data setiap kolom dan jumlah data. tahap selanjutnya statistik deskriptif untuk memahami distribusi data seperti mean median mode standard deviation dan range . tahap berikutnya identifikasi outlier atau nilai yang jauh berbeda dari data lainnya. outlier dapat mempengaruhi analisis dan model sehingga perlu diperiksa apakah akan dihapus atau ditangani dengan cara lain. tahap selanjutnya analisis korelasi dilakukan untuk melihat hubungan antara variabel variabel dalam dataset. korelasi membantu memahami bagaimana satu variabel mungkin mempengaruhi variabel lain. tahap terakhir visualisasi data menggunakan berbagai jenis grafik seperti histogram scatter plot dan box plot untuk lebih memahami distribusi dan pola dalam data. 3.3 preprocessing data preprocessing data sangat penting untuk memastikan bahwa data yang digunakan untuk pelatihan model machine learning memiliki kualitas yang tinggi relevan dan sesuai. alur proses data preprocessing pada penelitian ini dapat dilihat pada gambar 3.3. imputasi transformasi data reduksi dimensi gambar 3.3 alur proses data preprocessing 48 gambar 3.3 menunjukkan alur proses data preprocessing terdiri dari tahap pertama imputasi dilakukan untuk mengganti nilai yang outlier dengan nilai yang lebih representative. tahap kedua transformasi data untuk memperbaiki distribusi yang tidak normal. tahap ketiga reduksi dimensi yaitu mengurangi jumlah fitur dalam dataset guna menangani dataset dengan banyak fitur mengurangi kompleksitas model dan mencegah overfitting. 3.4 sistem peringatan pemeliharaan prediktif sistem peringatan pemeliharaan prediktif adalah sistem yang menggunakan teknik teknik analisis data terutama machine learning untuk memantau kondisi mesin dan memprediksi kegagalan yang mungkin terjadi. penelitian ini mengusulkan sistem untuk memberikan peringatan dini kepada operator atau tim pemeliharaan sehingga tindakan preventif dapat dilakukan sebelum kegagalan terjadi mengurangi downtime dan biaya pemeliharaan. gambaran alur kerja usulan sistem peringatan pemeliharaan prediktif dapat dilihat pada gambar 3.4. monitoring data realtime input data ke model prediksideteksi anomalianalisis prediksialert jadwal pemeliharaan gambar 3.4 alur kerja sistem peringatan pemeliharaan prediktif gambar 3.4 menggambarkan alur kerja dari sistem pemeliharaan prediktif pada mesin kompresor reciprocating berbasis machine learning. tahap awal adalah data operasional mesin dikumpulkan secara menerus menggunakan sensor bertujuan memastikan data terkini tersedia untuk dianalisis guna mendeteksi 49 tanda tanda awal kegagalan mesin. tahap kedua menginput data dari pemantauan realtime ke dalam model prediksi pemeliharaan yang telah dilatih. tahap ketiga mendeteksi adanya anomali atau pola yang tidak biasa agar tindakan pencegahan dapat diambil sebelum kerusakan terjadi. tahap keempat analisis prediksi untuk memperkirakan kapan dan bagaimana kegagalan akan terjadi sehingga pemeliharaan dapat direncanakan dengan tepat. tahap kelima alert atau peringatan untuk memberikan informasi kepada tim pemeliharaan agar dapat segera mengambil tindakan. tahap terakhir menjadwalkan tindakan pemeliharaan yang diperlukan berdasarkan peringatan untuk menghindari kegagalan mendadak dan meminimalkan downtime.,3.1 tahapan penelitian tahapan penelitian merupakan serangkaian langkah langkah yang dilakukan dalam penelitian. gambaran mengenai tahapan penelitian ini dapat dilihat pada gambar 3.1. studi literatur pengumpulan dan analisis data preprocessing data pemilihan algoritma yang efektif pengembangan model machine learning sistem peringatan pemeliharaan prediktif evaluasi identifikasi permasalahan integrasi model dengan sistem peringatan pemeliharaan prediktif gambar 3.1 tahapan penelitian gambar 3.1 menunjukkan tahapan penelitian sebagai dasar untuk pengembangan sistem pemeliharaan prediktif menggunakan teknik machine 45 learning. tahapan penelitian ini terdiri dari studi literatur untuk memahami keadaan yang terfokus terhadap tentang mesin kompresor reciprocating metode prediksi pemeliharaan mesin predictive maintenance dan machine learning. preprocessing data melibatkan pembersihan data normalisasi dan transformasi data agar siap digunakan dalam model machine learning. pemilihan algoritma machine learning yang efektif untuk membantu mencapai akurasi yang lebih tinggi dan efisiensi dalam prediksi. pengembangan dan melatih model machine learning menggunakan algoritma yang telah dipilih dengan data yang telah diperoses tahap ini melibatkan pembagian data menjadi set pelatihan dan set pengujian serta termasuk mengatur parameter model untuk mencapai kinerja terbaik. s istem peringatan pemeliharaan prediktif menggunakan model machine learning untuk memprediksi kegagalan mesin sistem ini bertujuan untuk memberikan peringatan dini sebelum terjadinya kerusakan atau kegagalan mesin. 46 tabel 3.1 deskripsi kolom dataset mesin kompresor reciprocating no nama kolom deskripsi 1. id identifikasi unik untuk setiap entri 2. rpm kecepatan putaran mesin dalam rotasi per menit 3. motor power daya motor dalam watt 4. torque torsi yang dihasilkan oleh mesin 5. outlet pressure bar tekanan keluaran dalam bar 6. air flow aliran udara yang diukur 7. noise db tingkat kebisingan dalam desibel 8. outlet temperature suhu keluaran 9. water pump outlet pressure tekanan keluaran pompa air 10. water inlet temperature suhu air masuk 11. water outlet temperature suhu air keluar 12. water pump power daya pompa air 13. water flow aliran air 14. oil pump power daya pompa oli 15. oil tank temperature suhu tangki oli 16. gacceleration x gaccx akselerasi gravitasi pada sumbu x 17. gacceleration y gaccy akselerasi gravitasi pada sumbu y 18. gacceleration z gaccz akselerasi gravitasi pada sumbu z 19. hacceleration x haccx akselerasi horizontal pada sumbu x 20. hacceleration y haccy akselerasi horizontal pada sumbu y 21. hacceleration z haccz akselerasi horizontal pada sumbu z 22. bearings kondisi bearing misalnya ok 23. water pump kondisi pompa air misalnya ok 24. radiator kondisi radiator misalnya clean 25. exvalve kondisi katup eksvalve misalnya clean 26. ac motor kondisi motor ac misalnya stable analisis data penelitian ini menggunakan metode exploratory data analysis eda bertujuan untuk memberikan gambaran umum tentang data dan mengidentifikasi pola atau anomali yang mungkin tidak terlihat dengan metode lain. 3.4 sistem peringatan pemeliharaan prediktif sistem peringatan pemeliharaan prediktif adalah sistem yang menggunakan teknik teknik analisis data terutama machine learning untuk memantau kondisi mesin dan memprediksi kegagalan yang mungkin terjadi. gambaran alur kerja usulan sistem peringatan pemeliharaan prediktif dapat dilihat pada gambar 3.4. monitoring data realtime input data ke model prediksideteksi anomalianalisis prediksialert jadwal pemeliharaan gambar 3.4 alur kerja sistem peringatan pemeliharaan prediktif gambar 3.4 menggambarkan alur kerja dari sistem pemeliharaan prediktif pada mesin kompresor reciprocating berbasis machine learning. tahap kelima alert atau peringatan untuk memberikan informasi kepada tim pemeliharaan agar dapat segera mengambil tindakan. tahap terakhir menjadwalkan tindakan pemeliharaan yang diperlukan berdasarkan peringatan untuk menghindari kegagalan mendadak dan meminimalkan downtime.
Erfiana Wahyuningsih_UK.txt,3.1 konsep penelitian untuk mempermudah dalam melakukan penelitian maka dibuat sebuah flowchart agar penelitian tidak menyimpang dan salah. berikut flowchart penelitian untuk rangkaian sram 6t low power dan high read stability dengan metode m gdi. gambar 10. alur penelitian sram 6t dengan metode m gdi dalam me mulai desain sram 6t dengan menggunakan metode m gdi diperlukan studi literatur terkait bebera pa penelitian dengan metode atau hasil serupa . setelah me mpelajari se luruh penelitian terkait maka di lakukan desain rangkaian sram dengan metode konvensional sebagai referensi untuk dilakukan proses m gdi. referensi rangkaian diperlukan untuk melihat hasil sebagai pembanding dengan rangkaian baru yang didesain dengan metode mgdi. dipilih desain berdasarkan penelitian sebelumnya yang dilakukan oleh ebrahim abiri dan abdolreza darabi 2015 sram 8t dengan low power dan high read stability menggunakan metode m gdi. berdasarkan penelit ian yang dilakukan rangkaian sram ini terbagi menjadi 3 blok yakni write bloc k sram 8t block dan read bloc k seperti pada gambar 11 be rikut gambar 11. desain sram 8t dengan metode m gdi lengkap gambar 11 akan dijadikan referensi untuk dilakukan tahap m gdi selanjutnya guna mendapatkan desain baru dengan kemampuan low power da n high read stability. penelitian tetap akan memusatkan pada desain rang kaian sram 8t untuk direduksi menjadi 6t dengan metode mgdi. hasil yang dihar apkan tetap mengacu pada low power dan mempertaha nkan pula kemampuan high read stability. rangkaian sram 8t diatas memiliki hasil simulasi seperti yang tamp ak pada gambar 12. gambar 12. simulasi sinyal masukan dan keluar an sram 8t dalam kondisi menulis write proses write pada rangkaian sram 8t dimulai ketika word line wl mencapai tegangan tinggi yang menyebabkan transistor akses access transistors menjadi aktif on. pada saat itu data disimpan dengan cepat pada node q dan qb yang terhubung ke gerbang da ri transistor pusat sel enr dan enl. setelah itu data mencapai keadaan permanen dengan bantuan sel sel m gdi . selama siklus write sel m gdi berperan dalam memastikan bahwa data yang ditulis ke dalam sel memori sram disimpan dengan stabil dan cepat yang merupakan bagian penting dari desain sram low power gambar 1 3. simulasi sinyal masukan dan keluar an hanya pada rangkaian blok sram 8t dalam blok rangkaian sram 8t input utama termasuk word line wl bit lines bl dan blb dan sinyal sinyal kontrol untuk operasi pembacaan dan penulisan. output dari rangkaian ini adalah data yang dibaca dari sel memori d_out dan d_outb . ketika melakukan operasi penulisan data yang akan ditulis ke dalam sel memori disuplai melalui bit lines bl dan blb. sinyal wl diaktifkan untuk menghubungkan sel memori dengan bit lines memungkinkan data untuk ditransfer ke dalam sel. setelah data ditu lis wl dinonaktifkan untuk mengisolasi sel dari bit lines dan menjaga data yang telah disimpan . selama operasi pembacaan wl diaktifkan untuk menghubungkan sel memori dengan bit lines memungkinkan data yang tersimpan di dalam sel untuk ditransfer keluar. data yang dibaca kemudian muncul pada output d_out dan d_outb dengan swing tegangan maksimum pad a output inverter .,berikut flowchart penelitian untuk rangkaian sram 6t low power dan high read stability dengan metode m gdi. gambar 10. alur penelitian sram 6t dengan metode m gdi dalam me mulai desain sram 6t dengan menggunakan metode m gdi diperlukan studi literatur terkait bebera pa penelitian dengan metode atau hasil serupa . referensi rangkaian diperlukan untuk melihat hasil sebagai pembanding dengan rangkaian baru yang didesain dengan metode mgdi. dipilih desain berdasarkan penelitian sebelumnya yang dilakukan oleh ebrahim abiri dan abdolreza darabi 2015 sram 8t dengan low power dan high read stability menggunakan metode m gdi. hasil yang dihar apkan tetap mengacu pada low power dan mempertaha nkan pula kemampuan high read stability.
Fitriana Indah Pramitasari_Kualifikasi.txt,3.1 alur penelitian alur penelitian menggambarkan alur dari awal hingga akhir penelitian dilaksanakan. alur penelitian ini diuraikan pada gambar 3.1 di bawah ini. gambar 3.1 alur penelitian 27 3.2 identifikasi masalah identifikasi masalah adalah salah satu langkah pertama yang dilakukan sebelum melakukan penelitian. identifikasi masalah merupakan suatu proses mencari dan mengetahui masalah yang ingin diselesaikan. identifikasi masalah ini membantu penelitian untuk memah ami tantangan yang dihadapi oleh petani kentang skala nasional dan merancang solusi yang tepat sesuai dengan kebutuhan mereka. identifikasi masalah pada penelitian ini berfokus pada mengidentifikasi proses perancangan model koperasi petani mengidentifikas i metode prediksi permintaan dengan ann di dalam blockchain yang digunakan untuk mengoptimalkan permintaan pelanggan di masa depan selama periode tertentu dan mengidentifikasi metode safety stock di dalam blockchain yang digunakan agar dapat mengoptimalkan stok dan permintaan. identifikasi masalah pada penelitian ini peneliti dapat lebih memahami kendala dan kebutuhan petani kentang skala nasional. perancangan model platform koperasi untuk meningkatkan efisiensi dan kerjasama antarpetani dengan koperasi sebagai mitranya. sement ara itu metode prediksi permintaan dengan menggunakan artificial neural network ann diharapkan dapat membantu petani mengelola produksi secara lebih tepat sesuai dengan kebutuhan pasar dan koperasi dapat menyesuaikan persediaan stok dan permintaan secar a dinamis dari hasil prediksi permintaan. selain itu identifikasi masalah juga mencakup penerapan metode safety stock untuk mengoptimalkan manajemen stok memastikan ketersediaan barang dan meningkatkan responsibilitas terhadap fluktuasi permintaan pasar. dengan penerapan ann dan metode safety stock di dalam blockchain semua prediksi dan manajemen stok dapat dicatat di dlaam buku besar yang tidak dapat diubah sehingga meningkatkan transparansi dan keamanan data dalam rantai pasok. sehingga koperasi ini dapat melakukan perencanaan yang lebih akurat meminimalkan pemborosan dan meningkatkan ketersediaan kentang sesuai dengan kebutuhan pelanggan. dengan demikian platform koperasi menjadi responsif terhadap perubahan permintaan pasar mendukung pertumbuhan ekonomi para petani memperkuat kolaborasi antar anggota koperasi serta memiliki transparansi dan keamanan pada rantai pasok. 28 3.3 studi literatur studi literatur yang dilakukan pada penelitian engembangan platform koperasi petani ini dimulai dari pencarian dan review literatur literatur terbaru dan relevan yang telah diterbitkan. studi literatur juga dapat dari teori teori buku yang relevan dengan metode yang digunakan. analisis literatur membantu untuk mengidentifikasi kerangka kerja metode dan teknologi yang telah digunakan pada penelitian sebelumnya. studi literatur dapat digunakan sebagai mencari solusi dan menganalisa penelitian yang dilakukan . studi literatur juga membantu dalam mengetahui tantangan dan peluang yang mungkin dihadapi dalam pengembangan platform koperasi petani kentang. sehingga informasi tersebut dapat memberikan sebuah wawasan terkait dengan penelitian yang dilakukan. 3.4 pengumpulan data pengumpulan data yang digunakan sebagai bahan dalam mengolah data. sehingga penelitian ini akan menghasilkan data yang sesuai dengan tujuan penelitian. penelitian ini mengumpulkan data sekunder dan data primer. pengumpulan data pada penelitian ini terdiri dari beberapa proses sebagai berikut. pengumpulan data sekunder memanfaatkan sumber informasi yang sudah ada seperti literatur ilmiah dokumen resmi dan data statistik yang relevan. proses ini memungkinkan peneliti untuk memahami konteks yang telah ada sebelumnya dan memanfaatkan pengetahua n serta data yang telah dihasilkan sebelumnya. berdasarkan data yang diperoleh dari badan pusat statistik bps tahun 2022 menjelaskan data produksi kentang di berbagai wilayah indonesia. beberapa wilayah indonesia berhasil dalam produksi kentang dan beberapa wilayah indonesia yang tidak dapat mempro duksi kentang. data tersebut memberikan gambaran lengkap mengenai kegiatan pertanian kentang di berbagai wilayah indonesia pada tahun 2022. berikut data bps tahun 2022 produksi kentang di berbagai wilayah indonesia. 29 tabel 3.1 data lokasi produksi kentang 2022 sumber badan pusat statistik 2023 pengumpulan data primer yaitu melakukan pencarian secara langsung untuk mengumpulkan data serta informasi baru sesuai dengan tujuan penelitian. metode ini seperti pengambilan data survei wawancara observasi atau eksperimen dengan tujuan untuk kebutuha n penelitian. data primer yang akan digunakan pada penelitian ini adalah kebutuhan pengguna aliran data dari petani dengan koperasi sebagai mitranya data musim data historis penjualan data produksi kentang dan data harga kentang. pengambilan data primer dilakukan di wonosobo jawa tengah. berdasarkan informasi yang didapatkan dari salah satu petani di wonosobo jawa tengah disana terdapat banyak petani kentang dan sayuran lainnya. menurut bps kabupaten wonosobo jawa tengah adalah wilayah yang terbanyak memproduksi kentang di provinsi jawa tengah. pola distribusi kentang di wonosobo jawa tengah terdiri dari 3 pola sebagai berikut zaenuri et al 2023. 30 gambar 3.2 pola distribusi kentang 3.5 blockchain pada penelitian ini untuk meningkatkan keamanan dan transparansi maka menggunakan teknologi blockchain untuk rantai pasok kentang. berikut flowchart kecerdasan buatan safety stock yang dikombinasikan di dalam blockchain. gambar 3.3 flowchart blockchain berdasarkan gambar di atas menggambarkan kombinasi ann dan safety stock di dalam blockchain. data rantai pasok yang telah dikumpulkan kemudian 31 dimasukkan ke dalam database. data tersebut diverifikasi dalam blockchain dengan proses pembuatan blok baru yang melibatkan perhitungan hash blok sebelumnya menyusun blok baru menghitung hash blok baru dan mencapai konsensus untuk menambahkan blok ke ra ntai. data yang diverifikasi kemudian diproses menggunakan model artificial neural network ann. tahapan dalam ann meliputi praproses data inisialisasi model pelatihan model validasi model dan evaluasi kinerja. hasil prediksi permintaan disimpan dalam blockchain dengan proses pembuatan blok baru yang sama seperti langkah sebelumnya. selanjutnya permintaan data diverifikasi dan jika valid safety stock dihitung menggunakan rumus safety stock yang sudah ada. hasil perhitungan safety stock disimpan dalam database dan dicatat dalam blockchain dengan pembuatan blok baru. semua data dari proses tersebut dicatat dalam blockchain untuk memastikan transparansi dan keamanan. proses validasi memastikan bahwa data permintaan dan pengelolaan stok selalu diperbarui dan valid sebelum digunakan untuk pengambilan keputusan. 3.6 design sistem dengan uml pengembangan platform koperasi petani kentang menggunakan metode unified modeling language uml untuk menggambarkan struktur fungsi dan interaksi komponen sistem secara visual. dimana proses metode uml ini diawali dengan identifikasi kebutuhan sistem dan pemahaman terhadap fungsionalitas yang terkait dengan economic sharing dan prinsip prinsip perkoperasian. 3.6.1 analisis kebutuhan sistem analisis kebutuhan sistem bertujuan untuk mengidentifikasi kebutuhan dari pengguna dan stakeholder sistem. pengumpulan informasi pada proses ini mengenai detail cara kerja sistem dan batasan batasan yang ada. pengumpulan data dilakukan melalui wawancara a tau survei. analisis kebutuhan sistem dapat menentukan arah dan ruang lingkup proyek pengembang sistem serta memastikan bahwa produk akhir akan memenuhi harapan dan memecahkan masalah yang dihadapi oleh pengguna. 32 gambar 3. 4 multi stakeholder cooperative gambar 3. 4 menjelaskan proses multi stakeholder cooperative dimana anggota koperasi termasuk dari workers community producers dan consumers . mereka memilih board of director dari para anggotanya. board of director merupakan struktur organisasi yang bertanggung jawab dalam mengawasi manajemen yang dijalankan oleh koperasi dengan setiap anggotanya memiliki tugas khusus sesuai dengan tujuan koperasi. dewan direksi berperan penting dalam menjaga keberlanjutan dan kese imbangan antara berbagai kepentingan dalam konteks multi stakeholder cooperative . gambar 3. 5 pengguna platform gambar 3. 5 mendeskripsikan pengguna platform koperasi petani yang melibatkan sejumlah pihak. pengguna platform ini terdiri dari consumers yang dapat mengakses produk pertanian secara langsung farmers yang memanfaatkan platform untuk memasarkan hasil panen companies yang terlibat dalam dukungan pengembangan teknologi dan partner cooperatives yang menjadi bagian dari 33 kolaborasi kerjasama antar koperasi untuk meningkatkan kesejahteraan bersama. keterlibatan seluruh pihak ini diharapkan platform koperasi petani menciptakan lingkungan yang saling mendukung dan berkelanjutan memperkuat konektivitas antar anggota untuk me ncapai tujuan bersama dalam dunia pertanian. 3.6.2 use case diagram model pertama uml adalah pemodelan use case diagram dimana menggambarkan skenario skenario utama pengguna platform koperasi. use case diagram digunakan untuk menunjukkan hubungan dan struktur kelas kelas yang terlibat dalam sistem termasuk entitas entitas seperti data permintaan stok kentang dan pengguna. gambar 3. 6 use case diagram gambar 3. 6 adalah diagram use case untuk platform koperasi petani yang menunjukkan berbagai interaksi antara pengguna dan sistem. pada diagram use 34 case terdapat aktor yang terdiri dari petani konsumen anggota koperasi dan admin. registrasi dilakukan oleh petani anggota koperasi dan konsumen. proses login untuk mengakses fitur yang terdapat pada website koperasi dapat dilakukan oleh petani anggot a koperasi konsumen dan admin. proses mencatat produksi hanya dilakukan oleh petani dimana petani mencatat data produksi kentang mereka yang kemudian akan dicatat di blockchain. proses melacak produksi dan distribusi menggunakan blockchain. proses verifi kasi produk kentang yang dihasilkan oleh petani. proses verifikasi dilakukan oleh admin dan anggota koperasi. proses melakukan pembelian melalui website dilakukan oleh konsumen. proses mengelola transaksi yang terjadi di dalam sistem memastikan semua tran saksi tercatat di blockchain dilakukan oleh admin. proses melihat laporan dan statistik dari data produksi distribusi dan transaksi yang terjadi di dalam sistem dilakukan oleh admin anggota koperasi dan petani. 3.6.3 activity diagram model kedua adalah activity diagram untuk menggambarkan alur kerja atau proses proses yang terjadi dalam platform koperasi. activity diagram dapat membantu dalam menguraikan langkah langkah yang diperlukan dari pemesanan kentang hingga manajemen stok. gambar 3. 7 activity diagram mencatat produksi 35 diagram pada gambar 3. 7 adalah proses mencatat produksi yang diawali dengan login seorang petani ke dalam platform koperasi petani. petani memulai dengan membuka platform dan memilih opsi untuk login. setelah login berhasil petani memasukkan data produksi kentang. sistem akan m emverifikasi data. selanjutnya sistem mencatat data di blockchain dan sistem memberikan notifikasi ke petani. gambar 3. 8 activity diagram melacak produksi dan distribusi diagram pada gambar 3. 8 merupakan alur proses melacak produksi dan distribusi. proses ini dilakukan dari anggota melakukan login ke sistem. selanjutnya anggota koperasi memilih menu pelacak produksi dan distribusi. sistem akan menampilkan tampilan menu pelacak produksi dan distr ibusi dan anggota koperasi memasukkan id produk. sistem akan mengambil data dari blockchain dan jika data telah dikirim oleh blockchain selanjutnya sistem akan menampilkan informasi pelacak. 36 gambar 3. 9 activity diagram pembelian diagram pada gambar 3. 9 menggambarkan proses pembelian yang dilakukan oleh konsumen. konsumen melakukan login di sistem koperasi petani. selanjutnya konsumen mencari produk yang ingin dibeli dan memilih produk serta memasukkan jumlah pembelian. sistem akan memverifikasi stok pr oduk. konsumen selanjutnya memasukkan informasi pembayaran. sistem akan mencatat transaksi di blockchain dan sistem akan mengirimkan notifikasi pembelian. gambar 3. 10 activity diagram kelola transaksi diagram pada gambar 3. 10 menggambarkan alur proses mengelola transaksi. pertama admin melakukan login pada sistem website koperasi. admin 37 memilih menu manajemen transaksi dan sistem menampilkan daftar transaksi yang terjadi. admin memverifikasi transaksi yang belum diverifikasi lalu mengubah status transaksi sesuai hasil verifikasi. sistem memperbarui data di blockchain. sistem akan menampil kan laporan transaksi yang berhasil diperbarui di blockchain. 3.6.4 sequence diagram model ketiga adalah sequence diagram dimana diagram ini dapat membantu dalam menggambarkan urutan peristiwa atau interaksi antar komponen dalam sistem seperti bagaimana data pemesanan diteruskan dan diproses. gambar 3. 11 sequence diagram mencatat produksi diagram pada gambar 3. 11 menggambarkan proses mencatat produksi kentang petani di website koperasi yang terintegrasi dengan blockchain. pertama petani memilih menu login dan mengisi form login ke website koperasi. website memverifikasi dan mengirimkan notifikasi login sukses. pe tani selanjutnya memilih menu data produksi dan melakukan input data produksi seperti nama produk kuantitas tanggal produksi dan sebagainya. website akan memverifikasi data yang di masukkan oleh petani. setelah data diverifikasi website koperasi mengir imkan data produksi ke blockchain. blockchain akan memverifikasi data tersebut melalui node node yang ada. data produksi yang telah diverifikasi oleh node blockchain selanjutnya dicatat di dalam blockchain. kemudian website 38 koperasi mengirimkan notifikasi ke petani bahwa data produksi telah berhasil tercatat gambar 3.1 2 sequence diagram melacak produksi dan distribusi diagram pada gambar 3.1 2 menunjukkan tahapan yang dilalui oleh seorang petani untuk melacak produksi dan distribusi kentang melalui website koperasi yang terintegrasi dengan blockchain. pertama anggota koperasi. mengirimkan permintaan login ke website koperasi. website akan mem verifikasi dan mengirimkan notifikasi login sukses. anggota koperasi memilih menu pelacak produksi dan distribusi pada website. anggota koperasi memasukkan id produk yang ingin dilacak. website koperasi mengirimkan permintaan untuk mengambil data pelacakan dari blockchain. blockchain mengirimkan data pelacakan yang diminta. selanjutnya website koperasi akan menampilkan informasi pelacakan yang diperoleh dari blockchain kepada anggota koperasi. gambar 3.1 3 sequence diagram pembelian 39 diagram pada gambar 3.1 3 menunjukkan alur proses pembelian yang dilakukan oleh konsumen untuk membeli produk kentang melalui website koperasi yang terintegrasi dengan blockchain. konsumen mengirimkan permintaan login ke website koperasi. website koperasi akan memverifikasi data l ogin yang diinput oleh konsumen dan mengirimkan notifikasi login sukses. konsumen memilih menu produk dan input nama produk yang akan dicari di sistem website. website koperasi akan menampilkan daftar produk yang diinginkan. konsumen memilih produk dan mem asukkan jumlah pembelian. website akan memverifikasi stok produk yang tersedia. setelah stok produk diverifikasi website akan mengirimkan informasi stok tersedia dan konsumen memasukkan informasi pembayaran. website koperasi akan mengirimkan data transaks i ke blockchain untuk dicatat. blockchain memverifikasi dan mencatat transaksi. website koperasi akan mengirimkan informasi kepada konsumen bahwa transaksi telah berhasil dicatat. gambar 3.1 4 sequence diagram mengelola transaksi diagram yang ditampilkan pada gambar 3.1 4 menggambarkan langkah langkah dalam mengelola transaksi di website koperasi. admin mengirimkan permintaan login ke website koperasi. website koperasi akan memverifikasi dan mengirimkan notifikasi login sukses. setelah berhasil login admin memilih menu 40 manajemen transaksi. admin melihat daftar transaksi yang terjadi. selanjutnya admin memilih transaksi yang belum diverifikasi dan melakukan verifikasi. website koperasi mengubah status transaksi berdasarkan hasil verifikasi. website mengirimkan permintaan untuk memperbarui status transaksi di blockchain. blockchain memverifikasi dan memperbarui status transaksi. admin melihat laporan transaksi yang telah diperbarui. website mengambil data laporan dari blockchain dan menampilkan kepada admin. 3.7 prediksi permintaan dengan ann metode prediksi permintaan dalam penelitian ini bertujuan untuk memprediksi jumlah kentang yang diminta oleh pasar atau konsumen pada periode waktu tertentu. metode yang digunakan pada prediksi permintaan adalah metode ann. input data yang akan digunakan a dalah data kuantitatif dan kualitatif yang dapat mempengaruhi permintaan di masa depan sehingga agar hasil prediksi permintaan dapat lebih akurat. dengan menerapkan metode ann pada prediksi permintaan ini penelitian dapat memberikan prediksi yang lebih tepat terkait kebutuhan pasar di masa mendatang sehingga dapat meningkatnya efektivitas rantai pasok. prediksi ini juga dapat memberikan petani wawas an yang berharga terkait potensi pasar dan membantu mereka mengoptimalkan produksi serta mitra koperasi dapat merencanakan strategi pemasaran yang lebih efektif. dengan menerapkan metode prediksi permintaan kentang menggunakan metode ann penelitian ini da pat memberikan kontribusi signifikan dalam mendukung keberlanjutan dan efisiensi dalam pertanian kentang. langkah pra pemrosesan data melibatkan pengumpulan data historis relevan seperti data penjualan sebelumnya data harga data produksi kentang serta data musim sebagai faktor eksternal yang dapat memengaruhi permintaan. kemudian dilakukan data cleaning di normalisasi dan di transformasi untuk memastikan bahwa ann yang akan dibangun dapat bekerja dengan efektif dan menghasilkan prediksi yang akurat. 41 langkah selanjutnya adalah desain dan pelatihan model ann. tahapan ini melibatkan proses pemilihan arsitektur jaringan yang tepat termasuk jumlah lapisan tersembunyi jumlah neuron per lapisan fungsi aktivasi dan algoritma pembelajaran. pelatihan model merupakan proses ann menyesuaikan bobotnya berdasarkan kesalahan prediksi melalui metode seperti backpropagation. pada tahapan ini penyesuaian parameter seperti kecepatan belajar dan momentum dilakukan untuk memperbaiki proses pembelajaran model. setelah dilatih dengan baik model akan mampu mengenali pola kompleks dan hubungan non linear dalam data. tahapan evaluasi model ann adalah tahapan dimana model yang telah dilatih dan diuji menggunakan dataset yang belum pernah dilihat sebelumnya untuk menilai kemampuannya dalam memprediksi permintaan dengan akurat. matrik evaluasi seperti mean square error mse atau mean absolute percentage error mape merupakan matrik yang sering digunakan untuk mengukur kinerja model. berdasarkan hasil evaluasi model prediksi yang akurat dari model ann ini berguna untuk perusahan dalam membuat keputusan strategis seperti inventory management . proses prediksi permintaan dengan ann akan menghasilkan data permintaan yang diharapkan informasi tersebut digunakan untuk proses inventory management . data prediksi permintaan tersebut akan digunakan sebagai dasar perhitungan safety stock . sehingga perusahaan dapat merencanakan dan menyesuaikan kuantitas inventory yang cukup untuk memenuhi permintaan dimana akan meminimalisir biaya penyimpanan dan mengurangi risiko kekurangan stok. tujuannya agar operasi bisnis dapat berjalan dengan lanca r dan efisien s erta mengoptimalkan ketersediaan produk. 3.8 inventory management proses inventory management menggunakan metode safety stock merupakan proses untuk menjaga ketersediaan persediaan dalam platform secara efektif. tahapan pertama penelitian ini memerlukan analisis data historis 42 permintaan kentang fluktuasi pasokan dan waktu panen sehingga dapat mengidentifikasi kebutuhan pasokan dan resiko keterlambatan. penerapan metode safety stock pada penelitian ini akan menentukan tingkat persediaan tambahan yang diperlukan untuk mengatasi ketidakpastian dalam permintaan atau keterlambatan pasokan. hal ini bertujuan untuk memberikan keandalan dan menghindari kekurangan persediaan yang dapat mengha mbat operasional koperasi. metode safety stock dalam pengembangan platform koperasi petani kentang pada penelitian ini untuk meningkatkan efisiensi manajemen persediaan. 3.9 integrasi tahapan integrasi merupakan proses menggabungkan sistem aplikasi atau teknologi yang berbeda menjadi satu kesatuan yang berfungsi secara harmonis. pada proses ini berbagai komponen yang sebelumnya beroperasi secara terpisah agar dapat berinteraksi satu sama lain dalam mencapai tujuan bersama. integrasi bertugas menyatukan aspek desain sistem dengan uml prediksi permintaan dan inventory management . proses integrasi menjamin bahwa data yang diolah sebelumnya dapat digunakan dengan baik untuk mendukung pe ngambilan keputusan. selain itu integrasi ini melibatkan penggunaan artificial neural network ann dan metode safety stock yang terintegrasi dalam blockchain untuk rantai pasok. data dari prediksi permintaan dan pengelolaan stok dicatat secara transparan dan aman dalam blockchain. website koperasi akan terintegrasi dengan blockchain untuk memastikan efisiensi dan transparansi dalam seluruh proses manajemen rantai pasok. 3.10 pengujian sistem tahapan pengujian sistem dalam penelitian merupakan langkah untuk mengevaluasi kinerja atau fungsionalitas sistem yang dikembangkan atau diuji pada penelitian. proses pengujian sistem mencakup implementasi prototipe atau model sistem hingga serangkaian u ji coba. tujuan dari tahapan pengujian sistem adalah mengidentifikasi adanya kegagalan mengukur efektivitas sebuah sistem serta 43 memastikan sistem berjalan sesuai dengan tujuan dan persyaratan yang telah ditetapkan sebelumnya. pada penelitian ini sistem platform koperasi petani diharapkan dapat berjalan sesuai dengan tujuan dan persyaratan perkoperasian serta sesuai dengan model platform economic sharing . platform koperasi petani kentang pada penelitian ini akan berbasis website dan dilengkapi dengan kecerdasan buatan yang dikombinasikan dengan blockchain. 3.11 evaluasi tahapan selanjutnya adalah evaluasi. evaluasi dilakukan untuk memastikan bahwa semua komponen sistem berfungsi sesuai rencana. evaluasi melibatkan penilaian kinerja pada sistem secara keseluruhan dan memeriksa apakah integrasi berjalan tanpa hambatan. ta hapan evaluasi juga dapat mengidentifikasi apakah hasil pengujian sistem sesuai dengan tujuan awal dan menentukan area yang mungkin memerlukan peningkatan. hasil dari tahap evaluasi menjadi petunjuk penting untuk membuat perubahan dan peningkatan sehingga sistem dapat bekerja lebih baik lagi. 3.12 analisis hasil analisis merupakan tahapan penelitian dimana menyimpulkan serta menguraikan informasi dari hasil data yang telah diolah dan diuji sebelumnya. tahapan analisis dapat memberikan makna dari temuan temuan tersebut. tahapan ini memberikan identifikasi faktor faktor yang dapat mempengaruhi kinerja sistem dan memberikan rekomendasi untuk peningkatan di masa yang akan datang.,3.1 alur penelitian alur penelitian menggambarkan alur dari awal hingga akhir penelitian dilaksanakan. alur penelitian ini diuraikan pada gambar 3.1 di bawah ini. gambar 3.1 alur penelitian 27 3.2 identifikasi masalah identifikasi masalah adalah salah satu langkah pertama yang dilakukan sebelum melakukan penelitian. identifikasi masalah merupakan suatu proses mencari dan mengetahui masalah yang ingin diselesaikan. identifikasi masalah ini membantu penelitian untuk memah ami tantangan yang dihadapi oleh petani kentang skala nasional dan merancang solusi yang tepat sesuai dengan kebutuhan mereka. identifikasi masalah pada penelitian ini berfokus pada mengidentifikasi proses perancangan model koperasi petani mengidentifikas i metode prediksi permintaan dengan ann di dalam blockchain yang digunakan untuk mengoptimalkan permintaan pelanggan di masa depan selama periode tertentu dan mengidentifikasi metode safety stock di dalam blockchain yang digunakan agar dapat mengoptimalkan stok dan permintaan. identifikasi masalah pada penelitian ini peneliti dapat lebih memahami kendala dan kebutuhan petani kentang skala nasional. perancangan model platform koperasi untuk meningkatkan efisiensi dan kerjasama antarpetani dengan koperasi sebagai mitranya. sement ara itu metode prediksi permintaan dengan menggunakan artificial neural network ann diharapkan dapat membantu petani mengelola produksi secara lebih tepat sesuai dengan kebutuhan pasar dan koperasi dapat menyesuaikan persediaan stok dan permintaan secar a dinamis dari hasil prediksi permintaan. selain itu identifikasi masalah juga mencakup penerapan metode safety stock untuk mengoptimalkan manajemen stok memastikan ketersediaan barang dan meningkatkan responsibilitas terhadap fluktuasi permintaan pasar. dengan penerapan ann dan metode safety stock di dalam blockchain semua prediksi dan manajemen stok dapat dicatat di dlaam buku besar yang tidak dapat diubah sehingga meningkatkan transparansi dan keamanan data dalam rantai pasok. sehingga koperasi ini dapat melakukan perencanaan yang lebih akurat meminimalkan pemborosan dan meningkatkan ketersediaan kentang sesuai dengan kebutuhan pelanggan. dengan demikian platform koperasi menjadi responsif terhadap perubahan permintaan pasar mendukung pertumbuhan ekonomi para petani memperkuat kolaborasi antar anggota koperasi serta memiliki transparansi dan keamanan pada rantai pasok. 28 3.3 studi literatur studi literatur yang dilakukan pada penelitian engembangan platform koperasi petani ini dimulai dari pencarian dan review literatur literatur terbaru dan relevan yang telah diterbitkan. studi literatur juga membantu dalam mengetahui tantangan dan peluang yang mungkin dihadapi dalam pengembangan platform koperasi petani kentang. sehingga penelitian ini akan menghasilkan data yang sesuai dengan tujuan penelitian. proses ini memungkinkan peneliti untuk memahami konteks yang telah ada sebelumnya dan memanfaatkan pengetahua n serta data yang telah dihasilkan sebelumnya. beberapa wilayah indonesia berhasil dalam produksi kentang dan beberapa wilayah indonesia yang tidak dapat mempro duksi kentang. data tersebut memberikan gambaran lengkap mengenai kegiatan pertanian kentang di berbagai wilayah indonesia pada tahun 2022. berikut data bps tahun 2022 produksi kentang di berbagai wilayah indonesia. data primer yang akan digunakan pada penelitian ini adalah kebutuhan pengguna aliran data dari petani dengan koperasi sebagai mitranya data musim data historis penjualan data produksi kentang dan data harga kentang. berdasarkan informasi yang didapatkan dari salah satu petani di wonosobo jawa tengah disana terdapat banyak petani kentang dan sayuran lainnya. 30 gambar 3.2 pola distribusi kentang 3.5 blockchain pada penelitian ini untuk meningkatkan keamanan dan transparansi maka menggunakan teknologi blockchain untuk rantai pasok kentang. berikut flowchart kecerdasan buatan safety stock yang dikombinasikan di dalam blockchain. data rantai pasok yang telah dikumpulkan kemudian 31 dimasukkan ke dalam database. data tersebut diverifikasi dalam blockchain dengan proses pembuatan blok baru yang melibatkan perhitungan hash blok sebelumnya menyusun blok baru menghitung hash blok baru dan mencapai konsensus untuk menambahkan blok ke ra ntai. hasil prediksi permintaan disimpan dalam blockchain dengan proses pembuatan blok baru yang sama seperti langkah sebelumnya. hasil perhitungan safety stock disimpan dalam database dan dicatat dalam blockchain dengan pembuatan blok baru. 3.6 design sistem dengan uml pengembangan platform koperasi petani kentang menggunakan metode unified modeling language uml untuk menggambarkan struktur fungsi dan interaksi komponen sistem secara visual. analisis kebutuhan sistem dapat menentukan arah dan ruang lingkup proyek pengembang sistem serta memastikan bahwa produk akhir akan memenuhi harapan dan memecahkan masalah yang dihadapi oleh pengguna. pengguna platform ini terdiri dari consumers yang dapat mengakses produk pertanian secara langsung farmers yang memanfaatkan platform untuk memasarkan hasil panen companies yang terlibat dalam dukungan pengembangan teknologi dan partner cooperatives yang menjadi bagian dari 33 kolaborasi kerjasama antar koperasi untuk meningkatkan kesejahteraan bersama. keterlibatan seluruh pihak ini diharapkan platform koperasi petani menciptakan lingkungan yang saling mendukung dan berkelanjutan memperkuat konektivitas antar anggota untuk me ncapai tujuan bersama dalam dunia pertanian. proses verifi kasi produk kentang yang dihasilkan oleh petani. website koperasi akan mengirimkan informasi kepada konsumen bahwa transaksi telah berhasil dicatat. website koperasi mengubah status transaksi berdasarkan hasil verifikasi. input data yang akan digunakan a dalah data kuantitatif dan kualitatif yang dapat mempengaruhi permintaan di masa depan sehingga agar hasil prediksi permintaan dapat lebih akurat. dengan menerapkan metode ann pada prediksi permintaan ini penelitian dapat memberikan prediksi yang lebih tepat terkait kebutuhan pasar di masa mendatang sehingga dapat meningkatnya efektivitas rantai pasok. kemudian dilakukan data cleaning di normalisasi dan di transformasi untuk memastikan bahwa ann yang akan dibangun dapat bekerja dengan efektif dan menghasilkan prediksi yang akurat. berdasarkan hasil evaluasi model prediksi yang akurat dari model ann ini berguna untuk perusahan dalam membuat keputusan strategis seperti inventory management . proses prediksi permintaan dengan ann akan menghasilkan data permintaan yang diharapkan informasi tersebut digunakan untuk proses inventory management . 3.8 inventory management proses inventory management menggunakan metode safety stock merupakan proses untuk menjaga ketersediaan persediaan dalam platform secara efektif. tahapan pertama penelitian ini memerlukan analisis data historis 42 permintaan kentang fluktuasi pasokan dan waktu panen sehingga dapat mengidentifikasi kebutuhan pasokan dan resiko keterlambatan. penerapan metode safety stock pada penelitian ini akan menentukan tingkat persediaan tambahan yang diperlukan untuk mengatasi ketidakpastian dalam permintaan atau keterlambatan pasokan. hal ini bertujuan untuk memberikan keandalan dan menghindari kekurangan persediaan yang dapat mengha mbat operasional koperasi. metode safety stock dalam pengembangan platform koperasi petani kentang pada penelitian ini untuk meningkatkan efisiensi manajemen persediaan. selain itu integrasi ini melibatkan penggunaan artificial neural network ann dan metode safety stock yang terintegrasi dalam blockchain untuk rantai pasok. website koperasi akan terintegrasi dengan blockchain untuk memastikan efisiensi dan transparansi dalam seluruh proses manajemen rantai pasok. 3.10 pengujian sistem tahapan pengujian sistem dalam penelitian merupakan langkah untuk mengevaluasi kinerja atau fungsionalitas sistem yang dikembangkan atau diuji pada penelitian. proses pengujian sistem mencakup implementasi prototipe atau model sistem hingga serangkaian u ji coba. tujuan dari tahapan pengujian sistem adalah mengidentifikasi adanya kegagalan mengukur efektivitas sebuah sistem serta 43 memastikan sistem berjalan sesuai dengan tujuan dan persyaratan yang telah ditetapkan sebelumnya. pada penelitian ini sistem platform koperasi petani diharapkan dapat berjalan sesuai dengan tujuan dan persyaratan perkoperasian serta sesuai dengan model platform economic sharing . platform koperasi petani kentang pada penelitian ini akan berbasis website dan dilengkapi dengan kecerdasan buatan yang dikombinasikan dengan blockchain. 3.11 evaluasi tahapan selanjutnya adalah evaluasi. evaluasi dilakukan untuk memastikan bahwa semua komponen sistem berfungsi sesuai rencana. evaluasi melibatkan penilaian kinerja pada sistem secara keseluruhan dan memeriksa apakah integrasi berjalan tanpa hambatan. ta hapan evaluasi juga dapat mengidentifikasi apakah hasil pengujian sistem sesuai dengan tujuan awal dan menentukan area yang mungkin memerlukan peningkatan. hasil dari tahap evaluasi menjadi petunjuk penting untuk membuat perubahan dan peningkatan sehingga sistem dapat bekerja lebih baik lagi. 3.12 analisis hasil analisis merupakan tahapan penelitian dimana menyimpulkan serta menguraikan informasi dari hasil data yang telah diolah dan diuji sebelumnya. tahapan analisis dapat memberikan makna dari temuan temuan tersebut. tahapan ini memberikan identifikasi faktor faktor yang dapat mempengaruhi kinerja sistem dan memberikan rekomendasi untuk peningkatan di masa yang akan datang.
Ike Putri Kusumawijaya (99216004).txt,"Penelitian ini melakukan pengembangan metode untuk deteksi pergerakan anomali pada kerumunan menggunakan Algoritma Generative Adversarial Network. Metodologi yang digunakan adalah sebagai berikut. 1. Studi literature. Pada tahap ini dilakukan studi terhadap beberapa artikel dan buku yang menguraikan mengenai pemrosesan video, deteksi pergerakan anomali pada kerumunan dan Algoritma Generative Adversarial Network
2. Merancang algoritma Generative Adversarial Network secara Real time untuk mengklasifikasi antara pergerakan normal dan anomali pada kerumunan
3. Menguji keunggulan Generative Adversarial Network secara Real time dengan algoritma sebelumnya

3.1 Sistem Struktur Anomali pada Kerumunan
    Pada gambar 3.1 merupakan konsep awal sistem struktur pada kerumunan, untuk medeteksi adanya kerumunan yang bersifat anomaly. Konsep ini berawal dari model dari kerumunan atau dapat dikatakan socialforce model yaitu sebuah model kerumunan yang memfokuskan pada karakteristik lokal orang jika terdapat tekanan dalam kerumunan, atau kerumunan yang memiliki kepadatan yang tinggi. Model ini dimulai dengan penempatan frame video untuk menghitung karakteristik lokal secara efisien dan mendapatkan nilai local pressure model atau local model. Model ini berfungsi untuk mengekstrak karakteristik tekanan lokal untuk mengetahui pola atau sifat perilaku dari kerumunan sesuai karakteristik penduduk/masyarakat. Pola ini menggunakan vektor fitur diekstraksi untuk frame video dengan pemanfaatan Histogram of Oriented Pressure (HOP), dan untuk pola anomali akan terlihat menggukan Support Vector Machine (SVM). Untuk klasifikasi, dan filter median diimplementasikan pada hasil klasifikasi. Filter median adalah metode penyaringan digital non-linear yang biasanya digunakan untuk menghilangkan noise dari sinyal atau gambar pada video. Penelitian ini menggunakan model sistem anomali untuk mengetahui pola pergerakan pada kerumunan khususnya pada objek peneliti atau masyarakat di Indonesia. Sehingga mendapat kebaharuan menjadi dataset untuk membedakan pola pergerakan pada kerumunan anomali ataupun normal pada penduduk Indonesia. 3.2 Kerangka Penelitian Perilaku Anomali pada Kerumunan
       Pada gambar 3.2 merupakan kerangka penelitian yang diadaptasi dari penelitian El-Etriby et al., tahun 2017. Pada penelitian ini, sumber kerumunan berasal dari video atau kamera pengawas, proses selanjutnya melalui segmentation video, yaitu proses yang dilakukan pada gambar 3.1 berfungsi untuk menemukan pola yang terjadi pada kerumunan. Pada proses segmentasi juga akan dianalisis pola yang memiliki kerumunan normal dan tidak normal/anomali, pola ini dijadikan formation block berfungsi untuk mendapatkan karakterisitik yang didapat dari kerumunan, karakteristik yang diambil adalah nilai kepadatan kerumunan, tekanan pada kerumunan, serta pergerakan kerumunan yang terjadi, hasil dari karakteristik ini menjadi point of interest atau hal utama dalam pengambilan pola, sehingga dari nilai ini akan diketahui analisis perilaku yang terekam dalam video. 3.3	Tahap Pelatihan dan Pengujian Menggunakan GAN
    Sistem training atau pelatihan dan testing atau pengujian menggunakan algoritma Generative Adversarial Network, terdapat pada gambar 3.3. Kerangka proses di atas dibagi menjadi dua tahap utama yaiu pelatihan jaringan dan pengujian jaringan untuk deteksi anomali. Tahap 1: Tahap pelatihan mengikuti langkah-langkah di bawah ini:
1. Ekstraksi representasi dynamic images untuk setiap frame input (perilaku normal). 2. Melatih dua DenoisingAutoencoder (DAE) yang berbeda, satu untuk frame input dan yang lainnya untuk dynamic images. 3. Ekstraksi high-levelfeatures dari frame input dan dynamic images dari Denoising Autoencoder (DAE) yang telah dilatih sebelumnya sesuai dengan tipe datanya. 4. Pelatihan dua Conditional Generative Adversarial Networks (CGAN) tentang high-level features yang diekstraksi dari frame input dan dynamic images. Tahap 2: Tahap pengujian mengikuti langkah-langkah di bawah ini:
1. Ekstraksi representasi dynamic images untuk setiap masukan frame pengujian. 2. Perhitungan high-level features untuk frame input dan representasi dynamic images yang sesuai. 3. Perhitungan generation error maps menggunakan pre-trained Conditional Generative Adversarial Networks (CGAN) untuk menghitung binary detection maps untuk setiap tingkat representasional. 4. Hasil deteksi akhir ditentukan berdasarkan penggabungan peta deteksi yang diekstraksi. Pada gambar 3.3 algoritma Generative adversarial network menggunakan dynamic image atau analisa urutan video yang dijadikan gambar. Pada gambar tersebut frame yang sudah dijadikan gambar dilakukan proses training. Pengembangan dari proses training dan testing ini adalah dengan menggunakan input video seperti pada gambar 3.4. Pada gambar 3.4 konsep masukkan video dilakukan melalui kamera pengawas yang terhubung kepada router untuk dapat melakukan streaming atau analisis secara realtime/online. Hasil proses training dan testing disimpan dalam server, sehingga saat kamera menangkap video maka akan membandingkan dengan server yang sudah terisi oleh konsep atau pola dari normal dan anomali. Sehingga penelitian yang akan diteliti memiliki konsep sebagai berikut. Pada gambar 3.5 konsep penelitian berawal dari kamera pengawas yang membandingkan masukkan dari train GAN yang disimpan di dalam server dengan video yang diambil dari kamera secara real time, hasil dari perbandingan video tersebut akan menghasilkan behaviour analysis atau pola gerakan anomali pada kerumunan sehingga outputnya akan menjadi suatu dataset untuk identifikasi gerakan anomali pada kerumunan.","Penelitian ini melakukan pengembangan metode untuk deteksi pergerakan anomali pada kerumunan menggunakan Algoritma Generative Adversarial Network. Metodologi yang digunakan adalah sebagai berikut. Pada tahap ini dilakukan studi terhadap beberapa artikel dan buku yang menguraikan mengenai pemrosesan video, deteksi pergerakan anomali pada kerumunan dan Algoritma Generative Adversarial Network
2. Merancang algoritma Generative Adversarial Network secara Real time untuk mengklasifikasi antara pergerakan normal dan anomali pada kerumunan
3. Untuk klasifikasi, dan filter median diimplementasikan pada hasil klasifikasi. Pada proses segmentasi juga akan dianalisis pola yang memiliki kerumunan normal dan tidak normal/anomali, pola ini dijadikan formation block berfungsi untuk mendapatkan karakterisitik yang didapat dari kerumunan, karakteristik yang diambil adalah nilai kepadatan kerumunan, tekanan pada kerumunan, serta pergerakan kerumunan yang terjadi, hasil dari karakteristik ini menjadi point of interest atau hal utama dalam pengambilan pola, sehingga dari nilai ini akan diketahui analisis perilaku yang terekam dalam video. Perhitungan generation error maps menggunakan pre-trained Conditional Generative Adversarial Networks (CGAN) untuk menghitung binary detection maps untuk setiap tingkat representasional. Hasil deteksi akhir ditentukan berdasarkan penggabungan peta deteksi yang diekstraksi. Pengembangan dari proses training dan testing ini adalah dengan menggunakan input video seperti pada gambar 3.4. Hasil proses training dan testing disimpan dalam server, sehingga saat kamera menangkap video maka akan membandingkan dengan server yang sudah terisi oleh konsep atau pola dari normal dan anomali. Sehingga penelitian yang akan diteliti memiliki konsep sebagai berikut. Pada gambar 3.5 konsep penelitian berawal dari kamera pengawas yang membandingkan masukkan dari train GAN yang disimpan di dalam server dengan video yang diambil dari kamera secara real time, hasil dari perbandingan video tersebut akan menghasilkan behaviour analysis atau pola gerakan anomali pada kerumunan sehingga outputnya akan menjadi suatu dataset untuk identifikasi gerakan anomali pada kerumunan."
KUALIFIKASI_Riya Widayanti.txt,bab ini menyajikan desain yang digunakan dalam penelitian ini. desain penelitian adalah rencana umum bagaimana penelitian akan dilakukan untuk menjawab pertanyaan dan pernyataan dalam penelitian. hal ini menentukan sumber dari mana data akan dikumpulkan dan bagaimana mengumpulkan dan menganalisis data ini. selanjutnya membahas masalah etika dan beberapa kendala yang dapat ditemui peneliti. ini menunjukkan bahwa peneliti telah memikirkan elemenelemen desain penelitian tertentu saunders lewis thornhill 2011. pada bab ini akan dibahas mengenai filosofi keilmuan dari data governance konsep teknolgi blockchain dan penerapan data governance dalam teknologi blockchain di bidang pendidikan yang akan memberikan pandangan utama saat melakukan penelitian. selanjutnya akan dijelaskan pendekatan yang digunakan penelitian dalam pengumpulan data menganalisis data yang digunakan serta etika lain yang akan dipatuhi terutama terkait kerahasiaan data yang digunakan. jadi metodologi penelitian memberikan gambaran jelas mengenai strategi penelitian pengambilan data pengumpulan pengolahan dan analisis dan serta keterbatasan penelitian. 3.1 filosofi keilmuan pengkajian ilmiah penelitian menurut aliran positivistik banyak dianut peneliti ilmu komputer merupakan upaya sistematis investigatif objektif logis hatihati dan terencana dengan selalu berusaha mencari kebenaran. penelitian dengan pendekatan positivistik adalah memiliki karakteristik analitik nomotetik dedikatif laboratorik pembuktian dengan logika kebenaran universal dan bersifat bebas nilainya. jazi eko istiyanto 2009. 3.2. skema penelitian untuk menyelesaikan penelitian dirancang kerangka pikir yang menggambarkan langkahlangkah yang harus ditempuh dapat dilihat penjelasan dan urutannya sebagai berikut 24 gambar 3.1 kerangka perancangan tata kelola sumber dama 2017 25 3.2.1 mendefinisikan tata kelola data untuk organisasi upaya tata kelola data harus mendukung strategi dan tujuan bisnis. strategi dan sasaran bisnis organisasi menginformasikan strategi data perusahaan dan bagaimana tata kelola data dan aktivitas manajemen data perlu dioperasionalkan dalam organisasi. tata kelola data memungkinkan tanggung jawab bersama untuk keputusan terkait data. kegiatan tata kelola data melintasi batasbatas organisasi dan sistem untuk mendukung tampilan data yang terintegrasi. tata kelola data membutuhkan pemahaman yang jelas tentang apa yang diatur dan siapa yang diatur serta siapa yang mengatur uraian lebih detil tentang proses transformasi warna ruang warna yang digunakan algoritma trasnformasinya. setiap univeritas merupakan node dimana masingmasing node mengajukan beberapa kesepatan yang diturunkan dalam fungsional requirement yang nantinya akan dituangkan dalam consensus yang terdalam di dalam smartcard gambar 3.2 arsitektur blockchain 26 3.2.2 mengidentikasi fungsional requirement berdasarkan kesepakatan fungsional requirement akan diusulkan smart contract gambar 3.3 usulan kelompok funsional requirement penilaian yang menggambarkan keadaan saat ini dari kemampuan manajemen informasi organisasi kematangan dan efektivitas sangat penting untuk merencanakan program unit bisnis. karena dapat digunakan untuk mengukur efektivitas program penilaian juga berharga dalam mengelola dan mempertahankan program unit binis. penilaian khas meliputi kematangan pengelolaan data memahami apa yang dilakukan organisasi dengan data mengukur kemampuan dan kapasitas manajemen datanya saat ini. fokusnya adalah pada kesan yang dimiliki personel bisnis tentang seberapa baik perusahaan mengelola data dan menggunakan data untuk keuntungannya serta pada kriteria objektif seperti penggunaan alat tingkat pelaporan dll. kesiapan kolaboratif penilaian ini mencirikan kemampuan organisasi untuk berkolaborasi dalam pengelolaan dan penggunaan data. karena penatalayanan menurut definisi melintasi area fungsional itu bersifat kolaboratif. jika sebuah organisasi tidak tahu bagaimana berkolaborasi budaya akan menjadi hambatan bagi penatalayanan. jangan pernah berasumsi bahwa sebuah organisasi tahu bagaimana berkolaborasi. ketika 27 dilakukan bersama dengan kapasitas perubahan penilaian ini menawarkan wawasan tentang kapasitas budaya untuk melaksanakan ditjen. penyelarasan bisnis terkadang disertakan dengan kapasitas perubahan penilaian keselarasan bisnis memeriksa seberapa baik organisasi menyelaraskan penggunaan data dengan strategi bisnis. seringkali mengejutkan untuk mengetahui bagaimana aktivitas terkait data ad hoc dapat terjadi. 3.2.3 membuat kerangka lapisan data logis dalam tahap ini setelah setiap node mnyepakati proses bisnis yang akan dipakai bersama dalam aplikasi blockchain menetapkan skema pada lapisan data logis. hasilnya ada bagaimana kerangka komunikasi dijelaskan dalam gambar 3.3. gambar 3.4 kerangka blockchain layer 3 nguyen binh truong 2019 28 pedoman membuat kerangka logis 1. mekanismen identitas mangement otoritas dan autentifikasi identitas mangement otorisasi dan mekanisme otentikasi sangat penting dalam sistem manajemen data karena hal tersebut terkait langsung dengan keamanan dan privasi sistem. dalam konsep desain entitas dalam jaringan blockchain harus diidentifikasi secara unik menggunakan kunci publik atau hash kunci publik dalam pasangan kunci kriptografi asimetris proses otentikasi dan otorisasi harus diterapkan dengan memanfaatkan teknik kriptografi kunci publik misalnya tanda tangan digital dan enkripsi. dalam hal izin bc lapisan kontrol akses tambahan dikonsolidasikan dengan menggunakan otoritas sertifikat ca dan penyedia layanan keanggotaan msp. 2. desain buku besar terdistribusi konten terdistribusi buku besar mencerminkan keadaan historis dan informasi terkini yang dicatat dalam buku besar yang dikelola oleh jaringan blockchain. platform manajemen data pribadi harus mengklarifikasi informasi apa dan model data terkait yang akan disimpan dalam buku besar. i informasi yang diperlukan agar tahan terhadap kerusakan transparan dan dapat dilacak harus dicatat dalam buku besar yang didistribusikan. setiap kumpulan data pribadi harus ditentukan oleh data subjek dan data controller menggunakan tanda tangan digital dalam buku besar yang didistribusikan kebijakan penggunaan data harus ditetapkan dengan jelas dan dicatat dalam buku besar yang didistribusikan aktivitas data harus dicatat dalam buku besar yang didistribusikan. log harus berisi informasi tentang siapa mengapa kapan apa dan bagaimana data pribadi diproses hash data pribadi dapat dicatat dalam buku besar terdistribusi untuk pemeriksaan integritas data. 29 ii desain buku besar yang didistribusikan harus memastikan node yang ditunjuk dalam jaringan blockchain dapat memverifikasi apakah suatu entitas adalah data subjeck atau data controller dari kumpulan data node yang ditunjuk dalam jaringan blockchain harus dapat memverifikasi apakah aktivitas entitas memenuhi kebijakan penggunaan data seperti yang dicatat dalam buku besar terdistribusi 3. kebijakan penggunaan data kebijakan tersebut menentukan tindakan tata kelola data termasuk hak izin dan kondisi. kebijakan penggunaan harus didefinisikan secara halus dan ekspresif menggunakan bahasa kebijakan seperti extensible access control markup language xacml dan modelbased security toolkit seckit yang ditujukan untuk domain iot. secara alami manajemen data pribadi berbasis blockchain mengikuti konsep desain yang diusulkan memberikan kemampuan kontrol akses yang halus karena pengguna individu dapat menyesuaikan kebijakannya sendiri pada setiap kumpulan data dengan memaksakan preferensi kontrol akses yang dicatat ke buku besar. 4. penyimpanan data offchain data pribadi harus disimpan offchain untuk skalabilitas yang lebih baik dan efisiensi yang lebih tinggi. selain itu menyimpan data pribadi langsung ke clockchain bahkan dalam bentuk terenkripsi dapat menimbulkan potensi kebocoran privasi dan mengakibatkan ketidakpatuhan terhadap gdpr. tergantung pada skenario tertentu dbms konvensional misalnya oracle atau mongodb layanan penyimpanan awan misalnya s3 aws atau azure atau sistem penyimpanan dapat digunakan untuk penyimpanan data. hanya referensi ke data yang disimpan secara onchain yaitu disimpan dalam buku besar terdistribusi. referensi disebut penunjuk data itu bisa menjadi hash string koneksi jalur absolut atau pengidentifikasi yang merujuk ke kumpulan data tergantung pada sistem penyimpanan offchain tertentu yang digunakan dalam platform. 30 3.3 kerangka pikir tahapan penelitian ini dimulai dengan pengumpulan data melalui proses wawancara analisis dokumen dan identifikasi masalah data. tahap selanjutnya adalah mengolah data yang terkumpul dengan kerangka tata kelola data yang dijelaskan dalam dmbok. tahap selanjutnya adalah merancang struktur tata kelola data sesuai dengan struktur pengelolan pengejaran merdeka belajar. dalam merancang struktur tata kelola data juga dilakukan penentuan peran area keputusan dan tanggung jawab yang dilakukan. perancangan peran dilakukan dengan menggunakan metode wawancara dan mengadaptasi kajian pada data subject tim pengelola merdeka belajar. struktur tata kelola data yang telah dirancang akan dikonfirmasikan dengan menggunakan kuesioner sehingga akan dihasilkan struktur yang dapat dipertanggungjawabkan. selanjutnya kesimpulan dibuat sebagai tahap terakhir dari penelitian ini. 31 gambar. 3.4 usulan kerangka pikir penelitian,hal ini menentukan sumber dari mana data akan dikumpulkan dan bagaimana mengumpulkan dan menganalisis data ini. ini menunjukkan bahwa peneliti telah memikirkan elemenelemen desain penelitian tertentu saunders lewis thornhill 2011. pada bab ini akan dibahas mengenai filosofi keilmuan dari data governance konsep teknolgi blockchain dan penerapan data governance dalam teknologi blockchain di bidang pendidikan yang akan memberikan pandangan utama saat melakukan penelitian. selanjutnya akan dijelaskan pendekatan yang digunakan penelitian dalam pengumpulan data menganalisis data yang digunakan serta etika lain yang akan dipatuhi terutama terkait kerahasiaan data yang digunakan. 3.2. skema penelitian untuk menyelesaikan penelitian dirancang kerangka pikir yang menggambarkan langkahlangkah yang harus ditempuh dapat dilihat penjelasan dan urutannya sebagai berikut 24 gambar 3.1 kerangka perancangan tata kelola sumber dama 2017 25 3.2.1 mendefinisikan tata kelola data untuk organisasi upaya tata kelola data harus mendukung strategi dan tujuan bisnis. strategi dan sasaran bisnis organisasi menginformasikan strategi data perusahaan dan bagaimana tata kelola data dan aktivitas manajemen data perlu dioperasionalkan dalam organisasi. tata kelola data memungkinkan tanggung jawab bersama untuk keputusan terkait data. kegiatan tata kelola data melintasi batasbatas organisasi dan sistem untuk mendukung tampilan data yang terintegrasi. tata kelola data membutuhkan pemahaman yang jelas tentang apa yang diatur dan siapa yang diatur serta siapa yang mengatur uraian lebih detil tentang proses transformasi warna ruang warna yang digunakan algoritma trasnformasinya. fokusnya adalah pada kesan yang dimiliki personel bisnis tentang seberapa baik perusahaan mengelola data dan menggunakan data untuk keuntungannya serta pada kriteria objektif seperti penggunaan alat tingkat pelaporan dll. tahap selanjutnya adalah mengolah data yang terkumpul dengan kerangka tata kelola data yang dijelaskan dalam dmbok. tahap selanjutnya adalah merancang struktur tata kelola data sesuai dengan struktur pengelolan pengejaran merdeka belajar. dalam merancang struktur tata kelola data juga dilakukan penentuan peran area keputusan dan tanggung jawab yang dilakukan. perancangan peran dilakukan dengan menggunakan metode wawancara dan mengadaptasi kajian pada data subject tim pengelola merdeka belajar. struktur tata kelola data yang telah dirancang akan dikonfirmasikan dengan menggunakan kuesioner sehingga akan dihasilkan struktur yang dapat dipertanggungjawabkan. selanjutnya kesimpulan dibuat sebagai tahap terakhir dari penelitian ini.
Kualifikasi_Andi Asnur Pranata M. H. (99219024).txt,"3.1 Tahapan Penelitian
     Metode penelitian ini dilakukan berdasarkan kerangka pemikiran dan kajian pustaka. Secara umum penelitian ini berupa studi kualitatif dengan pendekatan deskriptif, untuk teknik pengambilan sampel melalui observasi, wawancara, kuisioner, dan dokumentasi. Untuk tahapan penelitian secara garis besar, dapat dilihat pada gambar 3.1. digunakan untuk mengetahui output yang dibutuhkan oleh pihak-pihak yang terlibat di proyek konstruksi. 3.3.2 Data Sekunder
     Data sekunder dikumpulkan dari hasil studi literatur, review penelitian terdahulu, pencarian perangkat lunak dan beberapa template perangkat lunak yang banyak tersedia di internet. Data sekunder ini juga akan mengumpulkan mengenai informasi proyek, sumber dana, jenis laporan baik dari pemilik (pengguna jasa), konsultan dan kontraktor, serta jadwal pelaksanaan dan kemajuan fisik, termin pembayaran serta informasi orang-orang yang berkepentingan dalam proyek konstruksi. 3.3.3 Pengumpulan Data
     Teknik pengumpulan data yang dilakukan antara lain sebagai berikut :
a. Observasi dilakukan untuk mengetahui kebutuhan sistem informasi manajemen proyek yang dibutuhkan oleh para pihak-pihak yang terlibat pada proyek konstruksi. b. Wawancara dilakukan untuk mengetahui output mengenai sistem informasi manajemen yang dibutuhkan oleh para pihak-pihak yang terlibat pada proyek kosntruksi. c. Kuesioner dilakukan untuk menanyakan kembali kepada para pihak-pihak yang terlibat pada proyek konstruksi mengenai pembobotan atas atribut- atribut yang sudah dikelompokkan atas jawaban setiap atribut untuk dijadikan bobot kebutuhan dari para pihak-pihak yang terlibat pada proyek konstruksi. d. Dokumentasi dilakukan untuk mendapatkan data dengan cara membaca dan mengambil kesimpulan dari berkas-berkas proyek. 3.4 Pengembangan Sistem
     Untuk pengembangan sistem, sesuai yang telah disampaikan pada latar belakang bahwa metode yang digunakan dalam memodelkan sistem informasi manajemen proyek konstruksi adalah Rapid Application Development (RAD). Metode RAD digunakan karena modul yang terlalu banyak sehingga untuk fleksibilitas dalam pengembangan sistem dapat dikendalikan serta jika ada perubahan pada setiap modul, maka pengembang secara fleksibel dapat merubah modul tersebut dan modul yang berkaitan. RAD pada pengembangan sistem yang membuat terbagi dalam beberapa tahap, sedangkan tools yang digunakan untuk memodelkan beriorientasi objek adalah notasi Unified Modelling Language (UML). Metode RAD terdiri dari tiga tahap pengembangan, yaitu :
a. Requirement Planning Phase
     Pada tahap ini, akan dilakukan obeservasi untuk mengumpulkan informasi-informasi mengenai gambaran umum dari suatu perusahaan, termasuk logo perusahaan, visi dan misi, serta struktur perusahaannya. Tahap selanjutnya, akan dilakukan analisi pada sistem yang berjalan proses monitoring manajemen proyek serta bagaimana work breakdown structures yang sedang berlangsung, kemudian mengindentifikasi masalah dari sistem yang berjalan. Setelah itu, akan dibuat sistem usulan yang akan dijadikan sebagai rekomendasi untuk sistem tersebut. b. RAD Design Workshop
     Pada tahap ini, tahap perancangan proses sistem, basis data dan user interface yang akan dikerjakan untuk prototype sistem, kemudian menganalisis dan mengembangkan modul-modul yang dirancang. 1)	Perancangan Proses Sistem
     Pada tahap ini akan membuat use case diagram, activity diagram, class diagram, sequence diagram, component diagram, dan deployment diagram. 2)	Perancangan Basis Data
     Pada tahap ini, akan dirancang basis data yang berupa tabel-tabel serta hubungan antar label yang berdasarkan kebutuhan sistem informasi manajemen proyek. 3)	Perancangan User Interface
     Pada tahap ini, akan dirancang tampilan antar muka yang akan dibuat sesuai dengan kebutuhan pengembangan sistem. c. Implementation Phase
     Pada tahap ini, akan dilakukan implementasi pada sistem sehingga yang sudah dirancang dapat dilihat prosesnya ke dalam bentuk aplikasi. Tahap ini akan terdiri dari dua tahap, yaitu :
     1)	Tahap Pembangunan Sistem
     Jika perancangan siap dan sudah disetujui, maka proses sistem akan dibangun dengan menggunakan bahasa Laravel Framework sesuai dengan rancangan yang sudah dibuat. 2)	Tahap Pengujian Sistem
     Tahap ini akan memeriksa seluruh proses yang telah dibangun apakah dapat berjalan sesuai rancangan dan optimal. 3.4.1 Gambaran Umum Perusahaan
     Pada tahapan ini, akan dikumpukan informasi mengenai gambaran umum perusahaan seperti profil perusahaan, visi misi perusahaan, struktur organisasi perusahaan. 3.4.2 Analisis Sistem Berjalan
     Pada tahapan ini, akan dikumpulkan informasi mengenai sistem yang berj alan pada perusahaan, seperti informasi aktor-aktor yang menggunakan sistem tersebut. Untuk mengetahui tanggung jawab dari masing-masing Admin, User, dan Client yang menggunakan sistem tersebut. Dari sistem yang digunakan, akan dikumpulkan juga mengenai informasi kelebihan dan kekurangan sistem yang berjalan dari masing-masing aktor-aktor yang menggunakan sistem tersebut. 3.4.3 Identifikasi Masalah
     Berdasarkan hasil pengumpulan informasi pada tahap analisis sistem berjalan, yaitu mengetahui kelemahan sistem yang berjalan, maka peneliti akan mengindentifikasi permasalahan tersebut. 3.4.4 Analisis Sistem Usulan
     Berdasarkan hasil pengumpulan informasi mengenai kelemahan sistem yang berjalan dan hasil identifikasi masalah, maka untuk menyelesaikan permasalahan tersebut akan dianalisis untuk keperluan sistem usulan guna untuk melakukan pengembangan sistem informasi manajemen pada proyek konstruksi. 3.4.5 Perancangan Proses
3.4.5.1 Use Case Diagram
        Pada use case diagram akan memberikan deskripsi hubungan antara pengguna sistem (aktor) dengan aktivitas-aktivitas atau proses pada sistem informasi manajemen untuk proyek kosntruksi. Setelah use case diagram sudah terbentuk, maka selanjutnya akan membuat identifikasi use case diagram pada sistem informasi manajemen untuk proyek konstruksi. Setelah proses tersebut, maka selanjutnya membuat narasi use case untuk menjelaskan use case secara lebih rinci. 3.3 Pengumpulan Data
3.3.1 Data Primer
      Data primer dikumpulkan dari hasil wawancara dan diskusi dengan pengguna jasa dan penyedia jasa disertai dengan mempresentasikan mengenai konsep sistem informasi manajemen proyek konstruksi. 3.4.5.2	Activity Diagram
        Pada activity diagram ini akan menjelaskan mengenai aktivitas- aktivitas yang terjadi pada sistem informasi manajemen untuk proyek konstruksi. 3.4.5.3	Class Diagram
        Pada class diagram ini akan membuat hubungan relasi antara objek, memiliki atribut dan operasi yang ada pada objek. Tahap pertama akan dibuat daftar objek melalui analisis dari objek-objek pada proses sistem. Setalah membuat daftar objek, maka selanjutnya akan dibuat analisis daftar objek dan membuat atribut-abtribut pada objek. Dari hasil tersebut, akan terbentuk daftar usulan objek pada sistem tersebut, kemudian akan berlanjut pada tahap pembuatan class diagram pada sistem tersebut. 3.4.5.4	Sequence Diagram
        Pada sequence diagaram ini akan memberikan penjelasan mengenai 
urutan secara rinci pada proses objek-objek yang akan dilakukan pada sistem untuk 
mencapai tujuan dari use case. 3.4.5.5	Component Diagram
        Pada component diagram ini akan digambarkan komponen- komponen dari sistem dan memberikan penjelasan mengenai masing-masing komponen yang ada pada sistem tersebut. 3.4.5.6 Deployment Diagram
        Pada deployment diagram ini akan memberikan gambaran fisik untuk perangkat-perangkat yang akan digunakan. 3.4.6 Perancangan Basis Data
3.4.6.1 Mapping Database
        Pada mapping database ini akan dilakukan pemetaan skema database untuk menentukan relasi hubungan primary-key dan foreign-key dari antar tabel-tabel yang terbentuk. 3.4.6.2 Spesifikasi Database
        Pada spesifikasi database ini akan merancangan desain tabel sistem manajemen informasi untuk proyek konstruksi. 3.4.7 Perancangan Antar Muka
     Pada perancangan antar muka ini akan dirancang antar muka sistem informasi manajemen untuk proyek konstruksi yang nanti akan dibagi berdasarkan aktor-aktor pada case diagram. 3.5 Implementasi
3.5.1 Pembangunan Sistem
      Pada pembangunan sistem ini akan dibangun menggunakan hardware dan software sesuai dengan kebutuhan spesifikasi yang akan dibutuhkan. 3.5.2 Pengujiian Sistem
      Pada pengujian sistem ini akan menggunakan pengujian blackbox testing atau white-box testing. Pengujian ini dilakukan untuk mengetahui apakah semua modul yang sudah dibentuk berjalan sesuai rancangan atau tidak, serta mengetahui apakah ada kesalahan-kesalahan terhadap proses pada sistem informasi manajemen untuk proyek konstruksi. 3.6 Kesimpulan dan Saran
      Setelah tahap penelitian selesai, maka pada tahap akhir ini peneliti akan memberikan kesimpulan mengenai hasil penelitian yang sudah didapatkan. Pada tahap akhir ini juga, peneliti akan memberikan saran kepada peneliti berikut yang akan melakukan penelitian dengan tema yang sama, untuk memberikan gambaran dalam mengembangkan penelitian yang sudah dilakukan sebelumnya.","3.3.2 Data Sekunder
     Data sekunder dikumpulkan dari hasil studi literatur, review penelitian terdahulu, pencarian perangkat lunak dan beberapa template perangkat lunak yang banyak tersedia di internet. Data sekunder ini juga akan mengumpulkan mengenai informasi proyek, sumber dana, jenis laporan baik dari pemilik (pengguna jasa), konsultan dan kontraktor, serta jadwal pelaksanaan dan kemajuan fisik, termin pembayaran serta informasi orang-orang yang berkepentingan dalam proyek konstruksi. 3.4 Pengembangan Sistem
     Untuk pengembangan sistem, sesuai yang telah disampaikan pada latar belakang bahwa metode yang digunakan dalam memodelkan sistem informasi manajemen proyek konstruksi adalah Rapid Application Development (RAD). Metode RAD digunakan karena modul yang terlalu banyak sehingga untuk fleksibilitas dalam pengembangan sistem dapat dikendalikan serta jika ada perubahan pada setiap modul, maka pengembang secara fleksibel dapat merubah modul tersebut dan modul yang berkaitan. b. RAD Design Workshop
     Pada tahap ini, tahap perancangan proses sistem, basis data dan user interface yang akan dikerjakan untuk prototype sistem, kemudian menganalisis dan mengembangkan modul-modul yang dirancang. Tahap ini akan terdiri dari dua tahap, yaitu :
     1)	Tahap Pembangunan Sistem
     Jika perancangan siap dan sudah disetujui, maka proses sistem akan dibangun dengan menggunakan bahasa Laravel Framework sesuai dengan rancangan yang sudah dibuat. 3.4.4 Analisis Sistem Usulan
     Berdasarkan hasil pengumpulan informasi mengenai kelemahan sistem yang berjalan dan hasil identifikasi masalah, maka untuk menyelesaikan permasalahan tersebut akan dianalisis untuk keperluan sistem usulan guna untuk melakukan pengembangan sistem informasi manajemen pada proyek konstruksi. Dari hasil tersebut, akan terbentuk daftar usulan objek pada sistem tersebut, kemudian akan berlanjut pada tahap pembuatan class diagram pada sistem tersebut. 3.4.6.2 Spesifikasi Database
        Pada spesifikasi database ini akan merancangan desain tabel sistem manajemen informasi untuk proyek konstruksi. 3.4.7 Perancangan Antar Muka
     Pada perancangan antar muka ini akan dirancang antar muka sistem informasi manajemen untuk proyek konstruksi yang nanti akan dibagi berdasarkan aktor-aktor pada case diagram. 3.5 Implementasi
3.5.1 Pembangunan Sistem
      Pada pembangunan sistem ini akan dibangun menggunakan hardware dan software sesuai dengan kebutuhan spesifikasi yang akan dibutuhkan. Pengujian ini dilakukan untuk mengetahui apakah semua modul yang sudah dibentuk berjalan sesuai rancangan atau tidak, serta mengetahui apakah ada kesalahan-kesalahan terhadap proses pada sistem informasi manajemen untuk proyek konstruksi. 3.6 Kesimpulan dan Saran
      Setelah tahap penelitian selesai, maka pada tahap akhir ini peneliti akan memberikan kesimpulan mengenai hasil penelitian yang sudah didapatkan. Pada tahap akhir ini juga, peneliti akan memberikan saran kepada peneliti berikut yang akan melakukan penelitian dengan tema yang sama, untuk memberikan gambaran dalam mengembangkan penelitian yang sudah dilakukan sebelumnya."
Kualifikasi_Aris Gunaryati.txt,3.1 gambaran umum penelitian motivasi dari metodologi yang diusulkan adalah membu at suatu metode peramalan yang sesuai dengan data runtun waktu yang ada serta meni ngkatkan akurasinya dengan tetap memp erhatikan efisiensi w aktu ko mputasi nya. langkahlangkah yang dilakuk an dalam p enelitian ini adalah m enganalisis data jum lah kasus h arian covid 19 di jakarta berdasarkan dataset dari situs ht tpscorona.jakarta.go.id tangg al 6 maret 2020 sampai 30 juni 2021 sebagai data training dan nanti akan diprediksi untuk tanggal 1 juli 2021 sampai dengan 31 juli 2021 sebagai data uji dengan tahapan sebagai berikut 1. mempersiapk an data runtun w aktu yang akan dia nalisis 2. menganalisis data runtun waktu yang ada meng gunakan metode statistika arima 3. menganalisis data runtun waktu yang ada menggunakan metode quantum neural network 4. mengembangkan model hybrid arimaquantum neural network 5. menentukan mod el yang cocok untuk s etiap variabel 6. menguji kecocokan masingmasing model 7. melakuk an peramalan dengan menggunakan mo del yang cocok 8. melakuk an perbandingan tingkat aku rasi hasil peramalan dengan tiap model untuk mend apatkan model peramalan yang diharapkan sesuai dengan data runtun waktu yang ada maka perlu dilakukan pendekatan ilmiah yaitu dengan melihat pola d ata runtun waktu yang ada terlebih dahulu. dengan melihat pola data awal yang di miliki maka akan memud ahkan dalam memi lih model yang sesuai untuk data tersebut. pendekatan lainnya adalah me nggunakan too ls untuk m enentukan secara otomatis bentuk model statistik arima yang sesuai dengan runtun waktu yang ada lalu model tersebut dilatih menggunakan quantum neural network agar diket ahui polapola d ata yang sudah ada d an d apat d iuji akurasinya.17 tipe model pola tipikal acf pola tipikal pacf ar p menurun secara ekspon ensial sinusoidal terputus s etelah lag p ma q terputus s etelah lag q menurun secara ekspon ensial sinusoidal arma p q menurun secara ekspon ensial sinusoidal menurun secara ekspon ensial sinusoidal 3.2 model arima bentuk u mum model ar ima dapat dinyatakan dalam p ersamaan berikut ............................... ................................ ................................. 1 operator ar adalah ............................................................2 operator ma adalah ............................................................ ............. .3 1. autore gressive integrated moving average arima not asi model arima p d q p orde untuk pros es autoregressive ar d orde yang menyatakan banyaknya proses dife rensi d ilakuk an pada data time series yang tidak stasione r q orde yang menyatakan proses moving a verage ma. pola teoretis acf dan pacf dari proses yang stasio ner sumber aswi dan sukarna 2006 2. tahapan analisis time series arima a. membuat plot time series identifikasi asumsi s tasione ritas data runtun waktu. suatu de ret pengamatan dikat akan stasioner apabila proses tidak berubah seiring dengan perubahan waktu tidak stasioner dalam mean jika trend tidak datar tidak sejajar smbu waktu tidak stasioner dalam varian jika trend datar atau hampir datar tetapi data tersebar membangun pola m elebar atau m enyempit pola t eromp et18 tidak stasioner dalam mean varians j ika trend tidak datar dan data memb entuk po la terompet. augmented di ckey fuller uji formal untuk stasion eritas hipotesis h0 terdapat akar unit dan data tidak st asioner 0 h1 tid ak terdapat akar unit dan data stasioner 0 span taraf signifik ansi α statistik uji ............................................................... ...4 ............................................................................................... .5 ............................................................................................... .......... .6 kriteria uji h0 ditolak jika nilai mu tlak dari augmented di ckey fuller nilai kritis mackinnon atau nilai prob . α. b. menghitung membu at plot acf dan pacf mengidentifikasi model runtun w aktu yang mungkin mengestimasi p arameter model c. uji signifik ansi parameter hipotesis h0 danatau parameter tidak signifik an terhadap model h1 danatau parameter signifik an terhadap model taraf signifik ansi α statistik uji danatau 19 kriteria uji tolak h 0 jika atau p value alpha d. verifikasi mo del independensi residual hipotesis h0 tidak ada korelasi antarlag h1 paling sedikit ada satu dengan k12 24 36 48 ada ko relasi antarlag statistik uji kriteria uji tolak h 0 jika atau p value alpha dengan m l ag maksim um s jumlah p arameter yang diesti masi dan taraf signifik ans normalitas residual hipotesis h0 residual berdistriusi norm al h1 residual t idak berdistribusi norm al statistik uji fungsi peluang kumulatif r esidual distribusi ku mulatif yang diobs ervasi dari suatu sampel acak sebanyak n o servasi kriteria uji tolak h 0 jika atau p value alpha ukuran ketepatan ra malan mod el dengan uku ran ketepatan p eramalan yang baik ad alah model yang menghasilkan error yang kecil. nilai teng ah kesalahan kuadrat mean square er ror 20 berikut flo wchart langkahlangkah membu at model arima gambar 1. flowch art analisis runtun waktu arima 3.3 model neural network dalam buku jaringan syaraf tiruan dan pemrogramannya menggunakan matlab drs. jong jek siang m.sc menyebutkan bahwa jaringan syaraf tiruan adalah system pemroses informasi yang memiliki karakteristik mirip dengan jaringan syaraf biologi. jaringan syaraf t iruan dibentuk sebagai generalisasi model matematika dari jaringan syaraf biologi dengan asumsi bahwa pemrosesan informasi terjadi pada banyak elemen sederhana neuron a. sinyal dikirimkan di antara neuron neuron melalui penghubung penghubung b. penghubung ant ar neuron memiliki bobot yang akan memperkuat atau memperlemah sinyal c. untuk menentukan output setiap neuron menggunakan fungsi aktivasi biasanya bukan fungsi linier yang dikenakan pada jumlahan input yang diterima. besarnya output ini selanjutnya diband ingkan dengan suatu batas ambang treshhold 21 jaringan syaraf tiruan ditentukan oleh tiga hal a. pola hubungan antar neuron disebut arsitektur jaringan b. metode untuk menentukan bobot penghubung disebut metode traininglearning algoritma c. fungsi aktivasi gambar skematik tipikal neuron dapat dilihat pada gambar 2 gambar 2 syaraf biologis pemrosesan informasi dalam jaringan syaraf tiruan dapat disingkat sebagai berikut sinyal baik berupa aksi ataupun potensial muncul sebagai masukan unit sinapsis efek dari tiap sinyal ini dinyatakan sebagai bentuk perkalian dengan sebuah nilai bobot untuk mengindikasikan kekuatan dari sinapsis. semua sinyal yang diberi pengali bobo t ini kemudian dijumlahkan satu sama lain untuk menghasilkan unit aktivasi. jika aktivasi ini melampaui sebuah batas ambang tertentu maka unit tersebut akan memberikan keluaran dalam bentuk respon terhadap masukan. unit aktivasi ini kemudian dibandingkan d engan sebuah nilai ambang dan hasilnya dimasukkan kedalam fungsi transfer fungsi non linier yang akan menghasilkan sebuah keluaran. secara ringkas proses tersebut dapat digambarkan dalam gambar 3 gambar 3 neuron buatan mcculloch pitts sebagai operator matematis 22 aktivasi dari unit masukan diatur dan diteruskan melalui jaring hingga nilai dari keluaran dapat ditentukan. jaring berperan sebagai fungsi vektor yang mengambil satu vektor pada masukan dan mengeluarkan satu vektor lain pada keluaran. model jaringan syaraf tiruan dapat memiliki sebuah lapisan bobot dimana masukan dihubungkan langsung dengan keluaran atau beberapa lapisan yang didalamnya terdapat beberapa lapisan tersembunyi karena berada t ersembunyi diantara neuron masukan dan keluaran. jaring syaraf menggunakan unit tersembunyi untuk menghasilkan representasi pola masukan secara internal didalam jaring syaraf. fungsi transfer nonlinier yang digunakan dalam tiap neuron baik dilapisan masukan keluaran atau lapisan tersembunyi dapat berupa fungsi nilai ambang fungsi linier fungsi sigmoid ataupun fungsi gaussian tergantung dari karakter neuron sesuai keinginan kita. hal ini dapat dilihat pada gambar 4 gambar 4 tipikal sebuah jaringan syaraf tiruan 3.3.1 komponen jaringan syaraf terdapat beberapa tipe jaringan syaraf hampir semuanya memiliki komponen komponen yang sama. seperti halnya otak manusia jaringan syaraf juga terdiri atas beberapa neuron dan ada hubungan antar neu ron tersebut. neuron neuron tersebut akan mentransformasikan informasi yang diterima melalui sambungan keluarnya menuju ke neuron neuron yang lain. pada jaringan syaraf hubungan ini dikenal dengan nama bobot. informasi tersebut disimpan pada suatu nilai t ertentu pada bobot tersebut. neuron ini sebenarnya mirip dengan sel neuron biologis. neuron neuron buatan tersebut bekerja dengan cara yang sama pula dengan neuron biologis. informasi disebut dengan input akan dikirim ke neuron dengan bobot kedatangan t ertentu. input ini akan diproses oleh suatu fungsi perambatan yang akan menjumlahkan nilai nilai semua bobot yang datang. hasil penjumlahan ini kemudian akan dibandingkan dengan suatu nilai ambang threshold tertentu melalui fungsi aktivasi setiap neuron. apabila input tersebut melewati suatu nilai ambang tertentu 23 maka neuron tersebut akan diaktifkan tapi kalau tidak maka neuron tersebut tidak akan diaktifkan. apabila neuron tersebut diaktifkan maka neuron tersebut akan mengirimkan output melalui bobot bobot outputnya kesemua neuron yang berhubungan dengannnya. pada jaringan syaraf neuron neuron akan dikumpulkan dalam lapisan layer yang disebut dengan lapisan neuron neuron layer . neuron neuron pada satu lapisan akan dihubungkan dengan lapisan lapisan sebelum dan sesudahnya kecuali lapisan input dan lapisan output. informasi yang diberikan pada jaringan syaraf akan dirambatkan lapisan ke lapisan. mulai dari lapisan input sampai ke lapisan output melalui lapisan lainnya yang sering disebut sebagai lapisan tersembunyi hidden layer . 3.3.2 arsitektur jaringan syaraf 3.3.2.1 jaringan dengan lapisan tunggal single layer net jaringan dengan lapisan tunggal hanya memiliki satu lapisan dengan bobot bobot terhubung. jaringan ini hanya menerima input kemudian secara langsung akan mengolahnya menjadi output tanpa harus melalui lapisan tersembunyi. 3.3.3.3 jaringan dengan banyak lapisan multilayer net jaringan dengan banyak lapisan memiliki 1 atau lebih lapisan yang terletak diantara lapisan input dan lapisan output memiliki 1 atau lebih lapisan tersembunyi. umumnya ada lapisan bobot bobot yang terletak antara 2 lapisan yang bersebelahan. jaringan dengan banyak lapisan ini dapat menyelesaikan permasalahan yang lebih sulit daripada jaringan dengan lapisan tunggal tentu saja dengan pembelajaran yang lebih rumit. namun demikian pada banyak kasus pembelajaran pada jaringan dengan banyak lapisan ini lebih sukses dalam menyelesaikan masalah. 3.3.3. fungsi aktivasi ada beberapa fungsi aktivasi yang sering digunakan dalam jaringan syaraf tiruan antara lain a. fungsi undak biner hard limit jaringan dengan lapisan tunggal sering menggunakan fungsi undak step function untuk mengkonversikan input dari suatu variabel yang bernilai kontinu ke suatu output biner 0 atau 1 24 b. fungsi undak biner threshold fungsi undak biner dengan menggunakan nilai ambang sering juga disebut dengan fungsi nilai ambang threshold atau fungsi heaviside . c. fungsi bipolar symetric hard limit fungsi bipolar sebenarnya hampir sama dengan fungsi undak biner hanya saja output yang dihasilkan berupa 1 0 atau 1 d. fungsi bipolar dengan threshold fungsi bipolar sebenarnya hampir sama dengan fungsi undak biner dengan threshold. hanya saja keluaran yang dihaslkan berupa 1 0 atau 1 e. fungsi linear identitas fungsi linear memiliki nilai output yang sama dengan nilai inputnya. 25 f. fungsi saturating linear fungsi ini akan bernilai 0 jika inputnya kurang dari ½ dan akan bernilai 1 jika inputnya lebih dari ½. sedangkan jika nilai input terletak antara 12 dan ½ maka outputnya akan bernilai sama dengan nilai input ditambah ½ g. fungsi symetric saturating linear fungsi ini akan bernilai 1 jika inputnya kurang dari 1 dan akan bernilai 1 jika inputnya lebih dari 1. sedangkan jika nilai input terletak antara 1 dan 1 maka outputnya akan bernilai sama dengan nil ai inputnya. h. fungsi sigmoid biner fungsi ini digunakan untuk jaringan syaraf yang dilatih dengan menggunakan metode backpropagation. fungsi sigmoid biner memiliki nilai pada range 0 sampai 1. oleh karena itu fungsi ini sering digunakan untuk jaringan syaraf yang membutuhkan nilai output yang terletak pada interval 0 sampai 1. namun fungsi ini bisa juga digunakan oleh jaringan syaraf yang nilai outputnya 0 atau 1. 26 i. fungsi sigmoid bipolar fungsi sigmoid bipolar hampir sama dengan fungsi sigmoid biner ha nya saja output dari fungsi ini memiliki range antara 1 sampai 1 fungsi ni sangat dekat dengan fungsi hyperbolic tangent. keduanya memiliki range antara 1 sampai 1. untuk fungsi hyperbolic tangent 3.4 model hybrid arima neural network berdasarkan hasil peramalan model arima akan dilakukan proses analisis runtun waktu menggunakan metode jaringan syaraf tiruan. dengan kata lain output dari peramalan model arima akan menjadi input pada proses pengolahan data menggunakan metode jaringan syaraf tiruan. kemudian akan ditentukan model jaringan syaraf tiruan yang sesuai dan cocok untuk data runtun waktu tersebut. secara matemat is hasil ramalan secara keselu ruhan yang diperoleh adalah sebagai berikut zt merupakan hasil pe ramalan yang merupakan gabungan nilai ramalan dari model arima atau exponential smoothing dan nilai ramalan dari model jst. berikut ini adalah arsitektur model peramal an hybrid arima jst dan es jst gambar 5 model hybrid arima jst dan hybrid es jst 3.5 model quantum hybrid arima neural network ada b anyak pendekatan untuk pengembangan model quantum arima nn. modelmodel ini fokus pada yang berbeda aspek kompu tasi ku antum dan pemros esan saraf. dalam kompu tasi kuantum sebagai unit info rmasi terkecil bit kuantum atau qubit adalah si stem kuantum yang me nyatakan terletak di ruang hilb ert dua dimensi. seperti bit dalam klasik kompu ter qubit berlabel dan mengekspresikan satu bit informasi sesuai dengan bit 0 kompu ter klasik dan bit 1. keadaan qubit menyatakan superposisi keadaan yang kohere 27 ............................... ..................... .............. 7 di mana dan menentukan probabilitas yang sesuai. gerbang kuantum yang mencakup karakteristik kompu tasi kuantum me rupakan dasar untuk i mplem entasi fisik dari kompu tasi kuantum. himpunan logika unive rsal termasuk dalam logika kuantum. mirip dengan bit klasik gerbang dasar dapat memb entuk gerbang kuantum bemacammacam dan menyelesaikan keadaan ku antum dari beberapa logika tr ansformasi. berbasis elemen pada gerbang pergeseran fasa 1 bit dan gerbang kontro ltidak 2 bit dalam dinamika kuantum diambil sebagai fungsi aktivasi dalam jaringan saraf. untuk memud ahkan aplikasi fo rmulir berikut fungsi ko mpleks dib erikan untuk menyatakan keadaan kuantum ............................................................... .................. ................ .8 adalah bilangan imaginer adalah k uantum fase 3.6 pengukuran kinerja 3.6.1 mean squared error dalam statistik mean squared error mse sebuah estimator adalah nilai yang diharapkan dari kuadrat error . error yang ada menunjukkan seberapa besar perbedaan hasil estimasi dengan nilai yang akan diestimasi. perbedaan itu terjadi karena adanya keacakan pada data atau karena estimator tida k mengandung informasi yang dapat menghasilkan estimasi yang lebih akurat 3.6.2 komparasi hasil peramalan setelah nilai mean squared error dari kedua metode didapatkan maka akan dilakukan komparasi terhadap nilai mse yang didapatkan pada periode testing out sample jika nilai mse statistika mse ann maka metode statistika memiliki performa lebih baik dibandingkan metode ann karena memiliki tingkat kesalahan relatif lebih kecil. sebaliknya jika mse statistika mse ann maka metode statistika memilki perform a lebih buruk dibandingkan metode ann karena tingkat kesalahan yang dihasilkan relatif lebih besar.,3.1 gambaran umum penelitian motivasi dari metodologi yang diusulkan adalah membu at suatu metode peramalan yang sesuai dengan data runtun waktu yang ada serta meni ngkatkan akurasinya dengan tetap memp erhatikan efisiensi w aktu ko mputasi nya. langkahlangkah yang dilakuk an dalam p enelitian ini adalah m enganalisis data jum lah kasus h arian covid 19 di jakarta berdasarkan dataset dari situs ht tpscorona.jakarta.go.id tangg al 6 maret 2020 sampai 30 juni 2021 sebagai data training dan nanti akan diprediksi untuk tanggal 1 juli 2021 sampai dengan 31 juli 2021 sebagai data uji dengan tahapan sebagai berikut 1. mempersiapk an data runtun w aktu yang akan dia nalisis 2. menganalisis data runtun waktu yang ada meng gunakan metode statistika arima 3. menganalisis data runtun waktu yang ada menggunakan metode quantum neural network 4. mengembangkan model hybrid arimaquantum neural network 5. menentukan mod el yang cocok untuk s etiap variabel 6. menguji kecocokan masingmasing model 7. melakuk an peramalan dengan menggunakan mo del yang cocok 8. melakuk an perbandingan tingkat aku rasi hasil peramalan dengan tiap model untuk mend apatkan model peramalan yang diharapkan sesuai dengan data runtun waktu yang ada maka perlu dilakukan pendekatan ilmiah yaitu dengan melihat pola d ata runtun waktu yang ada terlebih dahulu. dengan melihat pola data awal yang di miliki maka akan memud ahkan dalam memi lih model yang sesuai untuk data tersebut. pendekatan lainnya adalah me nggunakan too ls untuk m enentukan secara otomatis bentuk model statistik arima yang sesuai dengan runtun waktu yang ada lalu model tersebut dilatih menggunakan quantum neural network agar diket ahui polapola d ata yang sudah ada d an d apat d iuji akurasinya.17 tipe model pola tipikal acf pola tipikal pacf ar p menurun secara ekspon ensial sinusoidal terputus s etelah lag p ma q terputus s etelah lag q menurun secara ekspon ensial sinusoidal arma p q menurun secara ekspon ensial sinusoidal menurun secara ekspon ensial sinusoidal 3.2 model arima bentuk u mum model ar ima dapat dinyatakan dalam p ersamaan berikut ............................... ................................ ................................. 1 operator ar adalah ............................................................2 operator ma adalah ............................................................ ............. .3 1. autore gressive integrated moving average arima not asi model arima p d q p orde untuk pros es autoregressive ar d orde yang menyatakan banyaknya proses dife rensi d ilakuk an pada data time series yang tidak stasione r q orde yang menyatakan proses moving a verage ma. pola teoretis acf dan pacf dari proses yang stasio ner sumber aswi dan sukarna 2006 2. tahapan analisis time series arima a. membuat plot time series identifikasi asumsi s tasione ritas data runtun waktu. suatu de ret pengamatan dikat akan stasioner apabila proses tidak berubah seiring dengan perubahan waktu tidak stasioner dalam mean jika trend tidak datar tidak sejajar smbu waktu tidak stasioner dalam varian jika trend datar atau hampir datar tetapi data tersebar membangun pola m elebar atau m enyempit pola t eromp et18 tidak stasioner dalam mean varians j ika trend tidak datar dan data memb entuk po la terompet. augmented di ckey fuller uji formal untuk stasion eritas hipotesis h0 terdapat akar unit dan data tidak st asioner 0 h1 tid ak terdapat akar unit dan data stasioner 0 span taraf signifik ansi α statistik uji ............................................................... ...4 ............................................................................................... .5 ............................................................................................... .......... .6 kriteria uji h0 ditolak jika nilai mu tlak dari augmented di ckey fuller nilai kritis mackinnon atau nilai prob . α. b. menghitung membu at plot acf dan pacf mengidentifikasi model runtun w aktu yang mungkin mengestimasi p arameter model c. uji signifik ansi parameter hipotesis h0 danatau parameter tidak signifik an terhadap model h1 danatau parameter signifik an terhadap model taraf signifik ansi α statistik uji danatau 19 kriteria uji tolak h 0 jika atau p value alpha d. verifikasi mo del independensi residual hipotesis h0 tidak ada korelasi antarlag h1 paling sedikit ada satu dengan k12 24 36 48 ada ko relasi antarlag statistik uji kriteria uji tolak h 0 jika atau p value alpha dengan m l ag maksim um s jumlah p arameter yang diesti masi dan taraf signifik ans normalitas residual hipotesis h0 residual berdistriusi norm al h1 residual t idak berdistribusi norm al statistik uji fungsi peluang kumulatif r esidual distribusi ku mulatif yang diobs ervasi dari suatu sampel acak sebanyak n o servasi kriteria uji tolak h 0 jika atau p value alpha ukuran ketepatan ra malan mod el dengan uku ran ketepatan p eramalan yang baik ad alah model yang menghasilkan error yang kecil. nilai teng ah kesalahan kuadrat mean square er ror 20 berikut flo wchart langkahlangkah membu at model arima gambar 1. flowch art analisis runtun waktu arima 3.3 model neural network dalam buku jaringan syaraf tiruan dan pemrogramannya menggunakan matlab drs. semua sinyal yang diberi pengali bobo t ini kemudian dijumlahkan satu sama lain untuk menghasilkan unit aktivasi. keduanya memiliki range antara 1 sampai 1. untuk fungsi hyperbolic tangent 3.4 model hybrid arima neural network berdasarkan hasil peramalan model arima akan dilakukan proses analisis runtun waktu menggunakan metode jaringan syaraf tiruan. dengan kata lain output dari peramalan model arima akan menjadi input pada proses pengolahan data menggunakan metode jaringan syaraf tiruan. kemudian akan ditentukan model jaringan syaraf tiruan yang sesuai dan cocok untuk data runtun waktu tersebut. secara matemat is hasil ramalan secara keselu ruhan yang diperoleh adalah sebagai berikut zt merupakan hasil pe ramalan yang merupakan gabungan nilai ramalan dari model arima atau exponential smoothing dan nilai ramalan dari model jst. berikut ini adalah arsitektur model peramal an hybrid arima jst dan es jst gambar 5 model hybrid arima jst dan hybrid es jst 3.5 model quantum hybrid arima neural network ada b anyak pendekatan untuk pengembangan model quantum arima nn. untuk memud ahkan aplikasi fo rmulir berikut fungsi ko mpleks dib erikan untuk menyatakan keadaan kuantum ............................................................... .................. ................ .8 adalah bilangan imaginer adalah k uantum fase 3.6 pengukuran kinerja 3.6.1 mean squared error dalam statistik mean squared error mse sebuah estimator adalah nilai yang diharapkan dari kuadrat error . error yang ada menunjukkan seberapa besar perbedaan hasil estimasi dengan nilai yang akan diestimasi. perbedaan itu terjadi karena adanya keacakan pada data atau karena estimator tida k mengandung informasi yang dapat menghasilkan estimasi yang lebih akurat 3.6.2 komparasi hasil peramalan setelah nilai mean squared error dari kedua metode didapatkan maka akan dilakukan komparasi terhadap nilai mse yang didapatkan pada periode testing out sample jika nilai mse statistika mse ann maka metode statistika memiliki performa lebih baik dibandingkan metode ann karena memiliki tingkat kesalahan relatif lebih kecil. sebaliknya jika mse statistika mse ann maka metode statistika memilki perform a lebih buruk dibandingkan metode ann karena tingkat kesalahan yang dihasilkan relatif lebih besar.
Kualifikasi_I Komang Sugiartha.txt,"3.1 Tahapan Penelitian
Penelitian ini memiliki fokus pengembangan model dalam melakukan prediksi terjadinya stunting pada suatu wilayah berbasis Generative Adversarial Networks. Gambar 3.1 adalah tahapan penelitian dalam membangun model prediksi terjadinya stunting pada suatu wilayah berbasis Generative Adversarial Networks. 1. Faktor indikator penyebab stunting menjelaskan tentang faktor-faktor yang menyebabkan terjadinya stunting pada balita. 2. Pengembangan model pada penelitian ini dibagi menjadi beberapa tahapan, antara lain:
(a) Akusisi Data memperoleh informasi yang dibutuhkan dalam rangka mencapai tujuan penelitian. (b) Preprocessing merupakan tahap pemrosesan data dari langkah akusisi data. (c) Proses Pengembangan Model Prediksi Stunting berbasis Generative Adversarial Networks untuk memprediksi keadaan stunting pada suatu wilayah. 3. Pengujian adalah tahap terakhir dari rangkaian cara kerja machine learning. Yaitu perbandingan kinerja pemodelan yang telah divalidasi sebelumnya kemudian dengan data uji, lalu mengaplikasikan data train dengan menciptakan prediksi berdasarkan data baru. 3.2 Faktor Indikator Penyebab Stunting
Faktor-faktor yang mempengaruhi terjadinya stunting pada balita terdiri dari beberapa faktor, diantaranya:
Hasil penelitian yang dilakukan oleh Umiyah and Hamidiyah (2021)menunjukkan bahwa ada hubungan antara berat badan lahir dengan kejadian stunting dengan nilai Pvalue = 0,009 (P < 0,05). Sedangkan sebaliknya untuk usia (Pvalue 0,095), jenis kelamin (Pvalue 0, 512), dan panjang badan lahir (Pvalue 0,334) tidak ada hubungan dengan kejadian stunting. Hasil penelitian yang dilakukan oleh Apriluana and Fikawati (2018)menunjukkan faktor status gizi dengan berat badan lahir < 2.500 gram memiliki pengaruh secara bermakna terhadap kejadian stunting pada anak dan memiliki risiko mengalami stunting sebesar 3,82 kali. Faktor pendidikan ibu rendah memiliki pengaruh secara bermakna terhadap kejadian stunting pada anak dan memiliki risiko mengalami stunting sebanyak 1,67 kali. Faktor pendapatan rumah tangga yang rendah diidentifikasi sebagai predictor signifikan untuk stunting pada balita sebesar 2,1 kali. Faktor sanitasi yang tidak baik memiliki pengaruh yang signifikan terhadap kejadian stunting pada balita dan memiliki risiko mengalami stunting hingga sebesar 5,0 kali. Hasil penelitian yang dilakukan oleh Romadoniyah et al. (2022) menunjukkan faktor yang memepengaruhi stunting pada balita adalah berat badan lahir < 2.500 gram, MPASI, faktor pendidikan ibu rendah, pendapatan rumah tangga, faktor sanitasi yang tidak baik. Hasil penelitian yang dilakukan oleh Ariati (2019)menunjukkan prevalensi stunting sebesar 32,5 % dan balita Normal 67,5%. Analisis uji statistik menunjukkan adanya hubungan bermakna antara faktor prenatal (usia ibu saat hamil, status gizi ibu saat hamil), faktor pascanatal (ASI Eksklusif, riwayat imunisasi, penyakit infeksi), Karakteristik keluarga (pendidikan ibu, pekerjaan ayah dan status sosial ekonomi) dengan kejadian stunting. Hasil penelitian yang dilakukan oleh Al-Rahmad et al. (2013)diperoleh kejadian stunting pada balita disebabkan rendahnya pendapatan kelu- arga (p=0,026; OR=3,1), pemberian ASI tidak eksklusif (p=0,002; OR=4,2), pemberian MP-ASI kurang baik (p=0,007; OR=3,4), serta imunisasi tidak lengkap (p=0,040; OR=3,5).Dari hasil penelitian tersebut, penulis melakukan keterhubungan antara faktor-faktor penyebab stunting dengan indikator PIS-PK. Hubungan faktor stunting dengan indikator PIS-PK dapat dilihat pada Tabel 3.1. Dari hasil keterhubungan faktor penyebab stunting dengan indikator PIS-PK, indikator yang digunakan ada 4 indikator. Indikator tersebut diantaranya:
  1. Indikator 3 (Bayi mendapat imunisasi dasar lengkap)
  2. Indikator 4 (Bayi mendapat air susu ibu (ASI) eksklusif)
  3. Indikator 5 (Balita mendapatkan pematauan pertumbuhan)
  4. Indikator 11 (Keluarga mempunyai akses sarana air bersih)

3.3 Pengembangan Model
Beberapa tahap yang dilakukan dalam pengembangan model stunting diantaranya akuisisi data, preprocessing dan proses pengembangan model. Gambar menunjukkan proses pengembangan model yang dilakukan dalam penelitian. 1. Akusisi Data memperoleh informasi yang dibutuhkan dalam rangka mencapai tujuan penelitian. 2. Preprocessing merupakan tahap pemrosesan data dari langkah akusisi data. 3. Proses Pengembangan Model Prediksi Stunting berbasis Generative Adversarial Networks untuk memprediksi keadaan stunting pada suatu wilayah. 3.3.1 Akuisisi Data
Tahap pertama dari pengembangan model terjadinya stunting adalah akuisisi data seperti pada Gambar 3.3. Akuisisi data adalah tahap pengumpulan data yang dibutuhkan dalam penelitian. Dalam penelitian ini data dikumpulkan dari data stunting dan data PIS-PK. Pada Gambar 3.3 merupakan tahapan yang dilakukan dalam melakukan akusisi data stunting dan data indiktor PIS-PK. 3.3.2 Preprocessing
Tahap preprocessing berfungsi untuk membersihkan data sehingga dapat menghindari penggunaan data yang bermasalah dan tidak konsisten (Surjandari et al., 2018). Proses ini digambarkan pada Gambar 3.4. Tahap preprocessing yang digunakan penelitian ini meliputi tahap pembersihan data, transformasi data dan data reduction. Tahapan tersebut digambarkan pada Gambar 3.5. Cleaning. Untuk membuat data layak digunakan untuk penelitian, dilakukan tahapan pembersihan data (cleaning). Transformation. Untuk mengubah data dalam bentuk yang sesuai dalam proses penelitian.Reduction. Untuk meningkatkan efisiensi penyimpanan serta analisis data. 3.3.3 Proses Pengembangan Model Prediksi Stunting
Pada tahap ini menjelaskan metode yang diusulkan menggunakan pendekatan kerangka Generative Adversarial Networks (GAN) untuk prediksi terjadi- nya stunting pada suatu wilayah. GAN diperkenalkan oleh Goodfellow et al. (2014) sebagai kerangka model deep learning untuk menangkap distribusi data pelatihan dengan menghasilkan data baru dari distribusi yang sama menggunakan model generator dan diskriminator. Arsitektur GAN mempe- lajari fitur tanpa pengawasan dengan proses pembelajaran yang kompetitif. GAN akan menghasilkan lebih banyak ruang fitur yang dapat dimanfaatkan, sehingga mengurangi potensi kelebihan fitur selama pelatihan. Berikut ini adalah gambaran arsitektur model GAN yang diusulkan pada Gambar 3.6. Gambar 3.6 Metode arsitektur yang diusulkan Model G (Generator) dilatih untuk menghasilkan data yang terlihat seperti data persebaran data indeks keluarga sehat di setiap wilayah, sedangkan model D (Discriminator) dilatih untuk membedakan antara data dari Generator dan data nyata. Kesalahan dari D digunakan untuk melatih G untuk mengalahkan D. Persaingan antara G dan D memaksa D untuk membedakan secara acak dari variabilitas nyata, secara formal GAN menyelesaikan permainan min-max dengan persamaan berikut:
   Diskriminator mengeluarkan nilai D(x) yang menunjukkan kemungkinan bahwa x adalah data nyata dengan tujuan memaksimalkan peluang untuk mengenali data nyata sebagai data nyata yang dihasilkan sebagai data palsu. Untuk mengukur kerugian model GAN menggunakan cross-entropy, p log (q), untuk data real p = 1. Di sisi generator, fungsi tujuan menggunakan model untuk menghasilkan D setinggi mungkin (x) nilai untuk membalikkan perbedaan. Seperti yang dijelaskan dalam makalah Goodfellow et al. (2014), D dan G memainkan permainan min-max di mana D mencoba memaksimalkan pro- babilitas dengan benar mengklasifikasikan real dan false (logD (x)), dan G mencoba meminimalkan probabilitas bahwa D akan memprediksi output yang salah ( log (1 - D (G (x))))). Dimana Pdata adalah distribusi data pelatihan nyata dari ke x, dan Pz adalah distribusi z vektor noise yang diambil. G adalah pemetaan dari z ke ruang x, sedangkan D memetakan input x ke nilai skalar yang mewakili x probabilistik menjadi sampel nyata. Generative Adversarial Networks (GAN) asli yang diusulkan oleh Goodfellow et al. (2014) menampilkan sebuah generator dan diskriminator; generator G dilatih untuk menghasilkan sampel palsu yang dapat menipu pembeda D, sedangkan yang terakhir dilatih untuk membedakan antara sampel asli dan palsu. Seperti yang ditunjukkan pada Gambar 3.7, generator menggu- nakan data stunting untuk membuat sampel palsu. Diskriminator memban- dingkan sampel asli dengan palsu sampel. Akhirnya kerugian diskriminator dihitung dan digunakan untuk memperbaharui diskriminator dan generator. Diberikan noise dari noise z dari data stunting p(z) , dan a sampel x dari distribusi data nyata pdata(x). Diskriminator keluarannya adalah D(x) untuk sampel asli dan DG(x) untuk sampel palsu. Diskriminator secara bersamaan berusaha meningkatkan kemampuannya untuk mengenali sampel nyata dengan memaksimalkan logD(x) ke 1, dan sampel palsu dengan memaksimalkan logD(1 - D(G(z))) ke 0. Generator menghindari pembangkitan sampel yang mudah dikenali oleh diskriminator dengan meminimalkan log (1 - D(G(z))). Permainan min-max yang dimainkan antara generator dan diskriminator diwakili pada (Persamaan (3.3)). Proses pelatihan GAN pertama-tama akan menghitung kerugian dan perbarui diskriminator (Persamaan (3.4)). Kemudian hitung dan perbarui generator (Persamaan (3.5)). Diskriminator diperbarui dengan menaikkan gradien stokastiknya V(c)d, sedangkan generator diperbarui dengan menurunkan gradien stokastiknya Veg. salah ukuran batch yang digunakan untuk pelatihan ini adalah sampelnya indeks. 3.4 Pengujian
Tujuan dari penelitian ini adalah membangun model prediksi berbasis Generative Adversarial Networks untuk memprediksi terjadinya stunting pada suatu wilayah yang dibutuhkan untuk mencegah terjadinya stunting dan mengoptimalkan perencanaan program pemerintah dalam penurunan terjadinya stunting. Data dipisahkan menjadi dua bagian untuk model pelatihan dan pengujian. Data yang digunakan adalah data dari tahun 2020-2022 untuk data stunting dan rentang waktu tahun 2019-2021 untuk data indikator PIS-PK.","3.1 Tahapan Penelitian
Penelitian ini memiliki fokus pengembangan model dalam melakukan prediksi terjadinya stunting pada suatu wilayah berbasis Generative Adversarial Networks. Gambar 3.1 adalah tahapan penelitian dalam membangun model prediksi terjadinya stunting pada suatu wilayah berbasis Generative Adversarial Networks. Pengembangan model pada penelitian ini dibagi menjadi beberapa tahapan, antara lain:
(a) Akusisi Data memperoleh informasi yang dibutuhkan dalam rangka mencapai tujuan penelitian. (c) Proses Pengembangan Model Prediksi Stunting berbasis Generative Adversarial Networks untuk memprediksi keadaan stunting pada suatu wilayah. Yaitu perbandingan kinerja pemodelan yang telah divalidasi sebelumnya kemudian dengan data uji, lalu mengaplikasikan data train dengan menciptakan prediksi berdasarkan data baru. 3.2 Faktor Indikator Penyebab Stunting
Faktor-faktor yang mempengaruhi terjadinya stunting pada balita terdiri dari beberapa faktor, diantaranya:
Hasil penelitian yang dilakukan oleh Umiyah and Hamidiyah (2021)menunjukkan bahwa ada hubungan antara berat badan lahir dengan kejadian stunting dengan nilai Pvalue = 0,009 (P < 0,05). Hasil penelitian yang dilakukan oleh Apriluana and Fikawati (2018)menunjukkan faktor status gizi dengan berat badan lahir < 2.500 gram memiliki pengaruh secara bermakna terhadap kejadian stunting pada anak dan memiliki risiko mengalami stunting sebesar 3,82 kali. Hasil penelitian yang dilakukan oleh Romadoniyah et al. Hasil penelitian yang dilakukan oleh Ariati (2019)menunjukkan prevalensi stunting sebesar 32,5 % dan balita Normal 67,5%. Hasil penelitian yang dilakukan oleh Al-Rahmad et al. (2013)diperoleh kejadian stunting pada balita disebabkan rendahnya pendapatan kelu- arga (p=0,026; OR=3,1), pemberian ASI tidak eksklusif (p=0,002; OR=4,2), pemberian MP-ASI kurang baik (p=0,007; OR=3,4), serta imunisasi tidak lengkap (p=0,040; OR=3,5).Dari hasil penelitian tersebut, penulis melakukan keterhubungan antara faktor-faktor penyebab stunting dengan indikator PIS-PK. Dari hasil keterhubungan faktor penyebab stunting dengan indikator PIS-PK, indikator yang digunakan ada 4 indikator. Indikator 11 (Keluarga mempunyai akses sarana air bersih)

3.3 Pengembangan Model
Beberapa tahap yang dilakukan dalam pengembangan model stunting diantaranya akuisisi data, preprocessing dan proses pengembangan model. Gambar menunjukkan proses pengembangan model yang dilakukan dalam penelitian. Proses Pengembangan Model Prediksi Stunting berbasis Generative Adversarial Networks untuk memprediksi keadaan stunting pada suatu wilayah. 3.3.3 Proses Pengembangan Model Prediksi Stunting
Pada tahap ini menjelaskan metode yang diusulkan menggunakan pendekatan kerangka Generative Adversarial Networks (GAN) untuk prediksi terjadi- nya stunting pada suatu wilayah. (2014) sebagai kerangka model deep learning untuk menangkap distribusi data pelatihan dengan menghasilkan data baru dari distribusi yang sama menggunakan model generator dan diskriminator. GAN akan menghasilkan lebih banyak ruang fitur yang dapat dimanfaatkan, sehingga mengurangi potensi kelebihan fitur selama pelatihan. Gambar 3.6 Metode arsitektur yang diusulkan Model G (Generator) dilatih untuk menghasilkan data yang terlihat seperti data persebaran data indeks keluarga sehat di setiap wilayah, sedangkan model D (Discriminator) dilatih untuk membedakan antara data dari Generator dan data nyata. Kesalahan dari D digunakan untuk melatih G untuk mengalahkan D. Persaingan antara G dan D memaksa D untuk membedakan secara acak dari variabilitas nyata, secara formal GAN menyelesaikan permainan min-max dengan persamaan berikut:
   Diskriminator mengeluarkan nilai D(x) yang menunjukkan kemungkinan bahwa x adalah data nyata dengan tujuan memaksimalkan peluang untuk mengenali data nyata sebagai data nyata yang dihasilkan sebagai data palsu. Di sisi generator, fungsi tujuan menggunakan model untuk menghasilkan D setinggi mungkin (x) nilai untuk membalikkan perbedaan. (2014) menampilkan sebuah generator dan diskriminator; generator G dilatih untuk menghasilkan sampel palsu yang dapat menipu pembeda D, sedangkan yang terakhir dilatih untuk membedakan antara sampel asli dan palsu. Diskriminator secara bersamaan berusaha meningkatkan kemampuannya untuk mengenali sampel nyata dengan memaksimalkan logD(x) ke 1, dan sampel palsu dengan memaksimalkan logD(1 - D(G(z))) ke 0. 3.4 Pengujian
Tujuan dari penelitian ini adalah membangun model prediksi berbasis Generative Adversarial Networks untuk memprediksi terjadinya stunting pada suatu wilayah yang dibutuhkan untuk mencegah terjadinya stunting dan mengoptimalkan perencanaan program pemerintah dalam penurunan terjadinya stunting."
Kualifikasi_Nur Azizah.txt,"3.1. Tahapan Penelitian
Penelitian ini berusaha mengembangkan model diagnosis Smear Negative Pulmonary Tuberculosis dengan metode Deep Learning menggunakan algoritma Faster R-CNN sebagai solusi dari masalah dan kekurangan dari teknik yang pernah dilakukan peneliti terdahulu yang dapat menghasilkan sebuah model diagnosis awal dan diagnosis akhir bagi pasien TB Negatif dengan menghasilkan tingkat akurasi tinggi. Rencana penelitian mencoba menggabungkan untuk mengembangkan deep learning dan mengembangkan metode yang dapat mengoptimalkan tingkat akurasi diagnosis. 3.1.1. Pengumpulan Dataset
Data yang yang dipakai dalam studi ini diambil dari dua sumber terbuka terdiri dari dataTuberculosis melalui laman https://www.kaggle.com/tawsifurrahman/tuberculosis-tb-chest-xray-dataset dan data normal yang dapat diakses melalui https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia Dari dua sumber ini, kami kemudian memformulasikan dataset gabungan dengan dua kelas yaitu Tuberculosis dan Normal yang masing-masing memiliki jumlah data latih 48 citra dan data validasi masing-masing 15 citra. Sedangkan untuk data uji, kelas Tuberculosis sebanyak 42 citra dan kelas normal sebanyak 234 citra. 3.1.2. Sistem yang dibagun
Setelah mengumpulkan dataset, langkah selanjutnya adalah membangun sistem untuk pelatihan dan mengevaluasi jaringan yang dibuat. Pada penelitian ini, kami menggunakan metode transfer learning, yang melakukan proses latih dengan menggunakan model yang sudah terlebih dahulu dilatih menggunakan dataset lain. Untuk kasus ini, kami menggunakan model ResNet yang sudah terlebih dahulu dilatih menggunakan ImagetNet dataset (Deng, dkk, 2009). Diagram alir dari sistem yang kami bangun dapat dilihat di Gambar 3. Berikut adalah langkah-langkah yang dilakukan dalam penelitian ini:

3.1.3. Evaluasi
Metriks yang digunakan untuk mengevaluasi model adalah akurasi, precision, recall dan F1-score. Untuk memahami metriks yang digunakan, sebelumnya akan didefinisikan terlebih dahulu true positive (TP), false positive (FP), false negative (FN) dan true negative (TN) seperti yang ditunjukkan pada confusion matrix Tabel 1. TP didefiniskan sebagai data positif yang diprediksi sebagai positif dan TN didefinisikan sebagai data negatif yang di prediksi sebagai negatif. Sedangkan FN merupakan kebalikan dari TP yaitu data positif yang diprediksi sebagai negative dan FP, kebalikan dari TN, yaitu data negatif yang diprediksi positif. Akurasi didefinisikan sebagai perbandingan jumlah data yang diprediksi secara benar terhadap total jumlah data. Precision menggambarkan perbandingan tp terhadap total data yang diprediksi positif. Recall didefinisikan sebagai perbandingan tp terhadap total data positif. Sedangkan F1 score adalah rataan harmonic antara precision dan recall. Persamaan 2, 3, 4, dan 5 merupakan rumus untuk precision, recall, Fl-score, dan akurasi.","3.1. Tahapan Penelitian
Penelitian ini berusaha mengembangkan model diagnosis Smear Negative Pulmonary Tuberculosis dengan metode Deep Learning menggunakan algoritma Faster R-CNN sebagai solusi dari masalah dan kekurangan dari teknik yang pernah dilakukan peneliti terdahulu yang dapat menghasilkan sebuah model diagnosis awal dan diagnosis akhir bagi pasien TB Negatif dengan menghasilkan tingkat akurasi tinggi. Rencana penelitian mencoba menggabungkan untuk mengembangkan deep learning dan mengembangkan metode yang dapat mengoptimalkan tingkat akurasi diagnosis. Sistem yang dibagun
Setelah mengumpulkan dataset, langkah selanjutnya adalah membangun sistem untuk pelatihan dan mengevaluasi jaringan yang dibuat. Sedangkan F1 score adalah rataan harmonic antara precision dan recall. Persamaan 2, 3, 4, dan 5 merupakan rumus untuk precision, recall, Fl-score, dan akurasi."
Miftakhul Zaen_KUALIFIKASI.txt,3.1 tahapan penel itian dalam penelitian mengenai pengembangan algoritma dbscan dengan kuantum terdapat langkahlangkah yang dilakukan seperti pada gambar 3.1. langkah langkah yang dilaukan d iantaranya yaitu pengumpulan data definisi qubits kriteria inisialis asi sistem kuantum hingga evaluasi klaster. data definisi qubits kriteria inisialisasi sistem kuantum penentuan eps dan minpts kuantum identifikasi core supplier dengan kuantum sirkuitidentifikasi noise supplier dengan kuantum sirkuit penanganan noise dengan kuantum stateformasi klaster supplier dengan kuantum measurementimplementasi quantum distance measure identifikasi core supplier dengan kuantum sirkuit evaluasi klaster1 2 3 4 5 6 9 10 117 8 gamb ar 3.1 tahapan penel itian 1. data tahap awal dalam penelitian di awali dengan pembuatan data dimana data yang digunakan pada penel itian ini adalah data s intetik. data sintetik digunakan untuk mendapatkan jumlah data yang besar sela in itu data sintetik juga b ersifat fleksibel kar ena ju mlah data yang digunakan dapat ditentukan sesuai dengan kebutuhan pengujian algo ritma yang dikembang kan. data sintetik yang dibuat berisikan nama supplier harga kualitas dan waktu pengiriman. 2. definis i qubits kriteria pada taha p ini kriteria yang digunak an untuk pengelompokan supplier diubah menjadi representasi kuantum menggunakan qubits. setia p kriteria mungkin diwakili ol eh satu atau lebih qubits tergantung pada kompleksitas yang diperlukan. kriteri a yang digunakan dalam peng elompokan supplier yaitu harga kualitas dan waktu pengiri man. 3. inisialisasi sistem kuantum pada tahapan ini melakukan p ersiapan awal dari komputer k uantum yaitu mengatur qubits ke state awal dan memas tikan semua qubits berada dalam keadaan awal sebelum operasi kuantum dijalankan. pada tahapan ini juga menentuk kan jumlah qubits yan g digunakan. 4. implementasi quantum distance measure pada tahapan ini melakukan p enerapan metode untuk mengukur jarak antar supplier dalam ruang kuantum dengan menggunakan prins ipprinsip mekanika kuantum . tahapan ini digunakan dalam proses pengelom pokkan data menggunakan quantum dbscan karena jarak antar supplier akan digunakan untuk menentukan klaster 5. penentuan eps dan minpts kuan tum pada tahap ini men entukan nilai nilai epsilo n atau eps dan minimum poi nts minpts dalam konteks kuantum untuk menentukan batas batas klaster . epsilon atau eps digunakan u ntuk menen tukan radius yang menentukan lingkungan di sekitar setiap titik data. dua titik dianggap ber tetangga jika jarak antara mereka kurang dari nilai e ps. minimum points atau min pts untuk menentukan jumlah minimum titik yang diperlukan untuk membentuk sebuah klaster . 6. identi fikasi core supplier dengan kuantum sirkuit pada tahapan ini m enggunakan rangka ian kuantum untuk mengident ifikasi supplier ini core suppl ier. supplier inti adalah supplier yang memiliki cukup banyak tetangga yang s esuai dengan minpts dalam radius epsilon yang telah ditentukan. 7. identifikasi noise supplier dengan kuantum sirkuit pada tahapan ini mengi dentifikasi supplier noise atau outlier yang mem iliki jarak tidak cukup dek at atau memiliki jarak yang jauh dengan supplier lain untuk dianggap bagian dari klaster . 8. penanganan noise dengan quantum state pada tahapan ini menge lola supplier noise yang telah diidentifikasi menggunakan teknik kuantu m untuk memisahkan atau mengelompok kan noise secara terpisah. dalam dbscan klasik noise adalah titik data yang tidak termasuk dalam klaster apa pun. titik titik ini tidak memiliki cuku p tetangga dalam radius epsilon eps atau tidak terhubung ke core poin t. 9. identifikasi core supplier dengan quantum circuit pada tahapan ini mengidentifikasi titik titik data yang berada dalam jarak epsilon atau e ps dari titik inti tetapi tidak memiliki cukup tetanga untuk masuk ke dalam klaster dengan menggunakan kuantum sirkuit . 10. formasi kluster supplier dengan quantum measurement pada tahapan ini m embentukan klaster supplier dengan mengukur state kuantum yang telah diubah melalui interaksi antar qubits yang mewakili supplier . 11. evaluasi kluster tahap terakhir di mana kualit as dan k eefektifan kluster yang ter bentuk dievaluasi. tahapan ini bertujuan untuk menilai seberapa baik kluster yang terbentuk mengguna kan. 3.2 rangkuman langkah langk ah penelitian setelah mengembangkan algoritma kuantum dbscan selanjutnya membandingk annya dengan algo ritma dbscan untuk mengetahui seberapa baik algoritma dbscan jika dibandingkan dengan algorit ma klasiknya . langkah langka h tersebut dapat dilihat pada gambar 3.2 rangkuman langkah langka h prosedur peneli tian. data definisi qubits kriteria inisialisasi sistem kuantum penentuan eps dan minpts kuantum identifikasi core supplier dengan kuantum sirkuitidentifikasi noise supplier dengan kuantum sirkuit penanganan noise dengan kuantum stateformasi klaster supplier dengan kuantum measurementimplementasi quantum distance measure identifikasi core supplier dengan kuantum sirkuitnormalisasi data penentuan epsilon dan minpts hitung jarak antar supplier identifikasi core supplier identifikasi core supplieridentifikasi noise supplier supplier tidak termasuk dalam klasterformasi klaster supplier evaluasi klasterusulan algoritma gambar 3.2 rangkuman langkah langkah prosedur penelitian,3.1 tahapan penel itian dalam penelitian mengenai pengembangan algoritma dbscan dengan kuantum terdapat langkahlangkah yang dilakukan seperti pada gambar 3.1. langkah langkah yang dilaukan d iantaranya yaitu pengumpulan data definisi qubits kriteria inisialis asi sistem kuantum hingga evaluasi klaster. data definisi qubits kriteria inisialisasi sistem kuantum penentuan eps dan minpts kuantum identifikasi core supplier dengan kuantum sirkuitidentifikasi noise supplier dengan kuantum sirkuit penanganan noise dengan kuantum stateformasi klaster supplier dengan kuantum measurementimplementasi quantum distance measure identifikasi core supplier dengan kuantum sirkuit evaluasi klaster1 2 3 4 5 6 9 10 117 8 gamb ar 3.1 tahapan penel itian 1. data tahap awal dalam penelitian di awali dengan pembuatan data dimana data yang digunakan pada penel itian ini adalah data s intetik. data sintetik digunakan untuk mendapatkan jumlah data yang besar sela in itu data sintetik juga b ersifat fleksibel kar ena ju mlah data yang digunakan dapat ditentukan sesuai dengan kebutuhan pengujian algo ritma yang dikembang kan. data sintetik yang dibuat berisikan nama supplier harga kualitas dan waktu pengiriman. 2. definis i qubits kriteria pada taha p ini kriteria yang digunak an untuk pengelompokan supplier diubah menjadi representasi kuantum menggunakan qubits. 10. formasi kluster supplier dengan quantum measurement pada tahapan ini m embentukan klaster supplier dengan mengukur state kuantum yang telah diubah melalui interaksi antar qubits yang mewakili supplier . tahapan ini bertujuan untuk menilai seberapa baik kluster yang terbentuk mengguna kan. 3.2 rangkuman langkah langk ah penelitian setelah mengembangkan algoritma kuantum dbscan selanjutnya membandingk annya dengan algo ritma dbscan untuk mengetahui seberapa baik algoritma dbscan jika dibandingkan dengan algorit ma klasiknya . data definisi qubits kriteria inisialisasi sistem kuantum penentuan eps dan minpts kuantum identifikasi core supplier dengan kuantum sirkuitidentifikasi noise supplier dengan kuantum sirkuit penanganan noise dengan kuantum stateformasi klaster supplier dengan kuantum measurementimplementasi quantum distance measure identifikasi core supplier dengan kuantum sirkuitnormalisasi data penentuan epsilon dan minpts hitung jarak antar supplier identifikasi core supplier identifikasi core supplieridentifikasi noise supplier supplier tidak termasuk dalam klasterformasi klaster supplier evaluasi klasterusulan algoritma gambar 3.2 rangkuman langkah langkah prosedur penelitian
Octaviani Hutapea_UK.txt,"3.1 Tahapan Penelitian
      Penelitian ini berusahan mengembangkan sistem Identifikasi Jenis dan Tingkat Kerusakan Jalan Serta Sebarannya Menggunakan Model Convolutional Neural Network dengan tahapan penelitian awal adalah analisis kebutuhan dari Direktorat Jenderal Bina Marga mengenai jenis kerusakan jalan serta tingkat kerusakannya. Akuisisi dan Analisis data dalam pengambilan citra kerusakan perkerasan jalan dilakukan mengikuti prosedur pedoman survei yang dikeluarkan Direktorat Jenderal Bina Marga. Pembentukan data set dilakukan dengan melabeli setiap citra yang dilakukan oleh pakar berdasarkan jenis dan tingkat kerusakan. Setelahnya dilakukan augmentasi data terhadap data set citra untuk mengurangi overfitting dengan cara meningkatkan dataset. Pembentukan model identifikasi jenis kerusakan dan tingkat kerusakan dibuat menggunakan model CNN dengan data latih yang diambil 90% dari dataset yang sudah dibuat. Pengujian dan validasi sistem dilakukan dengan memanfaatkan matrik konfusi untuk menghitung akurasi dari model yang sudah dibuat. Penentuan Koordinat Sebaran Kerusakan Pada Jalan akan dilakukan dengan penetapan titik koordinat setiap kerusakan pada peta lokasi. Berikut merupakan bagan tahapan penilitian ditunjukan pada gambar. 3.2 Analisis kebutuhan
      Jenis kerusakan yang dilakukan pencatatanya pada tabel komponen perkerasan berdasarkan pedoman survei pengumpulan data kondisi jaringan jalan yang dikeluarkan oleh Direktorat Jenderal Bina Marga sebagai berikut:
1) Retak Permukaan:
a) Retak Kulit Buaya
b) Retak Tepi
c) Retak Refleksi Sambungan
d) Retak Selip
2) Lubang
3) Alur
      Berdasarkan Indeks Kondisi Perkerasan tingkat kerusakan perkerasan jalan dalam Pedoman Bahan Konstruksi Bangunan Dan Rekayasa Sipil dibagi menjadi tiga tingkatan yaitu Rendah, Sedang, dan Tinggi untuk setiap masing-masing jenis kerusakan dijelaskan dalam tabel 3.1 (Nono & Hamdani, 2016). 3.3 Akuisisi dan Analisis Data
      Akuisisi data citra kerusakan dilakukan dengan menggunakan gambar video atau gambar digital yang berkoordinat, berikut merupakan beberapa syarat yang harus dipenuhi dalam proses akuisisi data citra berdasarkan pedoman survei pengumpulan data kondisi jaringan jalan yang dikeluarkan oleh Direktorat Jenderal Bina Marga:
1) Kamera yang digunakan harus dapat menghasilkan gambar digital dengan resolusi kamera minimum 1280*1920 pixel (setara dengan full HD video). 2) Gambar diambil dari kamera yang menghadap ke depan dengan sudut pandang minimum 120� dari garis depan kendaraan. 3) Interval pengambilan gambar maksimal 10 meter. 4) Setiap gambar yang diambil harus memiliki data koordinat yang dicatat secara menerus dengan GPS yang mempunyai ketelitian � 5 meter. 5) Gambar harus dapat ditampilkan dengan perangkat lunak video yang umum dioperasikan pada sistem operasi Windows. 6) Data kerusakan jalan dapat ditetapkan lokasinya dari gambar, dengan ketelitian 0,1 meter untuk ukuran dimensi dan jarak, dan � 10 meter untuk lokasi. 7) Penentuan unit-unit sampel. Semua data gambar harus memiliki koordinat berdasarkan pengukuran GPS, dan harus memenuhi beberapa persyaratan berikut:
1) Survei hanya dilakukan pada saat cuaca cerah, dan permukaan jalan kering. 2) Gambar harus jelas dan tidak terganggu karena adanya debu, butir air, serangga atau benda lainnya pada lensa kamera. 3) Ketika merekam data, kendaraan survei tidak boleh berialan menghadap sinar matahari. 4) Bayangan yang tampak pada gambar tidak boleh mengurangi mutu data gambar. Analisis Data dilakukan dengan menganalisis hasil dari gambar yang diambil saat melakukan survei kondisi perkerasan. Survei dengan metoda ini lebih disarankan dibandingkan dengan metoda survei dengan penilaian langsung di lapangan, karena:
1) Keselamatan
Kendaraan survei dijalankan dengan kecepatan normal, tidak memerlukan manajemen lalu lintas, dan petugas survei tidak terpapar langsung dengan lalu- lintas maupun cuaca. 2) Cepat
Perekaman data gambar di lapangan umumnya dapat mencapai 100 lajur km per hari dan penilaian per operator umumnya sekitar 30 km per hari. 3) Sumberdaya
Petugas penilai dapat ditugaskan lebih banyak untuk menilai kondisi dari gambar yang direkam. 4) Dapat diperiksa
Gambar-gambar yang digunakan, dan penilaian kondisi dapat diperiksa (diaudit) setiap saat untuk memeriksa konsistensi antar petugas penilai dan mutu penilaian. 3.4 Pembentukan Dataset
      Pembentukan dataset diawali dengan pelabelan data oleh pakar ke dalam tiga jenis yang akan diidentifikasi berdasarkan tingkat kerusakannya. Berikut merupakan alur dari pembentukan dataset. Berikut merupakan contoh pelabelan data citra berdasarkan jenis kerusakan (Nono & Hamdani, 2016) dalam tabel 3.2. 3.5 Pembentukan Model Klasfikasi
     Model klasifikasi dibentuk dengan menggunakan arsitektur CNN dengan data set yang telah terbentuk dari subbab 3.4 maka akan dibagi ke dalam dua bagian yaitu Data Latih dan Data Uji. Presentase pembagian dataset yang akan dilakukan baik untuk data latih maupun data uji tertera pada tabel. Alur pemodelan secara umum sampai dengan prediksi model diagram di bawah ini. 3.6 Pengujian Model
      Pengujian dilakukan dengan cara menguji satu per satu citra data uji sesuai dengan proses klasifikasi yang telah dijelaskan pada sub bab 3.4. Pengujian model dilakukan dengan memanfaatkan matrik konfusi untuk menghitung akurasi, presisi, recall, dan F1-Score seperti teori yang sudah dijelaskan pada subbab 2.5.6 dari model yang sudah dibuat. 3.7 Pengukuran Tingkat Kerusakan
      Perkerasan pada ruas yang telah dipilih dibagi menjadi beberapa unit perkerasan. Apabila perkerasan pada ruas tidak seragam, maka ruas perlu terlebih dulu dibagi menjadi seksi-seksi yang seragam dan kemudian tiap seksi dibagi menjadi unit-unit perkerasan. Selanjutnya, dari unit-unit perkerasan dipilih beberapa unit sampel yang akan disurvei. Survei dilakukan secara visual dan data yang dinilai dan dicatat pada saat suvei tiap unit sampel adalah jenis, tingkat keparahan, dan kuantitas kerusakan perkerasan. Peralatan yang digunakan untuk survei manual kondisi perkerasan adalah sebagai berikut:
1. Formulir survei atau alat yang dapat merekam sekurang-kurangnya informasi sebagai berikut: tanggal, lokasi, ruas, seksi, ukuran unit sampel, jumlah dan ukuran panel, jenis, tingkat keparahan, dan kuantitas kerusakan, dan nama-nama petugas survei. 2. Meteran roda yang dapat mengukur jarak dengan ketelitian 30 mm (0,1 feet) terdekat. 3. Mistar atau benang (untuk perkerasan beton aspal), 3 m (10 feet). 4. Mistar berskala 300 mm (12 in) yang dapat membaca jarak sampai 3 mm (1/8 in) atau lebih teliti. 5. Peta jaringan untuk jaringan jalan yang akan disurvei. 6. Perambuan sesuai Pd T-12-2003. Berdasarkan Indeks Kondisi Perkerasan cara pengukuran tingkat kerusakan perkerasan jalan dalam Pedoman Bahan Konstruksi Bangunan Dan Rekayasa Sipil Penilaian kondisi perkerasan dilakukan untuk setiap lajur jalan, dengan arah pengukuran 2 (dua) arah. Metoda penilaian kondisi perkerasan secara manual ini mencakup perekaman data gambar berkoordinat dan penilaian kondisi perkerasan dari gambar. Penilaian kondisi dapat dilakukan di kantor (Nono & Hamdani, 2016). Penentuan IKP ruas perkerasan beton aspal ditentukan berdasarkan IKP unit- unit sampel, maka untuk mendapatkan IKP ruas perlu terlebih dulu ditentukan IKP tiap unit sampel dan tiap unit khusus (bila ada). Tahapan penentuan IKP dapat dikelompokan menjadi empat tahap yang telah dijelaskan pada teori di subbab 2.6. yang ditunjukan pada gambar. 3.8 Penentuan Koordinat Sebaran Kerusakan Pada Jalan
      Penentuan Koordinat Sebaran Kerusakan Pada Jalan akan dilakukan dengan penetapan titik koordinat setiap kerusakan pada peta lokasi. Digambarkan dalam diagram alur di bawah ini. 3.9 Analisis dan Evaluasi Hasil
      Hasil pengukuran yang dihasilkan dari citra jenis kerusakan jalan perkerasan akan dianalisis dan evaluasi dengan pengolahan data secara manual yang dikerjakan oleh binamarga serta dihitung kembali akurasi dari pengukuran tingkat kerusakan tersebut.","3.1 Tahapan Penelitian
      Penelitian ini berusahan mengembangkan sistem Identifikasi Jenis dan Tingkat Kerusakan Jalan Serta Sebarannya Menggunakan Model Convolutional Neural Network dengan tahapan penelitian awal adalah analisis kebutuhan dari Direktorat Jenderal Bina Marga mengenai jenis kerusakan jalan serta tingkat kerusakannya. Akuisisi dan Analisis data dalam pengambilan citra kerusakan perkerasan jalan dilakukan mengikuti prosedur pedoman survei yang dikeluarkan Direktorat Jenderal Bina Marga. Pembentukan data set dilakukan dengan melabeli setiap citra yang dilakukan oleh pakar berdasarkan jenis dan tingkat kerusakan. Setelahnya dilakukan augmentasi data terhadap data set citra untuk mengurangi overfitting dengan cara meningkatkan dataset. Pembentukan model identifikasi jenis kerusakan dan tingkat kerusakan dibuat menggunakan model CNN dengan data latih yang diambil 90% dari dataset yang sudah dibuat. Pengujian dan validasi sistem dilakukan dengan memanfaatkan matrik konfusi untuk menghitung akurasi dari model yang sudah dibuat. Penentuan Koordinat Sebaran Kerusakan Pada Jalan akan dilakukan dengan penetapan titik koordinat setiap kerusakan pada peta lokasi. 3.2 Analisis kebutuhan
      Jenis kerusakan yang dilakukan pencatatanya pada tabel komponen perkerasan berdasarkan pedoman survei pengumpulan data kondisi jaringan jalan yang dikeluarkan oleh Direktorat Jenderal Bina Marga sebagai berikut:
1) Retak Permukaan:
a) Retak Kulit Buaya
b) Retak Tepi
c) Retak Refleksi Sambungan
d) Retak Selip
2) Lubang
3) Alur
      Berdasarkan Indeks Kondisi Perkerasan tingkat kerusakan perkerasan jalan dalam Pedoman Bahan Konstruksi Bangunan Dan Rekayasa Sipil dibagi menjadi tiga tingkatan yaitu Rendah, Sedang, dan Tinggi untuk setiap masing-masing jenis kerusakan dijelaskan dalam tabel 3.1 (Nono & Hamdani, 2016). 3.3 Akuisisi dan Analisis Data
      Akuisisi data citra kerusakan dilakukan dengan menggunakan gambar video atau gambar digital yang berkoordinat, berikut merupakan beberapa syarat yang harus dipenuhi dalam proses akuisisi data citra berdasarkan pedoman survei pengumpulan data kondisi jaringan jalan yang dikeluarkan oleh Direktorat Jenderal Bina Marga:
1) Kamera yang digunakan harus dapat menghasilkan gambar digital dengan resolusi kamera minimum 1280*1920 pixel (setara dengan full HD video). 3.4 Pembentukan Dataset
      Pembentukan dataset diawali dengan pelabelan data oleh pakar ke dalam tiga jenis yang akan diidentifikasi berdasarkan tingkat kerusakannya. 3.7 Pengukuran Tingkat Kerusakan
      Perkerasan pada ruas yang telah dipilih dibagi menjadi beberapa unit perkerasan. Survei dilakukan secara visual dan data yang dinilai dan dicatat pada saat suvei tiap unit sampel adalah jenis, tingkat keparahan, dan kuantitas kerusakan perkerasan. Formulir survei atau alat yang dapat merekam sekurang-kurangnya informasi sebagai berikut: tanggal, lokasi, ruas, seksi, ukuran unit sampel, jumlah dan ukuran panel, jenis, tingkat keparahan, dan kuantitas kerusakan, dan nama-nama petugas survei. Berdasarkan Indeks Kondisi Perkerasan cara pengukuran tingkat kerusakan perkerasan jalan dalam Pedoman Bahan Konstruksi Bangunan Dan Rekayasa Sipil Penilaian kondisi perkerasan dilakukan untuk setiap lajur jalan, dengan arah pengukuran 2 (dua) arah. 3.8 Penentuan Koordinat Sebaran Kerusakan Pada Jalan
      Penentuan Koordinat Sebaran Kerusakan Pada Jalan akan dilakukan dengan penetapan titik koordinat setiap kerusakan pada peta lokasi. Digambarkan dalam diagram alur di bawah ini. 3.9 Analisis dan Evaluasi Hasil
      Hasil pengukuran yang dihasilkan dari citra jenis kerusakan jalan perkerasan akan dianalisis dan evaluasi dengan pengolahan data secara manual yang dikerjakan oleh binamarga serta dihitung kembali akurasi dari pengukuran tingkat kerusakan tersebut."
Prameswari Rizcha Julianda_Kualifikasi.txt,"3.1 Tahapan Penelitian
       Tahapan yang dilakukan pada penelitian ini secara garis besar terdapat tiga kelompok tahapan yang mana luaran akhirnya adalah membangun sebuah model. Berikut beberapa tahapan tersebut yang dapat dilihat pada Gambar 3.1. Penjelasan lengkap mengenai tahapan penelitian secara lengkap dapat dilihat pada subbab berikutnya. Adapun penjelasan dari masing-masing tahapan penelitian ini adalah sebagai berikut. 3.2 Melakukan Studi Pustaka
       Tahapan pertama dari penelitian ini adalah melakukan studi pustaka. Pustaka atau referensi yang terkait dengan penelitian ini meliputi teori tentang stres, manajemen stres, analisis SWOT atau dalam hal ini fokus pada personal SWOT analysis, kemudian tentang balanced scorecard yang dalam hal ini fokus pada personal balanced scorecard, hingga Multi-criteria decision making (MCDM) yang mana dalam hal ini adalah Analytical Hierarchy Process (AHP). 3.3 Menentukan Objek Penelitian
       Penelitian yang dilakukan merupakan penelitian kuantitatif dengan pendekatan fenomenologis yang mana fenomena yang menjadi fokus penelitian ini adalah pada saat mahasiswa tingkat akhir dihadapkan pada penyusunan Capstone Design Project dan Skripsi. Menurut Kasiram (2008) dalam Wahyuningtiyas S, dkk (2019) bahwa penelitian kuantitatif yaitu proses dalam menemukan pengetahuan dengan menggunakan data yang berupa angka. Penelitian kuantitatif dalam melihat hubungan variabel terhadap objek yang diteliti lebih bersifat sebab akibat (kausal), sehingga dalam penelitiannya ada variabel indipenden dan dependen (Sugiyono, 2012 dalam Wahyuningtiyas S, dkk, 2019). Objek penelitian yang diteliti dalam penelitian ini adalah mahasiswa tingkat akhir Program Studi Teknik Industri Universitas Gunadarma. Terdapat 315 populasi mahasiswa tingkat akhir Program Studi Teknik Industri Gunadarma yang mana semuanya akan menjadi objek dalam penelitian ini. 3.4 Menyebarkan Kuesioner 1 dan Mendefinisikan Stres
       Pada penelitian ini, terdapat dua variabel yakni variabel bebas (X) dan variabel terikat (Y). Variabel bebas (X) dari penelitian ini adalah mengidentifikasi stres dalam kaitannya dengan stressor yang mana ini akan berbeda juga berdasarkan latar belakang masing-masing mahasiswa. Sedangkan variabel terikat (Y) dari penelitian ini adalah Manajemen Stres masing-masing mahasiswa. Penelitian ini menggunakan instrumen pengumpulan data berupa kuesioner, yaitu kuesioner tingkat stres dan kuesioner stresor mahasiswa dengan skala Likert 1 - 5. Kuesioner dibuat berdasarkan kebutuhan data yang akan dieksplorasikan dalam penelitian, yaitu mengidentifikasi tingkat stres mahasiswa dan mengungkap penyebab stres pada mahasiswa. Kuesioner bersifat terbuka dan tertutup. Kuesioner identifikasi stres yang dipakai dalam penelitian ini adalah adaptasi DASS (Depression, Anxiety, and Stress Scales) dari Ilmuwan Melbourne University; Lovibond, S.H dan Lovibon, P.F dan menggunakan metode K-Nearest Neighbor (K-NN). K-Nearest Neighbor (K-NN) adalah suatu metode yang menggunakan algoritma super-vised dimana hasil dari query instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori pada K-NN. Tujuan dari algoritma ini adalah mengklasifikasikan objek baru bedasarkan atribut dan training sample (Kartika, J. Santoso, E. & Sutrisno, 2017 dalam Aldi, 2020). Jarak Euclidean dapat dicari dengan menggunakan persamaan 1 berikut ini (Johar, T. A. Yanosma, D. & Anggriani, K, 2016 dalam Aldi, 2020). Tahapan algoritma K-NN:
1. Menentukan parameter k.
2. Menghitung jarak euclidean objek terhadap data training yang diberikan. 3. Mengurutkan hasil j arak euclidean secara ascending. 4. Mengumpulkan objek klasifikasi nearest neighbor berdasarkan nilai k yang telah ditetapkan. 5. Mencari kelas mayoritas berdasarkan hasil prediksi
       Sedangkan untuk identifikasi stresor dibuat kuesioner berdasarkan jenis-jenis stressor dari Prawirohusodo (1988) dalam Aldi (2020), yaitu stresor fisik-biologik, psikologis, dan sosial budaya. Data yang terkumpul dari responden akan dilakukan penyuntingan (editing), pengkodean (coding), kemudian ditabulasi. Data deskriptif yang didapat dari penelitian ini akan dianalisis dengan analisis statistika deskriptif berupa diagram. Contoh kuesioner 1 dapat dilihat pada Lampiran 1. 3.5 Menyebarkan Kuesioner 2 dan 3
       Kuesioner yang kedua ini merupakan kuesioner terbuka tentang personal SWOT analysis yang akan digunakan untuk mengidentifikasi kekuatan, kelemahan, peluang dan ancaman dari masing-masing mahasiswa. Masing-masing bagian terdiri dari tujuh pertanyaan yang dapat dilihat pada Lampiran 2. Kuesioner yang selanjutnya adalah kuesioner terbuka tentang personal balanced scorecard yang akan digunakan untuk mengidentifikasi empat perspektif, yaitu internal, eksternal, pengetahuan dan pembelajaran, serta keuangan. Adapun masing-masing pertanyaan kuesioner dapat dilihat pada Lampiran 3. 3.6 Personal SWOT Analysis
      Personal SWOT Analysis adalah cara mengevaluasi diri sendiri agar manusia dapat merencanakan kehidupan yang lebih baik dengan prinsip melakukan setiap pekerjaan yang paling disukai, sehingga dapat bekerja dengan suka cita dan penuh kecintaan serta semangat dalam melakukannya (Rangkuti, 2015). Maka, dalam melakukan PSA, manusia lebih mampu untuk mengidentiikasi SWOT yang ada pada diri sendiri. Cara melakukan Personal SWOT Analysis, seperti yang dijelaskan dalam buku Rangkuti, 2015 antara lain:
1. Menentukan indikator kekuatan, dengan mengidentifikasi semua indikator yang dapat dikendalikan, seperti: apa keahlian atau kelebihan yang berbeda, menonjol dan unik yang dimiliki, apa bukti dari keunggulan yang dimiliki, pengalaman dan wawasan apa dimiliki, siapa orang yang berpengaruh yang dikenal dengan sangat baik, sesuai dengan pekerjaan yang ingin dilakukan. 2. Menentukan indikator kelemahan yang ada, agar dapat memperbaiki diri dan meningkatkan kinerja, dengan mengidentifikasi kelemahan yang ada seperti apa kekurangan atau kebiasaan buruk yang ada pada diri manusia, menurut orang lain apa saja kelemahan kita dan tindakan apa yang bisa dilakukan untuk memperbaiki diri. 3. Tentukan indikator peluang, dengan mengidentifikasi beragam peluang yang ada, terkait perubahan yang dapat dimanfaatkan, seperti pemanfaatan tekhnologi, adanya posisi kosong atau proyek baru dan pelatihan apa yang dapat diikuti sebagai usaha meraih peluang tersebut. Pada mahasiswa tingkat akhir, peluang ini dapat diidentifikasikan seperti pemanfaatan teknologi dan informasi sehingga dapat mencapai tujuan yaitu lulus tepat waktu. 4. Tentukan indikator ancaman, dengan mengidentifikasi posisi pesaing, rekan kerja yang berposisi lebih baik, adanya penggunaan tekhnologi baru, beragam kendala internal atau eksternal yang dapat menghambat pekerjaan kita. Pada mahasiswa tingkat akhir, ancaman ini dapat diidentifikasikan seperti persaingan teman seangkatan yang akan meneliti tema yang sama, dan lain sebagainya. 3.7 Personal Balanced Scorecard
       Terdapat empat perspektif penilaian pada Personal Balanced Scorecard. Adapun keempat perspektif tersebut dapat dijelaskan sebagai berikut (Suwandi dan Harihayati, 2013). 1. Internal
Perspektif internal mendeskripsikan kesehatan fisik atau keadaan mental dari seorang pegawai. Contoh: Kejujuran dari seorang pegawai. Pada mahasiswa tingkat akhir, contoh perspektif internal ini adalah kejujuran dari seorang mahasiswa dalam kaitannya dengan menyontek pekerjaan orang lain
2. Eksternal
Perspektif eksternal mendeskripsikan hubungan seorang pegawai dengan keluarga, rekan kerja, atasan, customer atau lingkungan luar lainnya. Contoh: Tingkat komunikasi seorang pegawai dengan atasan, rekan kerja atau customer. Pada mahasiswa tingkat akhir, contoh perspektif eksternal adalah tingkat komunikasi mereka dengan orang tua atau wali, dosen, senior atau junior, dan teman seangkatan atau sepermainan. 3. Pengetahuan dan pembelajaran
Perspektif pengetahuan dan pembelajaran mendeskripsikan keterampilan, kemampuan pembelajaran dan pengetahuan pegawai. Contoh: Pengetahuan dan Penguasaan dalam Sistem Aplikasi. Pada mahasiswa tingkat akhir, contoh perspektif pengetahuan dan pembelajaran adalah pengetahuan dan penguasaan perangkat lunak sebagai alat bantu dalam mengerjakan tugas kuliah
4. Keuangan
Perspektif keuangan mendeskripsikan pencapaian target kerja seseorang dalam sisi keuangan. Contoh: Penghematan Penggunaan Dana Perusahaan Dalam Setiap Proyek. Pada mahasiswa tingkat akhir, contoh perspektif keuangan adalah pengalokasian penggunaan uang yang diberikan oleh orang tua. 3.8 Penentuan Key Performance Indicator (KPI) dan Analisis Output
       Hasil luaran dari Personal Balanced Scorecard kemudian selanjutnya akan ditentukan Key Performance Indicator yang mana mempertimbangkan hasil dari luaran Personal SWOT Analysis dan hasil dari identifikasi stres dan stressor. KPI ini selanjutnya akan dilakukan pembobotan dengan menggunakan Analitycal Hierarchy Process (AHP) dengan perangkat lunak Expert Choice yang nantinya akan menghasilkan model baru dari manajemen mahasiswa tingkat akhir dalam kaitannya dengan keahlian menyelesaikan masalah, atau dalam hal ini adalah stressor.","3.1 Tahapan Penelitian
       Tahapan yang dilakukan pada penelitian ini secara garis besar terdapat tiga kelompok tahapan yang mana luaran akhirnya adalah membangun sebuah model. Berikut beberapa tahapan tersebut yang dapat dilihat pada Gambar 3.1. Penjelasan lengkap mengenai tahapan penelitian secara lengkap dapat dilihat pada subbab berikutnya. Pustaka atau referensi yang terkait dengan penelitian ini meliputi teori tentang stres, manajemen stres, analisis SWOT atau dalam hal ini fokus pada personal SWOT analysis, kemudian tentang balanced scorecard yang dalam hal ini fokus pada personal balanced scorecard, hingga Multi-criteria decision making (MCDM) yang mana dalam hal ini adalah Analytical Hierarchy Process (AHP). 3.3 Menentukan Objek Penelitian
       Penelitian yang dilakukan merupakan penelitian kuantitatif dengan pendekatan fenomenologis yang mana fenomena yang menjadi fokus penelitian ini adalah pada saat mahasiswa tingkat akhir dihadapkan pada penyusunan Capstone Design Project dan Skripsi. Objek penelitian yang diteliti dalam penelitian ini adalah mahasiswa tingkat akhir Program Studi Teknik Industri Universitas Gunadarma. Terdapat 315 populasi mahasiswa tingkat akhir Program Studi Teknik Industri Gunadarma yang mana semuanya akan menjadi objek dalam penelitian ini. Penelitian ini menggunakan instrumen pengumpulan data berupa kuesioner, yaitu kuesioner tingkat stres dan kuesioner stresor mahasiswa dengan skala Likert 1 - 5. Kuesioner dibuat berdasarkan kebutuhan data yang akan dieksplorasikan dalam penelitian, yaitu mengidentifikasi tingkat stres mahasiswa dan mengungkap penyebab stres pada mahasiswa. K-Nearest Neighbor (K-NN) adalah suatu metode yang menggunakan algoritma super-vised dimana hasil dari query instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori pada K-NN. Pada mahasiswa tingkat akhir, peluang ini dapat diidentifikasikan seperti pemanfaatan teknologi dan informasi sehingga dapat mencapai tujuan yaitu lulus tepat waktu. Pada mahasiswa tingkat akhir, ancaman ini dapat diidentifikasikan seperti persaingan teman seangkatan yang akan meneliti tema yang sama, dan lain sebagainya. Pada mahasiswa tingkat akhir, contoh perspektif internal ini adalah kejujuran dari seorang mahasiswa dalam kaitannya dengan menyontek pekerjaan orang lain
2. Contoh: Tingkat komunikasi seorang pegawai dengan atasan, rekan kerja atau customer. Pada mahasiswa tingkat akhir, contoh perspektif eksternal adalah tingkat komunikasi mereka dengan orang tua atau wali, dosen, senior atau junior, dan teman seangkatan atau sepermainan. Pada mahasiswa tingkat akhir, contoh perspektif pengetahuan dan pembelajaran adalah pengetahuan dan penguasaan perangkat lunak sebagai alat bantu dalam mengerjakan tugas kuliah
4. Contoh: Penghematan Penggunaan Dana Perusahaan Dalam Setiap Proyek. Pada mahasiswa tingkat akhir, contoh perspektif keuangan adalah pengalokasian penggunaan uang yang diberikan oleh orang tua. 3.8 Penentuan Key Performance Indicator (KPI) dan Analisis Output
       Hasil luaran dari Personal Balanced Scorecard kemudian selanjutnya akan ditentukan Key Performance Indicator yang mana mempertimbangkan hasil dari luaran Personal SWOT Analysis dan hasil dari identifikasi stres dan stressor. KPI ini selanjutnya akan dilakukan pembobotan dengan menggunakan Analitycal Hierarchy Process (AHP) dengan perangkat lunak Expert Choice yang nantinya akan menghasilkan model baru dari manajemen mahasiswa tingkat akhir dalam kaitannya dengan keahlian menyelesaikan masalah, atau dalam hal ini adalah stressor."
Proposal Disertasi Kualifikasi Adam Huda Nugraha.txt,"3.1 Kerangka Metode Penelitian yang diusulkan
       Penelitian ini bertujuan untuk mendapatkan model arsitektur LSTM untuk suara digit desimal berbahasa Indonesia sehingga tingkat akurasi pengenalan dapat lebih tinggi dibandingkan dengan penelitian sebelumnya. Akurasi atau tingkat akurasi pengenalan adalah jumlah ketepatan suara digit desimal yang diucapkan dibagi dengan jumlah data suara yang akan diterjemahkan menjadi teks, sehingga semakin tinggi akurasi pengenalan menunjukkan semakin tingginya hasil ketepatan sistem menterjemahkan sinyal-sinyal suara ini menjadi teks. Model LSTM telah diterapkan untuk pengenalan suara berbahasa Bengali (Nahid, et al. 2017) (LSTM Nahid) dan Parsi (Daneshvar dan Veisi. 2016) (LSTM Daneshvar), kedua model ini diteliti untuk dibandingkan tingkat akurasi pengenalan dengan model yang diusulkan untuk mengenali digit desimal bahasa Indonesia. Kerangka metode yang diusulkan diilustrasikan pada gambar 3.1. 3.2 Pengumpulan Data Latih Dan Data Uji
       Tahapan penelitian dilakukan seperti yang terlihat pada gambar 3.1, meliputi tahap pengumpulan data, tahap pra-pengolahan data yang mengektraksi fitur suara menggunakan spektrogram atau MFCC, tahap pembelajaran untuk mencari model, tahap klasifikasi untuk menguji model yang dihasilkan dan evaluasi dengan menghitung akurasi pengenalan. Pada tahap pengumpulan data, dilakukan perekaman data suara digit desimal berbahasa Indonesia mulai dari digit 0 hingga digit 9, kemudian dilakukan pemotongan data suara sesuai dengan digit yang diucapkan dan dilanjutkan dengan pemberian label untuk setiap file digit yang disimpan. Tahap berikutnya adalah mentransformasi sinyal suara menjadi matriks koefisien spektrogram dan mengambil ciri data suara tersebut menjadi matriks koefisien MFCC. Proses pembelajaran dilakukan dengan menggunakan DTW, HMM, RNN, model LSTM Nahid, model LSTM Daneshvar dan model LSTM yang diusulkan terhadap matriks koefisien spektrogram dan MFCC. Untuk melihat tingkat akurasi, pengujian kemudian dilakukan terhadap DTW, HMM, RNN, model LSTM Daneshvar, model LSTM Nahid dan model LSTM yang diusulkan. Data digit suara 0-9 yang digunakan dalam penelitian ini diambil di dalam ruang kelas ujian komputer Universitas Gunadarma. Proses perekaman dilakukan dalam jangka waktu 4 minggu, sehingga didapatkan sekitar 799 mahasiswa. Pada minggu pertama perekaman, didapatkan 100 pembicara masing-masing mengucapkan digit 0 sampai 9 sehingga terdapat 1000 data suara. 1000 data ini kemudian dilakukan proses klasifikasi LSTM menggunakan fitur spektrogram dengan 900 data latih dan 100 data uji, akurasi yang dihasilkan masih rendah sekitar 60%. Minggu kedua dilakukan kembali perekaman data dengan total data yang didapatkan 3000 data suara. Klasifikasi LSTM yang dihasilkan dari 3000 data ini meningkat menjadi 70%. Perekaman dilanjutkan dengan mendapatkan 5000 data suara dari 500 pembicara, hasil klasifikasi LSTM meningkat menjadi 80%. Pada saat 6000 data suara diklasifikasi oleh LSTM, akurasi sudah mencapai 90%, begitu juga dengan 7000 data suara. Perekaman data kemudian dilanjutkan di sisa minggu ke-empat sehingga didapatkan 7990 data suara dari 799 pembicara. Pembicara adalah mahasiswa Gunadarma dengan rentang usia 19-22 tahun, terdiri dari 389 perempuan dan 410 laki-laki. Data suara yang diambil masing-masing mengucapkan digit dengan pelafalan dapat dilihat pada tabel 2.1, sehingga total basis data suara yang didapatkan sebanyak 7990 data. Data ini kemudian disebut dengan data latih. Proses perekaman dilakukan dengan telepon genggam iPhone 6 menggunakan aplikasi voice memos .yang terdapat pada telepon genggam, setiap mahasiswa mengucapkan digit 0 hingga digit 9 sekaligus dengan jeda antara setiap digit, kemudian data tersebut disimpan. Perekaman terjadi dalam lingkungan pengucapan yang terdapat suara AC, suara pintu tertutup dan terbuka, suara mahasiswa lain batuk, suara keyboard, walaupun tidak disemua data. Semua noise- noise ini pada background sinyal ikut terekam oleh microphone telepon genggam. Data suara tersebut kemudian di-transfer ke laptop dan disimpan dalam format wav, dengan sample-rate 8000Hz, karena suara manusia berbicara berada pada rentang 34- 3400Hz, dan menurut Nyquist Theorem harus mengambil s dua kali dari rentang paling tinggi, maka sample-rate yang ditentukan adalah 8000Hz. Data-data tersebut lalu dipotong sesuai dengan digit yang diucapkan, masing-masing digit disimpan dengan satu file dan di-labelkan sesuai dengan digit yang diucapkan. Pengucapan kata ""nol"" dari data suara mahasiswa A, data ini akan disimpan dalam satu file dengan diberi label seperti ""A-O"", sehingga terdapat 10 file pengucapan digit untuk satu mahasiswa. Noise-noise yang ada, tidak dihilangkan dari sinyal suara. Untuk melihat tingkat akurasi diambil, sekitar 10 % data dari data latih. Suara 79 mahasiswa menjadi data uji, terdiri dari 3 7 perempuan dan 42 laki-laki yang tidak terdapat dalam data latih, masing-masing mengucapkan 10 digit (0-9), sehingga total data untuk pengujian berjumlah 790 data. Proses perekaman dilakukan dengan cara yang sama seperti pada pembentukan data latih. Dengan proses pengumpulan basis data seperti ini, didapatkan model yang speaker independent, karena pembicara pada data latih berbeda dengan pembicara pada data uji. 3.3 Pra Pengolahan Data
       Pada tahap ini, data latih dan data uji, diubah menjadi spektrogram dan fitur MFCC. Data suara yang masih berbentuk amplitudo sangat sulit langsung digunakan untuk pengenalan suara. Nilai-nilai amplitudo tersebut sangat berfluktuasi tergantung dari tekanan pengucapan, kualitas microphone, noise, dan lain-lain. Algoritma untuk mengekstraksi ciri dari nilai-nilai amplitudo ini kemudian dikembangkan agar mempermudah proses pengenalan sinyal suara. MFCC digunakan untuk mengekstraksi ciri suara pada penelitian ini, karena hampir semua penelitian dengan DTW, HMM dan RNN menggunakan fitur MFCC pada digit bahasa Jepang (Sakoe dan Chiba. 1978), pada digit bahasa Spanyol Gomez. 2005), pada digit bahasa Arab (Alotaibi et al. 2010), pada digit bahasa Kroasia (Gulic, Lucanin dan Simic. 2011), pada digit bahasa Hindi (Dhandhania, Hansen, Kandi dan Ramesh. 2012; Saxena dan Wahi. 2015), pada digit bahasa Myanmar (Tun dan Srijuntongsiri. 2016), dan pada digit bahasa Gujarati (Pandit dan Bhatt. 2014). Spektrogram mulai digunakan oleh (Sak, et al. 2014) untuk pemodelan dengan LSTM dengan harapan agar LSTM dapat menemukan sendiri model akustik dari pembicara tanpa perlu fitur ekstraksi ciri seperti MFCC, sehingga spektrogram juga digunakan pada penelitian pengenalan suara digit desimal berbahasa Indonesia ini saat proses pra-pengolahan data. 3.3.1 Spektrogram
      Sinyal suara dapat dilihat dalam domain frekuensi yang menggambarkan kerapatan spektrum sinyal atau disebut dengan spektrogram. Untuk merubah sinyal dari domain waktu (amplitudo) ke domain frekuensi (spektrogram) dapat dilakukan dengan langkah seperti pada gambar 3.2. Sinyal suara dipotong per 20 milidetik, lalu dilakukan windowing dengan menggunakan metode hamming agar didapatkan rentang nilai magnitude di dalam sinyal, kemudian data yang telah di-windowing ini ditransformasi Fourier dan menghitung magnitude Fourier (log absolut). Untuk setiap 20 milidetik didapatkan 64 koefisien spektrogram. Pemotongan 20 milidetik atau 0.02 detik pada sample rate 8000Hz menghasilkan 160 sample. Jumlah sample didapatkan dari formula berikut:
Sample = sample-rate x time (detik)
=8000 x 0.02 =160
Pada gambar 3.3 diperlihatkan hasil pemotongan dari 20 milidetik atau 160 sample sinyal suara setelah dilakukan proses windowing dengan menggunakan hamming. Koefisien magnitude dari Fourier kemudian didapatkan dari sinyal suara yang telah di windowing. Nilai FFT size yang digunakan adalah 128, sehingga didapatkan 64 (128 dibagi 2) koefisien Fourier. Untuk data suara dengan sample rate 8000Hz, nilai fft size 128, 256, 512 tidak terlalu berpengaruh, sehingga bisa menggunakan nilai fft size yang paling kecil agar proses transformasi fourier lebih cepat. Flowchart mengubah sinyal amplitudo menjadi spektrogram pada Matlab dapat dilihat pada gambar 3.4. 3.3.2 Mel-Frequency Ceptral Ceptrum (MFCC)
       Ekstraksi ciri pada dasarnya adalah mengubah sinyal-sinyal amplitudo atau spektrogram menjadi hanya beberapa vektor koefisien yang diperkirakan mengandung informasi yang penting. Mel-Frequency Cepstrum Coefficient (MFCC) yang dikenalkan oleh Davis dan Mermelstein pada tahun 1990, mengambil sekitar 12 vektor frekuensi dari sinyal suara. Koefisien MFCC ini merupakan fitur ekstraksi ciri suara paling sering digunakan karena menunjukkan akurasi yang tinggi pada sinyal yang terdapat noise (Davis dan Mermelstein. 1990). Pada penelitian ini MFCC juga digunakan sebagai fitur ciri. Langkah awal untuk mencari koefisien MFFC mirip dengan spektrogram, namun setelah mendapatkan nilai koefisien fourier, ditambahkan menghitung log mel-frekuensi dan mencari ceptral koefisien dengan menghitung- Discrete Cosinus Transform (DCT) dari log mel-frekuensi. Langkah lengkap dapat dilihat pada gambar 3.5. Pada gambar 3.5, menunjukkan plot hasil 64 koefisien FFT kemudian dilakukan perhitungan mel-frekuensi dengan 12 parameter. Flowchart mengekstraksi ciri suara dengan MFCC pada Matlab dapat dilihat pada gambar 3.6. 3.4 Dynamic Time Warping (DTW)
       Untuk melakukan pengenalan suara dengan algoritma DTW, sebelumnya dilakukan pengambilan fitur ciri spektrogram atau MFCC dari 7990 data latih, begitu juga dengan 790 data uji. Koefisien spektrogram atau MFCC tersebut kemudian yang dijadikan data matriks untuk pengenalan dengan DTW. Terdapat 12 koefisien yang dihasilkan jika menggunakan MFCC, dan 64 koefisien jika menggunakan spektrogram. DTW membuat distance matriks untuk me-normalisasi perbedaan ukuran antara matriks yang akan dibandingkan, sehingga jika matriks data uji (A) menggunakan MFCC berukuran 83x12 dan matriks data latih (B) berukuran 52x12, maka distance matriks DTW akan berukuran 83x52. Distance matriks DTW menggunakan spektrogram juga berukuran yang sama, jika matriks data uji (A) berukuran 83x64 dan matriks data latih (B) berukuran 52x64, maka distance matriks DTW akan berukuran 83x52. Flowchart pengujian dengan DTW dapat dilihat pada gambar 3.7. Setiap sel distance matriks (D) berisi kombinasi antara jarak euclidean dan bobot 
antara sel A dan sel B. Proses ini dihitung hingga semua sel matriks D terisi. Jarak DTW antara matriks A dan matriks B kemudian didapatkan dari sel matriks D yang berada di sel terakhir dari distance matriks atau di sel pada baris 83 dan kolom 52.","3.1 Kerangka Metode Penelitian yang diusulkan
       Penelitian ini bertujuan untuk mendapatkan model arsitektur LSTM untuk suara digit desimal berbahasa Indonesia sehingga tingkat akurasi pengenalan dapat lebih tinggi dibandingkan dengan penelitian sebelumnya. Akurasi atau tingkat akurasi pengenalan adalah jumlah ketepatan suara digit desimal yang diucapkan dibagi dengan jumlah data suara yang akan diterjemahkan menjadi teks, sehingga semakin tinggi akurasi pengenalan menunjukkan semakin tingginya hasil ketepatan sistem menterjemahkan sinyal-sinyal suara ini menjadi teks. Model LSTM telah diterapkan untuk pengenalan suara berbahasa Bengali (Nahid, et al. 2016) (LSTM Daneshvar), kedua model ini diteliti untuk dibandingkan tingkat akurasi pengenalan dengan model yang diusulkan untuk mengenali digit desimal bahasa Indonesia. 3.2 Pengumpulan Data Latih Dan Data Uji
       Tahapan penelitian dilakukan seperti yang terlihat pada gambar 3.1, meliputi tahap pengumpulan data, tahap pra-pengolahan data yang mengektraksi fitur suara menggunakan spektrogram atau MFCC, tahap pembelajaran untuk mencari model, tahap klasifikasi untuk menguji model yang dihasilkan dan evaluasi dengan menghitung akurasi pengenalan. Pada tahap pengumpulan data, dilakukan perekaman data suara digit desimal berbahasa Indonesia mulai dari digit 0 hingga digit 9, kemudian dilakukan pemotongan data suara sesuai dengan digit yang diucapkan dan dilanjutkan dengan pemberian label untuk setiap file digit yang disimpan. Untuk melihat tingkat akurasi, pengujian kemudian dilakukan terhadap DTW, HMM, RNN, model LSTM Daneshvar, model LSTM Nahid dan model LSTM yang diusulkan. Data digit suara 0-9 yang digunakan dalam penelitian ini diambil di dalam ruang kelas ujian komputer Universitas Gunadarma. Pada minggu pertama perekaman, didapatkan 100 pembicara masing-masing mengucapkan digit 0 sampai 9 sehingga terdapat 1000 data suara. 1000 data ini kemudian dilakukan proses klasifikasi LSTM menggunakan fitur spektrogram dengan 900 data latih dan 100 data uji, akurasi yang dihasilkan masih rendah sekitar 60%. Klasifikasi LSTM yang dihasilkan dari 3000 data ini meningkat menjadi 70%. Perekaman dilanjutkan dengan mendapatkan 5000 data suara dari 500 pembicara, hasil klasifikasi LSTM meningkat menjadi 80%. Algoritma untuk mengekstraksi ciri dari nilai-nilai amplitudo ini kemudian dikembangkan agar mempermudah proses pengenalan sinyal suara. 2014) untuk pemodelan dengan LSTM dengan harapan agar LSTM dapat menemukan sendiri model akustik dari pembicara tanpa perlu fitur ekstraksi ciri seperti MFCC, sehingga spektrogram juga digunakan pada penelitian pengenalan suara digit desimal berbahasa Indonesia ini saat proses pra-pengolahan data. Jumlah sample didapatkan dari formula berikut:
Sample = sample-rate x time (detik)
=8000 x 0.02 =160
Pada gambar 3.3 diperlihatkan hasil pemotongan dari 20 milidetik atau 160 sample sinyal suara setelah dilakukan proses windowing dengan menggunakan hamming. 3.3.2 Mel-Frequency Ceptral Ceptrum (MFCC)
       Ekstraksi ciri pada dasarnya adalah mengubah sinyal-sinyal amplitudo atau spektrogram menjadi hanya beberapa vektor koefisien yang diperkirakan mengandung informasi yang penting. Pada gambar 3.5, menunjukkan plot hasil 64 koefisien FFT kemudian dilakukan perhitungan mel-frekuensi dengan 12 parameter. 3.4 Dynamic Time Warping (DTW)
       Untuk melakukan pengenalan suara dengan algoritma DTW, sebelumnya dilakukan pengambilan fitur ciri spektrogram atau MFCC dari 7990 data latih, begitu juga dengan 790 data uji. Koefisien spektrogram atau MFCC tersebut kemudian yang dijadikan data matriks untuk pengenalan dengan DTW. Terdapat 12 koefisien yang dihasilkan jika menggunakan MFCC, dan 64 koefisien jika menggunakan spektrogram. Setiap sel distance matriks (D) berisi kombinasi antara jarak euclidean dan bobot 
antara sel A dan sel B. Proses ini dihitung hingga semua sel matriks D terisi. Jarak DTW antara matriks A dan matriks B kemudian didapatkan dari sel matriks D yang berada di sel terakhir dari distance matriks atau di sel pada baris 83 dan kolom 52."
Ragiel.txt,"Berdasarkan dari penelitian tersebut, peneliti mengusulkan model paralelisasi antara proses transformasi subbyte dan shiftrows, sehingga diharapkan penggunaan resources dapat lebih efisien, mengoptimalkan kecepatan serta memberikan tingkat keamanan yang tinggi. Diagram alur pada gambar 3.1 menunjukkan usulan peneliti untuk melakukan pengembangan algoritma AES dengan model paralelisasi antara transformasi subbytes dan shiftrows. Adapun proses untuk algoritma AES sebagai berikut :
1. Proses perhitungan key schedule
Proses pertama dalam algoritma AES adalah mencari key schedule yang akan digunakan untuk 10 putaran. 2. Proses XOR antara plaintext dengan cipher key 
Proses kedua adalah melakukan XOR antara plaintext dengan cipher key untuk 
menghasilkan addroundkey. 3. Proses Transformasi Subbytes, Shiftrows, Mixcolumn, dan Addroundkey 
Transformasi subbytes dilakukan dengan cara subtitusi dengan tabel S-Box yang sudah disimpan dalam memory. Data yang dihasilkan merupakan matriks 4x4. Pada transformasi shiftrows dilakukan pergeseran untuk baris 0 tidak terjadi pergeseran, untuk baris 1 matriks terjadi pergeseran 1 byte ke kiri, untuk baris 2 matriks terjadi pergeseran 2 byte ke kiri, dan untuk baris 3 matriks terjadi pergeseran 3 byte ke kiri. Pada transformasi mixcolumn mengalikan (modulo polynomial g(X)) setiap kolom dalam state (diperlakukan sebagai polynomial) dengan polynomial a(X). Pada transformasi addroundkey terjadi proses XOR antara state (matriks) dengan key schedule. Proses ini berulang sebanyak 9 putaran. 4. Proses Transformasi Subbytes, Shiftrows, dan addroundkey
Pada proses ini hanya terdapat 3 proses transformasi yaitu subbytes, shiftrows dan addroundkey. Proses ini terjadi pada putaran terakhir dalam algoritma AES. Setelah proses ini selesai maka akan menghasilkan sebuah cipher text. Berdasarkan dari proses algoritma AES tersebut, penelitian ini mengusulkan sebuah metode untuk penggabungan dari proses transformasi subbytes dan shiftrows. Adapun proses penggabungan tersebut terjadi pada saat setiap byte data yang di subtitusi pada transformasi subbytes, dilakukan pengecekan apakah data tersebut mengalami pergeseran atau tidak terhadap data.","Berdasarkan dari penelitian tersebut, peneliti mengusulkan model paralelisasi antara proses transformasi subbyte dan shiftrows, sehingga diharapkan penggunaan resources dapat lebih efisien, mengoptimalkan kecepatan serta memberikan tingkat keamanan yang tinggi. Diagram alur pada gambar 3.1 menunjukkan usulan peneliti untuk melakukan pengembangan algoritma AES dengan model paralelisasi antara transformasi subbytes dan shiftrows. Adapun proses untuk algoritma AES sebagai berikut :
1. Proses ini terjadi pada putaran terakhir dalam algoritma AES. Setelah proses ini selesai maka akan menghasilkan sebuah cipher text."
Ragmar Faikar Eka_Kualifikasi.txt,bab metode penelitian menjelaskan mengenai tahapan yang dilakukan dalam penelitian beserta menjelaskan mengenai jadwal dan estimasi waktu tahapan yang dilakukan pada penelitian ini serta menjelaskan mengenai kegiatan yang dilakukan selama penelitian . tahapan penelitian dijelaskan dalam bentuk flowchart sehingga dapat menjelaskan proses yang dilakukan mulai dari studi literatur sampai dengan kesimpulan jadwal dan estimasi penelitian digambarkan dalam bentuk time table untuk menjadwalkan dan melakukan estimasi waktu dari tiap tahap yang dilakuk an. 3.1 tahapan penelitian terdapat beberapa tahapan yang dilakukan untuk melakukan penelitian ini beberapa tahapan yang dilakukan dapat dilihat pada gambar 3.1 . gambar 3.1 tahapan penelitian 31 3.1.1 studi literatur tahap pertama yang dilakukan yaitu studi literature yang bertujuan untuk mencari informasi atau pengetahuan dari paper atau buku sebagai teori pendukung untuk melakukan penelitian dan mencari novelty atau gap peneltian yang sudah dilakukan. paper dan buku yang digunakan dalam penelitian ini merupakan paper atau buku 5 tah un terakhir. 3.1.2 pengumpulan data tahap kedua yaitu pengumpulan data data yang digunakan pada penelitian ini adalah data citra digital kelapa sawit dengan tingkat kematangan belum matang setengah matang matang terlalu matang dan tandan buah yang kosong . data diambil dari beberapa sumber melalui website kaggle dan roboflow lalu dilakukan pemilihan gambar yang sesuai untuk dijadikan sebagai dataset. gambar 3.2 contoh data kelapa sawit dari masing masing kelas 3.1.3 preprocessing data sebelum data digunakan pada model machine learning yang dibuat data tersebut akan dilakukan preprocessing data agar data yang akan dilatih sesuai dengan keperluan yang dibutuhkan. preprocessing yang dilakukan adalah augmentasi data resize dan segmentasi data . gambar asli akan dilakukan resize menjadi ukuran 224x224 piksel lalu data tersebut akan di augmentasi untuk memperbanyak dan m emvariasi data agar dan hasil augmentasi akan dijadikan sebagai data latih untuk model yang dibuat. augmentasi yang dilakuk an adalah rotation_range width_shift_range height_shift_range brightness_range shear_range zoom_range horizontal_flip vertical_flip . segmentasi dilakukan untuk memisahkan objek gambar dengan latar belakang sehingga proses klasifikasi akan lebih terfokus pada objek yang akan diproses. 32 3.1.4 pembuatan model tahap keempat yaitu pembuatan model machine learning menggunakan mobilenetv3 smalllarge dan menggabungkannya dengan attention module cbam convolutional block attention module. 3.1.5 melatih model tahap kelima yaitu melatih model yang sudah dibuat menggunakan dataset yang sudah dikumpulkan pada tahap pengumpulan data . 3.1.6 evaluasi model tahap keenam yaitu evaluasi model dari hasil pelatihan model akan dievaluasi atay dinilai apakah model tersebut sudah baik atau belum . jika hasil dari evaluasi atau penilaian kinerja model kurang memuaskan maka tahap kelima akan dilakukan kembali. 3.1.7 deploy model tahap ketujuh yaitu deploy model model yang sudah dievaluasi dan dikatakan baik akan di deploy menjadi tflite sehingga dapat diintegrasikan ke dalam mobile device . 3.1.8 pembuatan aplikasi tahap kedelapan yaitu pembuatan aplikasi android tahap ini dilakukan menggunakan android studio untuk membuat aplikasi android . proses pembuatannya meliputi pembuatan tampilan user memasukan tflite ke dalam aplikasi sehingga aplikasi dapat menggunakan model machine learning untuk mengklasifikasi kematangan kelapa sawit menggunakan kamera smartphone. 3.1.9 pengujian aplikasi tahap kesembilan yaitu pengujian aplikasi yang sudah dibuat aplikasi akan diuji fungsi utamanya yaitu klasifikasi kematangan kelapa sawit. 3.1.10 kesimpulan penelitian tahap kesepuluh yaitu menulis kesimpulan mengenai penelitian yang sudah dilakukan kesimpulan ini mencakup kinerja model dan aplikasi yang sudah dibuat kelebihan dan kekurangan penelitian dan penelitian selanjutnya yang akan dilakukan. 33 3.2 jadwal estimasi penelitian jadwal estimasi penelitian menjelaskan mengenai rancangan kegiatan yang dilakukan selama penelitian beserta estimasi waktu tiap kegiatan yang dilakukan. gambar jadwal estimasi penelitian dapat dilihat pada gambar 3.3. gambar 3.3 jadwal estimasi penelitian gambar diatas merupakan jadwal atau estimasi penelitian kegiatan studi literatur dilakukan mulai dari bulan pertama pada tahun pertama studi literatur dilakukan untuk mencari pengetahuan dan informasi untuk mendapatkan novelty dan gap dari penelitian yan g akan dilakukan pengumpulan data dilakukan mulai dari bulan kedua tahun pertama beriringan dengan studi literature. pada bulan keenam ditahun pertama dilakukan kegiatan preprocessing data preprocessing data yang dilakukan adalah pemilihan data yang akan digunakan pada penelitian dari proses pengumpulan data melakukan resize augmentasi dan segmentasi data. pembuatan model model dilakukan pada bulan keenam beriringan dengan kegiatan pelatihan model serta melakukan evaluasi kinerja model yang sudah dilatih. bulan kesebelas melakukan kegiatan pembuatan aplikasi android kegiatan yang dilakukan yaitu membuat aplikasi android menggunakan android studio mendeploy model yang sudah dievaluasi menjadi format tflite lalu mengimplementasikannya ke dalam aplikasi android yang sudah dibuat. pada bulan ketiga ditahun kedua akan dilakukan pengujian kinerja aplikasi dengan data baru lalu dilakukan evaluasi atau penilaian terhadap fungsi fungsi aplikasi tersebut. bulan keempat tahun kedua merupakan kegiatan terakhir yaitu m enulis kesimpulan penelitian. 34 3.3 kegiatan penelitian kegiatan yang dilakukan untuk melakukan penelitian dari tahun pertama sampai tahun ketiga dapat dilihat pada tabel 2 berikut. tahun pertama tahun kedua tahun ketiga studi literatur evaluasi model submit jurnal pertama pembuatan proposal bab 1 sampai bab 3 deploy dimplementasi model pembuatan jurnal kedua pengumpulan dataset pembua tan aplikasi submit jurnal kedua preprocessing data menulis hasil penelitian bab 4 pembuatan model pengujian dan evaluasi aplikasi melatih model menulis hasil penelitian bab 4 dan bab 5 pembuatan jurnal pertama tabel 2. kegiatan penelitian kegiatan yang dilakukan pada tahun pertama yaitu melakukan studi literaur untuk pembuatan proposal penelitian bab 1 sam pai bab 3 lalu dilanjutkan dengan pengumpulan dan preprocessing data setelah mendapatkan data kegiatan pembuatan dan pelatihan m odel dapat dilakukan. pada tahun kedua dilakukan evaluasi model dan saat hasil evaluasi model sudah cukup baik model akan di deploy untuk dapat diimp lementasi ke dalam aplikasi yang sudah dibuat. aplikasi akan dievaluasi dan diuji kinerjanya sehingga mendapatkan kesimpulan dari peneli tian untuk ditulis da lam b ab 4 sampai bab 5 . pada akhir tahun kedu a setelah mendapatkan kesimpulan penelitian dilakukan pembuatan jurnal pertama dan dilanjutkan pada tahun ketiga untuk pembuatan jurnal kedua.,tahapan penelitian dijelaskan dalam bentuk flowchart sehingga dapat menjelaskan proses yang dilakukan mulai dari studi literatur sampai dengan kesimpulan jadwal dan estimasi penelitian digambarkan dalam bentuk time table untuk menjadwalkan dan melakukan estimasi waktu dari tiap tahap yang dilakuk an. 3.1.2 pengumpulan data tahap kedua yaitu pengumpulan data data yang digunakan pada penelitian ini adalah data citra digital kelapa sawit dengan tingkat kematangan belum matang setengah matang matang terlalu matang dan tandan buah yang kosong . gambar asli akan dilakukan resize menjadi ukuran 224x224 piksel lalu data tersebut akan di augmentasi untuk memperbanyak dan m emvariasi data agar dan hasil augmentasi akan dijadikan sebagai data latih untuk model yang dibuat. 32 3.1.4 pembuatan model tahap keempat yaitu pembuatan model machine learning menggunakan mobilenetv3 smalllarge dan menggabungkannya dengan attention module cbam convolutional block attention module. proses pembuatannya meliputi pembuatan tampilan user memasukan tflite ke dalam aplikasi sehingga aplikasi dapat menggunakan model machine learning untuk mengklasifikasi kematangan kelapa sawit menggunakan kamera smartphone. tahun pertama tahun kedua tahun ketiga studi literatur evaluasi model submit jurnal pertama pembuatan proposal bab 1 sampai bab 3 deploy dimplementasi model pembuatan jurnal kedua pengumpulan dataset pembua tan aplikasi submit jurnal kedua preprocessing data menulis hasil penelitian bab 4 pembuatan model pengujian dan evaluasi aplikasi melatih model menulis hasil penelitian bab 4 dan bab 5 pembuatan jurnal pertama tabel 2. kegiatan penelitian kegiatan yang dilakukan pada tahun pertama yaitu melakukan studi literaur untuk pembuatan proposal penelitian bab 1 sam pai bab 3 lalu dilanjutkan dengan pengumpulan dan preprocessing data setelah mendapatkan data kegiatan pembuatan dan pelatihan m odel dapat dilakukan. pada tahun kedua dilakukan evaluasi model dan saat hasil evaluasi model sudah cukup baik model akan di deploy untuk dapat diimp lementasi ke dalam aplikasi yang sudah dibuat. aplikasi akan dievaluasi dan diuji kinerjanya sehingga mendapatkan kesimpulan dari peneli tian untuk ditulis da lam b ab 4 sampai bab 5 . pada akhir tahun kedu a setelah mendapatkan kesimpulan penelitian dilakukan pembuatan jurnal pertama dan dilanjutkan pada tahun ketiga untuk pembuatan jurnal kedua.
Reza Al Husna_Kualifikasi.txt,3.1 tahapan penelitian secara garis besar penelitian ini terdiri dari beberapa tahapan yaitu akuisisi data preprocessing data pengembangan dan pelatihan model pengujian dan evaluasi model serta pengembangan system deteksi penyakit daun kakao ditunjukkan pada gambar 3.1. gambar 3.1 tahapan penelitian 3.2 akuisisi data penyakit daun tanaman kakao pengumpulan citra penyakit daun tanaman kakao dikumpulkan secara langsung oleh peneliti data primer dan juga menggunakan data yang dikumpulkan oleh peneliti lain data sekunder. terdapat 4 kelas penyakit dan satu kelas daun sehat yang akan digunakan dalam penelitian ini yaitu daun sehat penyakit antraknosa colletotrichum gloeosporioides penyakit vascular streak dieback vsd penyakit leaf blotch dan penyakit cocoa swollen shoot virus disease cssvd. 46 gambar 3.2 contoh 4 jenis penyakit daun tanaman kakao dataset primer akan dilakukan pengambilan foto penyakit daun tanaman kakao yang terdapat pada kebun kakao di daerah kabupaten solok provinsi sumatra barat. pengambilan akan dilakukan dari jarak 20cm dari kamera yang bertujuan menangkap detail kecil seperti bercak kecil atau lesi pada daun perubahan warna serta tekstur permukaan daun. dataset sekunder meng gunakan dataset yang telah digunakan u mum oleh para peneliti lain terkait penyakit daun tanaman kakao. 3.3 preprocessing 3.3.1 resiz e dataset perubahan ukuran citra dilakukan menggunakan metode nearest neighbor interpolation . cara kerja dari metode ini dengan cara mengambil nilai piksel terdekat dari citra asli untuk menentukan nilai piksel baru dalam citra yang akan diubah ukurannya. citra diubah ukurannya menjadi seragam 224x224 piksel . faktor skala dihitung dengan membandingkan dimensi citra asli dimana 𝑊𝐻 dengan dimensi baru 𝑊𝐻 yang akan diubah. untuk setiap piksel dalam citra baru dengan koordinat 𝑖𝑗 hitung koordinat terdekat di citra asli 𝑖𝑗. selanjutnya map nilai piksel yaitu mengambil nilai piksel dari citra asli pada koordinat 𝑖𝑗 dan menetapkan nilai ke piksel baru di koordinat 𝑖𝑗 dalam citra yang diubah ukurannya. 3.3.2 grayscale pada tahap ini citra rgb dikonversi ke grayscale untuk membantu menyederhanakan dan.memfokuskan informasi intensitas cahaya yang lebih relevan. gejala penyakit pada daun tanaman kakao seperti perubahan warna bi ntikbintik atau 47 nekrosis dapat lebih mudah diindetifikasi melalui variasi intensitas cahaya . grayscale dapat mempertahankan informasi penting dengan lebih sederhana. 3.3.3 augmentasi data set augmentasi data dilakukan untuk meningkatkan variasi pada data set yang akan digunakan serta untuk mencegah terjadinya overfitting. teknik augmentasi yang diterapkan penelitian ini seperti rotasi flipping zooming dan cropping . 3.3.4 ekstraksi fitur 3.3.4.1 histogram oriented of gradients hog ekstraksi fitur hog digunakan dalam penelitian ini untuk menangkap bentuk dan tekstur . hog berfokus pada gradien intensit as lokal dan arah tepi yang menggambarkan struktur dan tekstur dari daun yang terkena penyakit pada tanaman kakao hog dapat menangani perubahan dalam rotasi dan skala yang memungkinkan pendeteksian penyakit y ang konsisten pada pengambilan gambar dari sudut atau jarak yang berbeda. 3.3.4.2 local binary pattern lbp penerapan e kstraksi fitur lbp pada penelitian ini untuk menangkap tekstur lokal dalam citra. lbp membantu dalam menangkap informasi tekstur seperti berca k bercak lubang kecil yang terdapat pada daun perubahan wa rna daun yang tidak merata dan perubahan permukaan lainnya. 3.3.5 splitting data 3.3.5.1 data training data training digunakan untu melatih model untuk mengenali pola maupun karakteristik visual yang membedakan daun sehat dengan daun yang terinfeksi 48 penyakit. melalui proses pelatihan ini model mengoptimalkan parameter untuk memini malkan kesalahan dalam memprediksi. 3.3.5.2 data testing data testing digunakan untuk melakukan pengujian pada model yang telah dilatih sebelumnya untuk mengevaluasi kinerja model . 3.4 pengembangan dan pelatihan model data citra daun kakao yang telah melalui preprocessing dan ekstraksi fitur kemudian digunakan untuk pelatihan dan pembuatan model deep learning menggunakan pendekatan feature fusion berbasis attention yaitu fitur ekstraksi hog dan lbp digabungkan ke dalam vision transformer yang menggunakan attention mechanism . attention mechanism dalam vision transformer memberikan fokus yang berbeda pada fitur hog dan lbp . penggunaan attention mechanism dapat meningkatkan akurasi model dengan mengurangi pengaruh noise atau informasi yang tidak relevan dalam gambar. gambar 3.3 pengembangan dan pelatihan model 3.5 pengujian dan evaluasi model pengujian dan evaluasi model dilakukan untuk me lihat akurasi model saat mengidentifikasi penyakit daun tanaman kakao. proses evaluasi dimulai dengan pengujian yang terdiri dari data yang belum pernah dilihat oleh model selama melakukan fase pelatihan . matrik evaluasi digunakan untuk mengukur kinerja model 49 secara menyeluruh . matrik evaluasi yang digunakan seperti akurasi presisi recall dan f1score. gambar 3. 4 pengujian dan evaluasi model 3.6 pengembangan sistem deteksi penyakit daun kakao setelah melakukan pelatihan dan pengembangan model serta tahap pengujian dan evaluasi model system deteksi untuk penyakit daun kakao diimplemetasikan dengan melibatkan pengintegrasian model ke dalam aplikasi atau perang kat keras. pengembangan system menciptakan solusi yang efektif dan efisien dalam mengidentifikasi penyakit daun kakao. 50 gambar 3. 5 alur identifikasi penyakit tanaman kakao,3.1 tahapan penelitian secara garis besar penelitian ini terdiri dari beberapa tahapan yaitu akuisisi data preprocessing data pengembangan dan pelatihan model pengujian dan evaluasi model serta pengembangan system deteksi penyakit daun kakao ditunjukkan pada gambar 3.1. gambar 3.1 tahapan penelitian 3.2 akuisisi data penyakit daun tanaman kakao pengumpulan citra penyakit daun tanaman kakao dikumpulkan secara langsung oleh peneliti data primer dan juga menggunakan data yang dikumpulkan oleh peneliti lain data sekunder. terdapat 4 kelas penyakit dan satu kelas daun sehat yang akan digunakan dalam penelitian ini yaitu daun sehat penyakit antraknosa colletotrichum gloeosporioides penyakit vascular streak dieback vsd penyakit leaf blotch dan penyakit cocoa swollen shoot virus disease cssvd. 46 gambar 3.2 contoh 4 jenis penyakit daun tanaman kakao dataset primer akan dilakukan pengambilan foto penyakit daun tanaman kakao yang terdapat pada kebun kakao di daerah kabupaten solok provinsi sumatra barat. dataset sekunder meng gunakan dataset yang telah digunakan u mum oleh para peneliti lain terkait penyakit daun tanaman kakao. 3.4 pengembangan dan pelatihan model data citra daun kakao yang telah melalui preprocessing dan ekstraksi fitur kemudian digunakan untuk pelatihan dan pembuatan model deep learning menggunakan pendekatan feature fusion berbasis attention yaitu fitur ekstraksi hog dan lbp digabungkan ke dalam vision transformer yang menggunakan attention mechanism . penggunaan attention mechanism dapat meningkatkan akurasi model dengan mengurangi pengaruh noise atau informasi yang tidak relevan dalam gambar. gambar 3.3 pengembangan dan pelatihan model 3.5 pengujian dan evaluasi model pengujian dan evaluasi model dilakukan untuk me lihat akurasi model saat mengidentifikasi penyakit daun tanaman kakao. 4 pengujian dan evaluasi model 3.6 pengembangan sistem deteksi penyakit daun kakao setelah melakukan pelatihan dan pengembangan model serta tahap pengujian dan evaluasi model system deteksi untuk penyakit daun kakao diimplemetasikan dengan melibatkan pengintegrasian model ke dalam aplikasi atau perang kat keras. pengembangan system menciptakan solusi yang efektif dan efisien dalam mengidentifikasi penyakit daun kakao. 5 alur identifikasi penyakit tanaman kakao
Riezka Yunistika Fajriatifah_Kualifikasi.txt,"Dalam disertasi ini diusulkan metode klasifikasi anomali paru akibat Covid- 19 pada citra sinar-X paru yang terbagi ke dalam tiga tahap utama, seperti yang ditunjukkan pada Gambar 3.1. Gambar 3,1 menggambarkan peta penelitian yang terdiri dari tiga tahap utama: pelatihan model tunggal, ensemble learning dengan metode stacking dan implementasi model. Tahap pertama terdiri dari beberapa langkah, dimulai dari preprocessing dataset. Pada tahap preprocessing dilakukan augmentasi citra untuk meningkatkan variasi data pada dataset. Tahap selanjutnya adalah dataset splitting dengan membagi dataset menjadi tiga subset: dataset pelatihan, dataset validasi, dan dataset pengujian. Tahapan selanjutnya adalah pembuatan dan pelatihan model. Proses pembuatan dan pelatihan model memanfaatkan teknik transfer learning pada beberapa arsitektur (ConvNext, DenseNet, Vgg19, ResnetV2_50 dan Xception). Teknik transfer learning bertujuan untuk mempercepat proses pelatihan dengan mentransfer bobot dari model yang sudah dilatih sebelumnya. Proses pelatihan juga memanfaatkan teknik penjadwalan learning rate, agar model dapat mencapai nilai gradient optimal sehingga mendapatkan performa model terbaik dari proses pelatihan. Tahap berikutnya adalah mengevaluasi model yang telah dilatih menggunakan dataset uji yang tidak digunakan selama proses pelatihan. Evaluasi model dilakukan dengan mengukur performanya menggunakan classification metric seperti accuracy, precision, recall, danfl-score. Hasil dari proses ini berupa beberapa model terbaik. Hasil prediksi dari beberapa model ini disimpan untuk digunakan pada tahap kedua. Tahap kedua dalam penelitian ini adalah ensemble learning dengan metode stacking. Pada tahap ini hasil prediksi dari berbagai model tunggal yang telah dilatih sebelumnya dikumpulkan. Hasil prediksi ini mencakup probabilitas atau kelas yang diprediksi oleh masing-masing model tunggal untuk setiap sampel dalam dataset. Hasil prediksi yang dikumpulkan digunakan untuk membentuk dataset baru. Dataset baru ini terdiri dari fitur-fitur yang merupakan prediksi dari model-model tunggal sebelumnya, serta label asli dari dataset. Dataset baru ini bertujuan untuk merepresentasikan pola dan kesalahan yang dihasilkan oleh model- model tunggal. Dataset baru yang telah dibuat kemudian digunakan untuk melatih meta-model. Meta-model bertujuan untuk belajar dari kesalahan dan pola prediksi yang dihasilkan oleh model-model tunggal. Meta-model akan menggabungkan prediksi dari berbagai model tunggal untuk menghasilkan prediksi akhir yang lebih 
akurat. Setelah meta-model dilatih, dilakukan evaluasi model untuk memastikan kinerjanya. Langkah selanjutnya adalah tahap inferensi model, yang bertujuan untuk menerapkan model yang telah dilatih pada data baru agar dapat membuat prediksi. Proses inferensi ini menggunakan layanan web API (Application Programming Interface) berbasis web yang akan dimuat dalam sebuah gambar Docker, sehingga mempermudah proses penyebaran (deployment) pada perangkat dengan berbagai lingkungan yang berbeda. 3.1 Dataset COVID-19 Radiography
       Dataset COVID-19 Radiography adalah kumpulan data citra sinar-X paru- paru. Dataset ini terdiri dari 21165 citra sinar-X paru yang dikategorikan menjadi 4 kelas seperti: 3616 citra sinar-X positif Covid-19, 10119 citra normal, 6012 citra lung opacity dan 1345 citra pneumonia virus. Gambar 3.2 menunjukkan beberapa citra sinar-X paru COVID-19, viral pneumonia, lung opacity dan normal yang disediakan oleh database Radiografi COVID-19. Dataset Radiograph COVID-19 dapat diakses di platform Kaggle pada url COVID-19 Radiography Database . Gambar 3.2 di atas merupakan contoh sampel citra dari dataset Radiograph COVID-19. Citra sinar-X Normal memiliki paru-paru yang bersih tanpa bintik- bintik putih, yang menandakan bahwa paru-paru tidak mengalami peradangan atau infeksi. Citra sinar-X COVID-19 memiliki ciri khas berupa bintik-bintik putih, yang menunjukkan adanya cairan dan menandakan infeksi pada paru-paru. Citra sinar-X Lung Opacity menunjukkan ciri khas berupa bintik-bintik putih keabu- abuan. Pada citra sinar-X Viral Pneumonia, terdapat bintik-bintik putih pada bagian atas paru-paru yang menunjukkan infeksi pada saluran pernapasan atas. 3.2 Preprocessing Dataset
       Sebelum digunakan dalam model deep learning, dataset yang telah disiapkan perlu diolah terlebih dahulu melalui tahap preprocessing. Tahap preprocessing pertama yang dilakukan adalah augmentasi citra untuk memperkaya dataset dengan berbagai variasi citra. Tahap selanjutnya adalah mengubah ukuran (resize) semua citra dalam dataset agar sesuai dengan kebutuhan model. Tahap terakhir adalah normalisasi citra, dimana nilai piksel setiap citra diubah ke rentang 0 hingga 1. Memperlihatkan tahapanpreprocessing dataset yang digunakan dalam penelitian ini. Pada tahap preprocessing dataset, dilakukan augmentasi citra dengan memutar citra-citra dalam dataset untuk menghasilkan variasi gambar yang terotasi. Hasil augmentasi citra ini kemudian diubah ukurannya menjadi dimensi yang sama. Selanjutnya, tipe data citra diubah ke format torch.Tensor agar dapat diproses oleh GPU. Tahap selanjutnya adalah normalisasi citra menggunakan nilai rata-rata dan standar deviasi dari dataset ImageNet, penggunaan nilai tersebut memiliki tujuan agar hasil normalisasi sama dengan proses pelatihan bobot untuk transfer learning model. Proses preprocessing pada dataset pelatihan dilakukan menggunakan algoritma 3.1. Algoritma 3.1 Proses Preprocessing pada Dataset
Input:
Citra dari dataset COVID-19 Radiography
Output:
Citra yang telah dipreprocessing
Proses:
1. Mulai. 2. Melakukan random rotation pada citra menggunakan fungsi Random Rotation() dari PyTorch dengan parameter rotation_range sebesar 10. 3. Mengubah ukuran citra menjadi 224x224 menggunakan fungsi Resize() dengan parameter cfg['model']['input_size']. 4. Mengonversi citra menjadi tensor menggunakan fungsi ToTensor(). 5. Melakukan normalisasi citra menggunakan nilai mean dan std dari dataset ImageNet yang diberikan, yaitu mean=[0.485, 0.456, 0.406] dan std=[0.229, 0.224, 0.225] menggunakan fungsi Normalize(). 6. Selesai. Algoritma 3.1 menggambarkan langkah-langkah preprocessing yang dilakukan pada dataset. Langkah pertama adalah melakukan augmentasi citra menggunakan RandomRotation() dengan parameter rotation_range sebesar 10 derajat. Langkah selanjutnya adalah mengubah ukuran citra dengan nilai parameter (224, 224) menggunakan fungsi Resize(). Setelah itu, tipe data diubah menjadi torch.Tensor dengan fungsi ToTensor() untuk persiapan pengolahan pada perangkat. Terakhir, gambar dinormalisasi dengan fungsi Normalize() menggunakan nilai mean dan standard deviasi dari dataset ImageNet, yaitu [0.485, 0.456, 0.406] dan [0.229, 0.224, 0.225]. 3.3 Dataset Splitting
      Dataset Splitting merupakan proses membagi dataset menjadi beberapa subset. Pada penelitian ini dataset dibagi menjadi 3 subset: yaitu dataset pelatihan, dataset validasi, dan dataset pengujian. Dataset pelatihan dan dataset validasi digunakan untuk melatih model deep learning. Dataset latih digunakan untuk model dalam mengekstraksi fitur-fitur dari data tersebut. Fitur fitur yang diekstraksi kemudian divalidasi menggunakan dataset validasi. Dataset pengujian adalah subset yang digunakan untuk menguji kinerja model setelah proses pelatihan selesai. Proses pembagian dataset dapat dilihat pada gambar 3.3
       Gambar 3.3 menggambarkan tahapan dataset splitting. Proses pembagian dilakukan dengan membagi total dataset menjadi 80% untuk dataset pelatihan, 10% untuk dataset validasi dan 10% untuk dataset pengujian. Proses ini memastikan bahwa sebagian besar data digunakan untuk melatih model, sementara sisanya digunakan untuk memvalidasi performa model. Langkah berikutnya adalah membagi dataset menjadi beberapa batch pelatihan. Pembagian ini dilakukan untuk mengurangi beban pelatihan yang berat dan validasi hasil pelatihan dapat dilakukan secara perbatch. Algoritma 3.2 Proses Pemuatan Data dan Pembagian ke Batch Pelatihan, Validasi, dan Pengujian
Input:
Dataset pelatihan (train_set)
Dataset validasi (val_set)
Dataset uji (test_set)
Ukuran batch untuk dataset pelatihan (train_bs) 
Ukuran batch untuk dataset uji (test_bs)
Output:
DataLoader untuk dataset pelatihan (train_dl) 
DataLoader untuk dataset validasi (val_dl) 
DataLoader untuk dataset uji (test_dl)
Proses:
1. Mulai
2. Pemuatan Data Pelatihan:
� Atur kwargs dengan batch_size=self.train_bs, shuffle=True, num_workers=2. � Buat DataLoader untuk train_set dengan pin_memory=True dan kwargs. � Simpan DataLoader ke dalam train_dl. 3. Pemuatan Data Validasi:
� Atur kwargs dengan batch_size=self.train_bs, shuffle=True, num_workers=2. � Buat DataLoader untuk val_set dengan pin_memory=True dan kwargs. � Simpan DataLoader ke dalam val_dl. 4. Pemuatan Data Uji:
� Atur kwargs dengan batch_size=test_bs, shuffle=True, num_workers=2. � Buat DataLoader untuk test_set dengan pin_memory=True dan kwargs. � Simpan DataLoader ke dalam test_dl. 5. Kembalikan train_dl, val_dl, dan test_dl sebagai output. 6. Selesai. Algoritma di atas mengimplementasikan proses pemuatan dataset dan pembagian data ke dalam batch pelatihan. Setiap batch memiliki jumlah citra sesuai dengan nilai batch size yang ditentukan. Proses pemuatan ini juga mencakup pengacakan dataset untuk setiap batch size, sehingga variasi citra dalam setiap batch pelatihan menjadi lebih beragam. Selain itu, pengaturan parameter 'pin_memory=True' diaktifkan dengan tujuan untuk melakukan caching memory, sehingga proses pelatihan lebih cepat karena dataset hanya dimuat pada iterasi pertama dari setiap batch pelatihan. 3.4 Pembuatan dan Pelatihan Model
3.4.1 Model ConvNeXt
       Model ConvNeXt adalah salah satu variasi dari arsitektur Convolutional Neural Network (CNN). Model ini mengadopsi dasar Residual Network dan memperkenalkan empat tahapan proses konvolusi. Salah satu fitur utama dari ConvNeXt adalah penggunaan layer patchify yang mengimplementasikan konvolusi 7x7 dengan stride 2, diikuti oleh layer max-pooling. Selain itu, arsitektur ini menggunakan fungsi aktivasi GELU pada blok ConvNeXt, yang sering digunakan dalam model berbasis transformer. Model ConvNeXt juga menerapkan Layer Normalization (LN), yang merupakan perbedaan signifikan dengan model berbasis CNN lainnya. Contoh blok ConvNeXt dapat dilihat pada Gambar di bawah ini. Model ConvNeXt memiliki beberapa varian ukuran, yaitu tiny, small, base, large, dan xlarge. Ukuran model ditentukan oleh jumlah blok ConvNeXt yang digunakan. Semakin besar modelnya, semakin banyak blok yang digunakan. Pada penelitian ini, model ConvNeXt yang digunakan adalah ukuran base, yang terdiri dari 4 stage dengan komponen blok ConvNeXt sebanyak [3, 3, 9, 3], dan ukuran dimensi fitur untuk setiap stage adalah [96, 192, 384, 768]. 3.4.2 Transfer Learning
       Transfer Learning adalah proses dimana model dilatih pada sebuah dataset baru. Proses ini memanfaatkan model yang telah dilatih sebelumnya pada dataset lain (model pre-trained). Proses ini diawali dengan mengubah lapisan klasifikasi atau lapisan output agar sesuai dengan jumlah kelas yang ada pada dataset baru, yaitu 4. Langkah berikutnya adalah mentransfer bobot dari lapisan ekstraksi fitur, yang bertujuan untuk menginisialisasi model berdasarkan fitur-fitur yang telah dipelajari sebelumnya pada dataset ImageNet. Tujuan dari transfer learning adalah untuk mempercepat proses pelatihan dengan memanfaatkan bobot dari model pretrained. Implementasi proses transfer learning dapat dilihat pada Algoritma di bawah ini. Algoritma 3.3 Proses Implementasi Transfer Learning
Input:
Pretrained (boolean): Pilihan untuk menggunakan bobot yang sudah dilatih sebelumnya dari dataset ImageNet. num_classes (int): Jumlah kelas keluaran dari model. x (torch.Tensor): Batch dari Tensor Input. Output:
Probabilitas kelas setelah softmax (torch.Tensor). Proses:
1. Mulai
2. Inisialisasi Model
� Inisialisasi kelas ConvNext dengan argumen pretrained dan output_class. � Buat model convnext_base menggunakan paket timm dengan bobot pretrained dan jumlah num_classes sesuai output_class. 3. Metode Forward Pass
� Terima input tensor x. � Proses input melalui model convnext_base. � Terapkan fungsi softmax pada output untuk mendapatkan probabilitas kelas. 4. Metode Freeze Weights
� Iterasi melalui semua parameter di feature extractor dan set 
requires_grad ke False untuk membekukan bobot. � Iterasi melalui semua parameter di classifier dan set requires_grad 
ke True agar dapat dilatih. 5. Metode Unfreeze Weights
� Iterasi melalui semua parameter di feature extractor dan set 
requires_grad ke True untuk membuka pembekuan bobot. 6. Selesai
       Kelas ConvNeXt pada Algoritma di atas memiliki 4 buah method yaitu init, forward, freeze, dan unfreeze. Method init adalah sebuah fungsi yang bertujuan melakukan inisiasi model sesuai dengan inputan parameter yang ada, yaitu pretrained dan num_classes. Parameter pretrained merupakan sebuah inputan boolean agar model dapat melakukan proses inisiasi dengan menggunakan bobot pra-pelatihan yang tersedia dari pytorch, sedangkan parameter num_classes merupakan parameter yang bertujuan untuk menginisasi jumlah kelas output pada model. Method forward adalah sebuah fungsi yang menerima parameter x berupa torch.Tensor dan akan dilakukan proses forward pass pada model untuk melakukan prediksi pada model. Method freeze memiliki tujuan untuk membekukan bobot agar tidak terbaharui pada proses backpropagation, sedangkan method unfreeze memiliki tujuan untuk memasukan parameter model dan melakukan pembaharuan bobot pada proses backpropagation. Model yang telah dibuat akan digunakan pada proses pelatihan model. 3.5 Pelatihan Model
       Proses pelatihan model pada penelitian ini menerapkan teknik transfer learning yang terdiri dari dua tahap, yaitu tahap adaptation dan fine tuning. Pada tahap adaotation, pelatihan model hanya dilakukan pada lapisan klasifikasi dan lapisan output. Tujuan tahap ini adalah untuk menginisialisasi dan menyesuaikan bobot pada lapisan-lapisan tersebut. Tahap adaptation dalam proses pelatihan menggunakan nilai learning rate yang tinggi sehingga model dapat dengan cepat mencapai titik optimal gradien untuk memperoleh bobot yang tepat pada lapisan klasifikasi dan lapisan output. Tahap ini berperan penting dalam proses pengklasifikasian suatu hasil feature extraction menjadi suatu output prediksi. Proses fine tuning adalah tahap pelatihan model secara menyeluruh yang mencakup lapisan feature extraction dan lapisan klasifikasi. Pada tahap fine tuning, nilai learning rate yang kecil digunakan agar bobot pra-pelatihan model tidak mengalami perubahan signifikan yang dapat mengakibatkan hasil pelatihan model kurang baik. Selain itu, penggunaan nilai learning rate yang kecil membantu dalam mencapai konvergensi yang stabil. Dalam penelitian ini, teknik penjadwalan learning rate berbasis Cosine Learning Rate Decay diterapkan untuk mengoptimalkan proses pelatihan baik pada tahap adaptation maupun fine tuning. Gambar 3.8 merupakan rancangan alur pelatihan dalam penelitian ini. Alur pelatihan model terbagi menjadi 3 tahapan yakni proses inisialisasi, proses pelatihan dan proses log hasil pelatihan. Tahap pertama merupakan inisialisasi proses pelatihan berdasarkan dataset dan config yang diinput. Langkah pertama dalam inisialisasi adalah mengatur ulang bobot fungsi loss berdasarkan distribusi dataset untuk mengatasi masalah ketidakseimbangan data. Proses perhitungan bobot menggunakan rumus total data dibagi total kelas dikali total data pada kelas tersebut. Langkah selanjutnya adalah proses mengubah bobot menjadi tensor dan memindahkan bobot ke hardware yang digunakan. Bobot yang telah dipindahkan ke hardware kemudian akan dimasukan ke dalam fungsi CrossEntropyLoss untuk digunakan pada proses pelatihan model. Proses inisialisasi kedua adalah inisialisasi optimizer berdasarkan pilihan yang diinginkan. Proses inisiasi optimizer yang menggunakan fungsi get_optimizer() merupakan fungsi yang memiliki tujuan untuk mengembalikan inisiasi optimizer berdasarkan inputan konfigurasi yang ada. Proses berikutnya adalah initialisasi learning rate scheduler yang berfungsi untuk mengubah nilai learning rate secara adaptif selama proses pelatihan berlangsung. Proses learning rate scheduler memanfaatkan Cosine Learning Rate Decay dengan pengulangan. Proses kedua adalah proses pelatihan model ConvNeXt yang meliputi pelatihan, proses transfer learning model, dan juga proses penghitungan fungsi loss dan metrik performa. Proses melatih model akan dilakukan oleh komponen Trainer yang akan memproses pelatihan model ConvNeXt menggunakan Optimizer dan juga EarlyStopCallback. Proses pelatihan bagian adaptation akan dilakukan sebanyak 2 epoch pertama untuk mendapatkan bobot pada classification layer. Proses pelatihan bagian fine tuning akan dilakukan sebanyak total epoch dikurangi 2, contoh jika total epoch sama dengan 25, maka model akan melakukan proses pelatihan fine tuning sebanyak 23 epoch. Proses validasi performa model merupakan proses dimana hasil pelatihan model tiap peoch dilakukan validasi pada data validasi yang telah disiapkan sebelumnya dengan cara mengukur nilai loss dan juga performa metric yang meliputi accuracy, recall, precision, dan f1-score. Hal ini bertujuan agar model dapat memperbaharui nilai bobot pengetahuan dengan cara melakukan backpropagation berdasarkan nilai loss dan akan menyimpan file model terbaik berdasarkan hasil validasi metrik performa pada data validasi. Proses ketiga adalah log hasil pelatihan model yang meliputi menyimpan hasil output model dan juga log pelatihan menggunakan Comet logger. Comet logger akan menyimpan nilai hasil pelatihan seperti loss, learning rate, dan metrik performa pada platform CometML untuk visualisasi grafik pelatihan model. Proses penyimpanan log ini juga bertujuan agar dapat membandingkan berbagai hasil pelatihan dengan konfigurasi berbeda sehingga mendapatkan model dengan performa terbaik berdasarkan metrik performa pada data validasi. 3.7 Evaluasi Model
       Evaluasi Model adalah tahap di mana model yang telah dilatih dievaluasi menggunakan dataset uji. Tujuan dari pengujian ini adalah untuk mengevaluasi kinerja model pada data yang belum pernah dipelajari sebelumnya. Hasil pengujian model akan diukur menggunakan beberapa metrik performa, seperti akurasi, presisi, recall, dan fl-score. Gambar 3.9 merupakan alur pengujian model dalam penelitian ini. Pengujian model dimulai dengan input berupa dataset pengujian dan model yang telah dilatih sebelumnya. Semua model yang telah dilatih sebelumnya diuji menggunakan dataset ini untuk membandingkan performa tiap model pada dataset yang sama. Proses berikutnya adalah perhitungan metrik performa yang terdiri dari accuracy, precision, recall, dan Fl-score. Penggunaan metrik accuracy memiliki tujuan untuk mengetahui tingkat keakuratan prediksi yang dibuat oleh model terhadap data uji. Metrik ini dihitung dengan mengambil jumlah total prediksi yang benar baik true positives (kasus positif yang diprediksi dengan benar) dan true negatives (kasus negatif yang diprediksi dengan benar) dan membaginya dengan jumlah total sampel dalam dataset. Metrik precision mengukur ketepatan sebuah model dalam memprediksi label kelas positif. Precision merupakan proporsi dari true positives terhadap jumlah keseluruhan prediksi yang dinyatakan sebagai positif oleh model, termasuk false positives. Metrik recall bertujuan untuk mengukur seberapa akurat model dalam mengidentifikasi semua kasus positif yang ada dalam suatu kelas tertentu. Recall dihitung dengan membandingkan jumlah prediksi yang benar-benar positif yang berhasil diidentifikasi oleh model dengan jumlah total kasus positif sebenarnya dalam kelas tersebut. Penggunaan F1-score bertujuan untuk menilai keseimbangan antara precision dan recall dalam performa model klasifikasi. Metrik ini sangat penting untuk menghindari permasalahan di mana model memiliki precision tinggi tapi recall rendah, atau sebaliknya. 3.8 Metode Stacking
      Stacking atau Stacked generalization merupakan teknik ensemble learning yang mengintegrasikan prediksi dari beberapa model untuk menghasilkan prediksi yang lebih akurat. Stacking berbeda dari metode ensemble lain seperti bagging dan bosting. Fokus utama model stacking adalah menggabungkan kekuatan model yang berbeda melalui model meta. Komponen utama metode stacking terbagi menjadi 2, yaitu model dasar dan meta-model. Model dasar adalah beberapa model tunggal yang masing masing dilatih menggunakan dataset yang sama. Output dari setiap model dasar, yang merupakan prediksi digunakan sebagai input untuk meta-model. Meta-model adalah model yang dilatih pada prediksi yang dihasilkan oleh model dasar. Model ini berfungsi untuk mempelajari bagaimana prediksi dari model dasar dapat digabungkan untuk menghasilkan prediksi akhir yang lebih akurat. Proses implementasi metode stacking dalam penelitian ini terdiri dari 3 tahap. Tahap pertama, hasil prediksi dari beberapa model dasar dikumpulkan. Hasil prediksi yang dikumpulkan dari model-model dasar kemudian digunakan untuk membentuk dataset baru. Setelah dataset baru dibuat, yang terdiri dari prediksi dari model-model dasar sebagai fitur, langkah selanjutnya adalah pelatihan meta-model. 3.9 Evaluasi Model Stacking
       Pengujian model stacking dalam penelitian ini dilakukan seperti pengujian pada model tunggal. Pengujian dilakukan dengan menghitung metrik akurasi seperti accuracy, precision, recall, dan Fl-score. 3.10 Inference dan Deployment
       Inference dan Deployment adalah tahap di mana model terbaik yang telah melalui proses pengujian digunakan untuk membuat prediksi pada data baru. Inference dilakukan melalui sebuah API yang dibuat menggunakan framework Flask untuk menjalankan model. API yang dibuat menerima file citra X-ray dada sebagai input. Output dari API ini adalah hasil prediksi yang mencakup nama kelas dan confidence score untuk citra tersebut. confidence score ini mengindikasikan tingkat keyakinan model terhadap prediksinya. Tahapan deployment merupakan tahapan proses mengepak API yang telah dibuat ke dalam satu environment yang siap dijalankan. Proses pengepakan ini menggunakan teknologi Docker yang memiliki tujuan mengisolasi environment yang digunakan oleh API dengan environment yang ada pada mesin utama. Hal ini juga memudahkan proses deployment karena tahapan installasi semua komponen seperti library, framework, dan interpreter yang digunakan telah diinisiasi dalam bentuk docker image.","Dalam disertasi ini diusulkan metode klasifikasi anomali paru akibat Covid- 19 pada citra sinar-X paru yang terbagi ke dalam tiga tahap utama, seperti yang ditunjukkan pada Gambar 3.1. Gambar 3,1 menggambarkan peta penelitian yang terdiri dari tiga tahap utama: pelatihan model tunggal, ensemble learning dengan metode stacking dan implementasi model. Pada tahap preprocessing dilakukan augmentasi citra untuk meningkatkan variasi data pada dataset. Tahapan selanjutnya adalah pembuatan dan pelatihan model. Proses pembuatan dan pelatihan model memanfaatkan teknik transfer learning pada beberapa arsitektur (ConvNext, DenseNet, Vgg19, ResnetV2_50 dan Xception). Teknik transfer learning bertujuan untuk mempercepat proses pelatihan dengan mentransfer bobot dari model yang sudah dilatih sebelumnya. Proses pelatihan juga memanfaatkan teknik penjadwalan learning rate, agar model dapat mencapai nilai gradient optimal sehingga mendapatkan performa model terbaik dari proses pelatihan. Tahap berikutnya adalah mengevaluasi model yang telah dilatih menggunakan dataset uji yang tidak digunakan selama proses pelatihan. Evaluasi model dilakukan dengan mengukur performanya menggunakan classification metric seperti accuracy, precision, recall, danfl-score. Hasil dari proses ini berupa beberapa model terbaik. Hasil prediksi dari beberapa model ini disimpan untuk digunakan pada tahap kedua. Pada tahap ini hasil prediksi dari berbagai model tunggal yang telah dilatih sebelumnya dikumpulkan. Hasil prediksi ini mencakup probabilitas atau kelas yang diprediksi oleh masing-masing model tunggal untuk setiap sampel dalam dataset. Hasil prediksi yang dikumpulkan digunakan untuk membentuk dataset baru. Dataset baru ini terdiri dari fitur-fitur yang merupakan prediksi dari model-model tunggal sebelumnya, serta label asli dari dataset. Dataset baru ini bertujuan untuk merepresentasikan pola dan kesalahan yang dihasilkan oleh model- model tunggal. Meta-model bertujuan untuk belajar dari kesalahan dan pola prediksi yang dihasilkan oleh model-model tunggal. Meta-model akan menggabungkan prediksi dari berbagai model tunggal untuk menghasilkan prediksi akhir yang lebih 
akurat. 3.1 Dataset COVID-19 Radiography
       Dataset COVID-19 Radiography adalah kumpulan data citra sinar-X paru- paru. Dataset ini terdiri dari 21165 citra sinar-X paru yang dikategorikan menjadi 4 kelas seperti: 3616 citra sinar-X positif Covid-19, 10119 citra normal, 6012 citra lung opacity dan 1345 citra pneumonia virus. Gambar 3.2 menunjukkan beberapa citra sinar-X paru COVID-19, viral pneumonia, lung opacity dan normal yang disediakan oleh database Radiografi COVID-19. Gambar 3.2 di atas merupakan contoh sampel citra dari dataset Radiograph COVID-19. Citra sinar-X Normal memiliki paru-paru yang bersih tanpa bintik- bintik putih, yang menandakan bahwa paru-paru tidak mengalami peradangan atau infeksi. Citra sinar-X COVID-19 memiliki ciri khas berupa bintik-bintik putih, yang menunjukkan adanya cairan dan menandakan infeksi pada paru-paru. Citra sinar-X Lung Opacity menunjukkan ciri khas berupa bintik-bintik putih keabu- abuan. Pada citra sinar-X Viral Pneumonia, terdapat bintik-bintik putih pada bagian atas paru-paru yang menunjukkan infeksi pada saluran pernapasan atas. Tahap selanjutnya adalah mengubah ukuran (resize) semua citra dalam dataset agar sesuai dengan kebutuhan model. Pada tahap preprocessing dataset, dilakukan augmentasi citra dengan memutar citra-citra dalam dataset untuk menghasilkan variasi gambar yang terotasi. Hasil augmentasi citra ini kemudian diubah ukurannya menjadi dimensi yang sama. Tahap selanjutnya adalah normalisasi citra menggunakan nilai rata-rata dan standar deviasi dari dataset ImageNet, penggunaan nilai tersebut memiliki tujuan agar hasil normalisasi sama dengan proses pelatihan bobot untuk transfer learning model. Pembagian ini dilakukan untuk mengurangi beban pelatihan yang berat dan validasi hasil pelatihan dapat dilakukan secara perbatch. 3.4 Pembuatan dan Pelatihan Model
3.4.1 Model ConvNeXt
       Model ConvNeXt adalah salah satu variasi dari arsitektur Convolutional Neural Network (CNN). Tahap ini berperan penting dalam proses pengklasifikasian suatu hasil feature extraction menjadi suatu output prediksi. Pada tahap fine tuning, nilai learning rate yang kecil digunakan agar bobot pra-pelatihan model tidak mengalami perubahan signifikan yang dapat mengakibatkan hasil pelatihan model kurang baik. Gambar 3.8 merupakan rancangan alur pelatihan dalam penelitian ini. Alur pelatihan model terbagi menjadi 3 tahapan yakni proses inisialisasi, proses pelatihan dan proses log hasil pelatihan. Proses validasi performa model merupakan proses dimana hasil pelatihan model tiap peoch dilakukan validasi pada data validasi yang telah disiapkan sebelumnya dengan cara mengukur nilai loss dan juga performa metric yang meliputi accuracy, recall, precision, dan f1-score. Hal ini bertujuan agar model dapat memperbaharui nilai bobot pengetahuan dengan cara melakukan backpropagation berdasarkan nilai loss dan akan menyimpan file model terbaik berdasarkan hasil validasi metrik performa pada data validasi. Proses ketiga adalah log hasil pelatihan model yang meliputi menyimpan hasil output model dan juga log pelatihan menggunakan Comet logger. Comet logger akan menyimpan nilai hasil pelatihan seperti loss, learning rate, dan metrik performa pada platform CometML untuk visualisasi grafik pelatihan model. Proses penyimpanan log ini juga bertujuan agar dapat membandingkan berbagai hasil pelatihan dengan konfigurasi berbeda sehingga mendapatkan model dengan performa terbaik berdasarkan metrik performa pada data validasi. Hasil pengujian model akan diukur menggunakan beberapa metrik performa, seperti akurasi, presisi, recall, dan fl-score. Penggunaan metrik accuracy memiliki tujuan untuk mengetahui tingkat keakuratan prediksi yang dibuat oleh model terhadap data uji. Recall dihitung dengan membandingkan jumlah prediksi yang benar-benar positif yang berhasil diidentifikasi oleh model dengan jumlah total kasus positif sebenarnya dalam kelas tersebut. Penggunaan F1-score bertujuan untuk menilai keseimbangan antara precision dan recall dalam performa model klasifikasi. 3.8 Metode Stacking
      Stacking atau Stacked generalization merupakan teknik ensemble learning yang mengintegrasikan prediksi dari beberapa model untuk menghasilkan prediksi yang lebih akurat. Model dasar adalah beberapa model tunggal yang masing masing dilatih menggunakan dataset yang sama. Output dari setiap model dasar, yang merupakan prediksi digunakan sebagai input untuk meta-model. Meta-model adalah model yang dilatih pada prediksi yang dihasilkan oleh model dasar. Model ini berfungsi untuk mempelajari bagaimana prediksi dari model dasar dapat digabungkan untuk menghasilkan prediksi akhir yang lebih akurat. Tahap pertama, hasil prediksi dari beberapa model dasar dikumpulkan. Hasil prediksi yang dikumpulkan dari model-model dasar kemudian digunakan untuk membentuk dataset baru. Setelah dataset baru dibuat, yang terdiri dari prediksi dari model-model dasar sebagai fitur, langkah selanjutnya adalah pelatihan meta-model. 3.9 Evaluasi Model Stacking
       Pengujian model stacking dalam penelitian ini dilakukan seperti pengujian pada model tunggal. 3.10 Inference dan Deployment
       Inference dan Deployment adalah tahap di mana model terbaik yang telah melalui proses pengujian digunakan untuk membuat prediksi pada data baru. Inference dilakukan melalui sebuah API yang dibuat menggunakan framework Flask untuk menjalankan model. API yang dibuat menerima file citra X-ray dada sebagai input. Output dari API ini adalah hasil prediksi yang mencakup nama kelas dan confidence score untuk citra tersebut. confidence score ini mengindikasikan tingkat keyakinan model terhadap prediksinya."
RizqiaCahyaningtyas_Kualifikasi.txt,"3.1. Tahapan Penelitian
Pada Gambar 3.1 memperlihatkan diagram tahapan penelitian yang dijadikan dasar 
pelaksanaan penelitian sesuai dengan rumusan dan tujuan penelitian yang telah dirumuskan 
di dalam BAB I. Diawali oleh pengamatan yang dilakukan pada laboratorium PLTS yang ada 
di Institut Teknologi PLN. pengamatan yang dilakukan adalah tentang kerusakan pada modul 
surya, yang dilanjutkan dengan akuisisi dan analisis dataset dimana data dataset didapatkan 
dari website Kaggle (www.kaggle.com) yang berjumlah 559 data yang berformat .jpg. dikumpulkan dan disiapkan sehingga sesuai dengan kebutuhan dataset. Dataset kemudian 
dibagi menjadi 3 yaitu data latih untuk kebutuhan pelatihan model, data validasi untuk proses 
validasi dan penyetelan model, data uji untuk evaluasi model yang akan menghitung 
confusion matrix guna mencari nilai akurasi, presisi, recall, dan hasil F1-score. 3.2 Identifikasi Masalah
Institut Teknologi PLN memiliki laboratorium Pembangkit Listrik Tenaga Surya (PLTS) yang memiliki kapasitas 12.4 kWp, yang dalam sehari energi yang dihasilkan sekitar 40 - 60 kWh. Dengan perhitungan 1 modul photovoltaic sebesar 260 WP dengan jumlah modul sebanyak 60. Dari 60 modul 12 unit dilepas menjadi 48 unit dikalikan dengan 260 WP sebesar 12.4 kWp. jenis modul surya yang digunakan adalah polychristalyn dengan Umur dari modul surya kisaran 20 tahun. Dengan batasan usia dari modul surya dan berbagai faktor yang memperngaruhi kondisi modul surya maka perlu dilakukan pemeliharaan rutin agar dapat terus beroperasi secara optimal. pemeliharaan modul surya di Laboratorium PLTS IT PLN dilakukan secara rutin seminggu sekali dengan cara dibersihkan permukaannya, diperiksa koneksi listrik modul surya, pemeliharaan bulanan untuk menjaga modul surya agar langsung terkena cahaya tanpa ada gangguan misalkan pohon dan lain-lain, dan memeriksa sistem monitoring modul surya. Untuk memudahkan dalam pemeliharaan modul surya perlu dibuatkan sistem untuk deteksi, klasifikasi dan identifikasi kerusakan modul surya yang sering terjadi, diantaranya karena faktor lingkungan, kerusan produksi, korosi dan penuaan alami. Oleh itu dalam penelitian ini akan di kategorikan kondisi modul surya dalam 4 kategori yaitu Clean, dusty , electrical damage, Physical damage. 3.3. Akuisisi dan Analisis Data
Dalam mengumpulkan dataset kerusakan pada modul surya diperoleh dari website yaitu www.kaggle.com , dengan dataset yang digunakan yaitu berupa citra atau gambar dari solar panel dalam bentuk gambar format .png atau .jpg. kerusakan pada modul surya diklasifikasikan menjadi 4 kelas yaitu Clean, dusty , electrical damage, Physical damage. Data pada Gambar 3.2 merupakan dataset yang diambil dari website Kaggle dimana dataset dibagi menjadi 6 kelas, yaitu kelas bird-drop yang berisi kumpulan data gambar modul surya yang terdapat kotoran burung diatas permukaannya sejumlah 192 file gambar, kelas clean yang berisi kumpulan modul surya yang bersih sejumlah 194 file gambar, kelas dusty yang berisikan kumpulan data gambar debu pada permukaan modul surya sejumlah 191 file gambar, kemudian kelas electrical damage yang berisikan kumpulan data gambar modul surya ketika bagian tertentu dari sel surya menjadi lebih panas daripada yang lain, biasanya disebabkan oleh bayangan atau cacat dalam sel, Ini dapat menyebabkan kerusakan permanen pada sel surya sejumlah 104 file. Selanjutnya kelas physical damage yang berisikan kumpulan data gambar yang terkena Guncangan, getaran, atau benturan fisik pada sel surya dapat merusak sel surya atau koneksi antara sel, yang dapat mengurangi produksi daya sejumlah 70 file gambar dan terdapat juga kelas snow Covered berisi kumpulan modul surya yang tertutup oleh salju sejumlah 124 file gambar Dari gambara diatas maka penelitian akan dibatasi menjadi 4 kategori yaitu modul surya yang Clean, dusty , electrical damage, Physical damage. Tabel 3.1 diatas menjelaskan bahwa klasifikasi pada citra kerusakan modul surya yang didapat berjumlah 194 citra untuk kelas clean, 191 citra untuk data pada kelas dusty, 104 citra untuk data pada kelas electrical damage, 70 citra untuk data pada kelas physical damage. Kemudian dilakukan perubahan ukuran gambar pada dataset disama ratakan, sehingga gambar yang tadinya memiliki ukuran yang lebih besar ataupun kecil dapat menjadi satu ukuran yang sama yaitu 300x300. Dikarenakan data gambar dari dataset kerusakan modul surya memiliki beragam orientasi dan ukuran gambar yang terlalu besar atau kecil sehingga diperlukan pemrosesan pada dataset supaya data memiIiki ukuran yang sama. Selanjutnya pemrosesan dataset dinormalisasi, dan di augmentasi dengan menentukan rotasi acak, rentang pergeseran horizontal acak dari lebar gambar, rentang pergeseran vertikal acak dari tinggi gambar, rentang peregangan acak dengan sudut acak yang akan ditentukan. rentang persebaran gambar acak diperbesar, dan pemutaran gambar secara horizontal) sehingga dari pemrosesan dataset tersebut dapat mempercepat proses pelatihan. 3.4. Pembentukan Dataset
     Total keseluruhan dataset adalah sebahanyak 559 data gambar. data latih diberikan komposisi yang lebih besar untuk pembelajaran agar model semakin banyak memiliki pembelajaran. Dataset dibagi ke dalam tiga jenis, yaitu 80% sejumlah 447 data latih yang berguna melatih model, 10% sejumah 55 data validasi yang berguna untuk menghasilkan loss function dari proses latihan, dan 10% sejumlah 55 data pengujian modeI sebagai penggunaan model pada dunia nyata yang mana data uji tidak diperlihatkan oleh model yang dilatih sebelumnya. 3.5. Arsitektur dan Model CNN (Convolutional Neural Network)
       Proses pemodelan dilakukan dalam beberapa kali percobaan dengan ukuran batch size yang berbeda sehingga diperoleh model klasifikasi jenis kerusakan modul surya dengan hasil akurasi terbaik. Dengan arsitektur dari pretrained model menggunakan inceptionV3, untuk menyesuaikan dengan dataset, convolutional layer dengan 64 filter, ukuran kernel 3x3, dan activation function ReLu. convolutional layer dengan 32 filter, ukuran kernel 3x3, dan activation function ReLu. Dengan pooling layer menggunakan global average. fully connected layer atau dense layer dengan 4 neuron (sesuai jumlah kelas) dan activation functionnya menggunakan softmax dan untuk loss functionnya menggunakan categorical crossentropy. optimizationnya menggunakan adam untuk memperbaharui bobot secara berulang yang didasarkan pada proses pelatihan dan penggunaan learning rate. 3.6. Evaluasi Model
     Evaluasi model dilakukan dengan menggunakan confusion matrix guna mencari nilai akurasi, presisi, recall, dan hasil F1-score. Hasilnya akan digunakan sebagai landasan pertimbangan perlu tidaknya dilakukan pelatihan ulang pada model. Akurasi menentukan seberapa baik model mampu melakukan prediksi jenis kerusakan modul surya dengan benar. Presisi adalah akurasi atau kecocokan antara data yang diminta dengan hasil prediksi yang diberikan oleh model yang sudah disusun oleh arsitektur CNN. Recall adalah kesuksesan model yang dibuat oleh arsitektur CNN dalam mendapatkan kembali sebuah data. Sedangkan F1 score adalah perbandingan rerata presisi dan recall yang dibobotkan. Pada percobaan model inceptionV3 digunakan sebanyak 559 dataset. 3.7. Klasifikasi dan Identifikasi kerusakan Modul Surya
       Pada tahapan terakhir dibangun sebuah prototype perangkat lunak aplikasi yang dapat mengklasifikasikan jenis kerusakan yang dilihat dari bentuk, ukuran serta pelebaran kerusakan pada modul surya sehingga dapat teridentifikasi tingkat kerusakan dan penanganannya. 3.8. Rencana Penelitian
     Untuk mencapai target penelitian, maka penulis menyusun rencana Penelitian berupa jadwal kegiatan yang berguna untuk memastikan agar capaian yang ditetapkan dapat dipenuhi sesuai waktu yang telah ditetapkan termasuk target luaran berupa 2 (dua) buah publikasi.","3.1. pengamatan yang dilakukan adalah tentang kerusakan pada modul 
surya, yang dilanjutkan dengan akuisisi dan analisis dataset dimana data dataset didapatkan 
dari website Kaggle (www.kaggle.com) yang berjumlah 559 data yang berformat .jpg. Dataset kemudian 
dibagi menjadi 3 yaitu data latih untuk kebutuhan pelatihan model, data validasi untuk proses 
validasi dan penyetelan model, data uji untuk evaluasi model yang akan menghitung 
confusion matrix guna mencari nilai akurasi, presisi, recall, dan hasil F1-score. 3.2 Identifikasi Masalah
Institut Teknologi PLN memiliki laboratorium Pembangkit Listrik Tenaga Surya (PLTS) yang memiliki kapasitas 12.4 kWp, yang dalam sehari energi yang dihasilkan sekitar 40 - 60 kWh. Untuk memudahkan dalam pemeliharaan modul surya perlu dibuatkan sistem untuk deteksi, klasifikasi dan identifikasi kerusakan modul surya yang sering terjadi, diantaranya karena faktor lingkungan, kerusan produksi, korosi dan penuaan alami. kerusakan pada modul surya diklasifikasikan menjadi 4 kelas yaitu Clean, dusty , electrical damage, Physical damage. Arsitektur dan Model CNN (Convolutional Neural Network)
       Proses pemodelan dilakukan dalam beberapa kali percobaan dengan ukuran batch size yang berbeda sehingga diperoleh model klasifikasi jenis kerusakan modul surya dengan hasil akurasi terbaik. Evaluasi Model
     Evaluasi model dilakukan dengan menggunakan confusion matrix guna mencari nilai akurasi, presisi, recall, dan hasil F1-score. Hasilnya akan digunakan sebagai landasan pertimbangan perlu tidaknya dilakukan pelatihan ulang pada model. Akurasi menentukan seberapa baik model mampu melakukan prediksi jenis kerusakan modul surya dengan benar. Presisi adalah akurasi atau kecocokan antara data yang diminta dengan hasil prediksi yang diberikan oleh model yang sudah disusun oleh arsitektur CNN. Klasifikasi dan Identifikasi kerusakan Modul Surya
       Pada tahapan terakhir dibangun sebuah prototype perangkat lunak aplikasi yang dapat mengklasifikasikan jenis kerusakan yang dilihat dari bentuk, ukuran serta pelebaran kerusakan pada modul surya sehingga dapat teridentifikasi tingkat kerusakan dan penanganannya. Rencana Penelitian
     Untuk mencapai target penelitian, maka penulis menyusun rencana Penelitian berupa jadwal kegiatan yang berguna untuk memastikan agar capaian yang ditetapkan dapat dipenuhi sesuai waktu yang telah ditetapkan termasuk target luaran berupa 2 (dua) buah publikasi."
Tatya Atyanti Paramastri_Kualifikasi.txt,3.1. alur penelitian 1. identifikasi masalah gambar 3.1 alur penelitian identifikasi masalah dilakukan supaya permasalahan yang diangkat jelas. identifikasi masalah dilakukan dengan cara melihat permasalahan nyata melalui literatur seperti jurnal penelitian wawancara dengan ahli dan keresahan yang dirasakan oleh peneliti secara pribadi. permasalahan yang diangkat pada penelitian ini adalah motif batik indonesia sangat beragam dan memiliki maknanya masingmasing. namun tidak banyak masyarakat yang masih mengetahui nama makna dan pemakaian dari masingmasing motif batik. menurut dewan ahli ppbi paguyuban pecinta batik indonesia sekar jagad ibu mari s. condronegoro saat ini sering kali ditemukan kesalahan dalam pemakaian motif batik seperti mengenakan kain yang seharusnya digunakan pada upacara kematian ketika menghadiri acara pernikahan. solusi yang diusulkan adalah melakukan klasifikasi motif batik. metode klasifikas yang umum digunakan adalah cnn. namun cnn klasik memiliki kelemahan dalam memahami makna menyeluruh dari gambar terutama yang berkaitan dengan hubungan antar bagian gambar yang berbeda. selain itu cnn klasik juga rentan terhadap overfitting di mana model terlalu terlatih pada data pelatihan dan tidak dapat menggeneralisasi dengan baik ke data baru. 2. studi literatur studi literatur dilakukan supaya penelitian memiliki landasan yang jelas. studi literatur dilakukan dengan sumber jurnal penelitian terdahulu serta buku yang berisikan metode yang sesuai dengan penelitian. fokus studi literatur terbagi menjadi tiga topik yaitu klasifikasi motif batik komputasi kuantum dan deteksi tepi. 3. pengumpulan dataset pengumpulan data dilakukan berdasarkan keperluan penelitian. data yang dikumpulkan merupakan data primer yang akan dikumpulkan degan bantuan ahli yaitu dewan ahli ppbi sekar jagad ibu mari s. condronegoro. ppbi sekar jagad merupakan perkumpulan pecinta batik yang diawasi langsung penasehat utama oleh permaisuri gusti kanjeng ratu hemas istri dari sri sultan hamengku buwono x sehingga informasi yang didapatkan bisa dijamin kebenarannya. pengumpulan dataset primer ini dilakukan dengan diskusi wawancara serta bimbingan dewan ahli ppbi sekar jagad supaya dataset yang digunakan sesuai dengan kebenaran dan kebutuhan penelitian yang dilakukan. sehingga hasil yang didapatkan memuaskan dan akurat. dataset yang akan dikumpulkan merupakan citra motif batik daur hidup yogyakarta dari kain batik tradisional yang merupakan batik cap maupun batik tulis bukan printing. motif yang akan digunakan dalam penelitian akan didiskusikan terlebih dahulu dengan narasumber supaya dapat mewakili batik daur hidup yogyakarta yang sangat penting untuk diketahui dalam bersosialdi masyarakat. misalnya seperti motif batik yang memiliki makna khusus dan tak pantas untuk dikenakan pada berapa acara motif batik yang memiliki larangan dan lainnya. hal ini dilakukan karena batik daur hidup yogyakarta tercatat memiliki ratusan motif hingga tahun 2006 sekar jagad 2015. gambar 3.2 gawangan kain citra batik akan diambil dengan menggunakan kamera dimana kain akan dibentangkan pada gawangan untuk difoto di dalam ruangan dengan pecahayaan yang sama dan di luar ruangan dengan cuaca yang sama. citra batik akan diambil dari berbagai posisi supaya citra lebih beragam. kemudian motif batik yang akan diambil beragam namun akan dipisahkan terlebih dahulu berdasarkan jenisnya. hal ini dikarenakan untuk satu kelompok motif yang sama terkadang terdapat bentuk yang terlihat berbeda. sehingga dibutuhkan pengujian bertahap untuk melihat apakah model dapat mendeteksi motif dengan benar. a b c gambar 3.3. a motif parang gondosuli b motif batik parang barong c motif batik parang kusuma data primer digunakan karena terdapat kekurangan dari data sekunder yang dapat ditemukan. seperti motif batik yang terpotong sehingga tidak terlihat serta motif yang salah pada beberapa kelas. beberapa motif juga memiliki bentuk atau komponen serupa sehingga butuh dikonsultasikan lebih lanjut kepada ahli. a b gambar . a motif parang yang terpotong b motif batik ceplok namun memiliki konponen parang 4. prapengolahan data proses prapengolahan data dilakukan untuk menyiapkan data sebelum diimplementasikan dalam model klasifikasi citra. prapengolahan data meliputi resize mengubah ruang warna menjadi grayscale augmentasi dan split data. resize dilakukan untuk memperkecil ukuran gambar aslinya. hal ini dilakukan untuk memastikan semua citra memiliki ukuran yang sama sehingga algoritma dapat bekerja secara konsisten dan efisien. selain itu ukuran citra yang lebih kecil dapat mempercepat proses segmentasi dan klasifikasi tanpa kehilangan informasi penting. proses selanjutnya adalah mengubah ruang warna menjadi grayscale perubahan warna ini dilakukan karena dapat meningkatkan kontras meningkatkan efisiensi komputasi dan meningkatkan ketahanan terhadap variasi pencahayaan. citra grayscale memiliki rentang intensitas yang lebih kecil dibandingkan citra rgb sehingga kontras tepi lebih jelas. selain itu citra grayscale membutuhkan lebih sedikit memori dan sumber daya komputasi dibandingkan citra rgb. kemudian citra grayscale tidak terpengaruh oleh variasi pencahayaan sehingga tepi dapat dideteksi dengan lebih akurat. selanjutnya proses augmentasi digunakan untuk meningkatkan ukuran dataset dengan menambahkan data baru tanpa perlu melakukan pengumpulan data baru. hal ini bermanfaat untuk mengatasi masalah keterbatasan data. selain itudengan melakukan augmentasi data akan menjadi lebih bervariasi sehingga dapat mencegah terjadinya overfitting dan lebih stabil terhadap perubahan data. proses terakhir adalah melakukan pembagian data. data akan dibagi menjadi tiga dataset yaitu pelatihan pengujian dan validasi. 5. komputasi kuantum model komputasi kuantum merupakan usulan dalam penelitian ini. adapun kombinasi model pertama yang akan dilakukan meliputi segmentasi tepi berbasis kuantum dengan menggunakan metode canny dan klasifikasi dengan metode quantum convolutional neural network qcnn. komputasi kuantum diterapkan mulai dari proses segmentasi karena berdasarkan penelitian terdahulu deteksi tepi berbasis kuantum dapat mendeteksi lebih banyak tepi dibandingkan deteksi tepi klasik berdasarkan jumlah tepi yang dihasilkan sundani dkk. 2019. hal serupa juga berlaku pada metode qcnn yang memiliki hasil lebih baik dan akurat dibandingkan dengan metode cnn klasik. sehingga hipotesisnya hasil dari ekstraksi fitur dan klasifikasi akan menjadi lebih optimal. gambar 3.2 perbandingan hasil deteksi tepi berbasis kuantum dan klasik sundani dkk. 2019 6. komputasi kuatum dan klasik kombinasi model berikutnya adalah melakukan deteksi tepi berbasis kuatum dengan model canny. kemudian klasifikasi dilakukan dengan menggunakan cnn klasik. kombinasi ini dilakukan untuk mengetahui seberapa jauh pengaruh penggunaan komputasi kuantum pada deteksi tepi dengan model canny. 7. komputasi klasik dan kuantum komputasi klasik dan kuantum disini adalah kombinasi antara deteksi tepi klasik dengan model canny yang kemudian dilanjutkan dengan melakukan klasifikasi dengan menggunakan metode quantum convolutional neural network qcnn. hal ini dilakukan untuk melihat seberapa jauh pengaruh dari penerapan model qcnn yang menggunakan komputasi kuantum. 8. komputasi klasik pengolahan data dengan komputasi klasik dilakukan dengan menggunakan deteksi tepi canny kasik yang dikombinasikan dengan klasifikasi cnn klasik. pengolahan data ini dilakukan sebagai pembanding performa model segmentasi dan klasifikasi berbasis kuantum. pengolahan data kedua model komputasi kuantum dan komputasi klasik akan dilakukan dengan menggunakan komputer yang sama yaitu komputer klasik. 9. evaluasi performa tahap terakhir adalah melakukan evaluasi performa. performa akan dibandingkan dari akurasi yang dihasilkan. adapun akurasi akan dihitung menggunakan confusion matrix pada kedua model. evaluasi ini akan dilakukan pada keempat kombinasi model untuk mengetahui seberapa jauh perbedaan dan fungsi penerapan komputasi kuantup pada setiap model.,permasalahan yang diangkat pada penelitian ini adalah motif batik indonesia sangat beragam dan memiliki maknanya masingmasing. namun tidak banyak masyarakat yang masih mengetahui nama makna dan pemakaian dari masingmasing motif batik. menurut dewan ahli ppbi paguyuban pecinta batik indonesia sekar jagad ibu mari s. condronegoro saat ini sering kali ditemukan kesalahan dalam pemakaian motif batik seperti mengenakan kain yang seharusnya digunakan pada upacara kematian ketika menghadiri acara pernikahan. solusi yang diusulkan adalah melakukan klasifikasi motif batik. fokus studi literatur terbagi menjadi tiga topik yaitu klasifikasi motif batik komputasi kuantum dan deteksi tepi. sehingga hasil yang didapatkan memuaskan dan akurat. dataset yang akan dikumpulkan merupakan citra motif batik daur hidup yogyakarta dari kain batik tradisional yang merupakan batik cap maupun batik tulis bukan printing. motif yang akan digunakan dalam penelitian akan didiskusikan terlebih dahulu dengan narasumber supaya dapat mewakili batik daur hidup yogyakarta yang sangat penting untuk diketahui dalam bersosialdi masyarakat. selain itu ukuran citra yang lebih kecil dapat mempercepat proses segmentasi dan klasifikasi tanpa kehilangan informasi penting. proses selanjutnya adalah mengubah ruang warna menjadi grayscale perubahan warna ini dilakukan karena dapat meningkatkan kontras meningkatkan efisiensi komputasi dan meningkatkan ketahanan terhadap variasi pencahayaan. selanjutnya proses augmentasi digunakan untuk meningkatkan ukuran dataset dengan menambahkan data baru tanpa perlu melakukan pengumpulan data baru. komputasi kuantum diterapkan mulai dari proses segmentasi karena berdasarkan penelitian terdahulu deteksi tepi berbasis kuantum dapat mendeteksi lebih banyak tepi dibandingkan deteksi tepi klasik berdasarkan jumlah tepi yang dihasilkan sundani dkk. 2019. hal serupa juga berlaku pada metode qcnn yang memiliki hasil lebih baik dan akurat dibandingkan dengan metode cnn klasik. sehingga hipotesisnya hasil dari ekstraksi fitur dan klasifikasi akan menjadi lebih optimal. gambar 3.2 perbandingan hasil deteksi tepi berbasis kuantum dan klasik sundani dkk. pengolahan data ini dilakukan sebagai pembanding performa model segmentasi dan klasifikasi berbasis kuantum. pengolahan data kedua model komputasi kuantum dan komputasi klasik akan dilakukan dengan menggunakan komputer yang sama yaitu komputer klasik. performa akan dibandingkan dari akurasi yang dihasilkan.
Tia Haryanti_Kualifikasi.txt,3.1 kerangka umum penelitian ini bertujuan untuk mengembangkan sistem deteksi dini kantuk sebelum berkendara dengan menggunakan kombinasi data visual berupa data citra wajah dan data fisiologis. kondisi predriving mengacu pada kondisi sebelum pengemudi memulai perjalanan sehingga sistem ini sangat penting untuk mencegah risiko kecelakaan di jalan . sistem ini mengintegrasikan teknologi pengenalan wajah dan analisis data fisiologis untuk memberikan deteksi yang lebih akurat. blok d iagram secara umum yang digunakan pada penelitian ini dapat dilihat pada gambar 3.1 blok diagram. objek preprocessing data fisiologis ekstrasi fiturpenggabungan fitur klasifikasi data image data visual kantuk ya tidak gambar 3.1 blok diagram model ini terdiri dari tiga tahapan yaitu input proses dan output . penelitian deteksi dini kantuk untuk kondisi predriving menggabungkan data visual yaitu pengumpulan data citra wajah pengemudi yang diambil menggunakan kamera serta data fisiologis yang diukur berupa data ekg menggunakan perangkat wearable yaitu smartwatch dan pulse oximeter untuk mengukur saturasi oksigen spo2 . tahapan preprocessing dan ekstraksi fitur dilakukan pada kedua jenis data yaitu data citra gambar dan data fisiologis. model convolutional neural network cnn digunakan untuk mengekst raksi fitur dari data citra wajah yang merupakan data visual sementara long short term memory lstm digunakan untuk memproses data fisiologis yang bersifat timeseries. fitur fitur yang diekstraksi dari kedua model ini digabungkan untuk menghasilkan vect or fitur gabungan. vektor fitur ini kemudian digunakan sebagai input untuk model support vector machine 43 svm yang melakukan klasifikasi akhir untuk mendeteksi kantuk. hasil deteksi kemudian digunakan untuk memberikan peringatan kepada pengemudi layak tidak nya pengemudi untuk berkendara. 3.2 tahapan peneletian tahapan penelitian merupakan urutan atau langkah langkah yang dilakukan secara terstruktur dan sistematis pada penelitian ini secara garis besar terbagi menjadi empat tahapan. berikut adalah gambar 3.2 tahapan penelitian yang dilakukan pada penelitian ini. pengumpulan data data visualdata fisiologis pemilihan dan persiapan dataset preprocessing data pembuatan modelekstraksi fitur penggabungan fitur evaluasi pemisahan dataset pembangunan model pelatihan model evaluasi model implementasi gambar 3. 2 tahapan penelitian 44 3.3. pemilihan dan persiapan dataset tahapan ini merupakan tahapan identifikasi awal dari penelitian meliputi identifikasi masalah penelitian yang berfokus pada masalah utama yaitu mendeteksi kantuk pada pengemudi menggunakan pemrosesan citra dan fisiologis . tahapan ini dilakukan untuk memasikan bahwa hanya data yang relevan berkualitas tinggi dan siap untuk dip roses lebih lanjut yang digunakan. pemilihan dataset memastikan bahwa dataset yang dikumpulkan relevan dengan tujuan penelitian yaitu hanya menggunakan data yang berkaitan dengan kondisi predriving serta memastikan bahwa data visual dan data fisiologis diambil pada waktu yang sama. tahapan pengumpulan data dan preprocessing data merupakan tahap awal untuk mempersiapkan dataset yang akan digunakan. 3.3.1 pengumpulan data data dibagi menjadi dua kategori utama yaitu data primer dan data sekunder . data primer diperoleh berdasarkan pengumpulan dan pengamatan langsung oleh peneliti berdasarkan kondisi subjek penelitian dan rekaman aktivitas fisik atau ekspresi wajah menggunakan kamera serta pengukuran fisiologis yang menggunakan perangkat wearable . data primer ini berupa data objektif dengan mengumpulkan data citra wajah dan pengukuran fisiologis . berikut merupakan gambar 3. 3 pengumpulan data. matapengumpulan data data visual citra wajahdata fisiologis ekgsaturasi oksigen spo2kamera smartwatch pulse oximetry gambar 3 .3 pengumpulan data dataset visual berupa citra wajah yang berfokus pada wajah pengemudi yang diambil menggunakan kamera dengan spesifikasi 12 mp. data visual dan fisiologis berupa data yang diambil dari partisipan dalam kondisi terjaga dan mengantuk . data fisiologis mencakup pengukuran langsung dari respons tubuh berupa sinyal 45 ekg elektrokardiogram yang merekam detak jantung hr variabilitas detak jantung atau heart rate variability hrv menggunakan perangkat wearable dan pengukuran saturasi oksigen dalam darah spo2 yang diukur menggunakan pulse oximeter. 3.3.2 preprocessing data melakukan analisis eksploratif data untuk memahami karakteristik dataset sehingga meningkatkan kualitas deteksi . preprocessing yang dilakukan yaitu pre processing citra dan preprocessing data fisiologis. preprocessing citra yaitu dengan mendeteksi wajah dan mata normalisasi pencahayaan pemotongan area wajah yang relevan. ektraksi frame dari video menggunakan opencv . pre processing data fisiologis yaitu dengan normalisasi data dan segmentasi. berikut merupakan gambar 3. 4 preprocessing data. data acquisitionfacial landmark detectionroi extraction gambar dari kameraeye detection data fisiologis ekg spo2 noise removal normalizationsave processed datasegmentationresize imagesnormalize pixel value data augmentation gambar 3. 4 preprocessing data data set yang dikumpulkan kemudian diolah yang meliputi normalisasi penghilangan noise dan teknik pra pemrosesan lainnya untuk membuat data siap digunakan dalam ekstraksi fitur. langkah ini melibatkan pembersihan dan penyiapan data untuk analisis. proses preprocessing untuk data visual atau data gambar yaitu 1. pengumpulan data visual dengan mengambil gambar wajah pengemudi menggunakan kamera berfo kus pada mata. 2. deteksi wajah dan deteksi mata menggunakan algoritma deteksi wajah seperti haar cascades atau dlib untuk mendetekasi dan melokalisasi wajah dalam gambar. 3. deteksi mata yaitu mendeteksi mata daam area wajah yang terdeteksi. 46 4. ekstraksi roi region of interest dengan mengambil area mata dari gambar. 5. teknik normalisasi untuk mengubah ukuran gambar mata menjadi dimensi yang konsisten missal nya 64x64 pixel serta menormalisasi nilai pixel gambar dalam rentang 0 1 atau 1 1. 6. augmentasi gambar dilakukan untuk meningkatkan variasi data seperti rotasi flipping horizontal atau vertikal zooming dan perubahan cahaya 7. penyimpanan data yang diproses dengan menyimpan gambar yang telah diproses dan fitur yang diekstraski dalam format terstruktur csv atau database . proses preprocessing untuk data fisiologis yaitu 1. pengumpulan data fisiologis menggunakan wearable untuk merekam detak jantung hr variabilitas detak jantung hrv dan saturasi oksigen spo2. 2. pembersihan data dengan menghilangkan noise dengan menggunakan teknik filtering dan imputasi data hilang dengan mengisi data yang hilang menggunakan metode seperti mean median atau interpolasi. 3. normalisasi data dengan min max sehingga menyesuaikan denga n skala data ke rentang yang konsisten 0 1. 4. segmentasi data dilakukan dengan membagi data menjadi segmen dengan ukuran waktu tetap yaitu 30 detik. 5. normalisasi data untuk memastikan konsistensi skala antar subjek dan pengukuran. 6. penyimpanan data yang d iproses yaitu data fisiologis dalam format terstruktur. langkah selanjutnya yaitu sinkronisasi data dengan menggabungkan data visual serta data fisiologis berdasarkan timestamp. selanjutnya memastikan bahwa data visual dan fisiologis yang telah disinkronk an mencerminkan kondisi yang sama pada waktu yang sama. selanjutnya yaitu menyimpan data yang telah disinkronkan dalam format yang mudah diakses untuk dianalisis lebih lanjut. 3.4. pembuatan model pembuatan model merupakan proses implementasi dari desain arsitektur yang telah direncanakan . langkah dari pembuatan model yaitu penulisan kode untuk membangun model sesuai dengan desain arsitektur yaitu cnn lstm dan svm. 47 selanjutnya mengonfigurasi model dengan optimizer fungsi loss dan metrik evaluasi. kemu dian melakukan pelatihan model menggunakan dataset yang telah dibagi menjadi training set dan validation set pada tahapan preprocessing . selanjutnya dilakukan validasi serta tuning hyperparameters untuk mengoptimalkan kinerja model. 3.4.1 ekstraksi fitur ekstraksi fitur dilakukan untuk menangkap karakteristik penting dari data yang telah diproses. fitur ini akan digunakan sebagai input untuk model pembelajaran mesin. ektraksi fitur dilakukan pada data visual berupa data gambar dan data fisiologis. 1. data v isual a. eye aspect ratio ear digunakan untuk mendeteksi apakah mata terbuka atau tertutup. di mana pi adalah titik titik landmark mata. b. pupil dilation digunakan untuk mengukur perubahan ukuran pupil. c. redness of eyes mengukur tingkat kemerahan pada ma ta. d. eye openess mengukur bukaan mata berdasarkan jarak vertikal antara kelopak mata atas dan bawah. 2. data fisiologis a. heart rate hr mengukur detak jantung per menit. b. heart rate variability hrv mengukur variabilitas detak jantung. c. respiratory rate rr mengukur laju pernapasan. d. spo2 saturasi oksigen dalam darah ekstraksi fitur dengan convolutional neural network cnn adalah proses yang menggunakan lapisan konvolusi dan pooling untuk menangkap fitur penting dari data gambar. langkah langkah ekstraksi fitur dengan cnn 1. convolutional layer menggunakan filter untuk menangkap fitur spasial dari gambar. 48 2. pooling layer mengurangi dimensi peta fitur sambil mempertahankan fitur penting. 3. fully connected layer menghubungkan peta fitur yang telah diratakan untuk melakukan klasifikasi atau ekstraksi fitur. 4. pelatihan model menyesuaikan bobot filter melalui backpropagation dengan data latih. 5. ekstraksi fitur menggunakan model yang telah dilatih untuk mengekstraksi fitur dari gambar baru. ekstraksi fitur dengan long short term memory lstm adalah proses yang menggunakan jaringan lstm untuk menangkap pola temporal dan hubungan jangka panjang dalam data sekuensial seperti data fisiologis ekg hr hrv rr dan spo2. lstm sangat efektif dal am menangani data yang memiliki ketergantungan waktu. ekstraksi fitur dengan lstm melibatkan beberapa langkah penting 1. menyiapkan data menyiapkan data sekuensial dalam bentuk yang sesuai untuk input ke lstm. 2. membangun model lstm membangun model lstm dengan lapisan lstm dan dense untuk ekstraksi fitur. 3. melatih model lstm melatih model menggunakan data sekuensial untuk menyesuaikan bobot jaringan. 4. ekstraksi fitur menggunakan model yang telah dilatih untuk mengekstraksi fitur dari data seku ensial baru. 3.4.2 penggabungan fitur fitur fitur yang telah diekstraksi dari ekstraksi fitur dengan model cnn yaitu dari gambar visul dengan mengekstraksi bagian mata dan ektraksi fitur dari data sekuensial dengan menggunakan lstm berupa data fisiologis . selanju tnya penggabungan fitur visual dan fitur sekuensial menggunakan metode penggabungan concatenation digabungkan membentuk satu set fitur komprehensif yang akan digunakan untuk pelatihan model yaitu klasifikasi akhir menggunakan model sv m. 49 3.4.3 pemisahan dataset pembagian dataset merupakan langkah penting dalam proses pelatihan dan evaluasi model. merujuk pada penelitian li k. gong y. ren z. 2020 untuk pembagian dataset dibagi menjadi tiga bagian yaitu training set 40 validation set 10 dan test set 50 namun pada penelitian ini pembagian dataset yang terdiri dari data gambar dan data fisiologis dibagi menjadi berikut 1. training set 75 data yang digunakan untuk melatih melatih model . 2. validation set 15 digu nakan untuk tuning hyperparameters dan memilih model terbaik . 3. test set 15 digunakan untuk mengevaluasi kinerja akhir model . 3.4.4 desain arsitektur desain a rsitektur merupakan proses menentukan struktur dan komponen model yang akan dibangun yang terdiri dari jenis model jumlah dan jenis layer fungsi aktivasi teknik regularisasi dan konfigurasi model. jenis model penelitian ini melibatkan dua model utama yaitu convolutional neural network cnn untuk data visual dan long short term memory lstm untuk data fisiologis. hasil dari kedua model digabungkan dan diklasifikasikan menggunakan support vector machine svm. model ini terdiri dari tiga tahapan yaitu akuisisi data pre processing data ekstraksi fitur penggabungan fitur dan klasifikasi dengan svm dan output sistem . berikut merupakan gambar 3. 5 arsitektur model. ear pupil dilation redness of eyes eye openesspreprocessing alignmentmata akuisisi databehavioral fisiologis ekstraksi fiturklasifikasidriver mengantuk alarm atau notifikasi tidak layak alarm atau notifikasi layak yatidak perangkat sensor wearablemembaca hrv hr dan spo2object cnn lstmpenggabungan fitur ekstraksi gambar 3. 5 arsitektur model tahap ini mencakup perancangan arsitektur cnn yang akan digunakan termasuk pemilihan jumlah dan jenis layer fungsi aktivasi dan teknik regularisasi. 50 digunakan untuk mengolah data visual seperti mengenali mata tertutup atau mulut menguap sebagai indikator kantuk. lstm digunakan untuk menga nalisis data fisiologis yang berurutan seperti pola detak jantung yang m enunjukkan kelelahan atau penurunan kewaspadaan. menggabungkan fitur yang diekstrak dari cnn dan lstm untuk mendapatkan representasi data yang komprehensif memastikan bahwa model dapat mengidentifikasi kantuk berdasarkan kombinasi indikator visual dan fis iologis. selanjutnya yaitu menggunakan support vector machines svm untuk mengklasifikasikan data sebagai kantuk atau tidak kantuk. svm dipilih karena kemampuannya dalam mengklasifikasikan data yang kompleks dan memberikan batas keputusan yang jelas layak atau tidak layak pengemudi untuk berkendara. jika pengklasifikasi mendeteksi keadaan mengantuk maka pengklasifikasi menghasilkan alarm atau notifikasi pemberitahuan untuk memberi tahu bahwa pengemudi tidak layak untuk berkendara atau kembali ke f ase pertama dan memulai ulang prosedur. 3.4.5 pelatihan model dengan dataset pelatihan m odel dilakukan dengan menggunakan training set dengan tuning hyperparamaters berdasarkan kinerja pada validation set. pelatihan model dilakukan dengan model svm menggunakan training set. 3.5 evaluasi model gabungan ini dievaluasi menggunakan metrik seperti akurasi presisi recall dan f1score untuk memastikan performa dan keandalannya. implementasi sistem ini diharapkan dapat memberikan notifikasi atau peringatan kepada pengemudi jika tanda tanda kantuk terdeteksi selama kondisi predriving sehingga dapat meningkatkan keselamatan berkendara secara signifikan. berdasarkan hasil validasi model dapat ditune atau dioptimalkan untuk meningkatkan performa misalnya dengan mengubah arsitektur parameter atau teknik training . 3.6 implementasi setelah penyempurnaan model dianggap siap untuk digunakan. model ini harus dapat secara akurat mendeteksi kantuk pengemudi dalam berbagai kondisi dengan minimal kesalahan. langkah selanjutnya yaitu penerapan model dalam sistem nyata dan pemantauan efektivitasnya dalam kondisi pengemudi pada 51 lingkungan predriving. model yang telah dioptimalkan diintegrasikan ke dalam sistem deteksi dini kantuk untuk pengujian awal. selanjutn ya yaitu m elakukan uji coba lapangan untuk mengevaluasi efektivitas sistem dalam kondisi nyata memungkinkan pengumpulan feedback untuk perbaikan lebih lanjut. 3.7 rencana kegiatan tabel 3.1 rencana kegiatan no nama kegiatan bulan 1 2 3 4 5 6 7 8 9 10 11 12 1 kajian literatur 2 perencanaan penelitian 3. pengumpulan data 4. prapemrosesan data 5. pembuatan model 6. pelatihan dan evaluasi model 7. penyusunan laporan akhir 8. presentasi laporan akhir 9. publikasi jurnal ilmiah internasional 10. pengajuan hki,3.1 kerangka umum penelitian ini bertujuan untuk mengembangkan sistem deteksi dini kantuk sebelum berkendara dengan menggunakan kombinasi data visual berupa data citra wajah dan data fisiologis. kondisi predriving mengacu pada kondisi sebelum pengemudi memulai perjalanan sehingga sistem ini sangat penting untuk mencegah risiko kecelakaan di jalan . sistem ini mengintegrasikan teknologi pengenalan wajah dan analisis data fisiologis untuk memberikan deteksi yang lebih akurat. blok d iagram secara umum yang digunakan pada penelitian ini dapat dilihat pada gambar 3.1 blok diagram. objek preprocessing data fisiologis ekstrasi fiturpenggabungan fitur klasifikasi data image data visual kantuk ya tidak gambar 3.1 blok diagram model ini terdiri dari tiga tahapan yaitu input proses dan output . penelitian deteksi dini kantuk untuk kondisi predriving menggabungkan data visual yaitu pengumpulan data citra wajah pengemudi yang diambil menggunakan kamera serta data fisiologis yang diukur berupa data ekg menggunakan perangkat wearable yaitu smartwatch dan pulse oximeter untuk mengukur saturasi oksigen spo2 . tahapan preprocessing dan ekstraksi fitur dilakukan pada kedua jenis data yaitu data citra gambar dan data fisiologis. model convolutional neural network cnn digunakan untuk mengekst raksi fitur dari data citra wajah yang merupakan data visual sementara long short term memory lstm digunakan untuk memproses data fisiologis yang bersifat timeseries. fitur fitur yang diekstraksi dari kedua model ini digabungkan untuk menghasilkan vect or fitur gabungan. vektor fitur ini kemudian digunakan sebagai input untuk model support vector machine 43 svm yang melakukan klasifikasi akhir untuk mendeteksi kantuk. hasil deteksi kemudian digunakan untuk memberikan peringatan kepada pengemudi layak tidak nya pengemudi untuk berkendara. pengumpulan data data visualdata fisiologis pemilihan dan persiapan dataset preprocessing data pembuatan modelekstraksi fitur penggabungan fitur evaluasi pemisahan dataset pembangunan model pelatihan model evaluasi model implementasi gambar 3. 2 tahapan penelitian 44 3.3. pemilihan dan persiapan dataset tahapan ini merupakan tahapan identifikasi awal dari penelitian meliputi identifikasi masalah penelitian yang berfokus pada masalah utama yaitu mendeteksi kantuk pada pengemudi menggunakan pemrosesan citra dan fisiologis . pemilihan dataset memastikan bahwa dataset yang dikumpulkan relevan dengan tujuan penelitian yaitu hanya menggunakan data yang berkaitan dengan kondisi predriving serta memastikan bahwa data visual dan data fisiologis diambil pada waktu yang sama. 3.3.2 preprocessing data melakukan analisis eksploratif data untuk memahami karakteristik dataset sehingga meningkatkan kualitas deteksi . langkah dari pembuatan model yaitu penulisan kode untuk membangun model sesuai dengan desain arsitektur yaitu cnn lstm dan svm. 2. membangun model lstm membangun model lstm dengan lapisan lstm dan dense untuk ekstraksi fitur. 3.4.4 desain arsitektur desain a rsitektur merupakan proses menentukan struktur dan komponen model yang akan dibangun yang terdiri dari jenis model jumlah dan jenis layer fungsi aktivasi teknik regularisasi dan konfigurasi model. hasil dari kedua model digabungkan dan diklasifikasikan menggunakan support vector machine svm. 50 digunakan untuk mengolah data visual seperti mengenali mata tertutup atau mulut menguap sebagai indikator kantuk. menggabungkan fitur yang diekstrak dari cnn dan lstm untuk mendapatkan representasi data yang komprehensif memastikan bahwa model dapat mengidentifikasi kantuk berdasarkan kombinasi indikator visual dan fis iologis. selanjutnya yaitu menggunakan support vector machines svm untuk mengklasifikasikan data sebagai kantuk atau tidak kantuk. svm dipilih karena kemampuannya dalam mengklasifikasikan data yang kompleks dan memberikan batas keputusan yang jelas layak atau tidak layak pengemudi untuk berkendara. jika pengklasifikasi mendeteksi keadaan mengantuk maka pengklasifikasi menghasilkan alarm atau notifikasi pemberitahuan untuk memberi tahu bahwa pengemudi tidak layak untuk berkendara atau kembali ke f ase pertama dan memulai ulang prosedur. 3.5 evaluasi model gabungan ini dievaluasi menggunakan metrik seperti akurasi presisi recall dan f1score untuk memastikan performa dan keandalannya. implementasi sistem ini diharapkan dapat memberikan notifikasi atau peringatan kepada pengemudi jika tanda tanda kantuk terdeteksi selama kondisi predriving sehingga dapat meningkatkan keselamatan berkendara secara signifikan. berdasarkan hasil validasi model dapat ditune atau dioptimalkan untuk meningkatkan performa misalnya dengan mengubah arsitektur parameter atau teknik training . 3.6 implementasi setelah penyempurnaan model dianggap siap untuk digunakan. model ini harus dapat secara akurat mendeteksi kantuk pengemudi dalam berbagai kondisi dengan minimal kesalahan. langkah selanjutnya yaitu penerapan model dalam sistem nyata dan pemantauan efektivitasnya dalam kondisi pengemudi pada 51 lingkungan predriving. model yang telah dioptimalkan diintegrasikan ke dalam sistem deteksi dini kantuk untuk pengujian awal. selanjutn ya yaitu m elakukan uji coba lapangan untuk mengevaluasi efektivitas sistem dalam kondisi nyata memungkinkan pengumpulan feedback untuk perbaikan lebih lanjut. 3.7 rencana kegiatan tabel 3.1 rencana kegiatan no nama kegiatan bulan 1 2 3 4 5 6 7 8 9 10 11 12 1 kajian literatur 2 perencanaan penelitian 3. pengumpulan data 4. prapemrosesan data 5. pembuatan model 6. pelatihan dan evaluasi model 7. penyusunan laporan akhir 8. presentasi laporan akhir 9. publikasi jurnal ilmiah internasional 10. pengajuan hki
Yoga Panji Perdana Nugraha_Kualifikasi.txt,3.1 motivasi industri manufaktur memiliki berbagai macam produk yang ada di dalamnya. dalam upaya pemenuhan kualitas yang tinggi serta menjaga kepuasan pelanggan dan reputasi perusahaan maka mendeteksi produk yang cacat sedini mungkin merupakan aspek yang penting. sehingga motivasi dari disertasi ini adalah sebagai berikut. 1. pengembangan aplikasi pendeteksi cacat pada produk ini didasari keinginan peneliti untuk meningkatkan kinerja pengendalian kualitas pada industri manufaktur sehingga dapat membantu menjaga kualitas produk serta efisien si dalam kegiatan pengendalian kualitas. 2. untuk meminimalisir pemborosan waktu bahan baku biaya dan sumber daya lainnya karena deteksi cacat pada produk dilakukan sedini dan secepat mungkin. 3. meningkatkan efisiensi pada kegiatan inspeksi produk d engan mene rapkan otomatisasi mel alui aplikasi yang dikembangkan. 4. mengintegrasikan teknologi yang sedang berkembang seperti artificial intelligence dengan industri manufaktur sehingga tercipta manufaktur cerdas yang akan berakibat pendapatan profit perusahaan yang op timal. 5. memberikan kontribusi pemahaman dan pengembangan teknologi baru dalam deteksi objek sehingga bisa menjadi referensi untuk pembaca serta penelitian selanjutnya. 3.2 alur kerja riset alur kerja riset digambarkan melalui diagram alir. tujuannya agar penel itian dapat terstruktur sehingga tidak ada tah apan penelitian yang terlewat. secara umum b erikut ini merupakan diagram alir penelitian ini. tahap awal tahap pengembanganperancangan dan pembuatan prototype alat deteksi cacatpengumpulan data cacat objek uji coba prototype alat deketsi cacat objekperancangan model deteksi cacat objek menggunakan deep learning implementasi dan pelatihan model deteksi cacat objek evaluasi dan penyempurnaan model deteksi cacat objek pengujian model deteksi objek menggunakan deep learning pembuatan aplikasi pendeteksi objek cacattahap optimasi pengajuan hki dan jurnal internasional q 1 gambar 3. 1 diagram alir penelitian diagram alir penelitian di atas menggambarkan alur penelitian yang akan dilakukan. berikut ini adalah penjelasan dari diagram alir penelitian di atas. 1. tahap awal kegiatan yang dilakukan pada tahap awal ini adalah merancang dan membuat prototype alat deteksi cacat dan pengumpulan data cacat objek. prototype alat ini menggunakan ban berjalan dengan motor listrik sebagai penggeraknya dengan alat pencahayaan yang cukup. alat ini nantinya digunakan untuk mengumpulkan data yang akan digunakan untuk melakukan perancangan dan pelatihan m odel deteksi cacat objek. pengumpulan data dilakukan untuk memperoleh data yang dibutuhkan pada penelitian ini. data bisa berupa data primer dan data sekunder ataupun keduanya bergantung pada kebutuhan penelitian yang akan dilakukan. data primer dikumpulan dengan memotret objek pada ban berjalan menjadi citra baik citra bergerak maupun citra tak bergerak yang akan menjadi satu kesatuan yaitu dataset . data primer dikumpulkan menggunakan alat yang dirancang seperti di bawah ini. gambar 3. 2 rancangan prototipe alat gambar 3.2 di atas menggambarkan rancangan alat yang akan dikembangkan. alat tersebut pertama digunakan sebagai media untuk pengambilan data primer yaitu data citra dari objek yang akan dideteksi. objek berupa sekrup akan berjalan melalui ban berjalan conveyor yang nantinya akan ditangkap gambarnya oleh webcam atau kamera yang terhubung dengan komputer untuk menyimpan gambar tersebut untuk kebutuhan pelatihan model. sedangkan data sekunder dikumpulkan melalui website kaggle maupun website atau jurnal lain yang sejenis. hasil dari akuisisi citra ini akan digunakan untuk pelatihan dan pengujian data. sampel yang diambil adalah objek berupa sekrup yang terdapat kecacatan. data tersebut kemudian dikumpulkan menjadi sebuah da taset yang akan digunakan untuk melatih model. data data yang diambil kemudian dikelompokkan menjadi beberapa kelas sesuai dengan jenis cacat yang ada pada sekrup tersebut. luaran pada tahap ini adalah pengajuan hki untuk prototype alat pendeteksi cacat ob jek yang dirancang. 2. tahap pengembangan tahap ini terdapat beberapa kegiatan yang dilakukan. pertama adalah melakukan uji coba prototype alat deteksi cacat objek yang digambarkan pada gambar 3.2 di atas . uji coba dilakukan dengan menyesuaikan tinggi kamera tingkat pencahayaan kecepatan ban berjalan serta pengaturan tempat ban berjalan untuk menjaga efektivitas dan efisiensi dalam mendeteksi objek. kedua adalah merancang model untuk mendeteksi cacat objek dengan menggunakan deep learning . sebelum melatih da ta dilakukan preprocessing terlebih dahulu. kegiatan ini dilakukan dengan menggunakan website roboflow. preprocessing dilakukan mengoptimalkan pelatihan dengan menganotasi citra untuk menandai bagian penting dari citra region of interest menyamakan orie ntasi citra mengubah ukuran citra agar sama memperbanyak variasi data dengan augmentasi dan generalisir data sehingga menjadi satu kesatuan dataset yang lebih siap untuk dilatih. setelah preprocessing dilakukan maka diharapkan pelatihan data yang dilaku kan lebih optimal. pelatihan data dilakukan untuk melatih model mengenali citra yang akan dideteksi sehingga pada penerapannya mendapatkan hasil deteksi yang akurat dan optimal. pelatihan data dilakukan dengan menggunakan salah satu algoritma dari teknolog i kecerdasan artifisial yaitu deep learning dengan bahasa pemrograman yang digunakan adalah python. pada pelatihan data ini juga akan mendapatkan nilai pengukuran evaluasi measurment evaluation berupa accuracy recall and precision dan mean average prec ision map. pada umumnya pelatihan harus memiliki jumlah data dalam hal ini adalah citra yang lebih banyak dibandingkan pengujian. 3. tahap optimasi tahap pengembangan telah dilakukan kemudian masuk ke tahap optimasi. tahap ini terdapat kegiatan yaitu eval uasi dan penyempurnaan model deteksi cacat objek. evaluasi dan penyempurnaan dilakukan agar fitur yang ada pada aplikasi yang akan dikembangkan dapat ditampilkan dengan maksimal. fitur yang akan ditambahkan pada model pendeteksi objek berupa kemampuan komp uter untuk secara otomatis menyimpan hasil deteksi menjadi sebuah basis data. sehingga nantinya data tersebut dapat menjadi acuan bagi departemen terkait untuk inovasi ke depannya. setelah pelatihan data dilakukan maka selanjutnya adalah pengujian data. p engujian data dilakukan untuk menguji model sejauh mana dapat mendeteksi cacat dari suatu produk. pada pengujian data dilakukan dengan mengunggah data secara acak selain data yang digunakan pada pelatihan. pada akhirnya akan menampilkan output model dalam mendeteksi cacat pada produk. setelah itu maka dibangun aplikasi yang mampu mendeteksi cacat produk pada industri secara real time. aplikasi ini nantinya akan menampilkan hasil deteksi dari produk yang bergerak. informasi yang disampaikan antara lain kondi si dari produk cacat atau tidak serta bagian mana yang cacat akan ditandai oleh bounding box . hal ini akan dengan cepat membantu operator mengetahui cacat jenis apa yang terjadi. sehingga dapat ditindaklanjuti sesegera mungkin yang secara tidak langsung ju ga membantu dalam pengambilan keputusan. target penelitian ini adalah pengajuan hki serta publikasi artikeljurnal ilmiah internasional bereputasi q1 ieee access . 3.3 pendekatan pendekatan yang dilakukan adalah dengan menggunakan teknologi artificial int elligence dalam mengadopsi kemampuan manusia dalam mendeteksi objek. pendekatan ini menggabungkan antara pengolahan citra dan deep learning dengan memanfaatkan salah satu arsitektur yang dimilikinya. selain itu diterapkan juga pengukuran evaluasi seperti precision recall dan mean average precision map untuk memastikan model yang dikembangkan dapat digunakan dengan optimal. nantinya akan dikembangkan sebuah aplikasi yang kemungkinan berbasis web untuk mempermudah pengguna untuk mengambil gambar bergerak maupun tak bergerak yang kemudian mengirimnya ke sistem pendeteksi cacat dan menerima hasil deteksi secara real time. hasil deteksi secara real time dikehendaki agar produk dapat diperiksa selama proses produksi berlangsung sehingga cacat dapat dideteksi secepat dan seakurat mungkin. hal ini akan membantu operator untuk melakukan kegiatan inspeksi produk dengan efisien.,3.1 motivasi industri manufaktur memiliki berbagai macam produk yang ada di dalamnya. dalam upaya pemenuhan kualitas yang tinggi serta menjaga kepuasan pelanggan dan reputasi perusahaan maka mendeteksi produk yang cacat sedini mungkin merupakan aspek yang penting. 1. pengembangan aplikasi pendeteksi cacat pada produk ini didasari keinginan peneliti untuk meningkatkan kinerja pengendalian kualitas pada industri manufaktur sehingga dapat membantu menjaga kualitas produk serta efisien si dalam kegiatan pengendalian kualitas. 2. untuk meminimalisir pemborosan waktu bahan baku biaya dan sumber daya lainnya karena deteksi cacat pada produk dilakukan sedini dan secepat mungkin. 3. meningkatkan efisiensi pada kegiatan inspeksi produk d engan mene rapkan otomatisasi mel alui aplikasi yang dikembangkan. 4. mengintegrasikan teknologi yang sedang berkembang seperti artificial intelligence dengan industri manufaktur sehingga tercipta manufaktur cerdas yang akan berakibat pendapatan profit perusahaan yang op timal. 5. memberikan kontribusi pemahaman dan pengembangan teknologi baru dalam deteksi objek sehingga bisa menjadi referensi untuk pembaca serta penelitian selanjutnya. tahap awal tahap pengembanganperancangan dan pembuatan prototype alat deteksi cacatpengumpulan data cacat objek uji coba prototype alat deketsi cacat objekperancangan model deteksi cacat objek menggunakan deep learning implementasi dan pelatihan model deteksi cacat objek evaluasi dan penyempurnaan model deteksi cacat objek pengujian model deteksi objek menggunakan deep learning pembuatan aplikasi pendeteksi objek cacattahap optimasi pengajuan hki dan jurnal internasional q 1 gambar 3. 1. tahap awal kegiatan yang dilakukan pada tahap awal ini adalah merancang dan membuat prototype alat deteksi cacat dan pengumpulan data cacat objek. 2 rancangan prototipe alat gambar 3.2 di atas menggambarkan rancangan alat yang akan dikembangkan. tahap ini terdapat kegiatan yaitu eval uasi dan penyempurnaan model deteksi cacat objek. evaluasi dan penyempurnaan dilakukan agar fitur yang ada pada aplikasi yang akan dikembangkan dapat ditampilkan dengan maksimal. p engujian data dilakukan untuk menguji model sejauh mana dapat mendeteksi cacat dari suatu produk. pada akhirnya akan menampilkan output model dalam mendeteksi cacat pada produk. setelah itu maka dibangun aplikasi yang mampu mendeteksi cacat produk pada industri secara real time. aplikasi ini nantinya akan menampilkan hasil deteksi dari produk yang bergerak. selain itu diterapkan juga pengukuran evaluasi seperti precision recall dan mean average precision map untuk memastikan model yang dikembangkan dapat digunakan dengan optimal. nantinya akan dikembangkan sebuah aplikasi yang kemungkinan berbasis web untuk mempermudah pengguna untuk mengambil gambar bergerak maupun tak bergerak yang kemudian mengirimnya ke sistem pendeteksi cacat dan menerima hasil deteksi secara real time. hasil deteksi secara real time dikehendaki agar produk dapat diperiksa selama proses produksi berlangsung sehingga cacat dapat dideteksi secepat dan seakurat mungkin.
