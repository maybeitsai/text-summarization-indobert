judul
pengembangan optimasi rute menggunakan deep reinforcement learning pada model dynamic vehicle routing problem with time windows dvrp tw

latarbelakang
machine learning ml adalah cabang dari kecerdasan artifisial yang memanfaatkan teknik statistik dan algoritma agar dapat belajar dan membuat keputusan atau prediksi berdasarkan data. dengan menggunakan algoritma ml komputer dapat meningkatkan efisiensi dalam melakukan berbagai jenis pekerjaan tanpa harus di program secara eksplisit untuk setiap tugasnya karimi mamaghan m. mohammadi m. meyer p. karimi mamaghan a. m. talbi e. g 2022. ml memanfaatkan berbagai teknik dari statistik teori proba bilitas matematika dan ilmu komputer untuk membangun model dari dataset yang ada. machine learning adalah metode yang secara otomatis menganalisis data untuk mendapatkan aturan kemudian menggunakan aturan ini untuk memprediksi data yang tidak diketahui. algoritma dan metode statistik diterapkan untuk memberikan komputer kemampuan untuk belajar dari data dan meningkatkan kinerjanya dalam memecahkan masalah tanpa harus memprogram secara eksplisit untuk setiap masalah ni qiuping tang yuanxiang 2023 . machine learning dibagi menjadi tiga kategori supervised learning unsupervised learning dan reinforcement learning karimi mamaghan m. mohammadi m. meyer p. karimi mamaghan a. m. talbi e. g 2022 . reinforcement learning merupakan bagian dari machine learning yang membedakan dari supervised learning dan unsupervised learning yaitu pada reinforcement learning dengan trial and eror selama interaksi langsung dengan lingkungan sekitar panzer bender 2022 . reinforcement learning rl tidak memerlukan sinyal yang diawasi untuk belajar rl bergantung pada sinyal umpan balik dari individu agent di lingkungannya. umpan balik ini mengoreksi keadaan dan tindakan agen sehingga agen secara bertahap dapat mempelajari cara memaksi malkan hadiah reward dengan cara memaksimalkan nilai cumulative 5 reward hadiah atau imbalan yang dikumpulkan secara kumulatif dan mencapai kemampuan belajar mandiri yang kuat ni tang 2023 . algoritma rl dapat dibagi menjadi dua kategori yaitu pembelajaran berbasis model dan pembelajaran bebas model mousavi seyed sajad howley enda schukat michael 2018 . pembelajaran berbasis model memiliki pengetahuan sebelumnya tentang lingkungan yang dapat dioptimalkan terlebih dahulu . pembelajaran bebas model lebih rendah daripada yang pertama dalam hal kecepatan pelatihan tetapi lebih mudah diimplementasikan dan dapat dengan cepat menyesuaikan diri dengan keadaan yang lebih baik dalam skenario nyata. penerapan reinforcement learning telah menunju kkan hasil yang signifikan pada berbagai bidang seperti robotika jens kober j andrew bagnell jan peters 2013 permainan silver d. huang a. maddison c. et al. 2016 kesehatan liu siqisee kay choong ngiam kee yuan celi leo anthony sun xingzhi feng mengling 2020 dan distribusi logistik he zhenhua chen liang liu bin 2024 . perkembangan teknik machine learning khususnya dalam deep learning telah memungkinkan penggunaan arsitektur neural network yang lebih kompleks untuk memecahkan berbagai masalah yang sulit diselesaikan dengan pendekatan konvensional. salah satu teknik yang telah menunjukkan hasil signifikan adalah penggunaan mekanisme perhatian ganda atau multi head er attention . multiheade r attention adalah mekanisme di mana lapisan perhatian direplikasi beberapa kali untuk memungkinkan model fokus pada bagian berbeda dari urutan masukan secara bersamaan cordonnier loukas jaggi 2020 . mekanisme ini pertama kali diperkenalkan dalam konteks model transformer vaswani et al. 2017 dan telah digunakan secara luas dalam berbagai aplikasi salah satunya pengoptimalan rute logistik xin liang wen song zhiguang cao and jie zhang 2021 . distribusi merupakan kegiatan proses penyaluran produk dari produsen sampai ke tangan masyarakat atau konsumen secara tepat waktu dan efisien tjiptono diana 2020 . pengiriman yang tepat waktu merupakan salah satu tujuan dari proses distribusi yang dapat dilakukan dengan memahami lokasi tujuan distribusi. terdapat beberapa lokasi tujuan pada proses pendistribusian yang 6 mengakibatkan biaya transportasi yang cukup tinggi. biaya transportasi yang melebihi anggaran dikarenakan penentuan rute pendistribusian masih dilakukan secara manual atau acak yaitu penentuan jalur distribusi berdasarkan perkiraan saja. vehicle routing problem vrp merupakan masalah optimasi kombinatorial klasik yang pertama kali diusulkan oleh george danztig pada tahun 1959 dantzig ramser 1959 . vehicle routing problem vrp termasuk perma salahan nphard yang umum dalam optimasi kombinatorial dan telah dipelajari selama beberapa dekade. tujuan utama untuk menentukan rute optimal bagi armada kendaraan untuk melayani sekumpulan pelanggan. penelitian terkait vrp adalah kunci untuk meningkatkan daya saing pad a industri logistik . perkembangan distribusi di dunia nyata dengan bermacam macam karakteristik membuat banyaknya variasi vrp dari single objective hingga vrp multi objective. terdapat berbagai jenis vehicle routing problem vrp misalnya capacited vehicle routing problem cvrp vrp with time windows vrptw multi depot vehicle routing problem mdvrp abdirad maryam krishnan krishna gupta deepak 2022 dynamic vehicle routing problem with time windows dvrptw ghannam gleixner 2023 dan vehicle routing problem with pickup and delivery vrrppd m. liu q. song q. zhao l. li z. yang y . zhang 2022 . dynamic vehicle routing pro blem with time window dvrptw merupakan permasalahan optimas i rute yang menambahkan batasan jendela waktu ke dalam permasalahan. hal ini berarti bahwa pelanggan memiliki periode waktu tertentu untuk dilayani. dvrptw harus menjadwalkan pengiriman supaya barang diterima dalam rentang waktu yang ada sekaligus untuk meminimal kan biaya operasional liu et al. 2023 . permasalahan yang terjadi pada dvrptw yaitu perubahan dinamis seperti pesanan baru pembatalan atau keterlambatan serta ketidak pastian lalu lintas waktu pengiriman dan durasi pelayanan menambah komple ksitas permasalahan. pengiriman dilakukan dalam jendela waktu yang menjadi kendala penting pada proses distribusi . sebab jika pengiriman melebihi jendela waktu yang ditetapkan akan mengakibatkan ketidakpuasan pelanggan. permasalahan ini yang 7 perlu untuk diselesaikan untuk meningkat kan solusi yang optimal untuk permasalahan yang ada. terdapat berbagai solusi untuk menyelesaikan permasalahan ini diantaranya metode eksak metode heuristik dan metode metaheuristik m. liu q. zhao q. song y . zhang2023 . penggunaan machine learning khususnya reinforcement learning dalam penyelesaian vrp menawarkan pendekatan baru. ini memungkinkan pengembangan algoritma yang dapat secara otomatis belajar dari lingkungan untuk menghasilkan solusi optimal dalam kondisi yang dinamis seperti arus lalu lintas dan kedatangan pelanggan b aru. metode reinforcement learning yang umum untuk menyelesaikan vrp termasuk dynamic programing algoritma q learning algoritma deep q network dqn policy based reinforc e algorithms value and policy combined actor critic algorithms dan advantage actor critic algorithms ni tang 2023 . pada masalah dynamic veh icle routing problem with time window dvrptw sudah banyak metode yang digunakan untuk menyelesaikan permasalahan ini seperti brain storm optimization bso dan ant colony optimization aco liu et al. 2022 algoritma hybrid brain sto rm optimization bso liu et al. 2023 algoritma dynamic hybrid genetic search hgs ghannam gleixner 2023 . penggunaan reinforcement learning juga banyak digunakan untuk menyelesaikan permasalahan ini seperti yang dilakukan oleh joe lau 2020 menggabungkan deep reinforcement learning dan simulated annealing drlsa . penggunaan deep reinforcement learning juga dapat digunakan pada berbagai permasa lahan optimasi rute seperti yang d ilakukan oleh li et al. 2021 pada model permasalahan heterogeneous capacited vehicle routing problem hcvrp jiuxiu zhao 2020 meneliti pada vehicle routig pro blem . penggunaan deep q network seperti yang dilakukan oleh bdeir et al. 2021 untuk permasalahan route mengusulkan routing problem deep q network rpdqn untuk masalah vehicle routing problem hasilnya bahwa pendekatan rp dqn berhasil 8 meningkatkan kinerja dalam menyelesaikan masalah perutean kendaraan dengan memanfaatkan representasi status dinamis dan efisiensi sampel yang lebih baik. penggunaan model berbas is perhatian attent tion yang telah diteliti oleh kool et al. 2018 untuk menyelesaikan masalah optimisasi kombinatorial khususnya masalah perutean seperti travelling salesman problem tsp dan vehicle routing problem vrp. hasilnya menunjukkan bahwa model ya ng dibuat menunjuk kan fleksibilitas yang baik dalam menangani berbagai jenis masalah optimisasi kombinatorial dengan satu set hyperparameter s. berdasarkan penelitian terdahulu penerapan reinforcement learning dapat secara otomatis mengidentifikasi pola dan strategi terbaik untuk mengoptimalkan rute distribusi dalam menghadapi berbagai kondisi dinamis yang sering berubah seperti variabilitas arus lalu lintas yang tidak terduga dan permintaan pelanggan yang mun cul tidak dapat diprediksi . dengan kemampuan adaptasi ini algoritma berbasis reinforcement learning tidak hanya meningkatkan efisiensi logistik dengan menemukan solusi rute yang optimal teta pi juga meningkatkan responsivita s terhadap kebutuhan pelanggan yang berfluktuasi secara signifikan mengurangi waktu tunggu dan biaya operasional. pendekatan ini tidak hanya menjanjikan peningkatan dalam kinerja logistik tetapi juga menawarkan kemampuan untuk merespons secara lebih fleks ibel terhadap tantangan operasional yang kompleks memastikan kepuasan pelanggan dan keberlanjutan operasional dalam lingkungan bisnis yang semakin kompetitif. penelitian ini diharapkan mampu meingkatkan efisiensi dalam pemilihan rute pada konteks logistik dan distribusi yang dinamis khususnya dalam menghadapi dynamic vehicle routing problem with time windows dvrptw. masalah ini ditandai oleh kondisi yang terus berubah seperti fluktuasi dalam arus lalu lintas kedatangan pelanggan baru dan adanya jendela waktu untuk pengiriman serta kebutuhan untuk mengirim produk dalam berbagai jenis atau kategori akan mempengaruhi proses pengiriman . untuk mengatasi tantangan tersebut penelitian ini mengusulkan penerapan deep q network dqn yang diperkaya dengan mekanisme multi head er attention . pendekatan ini dirancang untuk memanfaatka n 9 kemampuan dqn dalam memahami dan beradaptasi dengan kondisi dinamis serta mengintegrasikan multi head er attention untuk meningkatkan pemrosesan informasi. penggabungan kedua teknologi ini diharapkan sistem dapat secara efektif mengidentifikasi rute optimal yang memenuhi semua kriteria dan batasan yang ada sekaligus menyesuaikan diri dengan perubahan kondisi yang ada .

rumusanmasalah
berdasarkan latar belakang yang telah dipaparkan maka rumusan masalah yang dapat disusun sebagai berikut . 1. bagaimana deep q network dqn dengan mekanisme multi header attention dapat diterapkan untuk menyelesaikan dynamic vehicle routing problem with time windows dvrptw 2. apakah penerapan dqn dengan multi header attention dapat meningkatkan efisiensi dan efektivitas dalam menentukan rute optimal pada kondisi dinamis seperti fluktuasi arus lalu lintas kedatangan pelanggan baru dan adanya jendela waktu

tujuanpenelitian
berdasarkan rumusan masalah yang telah dipaparkan tujuan penelitian dari penelitian ini sebagai berikut. 10 1. mengembangkan model berbasis deep q network dqn yang diperkaya dengan mekanisme multi header attention untuk menyelesaikan dynamic vehicle routing problem with time windows dvrptw. 2. menilai efektivitas model dqn dengan multi header attention dalam meningkatkan efisiensi pengiriman dan kepuasan pelanggan melalui penentuan rute optimal dalam kondisi yang dinamis. 3. menunjukkan kemampuan adaptasi model terhadap perubahan kondisi seperti fluktuasi lalu lintas kedatangan pelanggan baru dan batasan jendela waktu.

rangkumanpenelitianterkait
berdasarkan tabel perbandingan di atas dapat diketahui berbagai perbedaan pada masing masing penelitian. mode l permasalahan pada penelitian terdahulu terbagi menja di beberapa model yaitu dynamic vehicle routing problem dvrp dynamic vehicle routi ng problem with time window s dvrp tw capacited vehicle routing problem cvrp heterogeounus capac ited vehicle routing problem hcvrp trave l salesman problem tsp. terkait fokus permasalahan yang diambil pada berbagai penelitian mulai dari permintaan pelangg an yang tidak pasti keadaan lalu lintas dan terkait kendaraan yang diguna kan pada proses pengirim an. penyelesaian dilakukan dengan menggun akan metaheuristi k dianta ranya hybrid brain storm optimi zation bso ant colony optimizat ion dan yang lainnya . pengga bunga n algorit ma juga dilakuka n pada beberapa penelitian terdahulu seperti hybrid antara brain strom optimizat ion dengan ant colony optimization. pengguna an machine learning yaitu reinforcement learning deep reinforcement learning dan deep qnetwork digunaka n pada berbagai penelitian sebab memil iki kelebiha n yaitu lebih optimal pada data yang banyak. multi attention juga digunaka n pada penyelesaian permasalahan optimasi rute dan menunjuk an hasil yang optimal. pada penelitian selanjut nya fokus penelitian pada model masa lah dynamic vehicle routing problem with time windows dvrp tw dengan fokus terhadap ketidakpastian jalan raya serta ketidakpastian pelangga n yang berubahubah dimana pada prose s pengiriman ke pelangga n terdapat jende la waktu atau batasan waktu pengirim an sampai ke pelangga n. pada penelitian terdahulu hanya fokus pada salah satu saja seperti hanya fokus pada pelangga n yang tidak pasti atau ketidakpastian jalan raya. penyelesaian dilakukan dengan mengguna kan deep reinforcement learning pada hal ini mengguna kan metode deep qnetwork dqn dengan mengga bungk an multi header attention kedalam a rsitektur dqn.

metodologipenelitian
3.2 pengumpulan data langkah awal adalah mengumpulkan dataset yang akurat dan relevan. dataset didapatkan dari data sekunder dataset ini merupakan hal yang penting dari simulasi dan eksperimen mencakup koordinat lokasi yang mungkin meliputi lokasi depot dan titik pengiriman jendela waktu untuk setiap pengiriman yang menentukan batas awal dan akhir kapan pengiriman harus dilakukan serta jumlah kendaraan. data ini harus mencerminkan situasi dunia nyata untuk memastikan bahwa model yang dikembangkan dapat diaplikasikan secara praktis. 3.3 persiapan data langkah berikutnya adalah persiapan data. pada persiapan data dilakukan normalisasi data. n ormalisasi merupakan proses penting untuk menyamakan skala data memastikan bahwa model dapat memprosesnya dengan efisien. normalisasi minmax digunakan pada penelitian ini. minmax adalah teknik yang mengubah skala nilai data ke dalam rentang baru seperti 0 hingga 1 atau 1 hingga 1. teknik ini memastikan bahwa setiap fitur atau kolom data memberikan kontribusi yang seimbang dalam analisis tanpa membiarkan fitur dengan skala besar m endominasi. pengecekan matriks korelasi dilakukan untuk memahami hubungan antara variabel variabel dalam dataset. korelasi membantu mengidentifikasi fitur fitur yang saling terkait dan memberikan wawasan tentang bagaimana setiap fitur dapat mempengaruhi model prediksi rute. koefisien korelasi pearson digunakan untuk mengukur hubungan linear antara fitur. 3.4 desain model implementasi deep q network dqn dengan mekanisme attention untuk dynamic vehicle routing problem with time windows dvrptw melibatkan beberapa langkah utama mulai dari pemilihan kerangka kerja hingga pembuatan lingkungan simulasi. 1. pemilihan kerangka kerja 28 kerangka kerja yang digunakan yaitu tensorflow dimana kerangka kerja ini menawarkan lingkungan yang komprehensif dengan tensorboard untuk visualisasi serta dukungan terhadap tpu untuk akselerasi komputasi. tensorflow mungkin lebih cocok untuk produksi dan skala besar. 2. desain model dqn dan multi head erattention dqn adalah algoritma pembelajaran penguatan yang menggunakan jaringan saraf tiruan untuk memperkirakan fungsi nilai q yang merepresentasikan nilai maksimum hadiah kumulatif yang diharapkan diberikan sebuah state dan semua strategi yang mungkin diambil. i mplementasi dqn melibatkan beberapa komponen utama jaringan q jaringan ini memperkirakan nilai q untuk setiap aksi dari state tertentu. dalam kasus dvrptw input bisa berupa representasi dari state saat ini misalnya lokasi kendaraan status pengiriman dan output adalah nilai q untuk setiap kemungkinan aksi misalnya memilih lokasi pengiriman berikutnya. memory replay untuk meningkatkan stabilitas dan efisiensi pembelajaran dqn menggunakan teknik memory replay di mana transisi state aksi reward state baru disimpan dalam sebuah buffer . batch transisi ini kemudian digunakan untuk melatih jaringan q memungkinkan pengalaman dari masa lalu digunakan kembali. strategi eksplorasi seperti εgreedy di mana aksi acak dipilih dengan probabilitas ε untuk mendorong eksplorasi lingkungan. mekanis me attention terdiri dari tiga matriks utama query q key k dan value v untuk setiap head i. adapun langkah langkahnya implementasinya sebagai berikut a. definisikan ukuran input dan parameter mentukan jumlah fitur input dimensi embedding jumlah heads untuk mekanisme attention dan jumlah unit dalam lapisan tersembunyi dqn. serta jumlah tindakan yang mungkin dilakukan oleh agen. b. definisikan multi head er attention menerapkan mekanisme multi head er attention pada representasi vektor dari embedding layer . multi head er attention menggunakan query q key k 29 dan value v untuk menangkap hubungan kontekstual dalam data. proses ini membantu model untuk fokus pada aspek aspek penting dari data input. attention score dihitung dengan mengalikan query dengan key kemudian membaginya dengan skala biasanya akar dari dimensi key dan menerapkan fungsi softmax untuk mendapatkan bobot attention . output attention diperoleh dengan mengalikan bobot perhatian dengan value . multi head er attention melakukan proses ini beberapa kali secara paralel dengan beberapa heads dan hasilnya digabungkan untuk data i nput. c. definisikan dqn dengan lapisan tersembunyi memb uat beberapa lapisan tersembunyi hidden layers menggunakan fungsi aktivasi relu. lapisan tersembunyi ini memungkinkan jaringan untuk belajar representasi yang kompleks dari data input. output dari mekanisme attention diberikan sebagai input ke dqn. dqn memperkirakan q value s untuk setiap tindakan yang mungkin dilakukan oleh agen berdasarkan representasi state yang telah diperkaya. d. output layer untuk q value s lapisan output menghasilkan q value s untuk setiap tindakan yang mungkin dilakukan oleh agen. q value s ini menunjukkan seberapa baik setiap tindakan dalam memaksimalkan reward di masa depan. misalnya jika agen memiliki 5 kemungkinan tindakan lapisan output akan menghasilkan 5 q value s satu untuk setiap tindakan. e. memilih tindakan menggunakan strategi εgreedy implementasikan strategi εgreedy untuk memastikan agen mengeksplorasi lingkungan sekaligus mengeksploitasi pengetahuan yang ada. dengan probabilitas ε agen memilih tindakan secara acak untuk eksplorasi dan dengan probabilitas 1 ε agen memilih tindakan dengan q value tertinggi untuk eksploitasi. f. memperbarui model dengan pengalaman dari replay buffer mengg unakan replay buffer untuk menyimpan transisi state action reward next state dan menggunakannya untuk melatih model. batch transisi 30 diambil secara acak dari replay buffer untuk mengurangi korelasi antara sampel pelatihan dan meningkatkan stabilitas pelatihan. 3.5 pelatihan model selama fase pelatihan model secara berulang kali dihadapkan pada berbagai skenario dari masalah r ute kendaraan. untuk setiap episode model mengambil serangkaian aksi berdasarkan policy atau kebijakan saat ini yang awalnya adalah kebijakan acak dengan tujuan meminimalkan jarak total dan memenuhi jendela waktu pengiriman. setelah mengambil aksi model menerima feedback dari lingkungan berupa reward yang merupakan ukuran dari performa aksi tersebut dan state baru yang mencerminkan kondisi terkini dari lingk ungan setelah aksi diambil. informasi ini digunakan untuk memperbarui kebijakan model dengan cara mengoptimalkan parameter jaringan sehingga meningkatkan estimasi nilai q yang merepresentasikan hadiah kumulatif yang diharapkan. untuk meningkatkan stabilitas dan efisiensi pelatihan teknik seperti experience replay dan target networks digunakan. experience replay memungkinkan model untuk belajar dari pengalaman masa lalu yang disimpan dalam memory replay sedangkan target networks membantu mengurangi pergeseran target yang bergerak selama proses pembelajaran. melalui interaksi yang berulang dan proses optimisasi ini model secara bertahap belajar untuk memprediksi nilai q yang lebih akurat untuk setiap kombinasi state dan aksi yang mengarah pada pembentukan kebijakan rute yang lebih optimal. selain itu pada tahap pelatihan model ini juga dilakukan penyetelan hyperparameter untuk menemukan nilai optimal hyperparameter guna meningkatkan kinerja model. ini merupakan langkah penting dalam machine learning karena dapat menghasilkan peningkatan akurasi efisiensi dan generalizability model. penyetelan hyperparameter menggunakan teknik random search yaitu teknik yang digunakan untuk penyetelan hyperparameter yang melibatkan pemilihan acak dari ruang yang ditentukan untuk menemukan kombinasi terbaik yang mengoptimalkan kinerja model. teknik ini lebih efektif dan 31 efisien untuk penyetelan hyperparameter terutama pada kasus dengan ruang pencarian yang besar serta dapat menemukan solusi yang baik dalam waktu yang lebih singkat. 3.6 evaluasi model setelah fase pelatihan model deep q network dqn dengan multi header attention untuk dynamic vehicle routing problem with time windows dvrptw selesai langkah evaluasi menjadi penting untuk memahami seberapa efektif model dalam menyelesaikan masalah yang ditargetkan. evaluasi dilakukan dengan menguji model terhadap kumpulan data pengujian yang tidak terlibat selama proses pelatihan me mberikan masukan penting tentang kemampuan generalisasi model terhadap skenario baru dan belum pernah dilihat. dalam kont eks dvrptw metrik yang relevan seperti total jarak tempuh oleh semua kendaraan dan kepatuhan terhadap jendela waktu pengiriman menjadi fokus utama. total jarak tempuh mencerminkan efisiensi rute yang dihasilkan sementara kepatuhan terhadap jendela waktu mencerminkan kualitas layanan yang dapat dijamin oleh model. 3.7 analisis dan penyempurnaan langkah terakhir yaitu analisis secara mendalam kinerja model pada dataset pengujian. penyempurnaan dilakukan untuk mengatasi kelemahan yang telah dianalisis sebelumnya seperti penyempurnaan pada tuning hyperparameter untuk peningkatan kinerja modifikasi arsitektur dan pelatihan ulang model. 3.8 jadwal penelitian jadwal penelitian digunakan untuk meningkatkan efektivitas dalam proses penelitian. adanya jadwal penelitian ini setiap proses penelitian sudah terjadwal dalam tabel 3.1 sehingga penelitian lebih efektif dan optimal. 