3.1 Kerangka Umum Penelitian
     Berikut ini merupakan kerangka penelitian yang menjelaskan tahapan yang dilakukan dalam penelitan ini. Berikut gambar 3.1 diagram alir penelitian

Persiapan Data
Definisikan Ukuran Input dan Parameter
Definisikan Multi-Head Attention
Desain Model
Definisikan DQN dengan Lapisan Tersembunyi
Output Layer untuk Q-values
Fungsi untuk Memilih Tindakan menggunakan
Strategi e-greedy
Fungsi untuk Memperbarui Model dengan
Pengalaman dari Replay Buffer
Gambar 3.1 Diagram Alir Penelitian


3.2 Pengumpulan Data
     Langkah awal adalah mengumpulkan dataset yang akurat dan relevan. Dataset didapatkan dari data sekunder, dataset ini merupakan hal yang penting dari simulasi dan eksperimen, mencakup koordinat lokasi yang mungkin meliputi lokasi depot dan titik pengiriman, jendela waktu untuk setiap pengiriman yang menentukan batas awal dan akhir kapan pengiriman harus dilakukan, serta jumlah kendaraan. Data ini harus mencerminkan situasi dunia nyata untuk memastikan bahwa model yang dikembangkan dapat diaplikasikan secara praktis.

3.3 Persiapan Data
     Langkah berikutnya adalah persiapan data. Pada persiapan data dilakukan normalisasi data. Normalisasi merupakan proses penting untuk menyamakan skala data, memastikan bahwa model dapat memprosesnya dengan efisien. Normalisasi min-max digunakan pada penelitian ini. Min-max adalah teknik yang mengubah skala nilai data ke dalam rentang baru seperti 0 hingga 1 atau -1 hingga 1. Teknik ini memastikan bahwa setiap fitur atau kolom data memberikan kontribusi yang seimbang dalam analisis tanpa membiarkan fitur dengan skala besar mendominasi. Pengecekan matriks korelasi dilakukan untuk memahami hubungan antara variabel-variabel dalam dataset. Korelasi membantu mengidentifikasi fitur-fitur yang saling terkait dan memberikan wawasan tentang bagaimana setiap fitur dapat mempengaruhi model prediksi rute. Koefisien Korelasi Pearson digunakan untuk
mengukur hubungan linear antara fitur.


3.4 Desain model
     Implementasi Deep Q-Network (DQN) dengan mekanisme attention untuk Dynamic Vehicle Routing Problem with Time Windows (DVRPTW) melibatkan beberapa langkah utama, mulai dari pemilihan kerangka kerja hingga pembuatan lingkungan simulasi.
1. Pemilihan Kerangka Kerja
     Kerangka kerja yang digunakan yaitu TensorFlow, dimana kerangka kerja ini menawarkan lingkungan yang komprehensif dengan TensorBoard untuk visualisasi, serta dukungan terhadap TPU untuk akselerasi komputasi. TensorFlow mungkin lebih cocok untuk produksi dan skala besar.
2. Desain model DQN dan Multi header-attention
     DQN adalah algoritma pembelajaran penguatan yang menggunakan jaringan saraf tiruan untuk memperkirakan fungsi nilai Q, yang merepresentasikan nilai maksimum hadiah kumulatif yang diharapkan, diberikan sebuah state dan semua strategi yang mungkin diambil. Implementasi DQN melibatkan beberapa komponen utama:
Jaringan Q: Jaringan ini memperkirakan nilai Q untuk setiap aksi dari state tertentu. Dalam kasus DVRPTW, input bisa berupa representasi dari state saat ini (misalnya, lokasi kendaraan, status pengiriman) dan output adalah nilai Q untuk setiap kemungkinan aksi (misalnya, memilih lokasi pengiriman berikutnya).
Memory Replay: Untuk meningkatkan stabilitas dan efisiensi pembelajaran, DQN menggunakan teknik memory replay, di mana transisi (state, aksi, reward, state baru) disimpan dalam sebuah buffer. Batch transisi ini kemudian digunakan untuk melatih jaringan Q, memungkinkan pengalaman dari masa lalu digunakan kembali. Strategi Eksplorasi: Seperti e-greedy, di mana aksi acak dipilih dengan probabilitas e untuk mendorong eksplorasi lingkungan.
      Mekanisme attention terdiri dari tiga matriks utama: Query (Q), Key (K), dan Value (V) untuk setiap head i. Adapun langkah-langkahnya implementasinya sebagai berikut:
a. Definisikan Ukuran Input dan Parameter
      Mentukan jumlah fitur input, dimensi embedding, jumlah heads untuk mekanisme attention, dan jumlah unit dalam lapisan tersembunyi DQN. Serta jumlah tindakan yang mungkin dilakukan oleh agen.
b. Definisikan Multi-Header Attention
      Menerapkan mekanisme Multi-Header Attention pada representasi vektor dari embedding layer. Multi-Header Attention menggunakan Query (Q), Key (K), dan Value (V) untuk menangkap hubungan kontekstual dalam data. Proses ini membantu model untuk fokus pada aspek-aspek penting dari data input. Attention Score dihitung dengan mengalikan Query dengan Key, kemudian membaginya dengan skala (biasanya akar dari dimensi Key) dan menerapkan fungsi softmax untuk mendapatkan bobot attention. Output Attention diperoleh dengan mengalikan bobot perhatian dengan Value. Multi-Header Attention melakukan proses ini beberapa kali secara paralel (dengan beberapa "heads") dan hasilnya digabungkan untuk data input.
c. Definisikan DQN dengan Lapisan Tersembunyi
      Membuat beberapa lapisan tersembunyi (hidden layers) menggunakan fungsi aktivasi ReLU. Lapisan tersembunyi ini memungkinkan jaringan untuk belajar representasi yang kompleks dari data input. Output dari mekanisme attention diberikan sebagai input ke DQN. DQN memperkirakan Q-values untuk setiap tindakan yang mungkin dilakukan oleh agen berdasarkan representasi state yang telah diperkaya.
d. Output Layer untuk Q-values
      Lapisan output menghasilkan Q-values untuk setiap tindakan yang mungkin dilakukan oleh agen. Q-values ini menunjukkan seberapa baik setiap tindakan dalam memaksimalkan reward di masa depan. Misalnya, jika agen memiliki 5 kemungkinan tindakan, lapisan output akan menghasilkan 5 Q-values, satu untuk setiap tindakan.
e. Memilih Tindakan menggunakan Strategi e-greedy
      Implementasikan strategi e-greedy untuk memastikan agen mengeksplorasi lingkungan sekaligus mengeksploitasi pengetahuan yang ada. Dengan probabilitas e, agen memilih tindakan secara acak untuk eksplorasi, dan dengan probabilitas 1- e, agen memilih tindakan dengan Q-value tertinggi untuk eksploitasi.
f. Memperbarui Model dengan Pengalaman dari Replay Buffer
      Menggunakan replay buffer untuk menyimpan transisi (state, action, reward, next state) dan menggunakannya untuk melatih model. Batch transisi diambil secara acak dari replay buffer untuk mengurangi korelasi antara sampel pelatihan dan meningkatkan stabilitas pelatihan.

3.5 Pelatihan Model
     Selama fase pelatihan, model secara berulang kali dihadapkan pada berbagai skenario dari masalah rute kendaraan. Untuk setiap episode, model mengambil serangkaian aksi berdasarkan policy atau kebijakan saat ini yang awalnya adalah kebijakan acak dengan tujuan meminimalkan jarak total dan memenuhi jendela waktu pengiriman. Setelah mengambil aksi, model menerima feedback dari lingkungan berupa reward yang merupakan ukuran dari performa aksi tersebut dan state baru yang mencerminkan kondisi terkini dari lingkungan setelah aksi diambil. Informasi ini digunakan untuk memperbarui kebijakan model dengan cara mengoptimalkan parameter jaringan sehingga meningkatkan estimasi nilai Q, yang merepresentasikan hadiah kumulatif yang diharapkan.
     Untuk meningkatkan stabilitas dan efisiensi pelatihan, teknik seperti experience replay dan target networks digunakan. Experience replay memungkinkan model untuk belajar dari pengalaman masa lalu yang disimpan dalam memory replay, sedangkan target networks membantu mengurangi pergeseran target yang bergerak selama proses pembelajaran. Melalui interaksi yang berulang dan proses optimisasi ini, model secara bertahap belajar untuk memprediksi nilai Q yang lebih akurat untuk setiap kombinasi state dan aksi, yang mengarah pada pembentukan kebijakan rute yang lebih optimal.
     Selain itu pada tahap pelatihan model ini juga dilakukan penyetelan hyperparameter untuk menemukan nilai optimal hyperparameter guna meningkatkan kinerja model. Ini merupakan langkah penting dalam machine learning karena dapat menghasilkan peningkatan akurasi, efisiensi, dan generalizability model. Penyetelan hyperparameter menggunakan teknik random search, yaitu teknik yang digunakan untuk penyetelan hyperparameter yang melibatkan pemilihan acak dari ruang yang ditentukan untuk menemukan kombinasi terbaik yang mengoptimalkan kinerja model. Teknik ini lebih efektif dan efisien untuk penyetelan hyperparameter terutama pada kasus dengan ruang pencarian yang besar, serta dapat menemukan solusi yang baik dalam waktu yang lebih singkat.

3.6 Evaluasi Model
     Setelah fase pelatihan model Deep Q-Network (DQN) dengan multi-header attention untuk Dynamic Vehicle Routing Problem with Time Windows (DVRPTW) selesai, langkah evaluasi menjadi penting untuk memahami seberapa efektif model dalam menyelesaikan masalah yang ditargetkan. Evaluasi dilakukan dengan menguji model terhadap kumpulan data pengujian yang tidak terlibat selama proses pelatihan, memberikan masukan penting tentang kemampuan generalisasi model terhadap skenario baru dan belum pernah dilihat. Dalam konteks DVRPTW, metrik yang relevan seperti total jarak tempuh oleh semua kendaraan dan kepatuhan terhadap jendela waktu pengiriman menjadi fokus utama. Total jarak tempuh mencerminkan efisiensi rute yang dihasilkan, sementara kepatuhan terhadap jendela waktu mencerminkan kualitas layanan yang dapat dijamin oleh model.

3.7 Analisis dan Penyempurnaan
     Langkah terakhir yaitu analisis secara mendalam kinerja model pada dataset pengujian. Penyempurnaan dilakukan untuk mengatasi kelemahan yang telah dianalisis sebelumnya seperti penyempurnaan pada tuning hyperparameter untuk peningkatan kinerja, modifikasi arsitektur dan pelatihan ulang model.

3.8 Jadwal Penelitian
     Jadwal penelitian digunakan untuk meningkatkan efektivitas dalam proses penelitian. Adanya jadwal penelitian ini setiap proses penelitian sudah terjadwal dalam tabel 3.1 sehingga penelitian lebih efektif dan optimal.
Tabel 3.1 Jadwal Penelitian