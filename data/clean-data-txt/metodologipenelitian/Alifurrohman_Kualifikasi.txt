3.2 pengumpulan data langkah awal adalah mengumpulkan dataset yang akurat dan relevan. dataset didapatkan dari data sekunder dataset ini merupakan hal yang penting dari simulasi dan eksperimen mencakup koordinat lokasi yang mungkin meliputi lokasi depot dan titik pengiriman jendela waktu untuk setiap pengiriman yang menentukan batas awal dan akhir kapan pengiriman harus dilakukan serta jumlah kendaraan. data ini harus mencerminkan situasi dunia nyata untuk memastikan bahwa model yang dikembangkan dapat diaplikasikan secara praktis. 3.3 persiapan data langkah berikutnya adalah persiapan data. pada persiapan data dilakukan normalisasi data. n ormalisasi merupakan proses penting untuk menyamakan skala data memastikan bahwa model dapat memprosesnya dengan efisien. normalisasi minmax digunakan pada penelitian ini. minmax adalah teknik yang mengubah skala nilai data ke dalam rentang baru seperti 0 hingga 1 atau 1 hingga 1. teknik ini memastikan bahwa setiap fitur atau kolom data memberikan kontribusi yang seimbang dalam analisis tanpa membiarkan fitur dengan skala besar m endominasi. pengecekan matriks korelasi dilakukan untuk memahami hubungan antara variabel variabel dalam dataset. korelasi membantu mengidentifikasi fitur fitur yang saling terkait dan memberikan wawasan tentang bagaimana setiap fitur dapat mempengaruhi model prediksi rute. koefisien korelasi pearson digunakan untuk mengukur hubungan linear antara fitur. 3.4 desain model implementasi deep q network dqn dengan mekanisme attention untuk dynamic vehicle routing problem with time windows dvrptw melibatkan beberapa langkah utama mulai dari pemilihan kerangka kerja hingga pembuatan lingkungan simulasi. 1. pemilihan kerangka kerja 28 kerangka kerja yang digunakan yaitu tensorflow dimana kerangka kerja ini menawarkan lingkungan yang komprehensif dengan tensorboard untuk visualisasi serta dukungan terhadap tpu untuk akselerasi komputasi. tensorflow mungkin lebih cocok untuk produksi dan skala besar. 2. desain model dqn dan multi head erattention dqn adalah algoritma pembelajaran penguatan yang menggunakan jaringan saraf tiruan untuk memperkirakan fungsi nilai q yang merepresentasikan nilai maksimum hadiah kumulatif yang diharapkan diberikan sebuah state dan semua strategi yang mungkin diambil. i mplementasi dqn melibatkan beberapa komponen utama jaringan q jaringan ini memperkirakan nilai q untuk setiap aksi dari state tertentu. dalam kasus dvrptw input bisa berupa representasi dari state saat ini misalnya lokasi kendaraan status pengiriman dan output adalah nilai q untuk setiap kemungkinan aksi misalnya memilih lokasi pengiriman berikutnya. memory replay untuk meningkatkan stabilitas dan efisiensi pembelajaran dqn menggunakan teknik memory replay di mana transisi state aksi reward state baru disimpan dalam sebuah buffer . batch transisi ini kemudian digunakan untuk melatih jaringan q memungkinkan pengalaman dari masa lalu digunakan kembali. strategi eksplorasi seperti εgreedy di mana aksi acak dipilih dengan probabilitas ε untuk mendorong eksplorasi lingkungan. mekanis me attention terdiri dari tiga matriks utama query q key k dan value v untuk setiap head i. adapun langkah langkahnya implementasinya sebagai berikut a. definisikan ukuran input dan parameter mentukan jumlah fitur input dimensi embedding jumlah heads untuk mekanisme attention dan jumlah unit dalam lapisan tersembunyi dqn. serta jumlah tindakan yang mungkin dilakukan oleh agen. b. definisikan multi head er attention menerapkan mekanisme multi head er attention pada representasi vektor dari embedding layer . multi head er attention menggunakan query q key k 29 dan value v untuk menangkap hubungan kontekstual dalam data. proses ini membantu model untuk fokus pada aspek aspek penting dari data input. attention score dihitung dengan mengalikan query dengan key kemudian membaginya dengan skala biasanya akar dari dimensi key dan menerapkan fungsi softmax untuk mendapatkan bobot attention . output attention diperoleh dengan mengalikan bobot perhatian dengan value . multi head er attention melakukan proses ini beberapa kali secara paralel dengan beberapa heads dan hasilnya digabungkan untuk data i nput. c. definisikan dqn dengan lapisan tersembunyi memb uat beberapa lapisan tersembunyi hidden layers menggunakan fungsi aktivasi relu. lapisan tersembunyi ini memungkinkan jaringan untuk belajar representasi yang kompleks dari data input. output dari mekanisme attention diberikan sebagai input ke dqn. dqn memperkirakan q value s untuk setiap tindakan yang mungkin dilakukan oleh agen berdasarkan representasi state yang telah diperkaya. d. output layer untuk q value s lapisan output menghasilkan q value s untuk setiap tindakan yang mungkin dilakukan oleh agen. q value s ini menunjukkan seberapa baik setiap tindakan dalam memaksimalkan reward di masa depan. misalnya jika agen memiliki 5 kemungkinan tindakan lapisan output akan menghasilkan 5 q value s satu untuk setiap tindakan. e. memilih tindakan menggunakan strategi εgreedy implementasikan strategi εgreedy untuk memastikan agen mengeksplorasi lingkungan sekaligus mengeksploitasi pengetahuan yang ada. dengan probabilitas ε agen memilih tindakan secara acak untuk eksplorasi dan dengan probabilitas 1 ε agen memilih tindakan dengan q value tertinggi untuk eksploitasi. f. memperbarui model dengan pengalaman dari replay buffer mengg unakan replay buffer untuk menyimpan transisi state action reward next state dan menggunakannya untuk melatih model. batch transisi 30 diambil secara acak dari replay buffer untuk mengurangi korelasi antara sampel pelatihan dan meningkatkan stabilitas pelatihan. 3.5 pelatihan model selama fase pelatihan model secara berulang kali dihadapkan pada berbagai skenario dari masalah r ute kendaraan. untuk setiap episode model mengambil serangkaian aksi berdasarkan policy atau kebijakan saat ini yang awalnya adalah kebijakan acak dengan tujuan meminimalkan jarak total dan memenuhi jendela waktu pengiriman. setelah mengambil aksi model menerima feedback dari lingkungan berupa reward yang merupakan ukuran dari performa aksi tersebut dan state baru yang mencerminkan kondisi terkini dari lingk ungan setelah aksi diambil. informasi ini digunakan untuk memperbarui kebijakan model dengan cara mengoptimalkan parameter jaringan sehingga meningkatkan estimasi nilai q yang merepresentasikan hadiah kumulatif yang diharapkan. untuk meningkatkan stabilitas dan efisiensi pelatihan teknik seperti experience replay dan target networks digunakan. experience replay memungkinkan model untuk belajar dari pengalaman masa lalu yang disimpan dalam memory replay sedangkan target networks membantu mengurangi pergeseran target yang bergerak selama proses pembelajaran. melalui interaksi yang berulang dan proses optimisasi ini model secara bertahap belajar untuk memprediksi nilai q yang lebih akurat untuk setiap kombinasi state dan aksi yang mengarah pada pembentukan kebijakan rute yang lebih optimal. selain itu pada tahap pelatihan model ini juga dilakukan penyetelan hyperparameter untuk menemukan nilai optimal hyperparameter guna meningkatkan kinerja model. ini merupakan langkah penting dalam machine learning karena dapat menghasilkan peningkatan akurasi efisiensi dan generalizability model. penyetelan hyperparameter menggunakan teknik random search yaitu teknik yang digunakan untuk penyetelan hyperparameter yang melibatkan pemilihan acak dari ruang yang ditentukan untuk menemukan kombinasi terbaik yang mengoptimalkan kinerja model. teknik ini lebih efektif dan 31 efisien untuk penyetelan hyperparameter terutama pada kasus dengan ruang pencarian yang besar serta dapat menemukan solusi yang baik dalam waktu yang lebih singkat. 3.6 evaluasi model setelah fase pelatihan model deep q network dqn dengan multi header attention untuk dynamic vehicle routing problem with time windows dvrptw selesai langkah evaluasi menjadi penting untuk memahami seberapa efektif model dalam menyelesaikan masalah yang ditargetkan. evaluasi dilakukan dengan menguji model terhadap kumpulan data pengujian yang tidak terlibat selama proses pelatihan me mberikan masukan penting tentang kemampuan generalisasi model terhadap skenario baru dan belum pernah dilihat. dalam kont eks dvrptw metrik yang relevan seperti total jarak tempuh oleh semua kendaraan dan kepatuhan terhadap jendela waktu pengiriman menjadi fokus utama. total jarak tempuh mencerminkan efisiensi rute yang dihasilkan sementara kepatuhan terhadap jendela waktu mencerminkan kualitas layanan yang dapat dijamin oleh model. 3.7 analisis dan penyempurnaan langkah terakhir yaitu analisis secara mendalam kinerja model pada dataset pengujian. penyempurnaan dilakukan untuk mengatasi kelemahan yang telah dianalisis sebelumnya seperti penyempurnaan pada tuning hyperparameter untuk peningkatan kinerja modifikasi arsitektur dan pelatihan ulang model. 3.8 jadwal penelitian jadwal penelitian digunakan untuk meningkatkan efektivitas dalam proses penelitian. adanya jadwal penelitian ini setiap proses penelitian sudah terjadwal dalam tabel 3.1 sehingga penelitian lebih efektif dan optimal.