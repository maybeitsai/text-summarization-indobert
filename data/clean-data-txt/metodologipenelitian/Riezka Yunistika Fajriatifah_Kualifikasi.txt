Dalam disertasi ini diusulkan metode klasifikasi anomali paru akibat Covid- 19 pada citra sinar-X paru yang terbagi ke dalam tiga tahap utama, seperti yang ditunjukkan pada Gambar 3.1.
Gambar 3,1 menggambarkan peta penelitian yang terdiri dari tiga tahap utama: pelatihan model tunggal, ensemble learning dengan metode stacking dan implementasi model. Tahap pertama terdiri dari beberapa langkah, dimulai dari preprocessing dataset. Pada tahap preprocessing dilakukan augmentasi citra untuk meningkatkan variasi data pada dataset. Tahap selanjutnya adalah dataset splitting dengan membagi dataset menjadi tiga subset: dataset pelatihan, dataset validasi, dan dataset pengujian. Tahapan selanjutnya adalah pembuatan dan pelatihan model.
Proses pembuatan dan pelatihan model memanfaatkan teknik transfer learning pada beberapa arsitektur (ConvNext, DenseNet, Vgg19, ResnetV2_50 dan Xception). Teknik transfer learning bertujuan untuk mempercepat proses pelatihan dengan mentransfer bobot dari model yang sudah dilatih sebelumnya. Proses pelatihan juga memanfaatkan teknik penjadwalan learning rate, agar model dapat mencapai nilai gradient optimal sehingga mendapatkan performa model terbaik dari proses pelatihan. Tahap berikutnya adalah mengevaluasi model yang telah dilatih menggunakan dataset uji yang tidak digunakan selama proses pelatihan. Evaluasi model dilakukan dengan mengukur performanya menggunakan classification metric seperti accuracy, precision, recall, danfl-score. Hasil dari proses ini berupa beberapa model terbaik. Hasil prediksi dari beberapa model ini disimpan untuk digunakan pada tahap kedua.
Tahap kedua dalam penelitian ini adalah ensemble learning dengan metode stacking. Pada tahap ini hasil prediksi dari berbagai model tunggal yang telah dilatih sebelumnya dikumpulkan. Hasil prediksi ini mencakup probabilitas atau kelas yang diprediksi oleh masing-masing model tunggal untuk setiap sampel dalam dataset. Hasil prediksi yang dikumpulkan digunakan untuk membentuk dataset baru. Dataset baru ini terdiri dari fitur-fitur yang merupakan prediksi dari model-model tunggal sebelumnya, serta label asli dari dataset. Dataset baru ini bertujuan untuk merepresentasikan pola dan kesalahan yang dihasilkan oleh model- model tunggal. Dataset baru yang telah dibuat kemudian digunakan untuk melatih meta-model. Meta-model bertujuan untuk belajar dari kesalahan dan pola prediksi yang dihasilkan oleh model-model tunggal. Meta-model akan menggabungkan prediksi dari berbagai model tunggal untuk menghasilkan prediksi akhir yang lebih 
akurat. Setelah meta-model dilatih, dilakukan evaluasi model untuk memastikan kinerjanya.
Langkah selanjutnya adalah tahap inferensi model, yang bertujuan untuk menerapkan model yang telah dilatih pada data baru agar dapat membuat prediksi. Proses inferensi ini menggunakan layanan web API (Application Programming Interface) berbasis web yang akan dimuat dalam sebuah gambar Docker, sehingga mempermudah proses penyebaran (deployment) pada perangkat dengan berbagai lingkungan yang berbeda.

3.1 Dataset COVID-19 Radiography
       Dataset COVID-19 Radiography adalah kumpulan data citra sinar-X paru- paru. Dataset ini terdiri dari 21165 citra sinar-X paru yang dikategorikan menjadi 4 kelas seperti: 3616 citra sinar-X positif Covid-19, 10119 citra normal, 6012 citra lung opacity dan 1345 citra pneumonia virus. Gambar 3.2 menunjukkan beberapa citra sinar-X paru COVID-19, viral pneumonia, lung opacity dan normal yang disediakan oleh database Radiografi COVID-19. Dataset Radiograph COVID-19 dapat diakses di platform Kaggle pada url COVID-19 Radiography Database .
       Gambar 3.2 di atas merupakan contoh sampel citra dari dataset Radiograph COVID-19. Citra sinar-X Normal memiliki paru-paru yang bersih tanpa bintik- bintik putih, yang menandakan bahwa paru-paru tidak mengalami peradangan atau infeksi. Citra sinar-X COVID-19 memiliki ciri khas berupa bintik-bintik putih, yang menunjukkan adanya cairan dan menandakan infeksi pada paru-paru. Citra sinar-X Lung Opacity menunjukkan ciri khas berupa bintik-bintik putih keabu- abuan. Pada citra sinar-X Viral Pneumonia, terdapat bintik-bintik putih pada bagian atas paru-paru yang menunjukkan infeksi pada saluran pernapasan atas.

3.2 Preprocessing Dataset
       Sebelum digunakan dalam model deep learning, dataset yang telah disiapkan perlu diolah terlebih dahulu melalui tahap preprocessing. Tahap preprocessing pertama yang dilakukan adalah augmentasi citra untuk memperkaya dataset dengan berbagai variasi citra. Tahap selanjutnya adalah mengubah ukuran (resize) semua citra dalam dataset agar sesuai dengan kebutuhan model. Tahap terakhir adalah normalisasi citra, dimana nilai piksel setiap citra diubah ke rentang 0 hingga 1. Memperlihatkan tahapanpreprocessing dataset yang digunakan dalam penelitian ini. Pada tahap preprocessing dataset, dilakukan augmentasi citra dengan memutar citra-citra dalam dataset untuk menghasilkan variasi gambar yang terotasi. Hasil augmentasi citra ini kemudian diubah ukurannya menjadi dimensi yang sama. Selanjutnya, tipe data citra diubah ke format torch.Tensor agar dapat diproses oleh GPU. Tahap selanjutnya adalah normalisasi citra menggunakan nilai rata-rata dan standar deviasi dari dataset ImageNet, penggunaan nilai tersebut memiliki tujuan agar hasil normalisasi sama dengan proses pelatihan bobot untuk transfer learning model. Proses preprocessing pada dataset pelatihan dilakukan menggunakan algoritma 3.1.

Algoritma 3.1 Proses Preprocessing pada Dataset
Input:
Citra dari dataset COVID-19 Radiography
Output:
Citra yang telah dipreprocessing
Proses:
1. Mulai.
2. Melakukan random rotation pada citra menggunakan fungsi Random Rotation() dari PyTorch dengan parameter rotation_range sebesar 10.
3. Mengubah ukuran citra menjadi 224x224 menggunakan fungsi Resize() dengan parameter cfg['model']['input_size'].
4. Mengonversi citra menjadi tensor menggunakan fungsi ToTensor().
5. Melakukan normalisasi citra menggunakan nilai mean dan std dari dataset ImageNet yang diberikan, yaitu mean=[0.485, 0.456, 0.406] dan std=[0.229, 0.224, 0.225] menggunakan fungsi Normalize().
6. Selesai.
       Algoritma 3.1 menggambarkan langkah-langkah preprocessing yang dilakukan pada dataset. Langkah pertama adalah melakukan augmentasi citra menggunakan RandomRotation() dengan parameter rotation_range sebesar 10 derajat. Langkah selanjutnya adalah mengubah ukuran citra dengan nilai parameter (224, 224) menggunakan fungsi Resize(). Setelah itu, tipe data diubah menjadi torch.Tensor dengan fungsi ToTensor() untuk persiapan pengolahan pada perangkat. Terakhir, gambar dinormalisasi dengan fungsi Normalize() menggunakan nilai mean dan standard deviasi dari dataset ImageNet, yaitu [0.485, 0.456, 0.406] dan [0.229, 0.224, 0.225].

3.3 Dataset Splitting
      Dataset Splitting merupakan proses membagi dataset menjadi beberapa subset. Pada penelitian ini dataset dibagi menjadi 3 subset: yaitu dataset pelatihan, dataset validasi, dan dataset pengujian. Dataset pelatihan dan dataset validasi digunakan untuk melatih model deep learning. Dataset latih digunakan untuk model dalam mengekstraksi fitur-fitur dari data tersebut. Fitur fitur yang diekstraksi kemudian divalidasi menggunakan dataset validasi. Dataset pengujian adalah subset yang digunakan untuk menguji kinerja model setelah proses pelatihan selesai. Proses pembagian dataset dapat dilihat pada gambar 3.3
       Gambar 3.3 menggambarkan tahapan dataset splitting. Proses pembagian dilakukan dengan membagi total dataset menjadi 80% untuk dataset pelatihan, 10% untuk dataset validasi dan 10% untuk dataset pengujian. Proses ini memastikan bahwa sebagian besar data digunakan untuk melatih model, sementara sisanya digunakan untuk memvalidasi performa model.
       Langkah berikutnya adalah membagi dataset menjadi beberapa batch pelatihan. Pembagian ini dilakukan untuk mengurangi beban pelatihan yang berat dan validasi hasil pelatihan dapat dilakukan secara perbatch.
Algoritma 3.2 Proses Pemuatan Data dan Pembagian ke Batch Pelatihan, Validasi, dan Pengujian
Input:
Dataset pelatihan (train_set)
Dataset validasi (val_set)
Dataset uji (test_set)
Ukuran batch untuk dataset pelatihan (train_bs) 
Ukuran batch untuk dataset uji (test_bs)
Output:
DataLoader untuk dataset pelatihan (train_dl) 
DataLoader untuk dataset validasi (val_dl) 
DataLoader untuk dataset uji (test_dl)
Proses:
1. Mulai
2. Pemuatan Data Pelatihan:
� Atur kwargs dengan batch_size=self.train_bs, shuffle=True, num_workers=2.
� Buat DataLoader untuk train_set dengan pin_memory=True dan kwargs.
� Simpan DataLoader ke dalam train_dl.
3. Pemuatan Data Validasi:
� Atur kwargs dengan batch_size=self.train_bs, shuffle=True, num_workers=2.
� Buat DataLoader untuk val_set dengan pin_memory=True dan kwargs.
� Simpan DataLoader ke dalam val_dl.
4. Pemuatan Data Uji:
� Atur kwargs dengan batch_size=test_bs, shuffle=True, num_workers=2.
� Buat DataLoader untuk test_set dengan pin_memory=True dan kwargs.
� Simpan DataLoader ke dalam test_dl.
5. Kembalikan train_dl, val_dl, dan test_dl sebagai output.
6. Selesai.
       Algoritma di atas mengimplementasikan proses pemuatan dataset dan pembagian data ke dalam batch pelatihan. Setiap batch memiliki jumlah citra sesuai dengan nilai batch size yang ditentukan. Proses pemuatan ini juga mencakup pengacakan dataset untuk setiap batch size, sehingga variasi citra dalam setiap batch pelatihan menjadi lebih beragam. Selain itu, pengaturan parameter 'pin_memory=True' diaktifkan dengan tujuan untuk melakukan caching memory, sehingga proses pelatihan lebih cepat karena dataset hanya dimuat pada iterasi pertama dari setiap batch pelatihan.

3.4 Pembuatan dan Pelatihan Model
3.4.1 Model ConvNeXt
       Model ConvNeXt adalah salah satu variasi dari arsitektur Convolutional Neural Network (CNN). Model ini mengadopsi dasar Residual Network dan memperkenalkan empat tahapan proses konvolusi. Salah satu fitur utama dari ConvNeXt adalah penggunaan layer patchify yang mengimplementasikan konvolusi 7x7 dengan stride 2, diikuti oleh layer max-pooling. Selain itu, arsitektur ini menggunakan fungsi aktivasi GELU pada blok ConvNeXt, yang sering digunakan dalam model berbasis transformer. Model ConvNeXt juga menerapkan Layer Normalization (LN), yang merupakan perbedaan signifikan dengan model berbasis CNN lainnya. Contoh blok ConvNeXt dapat dilihat pada Gambar di bawah ini.
       Model ConvNeXt memiliki beberapa varian ukuran, yaitu tiny, small, base, large, dan xlarge. Ukuran model ditentukan oleh jumlah blok ConvNeXt yang digunakan. Semakin besar modelnya, semakin banyak blok yang digunakan. Pada penelitian ini, model ConvNeXt yang digunakan adalah ukuran base, yang terdiri dari 4 stage dengan komponen blok ConvNeXt sebanyak [3, 3, 9, 3], dan ukuran dimensi fitur untuk setiap stage adalah [96, 192, 384, 768].

3.4.2 Transfer Learning
       Transfer Learning adalah proses dimana model dilatih pada sebuah dataset baru. Proses ini memanfaatkan model yang telah dilatih sebelumnya pada dataset lain (model pre-trained). Proses ini diawali dengan mengubah lapisan klasifikasi atau lapisan output agar sesuai dengan jumlah kelas yang ada pada dataset baru, yaitu 4. Langkah berikutnya adalah mentransfer bobot dari lapisan ekstraksi fitur, yang bertujuan untuk menginisialisasi model berdasarkan fitur-fitur yang telah dipelajari sebelumnya pada dataset ImageNet. Tujuan dari transfer learning adalah untuk mempercepat proses pelatihan dengan memanfaatkan bobot dari model pretrained. Implementasi proses transfer learning dapat dilihat pada Algoritma di bawah ini.
Algoritma 3.3 Proses Implementasi Transfer Learning
Input:
Pretrained (boolean): Pilihan untuk menggunakan bobot yang sudah dilatih sebelumnya dari dataset ImageNet. num_classes (int): Jumlah kelas keluaran dari model. x (torch.Tensor): Batch dari Tensor Input.
Output:
Probabilitas kelas setelah softmax (torch.Tensor).
Proses:
1. Mulai
2. Inisialisasi Model
� Inisialisasi kelas ConvNext dengan argumen pretrained dan output_class.
� Buat model convnext_base menggunakan paket timm dengan bobot pretrained dan jumlah num_classes sesuai output_class.
3. Metode Forward Pass
� Terima input tensor x.
� Proses input melalui model convnext_base.
� Terapkan fungsi softmax pada output untuk mendapatkan probabilitas kelas.
4. Metode Freeze Weights
� Iterasi melalui semua parameter di feature extractor dan set 
requires_grad ke False untuk membekukan bobot.
� Iterasi melalui semua parameter di classifier dan set requires_grad 
ke True agar dapat dilatih.
5. Metode Unfreeze Weights
� Iterasi melalui semua parameter di feature extractor dan set 
requires_grad ke True untuk membuka pembekuan bobot.
6. Selesai
       Kelas ConvNeXt pada Algoritma di atas memiliki 4 buah method yaitu init, forward, freeze, dan unfreeze. Method init adalah sebuah fungsi yang bertujuan melakukan inisiasi model sesuai dengan inputan parameter yang ada, yaitu pretrained dan num_classes. Parameter pretrained merupakan sebuah inputan boolean agar model dapat melakukan proses inisiasi dengan menggunakan bobot pra-pelatihan yang tersedia dari pytorch, sedangkan parameter num_classes merupakan parameter yang bertujuan untuk menginisasi jumlah kelas output pada model. Method forward adalah sebuah fungsi yang menerima parameter x berupa torch.Tensor dan akan dilakukan proses forward pass pada model untuk melakukan prediksi pada model. Method freeze memiliki tujuan untuk membekukan bobot agar tidak terbaharui pada proses backpropagation, sedangkan method unfreeze memiliki tujuan untuk memasukan parameter model dan melakukan pembaharuan bobot pada proses backpropagation. Model yang telah dibuat akan digunakan pada proses pelatihan model.

3.5 Pelatihan Model
       Proses pelatihan model pada penelitian ini menerapkan teknik transfer learning yang terdiri dari dua tahap, yaitu tahap adaptation dan fine tuning. Pada tahap adaotation, pelatihan model hanya dilakukan pada lapisan klasifikasi dan lapisan output. Tujuan tahap ini adalah untuk menginisialisasi dan menyesuaikan bobot pada lapisan-lapisan tersebut. Tahap adaptation dalam proses pelatihan menggunakan nilai learning rate yang tinggi sehingga model dapat dengan cepat mencapai titik optimal gradien untuk memperoleh bobot yang tepat pada lapisan klasifikasi dan lapisan output. Tahap ini berperan penting dalam proses pengklasifikasian suatu hasil feature extraction menjadi suatu output prediksi.
       Proses fine tuning adalah tahap pelatihan model secara menyeluruh yang mencakup lapisan feature extraction dan lapisan klasifikasi. Pada tahap fine tuning, nilai learning rate yang kecil digunakan agar bobot pra-pelatihan model tidak mengalami perubahan signifikan yang dapat mengakibatkan hasil pelatihan model kurang baik. Selain itu, penggunaan nilai learning rate yang kecil membantu dalam mencapai konvergensi yang stabil. Dalam penelitian ini, teknik penjadwalan learning rate berbasis Cosine Learning Rate Decay diterapkan untuk mengoptimalkan proses pelatihan baik pada tahap adaptation maupun fine tuning.
       Gambar 3.8 merupakan rancangan alur pelatihan dalam penelitian ini. Alur pelatihan model terbagi menjadi 3 tahapan yakni proses inisialisasi, proses pelatihan dan proses log hasil pelatihan. Tahap pertama merupakan inisialisasi proses pelatihan berdasarkan dataset dan config yang diinput. Langkah pertama dalam inisialisasi adalah mengatur ulang bobot fungsi loss berdasarkan distribusi dataset untuk mengatasi masalah ketidakseimbangan data.
       Proses perhitungan bobot menggunakan rumus total data dibagi total kelas dikali total data pada kelas tersebut. Langkah selanjutnya adalah proses mengubah bobot menjadi tensor dan memindahkan bobot ke hardware yang digunakan. Bobot yang telah dipindahkan ke hardware kemudian akan dimasukan ke dalam fungsi CrossEntropyLoss untuk digunakan pada proses pelatihan model. Proses inisialisasi kedua adalah inisialisasi optimizer berdasarkan pilihan yang diinginkan.
       Proses inisiasi optimizer yang menggunakan fungsi get_optimizer() merupakan fungsi yang memiliki tujuan untuk mengembalikan inisiasi optimizer berdasarkan inputan konfigurasi yang ada. Proses berikutnya adalah initialisasi learning rate scheduler yang berfungsi untuk mengubah nilai learning rate secara adaptif selama proses pelatihan berlangsung. Proses learning rate scheduler memanfaatkan Cosine Learning Rate Decay dengan pengulangan.
       Proses kedua adalah proses pelatihan model ConvNeXt yang meliputi pelatihan, proses transfer learning model, dan juga proses penghitungan fungsi loss dan metrik performa. Proses melatih model akan dilakukan oleh komponen Trainer yang akan memproses pelatihan model ConvNeXt menggunakan Optimizer dan juga EarlyStopCallback. Proses pelatihan bagian adaptation akan dilakukan sebanyak 2 epoch pertama untuk mendapatkan bobot pada classification layer. Proses pelatihan bagian fine tuning akan dilakukan sebanyak total epoch dikurangi 2, contoh jika total epoch sama dengan 25, maka model akan melakukan proses pelatihan fine tuning sebanyak 23 epoch. Proses validasi performa model merupakan proses dimana hasil pelatihan model tiap peoch dilakukan validasi pada data validasi yang telah disiapkan sebelumnya dengan cara mengukur nilai loss dan juga performa metric yang meliputi accuracy, recall, precision, dan f1-score. Hal ini bertujuan agar model dapat memperbaharui nilai bobot pengetahuan dengan cara melakukan backpropagation berdasarkan nilai loss dan akan menyimpan file model terbaik berdasarkan hasil validasi metrik performa pada data validasi.
       Proses ketiga adalah log hasil pelatihan model yang meliputi menyimpan hasil output model dan juga log pelatihan menggunakan Comet logger. Comet logger akan menyimpan nilai hasil pelatihan seperti loss, learning rate, dan metrik performa pada platform CometML untuk visualisasi grafik pelatihan model. Proses penyimpanan log ini juga bertujuan agar dapat membandingkan berbagai hasil pelatihan dengan konfigurasi berbeda sehingga mendapatkan model dengan performa terbaik berdasarkan metrik performa pada data validasi.

3.7 Evaluasi Model
       Evaluasi Model adalah tahap di mana model yang telah dilatih dievaluasi menggunakan dataset uji. Tujuan dari pengujian ini adalah untuk mengevaluasi kinerja model pada data yang belum pernah dipelajari sebelumnya. Hasil pengujian model akan diukur menggunakan beberapa metrik performa, seperti akurasi, presisi, recall, dan fl-score.
       Gambar 3.9 merupakan alur pengujian model dalam penelitian ini. Pengujian model dimulai dengan input berupa dataset pengujian dan model yang telah dilatih sebelumnya. Semua model yang telah dilatih sebelumnya diuji menggunakan dataset ini untuk membandingkan performa tiap model pada dataset yang sama. Proses berikutnya adalah perhitungan metrik performa yang terdiri dari accuracy, precision, recall, dan Fl-score.
       Penggunaan metrik accuracy memiliki tujuan untuk mengetahui tingkat keakuratan prediksi yang dibuat oleh model terhadap data uji. Metrik ini dihitung dengan mengambil jumlah total prediksi yang benar baik true positives (kasus positif yang diprediksi dengan benar) dan true negatives (kasus negatif yang diprediksi dengan benar) dan membaginya dengan jumlah total sampel dalam dataset. Metrik precision mengukur ketepatan sebuah model dalam memprediksi label kelas positif. Precision merupakan proporsi dari true positives terhadap jumlah keseluruhan prediksi yang dinyatakan sebagai positif oleh model, termasuk false positives. Metrik recall bertujuan untuk mengukur seberapa akurat model dalam mengidentifikasi semua kasus positif yang ada dalam suatu kelas tertentu. Recall dihitung dengan membandingkan jumlah prediksi yang benar-benar positif yang berhasil diidentifikasi oleh model dengan jumlah total kasus positif sebenarnya dalam kelas tersebut. Penggunaan F1-score bertujuan untuk menilai keseimbangan antara precision dan recall dalam performa model klasifikasi. Metrik ini sangat penting untuk menghindari permasalahan di mana model memiliki precision tinggi tapi recall rendah, atau sebaliknya.

3.8 Metode Stacking
      Stacking atau Stacked generalization merupakan teknik ensemble learning yang mengintegrasikan prediksi dari beberapa model untuk menghasilkan prediksi yang lebih akurat. Stacking berbeda dari metode ensemble lain seperti bagging dan bosting. Fokus utama model stacking adalah menggabungkan kekuatan model yang berbeda melalui model meta. Komponen utama metode stacking terbagi menjadi 2, yaitu model dasar dan meta-model. Model dasar adalah beberapa model tunggal yang masing masing dilatih menggunakan dataset yang sama. Output dari setiap model dasar, yang merupakan prediksi digunakan sebagai input untuk meta-model. Meta-model adalah model yang dilatih pada prediksi yang dihasilkan oleh model dasar. Model ini berfungsi untuk mempelajari bagaimana prediksi dari model dasar dapat digabungkan untuk menghasilkan prediksi akhir yang lebih akurat.
       Proses implementasi metode stacking dalam penelitian ini terdiri dari 3 tahap. Tahap pertama, hasil prediksi dari beberapa model dasar dikumpulkan. Hasil prediksi yang dikumpulkan dari model-model dasar kemudian digunakan untuk membentuk dataset baru. Setelah dataset baru dibuat, yang terdiri dari prediksi dari model-model dasar sebagai fitur, langkah selanjutnya adalah pelatihan meta-model.

3.9 Evaluasi Model Stacking
       Pengujian model stacking dalam penelitian ini dilakukan seperti pengujian pada model tunggal. Pengujian dilakukan dengan menghitung metrik akurasi seperti accuracy, precision, recall, dan Fl-score.

3.10 Inference dan Deployment
       Inference dan Deployment adalah tahap di mana model terbaik yang telah melalui proses pengujian digunakan untuk membuat prediksi pada data baru. Inference dilakukan melalui sebuah API yang dibuat menggunakan framework Flask untuk menjalankan model. API yang dibuat menerima file citra X-ray dada sebagai input. Output dari API ini adalah hasil prediksi yang mencakup nama kelas dan confidence score untuk citra tersebut. confidence score ini mengindikasikan tingkat keyakinan model terhadap prediksinya. Tahapan deployment merupakan tahapan proses mengepak API yang telah dibuat ke dalam satu environment yang siap dijalankan. Proses pengepakan ini menggunakan teknologi Docker yang memiliki tujuan mengisolasi environment yang digunakan oleh API dengan environment yang ada pada mesin utama. Hal ini juga memudahkan proses deployment karena tahapan installasi semua komponen seperti library, framework, dan interpreter yang digunakan telah diinisiasi dalam bentuk docker image.