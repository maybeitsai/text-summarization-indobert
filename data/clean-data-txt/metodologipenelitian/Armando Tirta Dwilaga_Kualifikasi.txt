3.1 Gambaran Umum Penelitian
     Penelitian ini digunakan untuk mengatasi sensitivitas terhadap cacat pada gambar ban dengan melibatkan penggunaan jaringan syaraf menggunakan algoritma Convolutional Neural Network (CNN) dan membangun model atau kerangka kerja menggunakan Keras. Berikut adalah Gambar 3.1 Blok Diagram Gambaran Umum Penelitian.

      Berdasarkan Gambar 3.1 Blok Diagram Gambaran Umum Penelitian maka dapat dijelaskan di blok tersebut terbagi menjadi 3 bagian yaitu bagian pertama adalah unit 
masukan berisikan data preparation di mana gambar ban dimuat, diubah menjadi format yang sesuai dipersiapkan untuk pelatihan model Convolutional Neural Network (CNN) 
seperti pemrosesan gambar ban, selanjutnya data augmentation di mana data dibuat lebih ber variasi dari training data yang ada sehingga dapat meningkatkan keberagaman 
training data tanpa harus mengambil data baru, mencakup (rotasi, pergeseran horizontal/vertikal, perbesar gambar, perubahan kecerahan gambar, sampai mengubah nilai pixel), selanjutnya data di mana dataset yang telah di augmentasi dan disiapkan dibagi menjadi subset yang berbeda untuk training untuk melatih model, validation untuk menyempurnakan model serta memvalidasi performanya selama pelatihan, dan testing untuk mengevaluasi kinerja model akhir. Dataset dibagi menjadi training data, validation data, dan testing data dalam proporsi tertentu.
      Bagian kedua adalah unit pemrosesan yang bertindak adalah model training (forward Pass, tahap di mana input diproses melalui model untuk menghasilkan prediksi), tujuannya melatih model Convolutional Neural Network (CNN) menggunakan dataset pelatihan di mana data dari unit masukan diteruskan melalui jaringan neural di lakukan transformasi linier (konvulasi) dan non-linier (fungsi aktivasi) dilakukan pada data di setiap lapisan untuk menghasilkan output prediksi yang melibatkan komputasi di setiap neuron dan lapisan jaringan, yang merupakan inti dari proses pembelajaran dalam jaringan saraf.
      Selanjutnya unit pemrosesan Fine-tuning tujuannya dilakukan untuk 
menyempurnakan model lebih lanjut setelah pelatihan awal dengan dataset yang lebih kecil atau lebih spesifik nantinya. Proses di dalam Fine-tuning menyesuaikan bobot 
(menggunakan kumpulan data yang lebih kecil untuk menyesuaikan bobot model untuk performa yang lebih baik), pelatihan khusus (fokus pada fitur data yang lebih relevan 
dengan objek).
      Bagian ketiga adalah unit keluaran yang bertindak ada proses model evaluatioan (backwardpass, tahap di mana gradien (memperbarui parameter model dalam arah yang 
akan mengurangi fungsi loss) dari fungsi loss (metrik yang mengukur seberapa baik atau buruk model melakukan prediksi dibandingkan nilai aktualnya) dihitung dan digunakan 
untuk memperbarui parameter model selama pelatihan) tujuannya mengevaluasi performa model yang dilatih dan model dievaluasi menggunakan metrik yang relevan (accuracy, 
precision, recall, dan F1- score) berdasarkan prediksi yang dihasilkan dari model terhadap validasi atau uji data. Output dari proses ini adalah tentang hasil evaluasi model, yang memberikan informasi kinerja model. Selanjutnya ada dua alur pilihan yang bisa dilakukan, alur pertama jika hasil prediksi sudah sesuai dengan keinginan maka bisa langsung masuk ke model deployment (inference), dan alur kedua jika hasil prediksi masih perlu diperbaiki pada bagian unit pemrosesan terlebih dahulu fine tuning untuk penggunaan data set lebih kecil (jika menunjukan model belum mencapai performa yang diharapkan) baru masuk ke model deployment (inference) tujuannya menerapkan model 
terlatih untuk membuat prediksi pada data baru yang belum terlihat.
      Model deployment (inference) yang telah dilatih digunakan untuk membuat prediksi pada data baru atau dalam situasi dunia nyata, tahap di mana model menerima input baru dan menghasilkan output berdasarkan pada pembelajaran yang dilakukan selama proses pelatihan dan merupakan output akhir dari keseluruhan proses, di mana model "mengambil keputusan" atau "membuat prediksi" berdasarkan pada pengalaman yang telah diperoleh selama pelatihan.

3.2. Tahapan Penelitian
      Penelitian ini di dalamnya terdapat tahapan-tahapan yang dilakukan untuk membentuk satu kesatuan yang utuh dari awal sampai akhir dan membentuk kerangka penelitian mengenai klasifikasi pada produk ban menggunakan algoritma Convolutional Neural Network (CNN). Berikut Gambar 3.2 Tahapan penelitian.

      Berdasarkan Gambar 3.2 Tahapan Penelitian maka dapat dijelaskan proses yang terlibat di dalamnya ada 8 yaitu studi literatur, data aquisition, data preprocessing, data augmentation, texture feature extraction, data splitting, model building, dan model evaluation & testing di mana tahap ke dua sampai lima merupakan tahap proses 
menyiapkan sebuah data sebelum dilakukan pemodelan.

3.2.1 Studi Literatur
      Tahap pertama adalah studi literatur di mana studi yang dilakukan berasal dari artikel ilmiah dan buku yang menunjang dalam menganalisis terkait dengan metode 
pengukuran kualitas, mengenai klasifikasi produk ban, meninjau penggunaan pembelajaran mesin algoritma Convolutional Neural Network (CNN) dari beberapa tahun ke belakang dalam konteks pengukuran kualitas untuk klasifikasi terhadap kondisi-kondisi produk ban. Sehingga dapat menemukan teknik terbaik yang dapat diaplikasikan 
pada masalah yang ada. Berikut merupakan Gambar 3.3 Tahapan Study Literature.

3.2.2 Data Aquisition
     Tahap kedua adalah data aquisition dengan mengumpulkan kumpulan data sesuai tujuan penelitian dengan target untuk kumpulan data gambar ban untuk training data, 
validation data, dan testing data, memastikan bahwa kumpulan data tersebut memiliki varian yang secara akurat memang mewakili kondisi produk ban dan diperoleh dari sumber-sumber terpercaya. Berikut merupakan Gambar 3.4 Tahapan Data Aquisition.

3.2.3 Data Preprocessing
     Tahap ketiga adalah data preprocessing melakukan pra-pemrosesan data untuk menyiapkan gambar untuk model pelatihan dan pengujian proses ini meliputi normalisasi 
dan penskalaan dengan fitur dalam program (ImageDataGenerator). Bermaksud merapikan, menata, dan menyiapkan data untuk pemeriksaan tambahan. Normalisasi data, pengkodean variabel, mengatasi nilai yang hilang, menghapus data yang tidak relevan atau hilang, dan modifikasi data lainnya untuk memenuhi persyaratan analisis adalah persiapan data. Berikut merupakan Gambar 3.5 Tahapan Data Preprocessing.

3.2.4 Data Augmentation
     Tahap keempat adalah data augmentation meningkatkan variasi dalam dataset dengan teknik augmentasi data, menggunakan operasi seperti rotasi, pergeserarn 
horizontal/vertikal, perbesar gambar, perubahan kecerahan gambar, sampai mengubah nilai pixel untuk memperkaya dataset dan mengurangi overfitting (Saat disajikan dengan 
data baru yang belum pernah dilihat sebelumnya, performa model akan menurun drastis karena model tersebut dapat menyesuaikan diri dengan kumpulan training data dengan 
sangat efektif). Augmentasi data dilakukan dengan dua cara secara statis dan dinamis yang artinya secara statis yaitu menambah data secara fisiknya dan dinamis tidak menambah secara fisik tetapi secara kuantitas dataset yang dapat diakses secara fisik di komputer tidak bertambah ketika ImageDataGenerator digunakan pada dataset. Sebaliknya, pada saat runtime hanya menghasilkan variasi dari gambar yang sudah ada dibuat secara dinamis dan cukup bagi model untuk berlatih dari berbagai kondisi gambar ban yang ada pada kenyataaanya.
     Secara lebih jelas nilai teknik augmentasi pertama dilakukan dengan manual menggunakan bantuan dari website roboflow dengan resize gambar menjadi 640 x 640, 
pada augmentasinya menggunakan model flip (horizontal dan vertikal), 90� pemutaran (searah jarum jam, berlawanan arah jarum jam, dan terbalik), rotasi ( -45� dan 45�), shear (�5� horizontal dan �5� vertikal), brightness (-20% sampai 20%). Data asli pada dataset berjumlah 1.028 data gambar ban setelah dilakukan augmentasi secara fisik menggunakan website roboflow ada data yang tidak dapat diidentifikasi ada 3 gambar sehingga total gambar asli yang berhasil di upload dan dijadikan data asli yang tetap berjumlah 1.025 data gambar dan setelah di augmentasi bertambah menjadi 2.050 data gambar ban. Rinciannya pada data asli training adalah 560 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 1.121 gambar. Rincian data asli pada Validation data berjumlah 140 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 279 gambar. Rincian data asli pada testing data berjumlah 328 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 650 gambar. Testing data pada prosesnya 
sebenarnya tidak mengalami augmentasi karena pada proses pengujian atau evaluasi model, ingin menggunakan data asli yang sebenarnya untuk melihat kinerja model pada 
kasus-kasus yang belum pernah dilihat sebelumnya.
     Augmentasi kedua yaitu dilakukan rotasi melakukan pemutaran gambar secara penuh dan secara acak dengan nilai 360 atau rentang nilai 0-360 derajat, kedua width shift 
range, yang menggeser gambar secara acak ke kiri atau kanan dengan nilai 0.05 atau gambar dapat digeser sampai 5% dari lebar aslinya. Ketiga height shift range gambar 
dapat digeser secara vertikal dengan nilai 0.05 atau gambar dapat digeser sampai 5% dari tinggi aslinya. Keempat shear range untuk menggeser gambar dengan sudut geser 
berlawanan arah jarum jam dengan nilai 0.05. kelima zoom range memperbesar gambar sebanyak 0.05 atau gambar dapat diperbesar sampai 5%. Keenam horizontal flip adalah memberikan variasi tambahan dengan mengubah orientasi gambar secara horizontal acak dengan keterangan nilai true. ketujuh vertikal flip adalah memberikan variasi tambahan dengan mengubah orientasi gambar secara vertikal acak dengan keterangan nilai true. Kedelapan brightness range mengubah atau menentukan kecerahan pada gambar secara acak dengan nilai rentan [0.75, 1.25] atau kecerahan dapat diubah mulai dari rentnag 75% sampai 125% dari kecerahan asli gambarnya. Kesembilan resecale mengubah nilai skala piksel 0.1 dengan membaginya setiap nilai piksel pada nilai 255, sehingga dapat membantu untuk normalisasi data. Kesepuluh validation split mengatur pembagian data untuk validasi dengan nilai 0.2 atau 20% data dari keseluruhan data untuk alokasi validation data dan 80% untuk alokasi training data.
     Merupakan pendekatan augmentasi awal di mana sebelum data masuk ke model untuk proses pelatihan dan akan diperbesar sebelum pembagian dataset menjadi batch untuk setiap epoch nya, sehingga model akan dilatih menggunakan dataset yang telah diaugmentsi sejak awal dan seluruh augmentasi akan diterapkan pada setiap epoch nya dengan penggunaan ukuran batch 64 dengan jumlah batch training 36 dan validasi 10. Rinciannya data asli pada training data berjumlah 1.121 gambar dan setelah dilakukan augmentasi bertambah sebanyak 1.152 gambar, sehingga data pada training data berjumlah total menjadi 2.273 gambar. Rincian data asli pada Validation data berjumlah 279 gambar dan setelah dilakukan augmentasi bertambah sebanyak 320 gambar, sehingga data pada Validation data berjumlah total menjadi 599 gambar. Testing data tidak mengalami augmentasi karena pada proses pengujian atau evaluasi model, ingin menggunakan data asli yang sebenarnya untuk melihat kinerja model pada kasus-kasus yang belum pernah dilihat sebelumnya. Berikut merupakan Gambar 3.6 Tahapan Data Augmentation.

3.2.5 Data Splitting
     Tahap kelima data splitting dengan membagi file dataset menjadi subset training data, validation data, dan testing data berisikan gambar ban normal dan gambar ban tidak normal sehingga subset training data digunakan untuk melatih model, sedangkan subset validation data digunakan untuk menguji kinerja model. Sebenarnya langkah-langkah dalam proses pra-pemrosesan data yang mempersiapkan data mentah untuk digunakan dalam pelatihan model adalah tahapan yang sudah disebutkan sebelumnya data 
aquisition, dataprerocessing, data augmentation, dan splitting data. Prosedur yang disebutkan di atas berkonsentrasi pada pengumpulan, sanitasi, pengorganisasian, dan 
penambahan jumlah data yang diperlukan untuk pelatihan model.
     Rinciannya yaitu file yang tersimpan di dalam komputer total data gambar sebanyak 2.050 gambar yang dibagi menjadi dua pertama adalah file testing data dengan 
jumlah data tersimpan sebanyak 650 gambar yang dibagi menjadi sub file "crack" berjumlah 420 dan sub file "normal' berjumlah 230 data. kedua adalah file training data 
dengan jumlah data tersimpan sebanyak 1400 gambar yang dibagi menjadi sub file "crack" berjumlah 654 dan sub file "normal' berjumlah 746 data. maka ketika dilakukan 
data splitting pada program secara otomatis yang pada data augmentasi diatur menjadi pembagian 80% untuk training data dan 20% untuk validation data yaitu untuk Train 
Data sebanyak 1.121 gambar dengan 2 kelas, validation data sebanyak 279 gambar dengan 2 kelas, dan Test Data sebanyak 650 gambar dengan 2 kelas. Testing data bernilai tetap hal ini bertujuan agar kuantitas data awal yang telah ditentukan sebelumnya tetap terjaga dan testing data tidak terpengaruh oleh prosedur pemisahan. Setelah model dilatih dan divalidasi, testing data digunakan untuk mengevaluasi performa akhir model. Akibatnya, testing data tidak terbagi, dan rincian asli 648 foto masih berlaku. Berikut merupakan Gambar 3.7 Tahapan Splitting Data.
     

3.2.6 Model Building
      Tahap keenam adalah model building (membangun model Convolutional Neural Network (CNN) dengan Keras) membangun arsitektur model Convolutional Neural Network (CNN) menggunakan Keras, mengatur lapisan-lapisan seperti convolutional, MaxPooling2D, Flatten, dan Dense untuk membangun model. learning rate dalam penggunaan algoritma optimasi menggunakan Adaptive Momentum (Adam) untuk menghasilkan pembelajaran yang adaptif, pemilihan penggunaan Adaptive Momentum (Adam) jika dibandingkan dengan learning rate lain seperti Stochastic Gradient Descent (SGD) karena kecepatan pembelajaran adaptif untuk Adaptive Momentum (Adam) bisa secara otomatis menyesuaikan learning rate untuk setiap parameter dalam model klasifikasi ban sedangkan Stochastic Gradient Descent (SGD) memiliki learning rate tetap selama pelatihan model klasifikasi ban yang penentuannya dari user dan tidak bisa menyesuaikan learning rate secara otomatis berdasarkan kondisi aktual dari setiap parameter. Selanjutnya secara kestabilan dan konvergensi Adaptive Momentum (Adam) menyambung dari awal dapat mengubah kecepatan pembelajaran secara adaptif, sehingga membuatnya lebih stabil dan kecil kemungkinannya terjebak pada tingkat minimum lokal (nilai yang dianggap sebagai titik terendah dari loss function dalam model) sehingga Adaptive Momentum (Adam) cenderung mencapai konvergensi (tingkat kinerja yang diharapkan) lebih cepat dan andal dalam berbagai keadaan, sedangkan Stochastic Gradient Descent (SGD) mungkin lebih stuck pada nilai minimum atau terjebak pada nilai minimum lokal yang disebabkan oleh kemungkinan bergantung pada seberapa tepat kecepatan pemelajaran dipilih, kecepatan pemelajaran yang tetap dapat membuat model mencapai konvergensi terlalu cepat atau terlalu lambat.
      Adapun melakukan pendekatan kedua penambahan data ketika masuk ke model building dan terjadi proses pemodelan setelah menggunakan epoch. Augmentasi data diterapkan setelah data melewati beberapa epoch selama proses pelatihan, sehingga variasi data yang dihasilkan akan berbeda-beda pada setiap epoch dan model dapat terus-menerus terlatih dengan variasi data yang lebih besar. Menggunakan 100 epoch, sehingga total training data yang diproses menjadi 230.400 gambar dan validation data menjadi 6.400 gambar. Sehingga jumlah data yang diproses selama pelatihan menjadi sangat besar dan pada akhirnya nanti akan menyiapkan model kompilasi dalam mengatur pengoptimal (Adam), fungsi kerugian (biner crossentropy), dan metrik evaluasi (akurasi). Berikut merupakan Gambar 3.8 Tahapan Building Model.

      Berdasarkan hasil analisis sebelumnya maka dapat diketahui untuk jumlah data asli (training data) adalah 1.121, jumlah data asli (validation data) adalah 279, jumlah data asli (training data) setelah augmentasi adalah 1152, jumlah data asli (validation data) setelah augmentasi adalah 320, jumlah epoch yang digunakan sebanyak 100, dan ukuran batch adalah 64. Berikut merupakan perhitungan manualnya ketika masuk ke model building dan terjadi proses pemodelan setelah menggunakan epoch.
1. Jumlah batch per epoch untuk training data.
2. Jumlah batch per epoch untuk validation data.
3. Total jumlah data setelah augmentasi untuk semua epoch.
a. Training Data
b. Validation Data

3.2.7 Model Evaluation & Testing
      Tahap kedelapan adalah model evaluation & testing digunakan sebagai bahan terusan pada model building yang dibuat untuk melakukan evaluasi performanya dengan 
menggunakan bagian pengujian, dan parameter yang digunakan pada metrik evaluasi seperti akurasi, presisi, recall, dan Fl-score. Berikut merupakan Gambar 3.9 Tahapan 
Model Evaluation & Testing.

3.3 Arsitektur Convolutional Neural Network (CNN)
     Convolutional Neural Network (CNN) yang dibangun menggunakan model atau kerangka kerja yang pada dasarnya menggunakan Keras dan juga tensorflow dengan menambahkan beberapa model lapisan-lapisan seperti lapisan convolutional (Conv2D), laposan pooling (MaxPooling2D), Flatten, dan lapisan fully connected (Dense). Berikut merupakan Gambar 3.10 Tahapan Convolutional Neural Network (CNN) dengan Model Keras.

      Berdasarkan Gambar 3.10 Tahapan Convolutional Neural Network (CNN) dengan Model Keras maka dapat dijelaskan mulai dari yang mencakup lapisan-lapisan konvolusi yang telah dilatih pada dataset besar seperti ImageNet untuk mengekstrak fitur dari gambar-gambar, penggunaan image size diatur dengan (379, 379) batch size 64, kernel size 3, strides (2 untuk cov2d dan 2 untuk maxpooling2d), dan pool size 2. Selanjutnya Conv2D yang merupakan convolutional layer pertama yang berfungsi untuk mengekstrak fitur-fitur visual dari gambar. Filter convolutional layer pertama yang berfungsi untuk mengekstrak fitur-fitur visual diterapkan pada gambar untuk menghasilkan fitur-fitur yang lebih abstrak, formula untuk mengetahui jumlah training datanya dengan. Selanjutnya MaxPooling2D di mana tahap pooling digunakan untuk mengurangi dimensi spasial dari setiap feature map yang dihasilkan oleh layer sebelumnya. Max pooling memilih nilai maksimum di dalam jendela pooling untuk mengurangi ukuran fitur dan mempertahankan informasi penting. Selanjutnya Conv2D dan MaxPooling2D diulang sampai 4 layer karena untuk terus mengekstrak fitur-fitur yang semakin kompleks dari gambar. Selanjutnya flatten (digunakan untuk mengubah tensor multi-dimensi menjadi tensor satu dimensi) di mana setelah serangkaian layer konvolusi dan pooling, masukan dari layer terakhir perlu diubah menjadi vektor tunggal sebelum dimasukkan ke dalam layer dense. Flatten layer melakukan hal ini dengan mengubah matriks output menjadi array satu dimensi.
      Selanjutnya dense layers (lapisan dense digunakan sebagai lapisan output dalam model klasifikasi, di mana jumlah neuron dalam lapisan output sesuai dengan jumlah kelas yang harus diprediksi) di mana ada tiga lapisan dense ditambahkan, dengan fungsi pertama dan kedua menggunakan ReLu sebagai f (x) = max(0, x) yang artinya menunjukkan bahwa keluarannya nol jika masukannya negatif atau nol dan output x jika masukannya positif dengan 128 unit neuron dan pada dense kedua 64 unit neuron karena tugasnya mengurangi dimensi representasi pada lapisan dense pertama maka model dapat mempelajari pola yang lebih rumit dan mendalam dari data dengan menambahkan lapisan yang lebih padat, yang dapat meningkatkan performa model dalam tugas klasifikasi gambar. Lapisan dense ketiga dengan fungsi aktivasi sigmoid untuk output biner, dengan menunjukan kelas prediksi dari gambar yaitu normal atau crack. Di antara tiga lapisan dense di ikuti dengan lapisan dropout (untuk mencegah overfitting di mana model pembelajaran mesin terlalu menghafal pola dari training data yang tersedia, sehingga kinerjanya menurun secara signifikan saat diuji dengan data baru yang tidak dilihat sebelumnya) juga dimasukkan setelah setiap lapisan dense untuk mencegah overfitting dengan secara acak menonaktifkan sebagian unit sebanyak 0.2 atau 20% dari neuron selama pelatihan.
       Selanjutnya training di mana model diterapkan pada training data dengan menggunakan metode fit dan callback. Model fit digunakan untuk melatih model dengan training data dan model callback menggunakan "modelCheckpoint" untuk menyimpan model terbaik selama pelatihan berkaitan dengan performa pada validation data mengontrol proses pelatihan. Terakhir evaluation di mana performa model pada testing data dinilai menggunakan hasil klasifikasi dan confusion matrix untuk memahami kinerjanya testing data.
      Banyaknya parameter atau bobot dan jumlah data yang harus dipelajari selama pelatihan bergantung pada jumlah neuron pada lapisan. Jumlah data yang harus dipelajari 
model selama pelatihan tercermin dalam jumlah parameter ini. Berikut merupakan perhitungan dalam mengetahui total neuron yang dikerjakan oleh setiap lapisan.
1. First Conv2D
Kedalaman gambar yang diproses lapisan konvolusi sebenarnya ditunjukkan oleh jumlah saluran masukan. Tiga saluran merah, hijau, dan biru membentuk sebuah gambar jika diwarnai, artinya ada tiga saluran masukan. Karena kata grayscale digunakan untuk mendeskripsikan gambar ini, hanya ada satu saluran warna dan bernilai 1. Sehingga jumlah neuronnya 1280, yang berarti ada 1280 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan. Selanjutnya adalah dalam penentuan ukuran spasialnya setiap filter diubah menjadi setengah dari ukuran input nya (379x379) menjadi (189x189) sebagai berikut. 
2. First Maxpooling2D
Tidak ada parameter baru yang ditambahkan, dan jumlah neuron (dalam contoh ini, lapisan konvolusi pertama) tetap sama. Setiap filter diubah menjadi setengah dari ukuran inputnya (189x189) menjadi (94x94) dan jumlah neuronnya 1280 mengikuti lapisan konvolusi pertama. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 
3. Second Cov2D
Total Neuron = (Ukuran Filter x Jumlah Channel Input + 1) x Filter = (3 x 3 x 128 + 1)x 64 = (1153)x 128 = 73792
Jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. Sehingga jumlah neuronnya 73792, yang berarti ada 73792 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan. Selanjutnya adalah dalam penentuan ukuran spasialnya setiap filter diubah menjadi setengah dari ukuran input nya (94x94) menjadi (46x46) sebagai berikut.
4. Second Maxpooling2D
Tidak ada parameter baru yang ditambahkan, dan jumlah neuron (dalam contoh ini, lapisan konvolusi kedua) tetap sama. Setiap filter diubah menjadi setengah dari ukuran 
inputnya (46x46) menjadi (23x23) dan jumlah neuronnya 73792 mengikuti lapisan konvolusi kedua. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 
5. Third Cov2D
Total Neuron = (Ukuran Filter x Jumlah Channel Input + 1) x Filter = (3 x 3 x 64 + 1)x 32 = (577)x 32 = 18464
Jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. Sehingga jumlah neuronnya 18464, yang berarti ada 18464 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan. Selanjutnya adalah dalam penentuan ukuran spasialnya setiap filter diubah menjadi setengah dari ukuran input nya (23x23) menjadi (11x11) sebagai berikut.
6. Third Maxpooling2D
Tidak ada parameter baru yang ditambahkan, dan jumlah neuron (dalam contoh ini, lapisan konvolusi ketiga) tetap sama. Setiap filter diubah menjadi setengah dari ukuran 
inputnya (11x11) menjadi (5x5) dan jumlah neuronnya 18464 mengikuti lapisan konvolusi ketiga. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 
7. Fourth Cov2D
Total Neuron = (Ukuran Filter x Jumlah Channel Input + 1) x Filter = (3 x 3 x 32 + 1)x 16 = (289)x 16 = 4624
Jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. Sehingga jumlah neuronnya 4624, yang berarti ada 4624 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan. Selanjutnya adalah dalam penentuan ukuran spasialnya setiap filter diubah menjadi setengah dari ukuran input nya (5x5) menjadi (2x2) sebagai berikut. 
8. Fourth Maxpooling2D
Tidak ada parameter baru yang ditambahkan, dan jumlah neuron (dalam contoh ini, lapisan konvolusi keempat) tetap sama. Setiap filter diubah menjadi setengah dari ukuran inputnya (2x2) menjadi (1x1) dan jumlah neuronnya 4624 mengikuti lapisan konvolusi keempat. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 
9. Flatten
Tidak mengubah parameter yang ada karena fungsi flatten hanya mengubah matriks multidimensi menjadi vektor tunggal berdasarkan hasil dari jumlah filter pada lapisan 
lima cov2d yaitu 8 dan maxpooling2D dengan ukuran inputnya (1x1) sehingga menjadi matriks multidimensi (1, 1, 16) diubah menjadi nilai vektor tunggal dengan panjang 16 atau menjadi jumlah neuron sebanyak 16.
10. Dense Layer 1
Total Neuron = (Jumlah Neuron Input + 1) x Jumlah Neuron Output = (16+ 1)x 128 = (17)x 128 = 2176
11. Dropout Layer 1
Menggunakan 0.2 yang artinya sebanyak 20% dari neuron dalam dense layer 1 akan dinonaktifkan secara acak.
12. Dense Layer 2
Total Neuron = (Jumlah Neuron /nput + 1) x Jumlah Neuron Output = (128 + 1)x 64 = (129)x 64 = 8256
13. Dropout Layer 2
Menggunakan 0.2 yang artinya sebanyak 20% dari neuron dalam dense layer 2 akan dinonaktifkan secara acak.
14. Dense Layer 2
Total Neuron = (Jumlah Neuron /nput + 1) x Jumlah Neuron Output = (64 + 1)x 1 = (65)x 1 = 65
Ketika dimensi spasial (tinggi dan lebar) dikurangi menggunakan operasi lapisan pooling seperti maxpooling, jumlah neuron di setiap lapisan pooling akan menurun. Misalnya, dimensi spasial setiap filter (tinggi dan lebar) di lapisan maxpooling disesuaikan menjadi setengah dari dimensi masukannya. Karena hanya separuh dari masukan yang diproses lebih lanjut, hal ini juga menyebabkan berkurangnya jumlah neuron pada lapisan tersebut. Sedangkan penurunan pada dense terjadi karena penentuan jumlah neuron.