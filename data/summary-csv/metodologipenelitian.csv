nama_dokumen,summary
Alfharizky Fauzi_Kualifikasi.txt,"Pada metodologi penelitian ini menjelaskan mengenai bagaimana proses dari analisis system, perancangan, dan analisis program yang dilakukan pada penelitian ini. Berikut analisis dan perancangan pada penelitian ini. Tahapan penelitian yang dilakukan terdiri dari 9 tahapan, yaitu dimulai dari studi literatur sebagai dasar penelitian, analisis kebutuhan pada system yang akan dibangun, Pengumpulan dataset, preprocessing data, membangun model, training model, evaluasi model, deployment model, dan implementasi model yang telah dibuat ke dalam smartphone. Saat program telah dijalankan, program akan mengakuisisi dataset kemudian dataset akan melalui tahap 
preprocessing untuk menormalkan data kemudian setelah melalui tahap preprocessing selanjutnya mentraining dataset yang sudah didapatkan jika dataset berhasil dilatih dan 
juga divalidasi maka berlanjut ke tahap berikutnya yaitu tahapan testing dengan menerapkan model yang dibuat kedalam mobile phone atau smartphone. tahap selanjutnya 
jika camera telah menyala maka artinya sudah siap untuk mendeteksi objek jenis penyakit kulit. Pada tahap terakhir yaitu saat ada objek jenis penyakit kulit yang masuk atau terdeteksi oleh camera, maka citra tersebut sudah dapat dilakukan proses klasifikasi kemudian divalidasikan bahwa data tersebut sama dengan yang ada pada database untuk memunculkan label nama pada dataset serta memunculkan nilai confidence pada citra jenis penyakit kulit yang terdeteksi. 3.2 Analisis Kebutuhan
       Analisis kebutuhan merupakan menganalisis komponen yang diperlukan dalam pembuatan dan menjalankan program, Proses ini mencakup evaluasi, identifikasi, dan pemetaan kebutuhan dari berbagai perangkat yang terlibat dalam pembuatan system dan program pada penelitian ini. berikut analisis kebutuhan dari penelitian yang dibuat. Analisis Kebutuhan Perangkat Keras
       Perangkat keras yang digunakan dalam penelitian identifikasi penyakit kulit pada manusia menggunakan laptop Acer Predator Helios Neo 16 dan mobile phone atau 
smartphone Xiaomi Redmi Note 7 dengan bahasa pemrograman python, dengan spesifikasi yang dapat dilihat pada Tabel 3.1. Analisis Kebutuhan Perangkat Lunak
       Perangkat lunak yang digunakan dalam penelitian identifikasi penyakit kulit pada manusia menggunakan Operating System Windows Jupyter Lab dengan bahasa pemrograman python, dan Visual Studio sebagai text editor, yang dapat dilihat pada tabel 3.2. Analisis Objek
       Program dengan menggunakan Metode Bi-Directional Image-Text Matching Deep Learning ini mempunyai beberapa objek yang diterapkan pada penelitian ini, yaitu:
1. Identifikasi berbagai macam jenis penyakit kulit dengan memunculkan citra gambar yang didapat dan deskripsi mengenai penyakit kulit yang teridentifikasi dibawah citra gambar untuk setiap objek penyakit kulit yang terdeteksi, data yang digunakan memiliki variasi jenis penyakit kulit dengan kategori 2 penyakit kulit menular (Candidiasis dan Molluscum) dan 2 penyakit kulit tidak menular (Eczhema dan Melanoma) dengan masing masing kelas memiliki 1000 citra penyakit kulit yang di dapat pada website international Dermnet NZ (dermnetnz.org, 2024) dan The International Skin Imaging Collaboration (ISIC) (isic-archive.com, 2024). Program identifikasi berbagai macam objek penyakit kulit pada manusia ditampilkan secara real-time menggunakan file upload kamera mobile phone. Website ini menyediakan gambar-gambar resolusi tinggi dari berbagai penyakit kulit, baik yang menular maupun tidak menular, serta memberikan deskripsi lengkap tentang penyakti tersebut meliputi gejala dan pengobatan. Citra yang diperoleh kemudian diseleksi berdasarkan fokus penelitian, yaitu identifikasi penyakit kulit menular (Candidiasis dan Molluscum) dan tidak menular (Eczhema dan Melanoma). Dataset Penyakit Kulit
       Dataset pada penelitian ini dibagi menjadi 2 bagian yaitu 80% data training, dan 20% data testing objek jenis penyakit kulit. Dataset bersumber dari citra (data image) dan deskripsi (data teks) beberapa jenis penyakit kulit sejumlah 4000 citra dengan 4 jenis penyakit kulit yang terdiri dari Echzema, Melanoma, Candidiasis, dan Molluscum dengan memiliki 1000 citra berbeda setiap jenis penyakit kulit. Dari keempat jenis penyakit kulit tersebut dibagi menjadi 2 kelompok sebagai penyakit kulit menular dan tidak menular. Data Gambar
       Data image ini mencakup berbagai jenis gambar yang menampilkan gejala dan karakteristik penyakit kulit yang digunakan pada peneltian ini (Eczhema, Melanoma, 
Candidiasis, dan Molluscum) seperti ruam, bintik-bintik, lepuhan, atau lesi kulit lainnya. Penggunaan data gambar 
sangat penting dalam penelitian ini untuk membandingkan dan mempelajari pola visual yang terkait dengan berbagai penyakit kulit. Data image pada penelitian ini terdiri 4000 gambar dari 4 jenis penyakit kulit yaitu Eczhema, Melanoma, Candidiasis, dan Molluscum yang dibagi menjadi 2 kelompok menular dan tidak menular. Data Teks
       Data teks penyakit kulit merujuk kepada informasi tertulis yang berisi deskripsi dan karakteristik berbagai kondisi dermatologis. Informasi ini penting untuk diagnosis dan pemahaman lebih lanjut tentang berbagai penyakit kulit seperti dermatitis, eksim, psoriasis, dan infeksi jamur kulit. Berikut data teks yang digunakan pada penelitian ini dapat dilihat pada Tabel 3.3
       
3.4 Pre-Processing Data
       Pada tahapan ini data gambar penyakit kulit, preprocessing mencakup berbagai teknik seperti pengubahan ukuran gambar, normalisasi piksel, peningkatan kontras, 
penghapusan noise serta melakukan segmentasi dan fitur ekstraksi. Teknik ini bertujuan untuk meningkatkan kualitas gambar dan memastikan konsistensi data, sehingga fitur-fitur penting dapat diekstraksi dengan lebih efektif oleh algoritma analisis atau model kecerdasan buatan. Ketiga, peningkatan kontras (contrast enhancement) dan penghapusan noise 
bertujuan untuk memperjelas fitur-fitur penting dalam gambar, seperti tepi atau tekstur, yang mungkin relevan untuk diagnosis penyakit kulit. Ukuran dan bentuk citra hasil resizing disimpan pada folder output masing-masing penyakit kulit, yang selanjutnya akan diproses pada tahap berikutnya. Dengan meningkatkan perbedaan antara nilai intensitas piksel, proses ini membantu dalam meningkatkan ketajaman citra dan membuatnya lebih mudah untuk dianalisis. Dengan menghapus noise maka citra yang dihasilkan menjadi lebih bersih, proses ini membantu dalam meningkatkan fokus citra terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. Dengan menghapus nilai-nilai pada 
citra yang tidak terpakai maka citra yang dihasilkan menjadi lebih bersih, proses ini membantu dalam menentukan focus objek terhadap penyakit kulit dan membuatnya lebih 
mudah untuk dianalisis. Langkah-langkah ekstraksi fitur warna dapat dilihat pada Algoritma 3.6.Nilai hasil Ektraksi fitur warna menggunakan RGB disimpan, yang selanjutnya akan diproses pada tahap berikutnya. Dengan mendapatkan nilai-nilai pada setiap kanal RGB maka informasi yang didapat akan semakin kompleks, proses ini membantu dalam menentukan setiap warna yang paling dominan pada objek terhadap penyakit kulit dan 
membuatnya lebih mudah untuk dianalisis. Dengan mendapatkan nilai-nilai bentuk maka informasi yang didapat akan semakin kompleks, proses ini membantu dalam menentukan setiap bentuk yang paling dominan pada objek terhadap penyakit kulit dan membuatnya lebih mudah untuk dianalisis. Langkah-langkah ekstraksi fitur tekstur dengan menggunakan metode GLCM sebagai acuan tekstur dapat dilihat pada Algoritma 3.8. Nilai hasil Ektraksi fitur tekstur menggunakan GLCM disimpan, yang selanjutnya akan diproses pada tahap berikutnya. Sehingga tampilan hasil program terlihat pada gambar 3.13 berikut. Seperti terlihat pada gambar, proses ektraksi fitur menggunakan GLCM menunjukan hasil nilai untuk setiap 
citra ditujukan untuk memisahkan informasi tekstur menjadi contrast, dissimilarity, homogeneity, energy dan correlation. Dengan mendapatkan nilai-nilai tekstur maka 
informasi yang didapat akan semakin kompleks, proses ini membantu dalam menentukan setiap tekstur yang paling dominan pada objek terhadap penyakit kulit dan membuatnya 
lebih mudah untuk dianalisis. Proses ini krusial karena data teks sering kali tidak terstruktur dan dapat mengandung berbagai jenis noise atau informasi yang tidak relevan yang dapat mempengaruhi hasil analisis. Terakhir Tagging proses ini menandai atau menempatkan label pada kata- kata atau token dalam teks untuk mengidentifikasi informasi tertentu atau untuk mempersiapkan 
data untuk analisis lebih lanjut. 3.5 Training Model
       Sebelum memulai pelatihan model, data yang telah diproses melalui tahap preprocessing, seperti segmentasi dan ekstraksi fitur, menjadi input yang sangat penting. Segmentasi membantu dalam memisahkan area lesi kulit dari bagian yang tidak relevan, sedangkan ekstraksi fitur membantu dalam mengidentifikasi karakteristik spesifik dari lesi kulit tersebut. Setelah melakukan 2 pemodelan antara modelling gambar dan modelling teks, maka tahapan selanjutnya menggabungkan ke 2 
model tersebut dengan metode Bi-Directional Image-Text Matching untuk menghasilkan algoritma yang dapat mengindentifikasi melalui kedua jenis objek data. Gambaran Bi-
Directional Image-Text Matching dapat dilihat pada gambar berikut
       
3.7 Evaluasi Model
       Evaluasi terhadap model dilakukan untuk melihat akurasi model saat mengidentifikasi penyakit. Set uji ini dirancang untuk mensimulasikan kondisi dunia nyata, di mana model harus membuat 
prediksi tanpa bias dari data pelatihan. 3.8 Implenientasi Model
       Tahap ini melibatkan integrasi model yang telah dilatih ke dalam lingkungan klinis atau aplikasi yang akan digunakan oleh para profesional medis untuk mendukung 
diagnosis dan pengobatan penyakit kulit. Implementasi model membutuhkan pemikiran yang cermat dan strategi yang terkoordinasi untuk memastikan keberhasilannya dalam 
praktik medis."
Alifurrohman_Kualifikasi.txt,"3.1 Kerangka Umum Penelitian
     Berikut ini merupakan kerangka penelitian yang menjelaskan tahapan yang dilakukan dalam penelitan ini. Berikut gambar 3.1 diagram alir penelitian

Persiapan Data
Definisikan Ukuran Input dan Parameter
Definisikan Multi-Head Attention
Desain Model
Definisikan DQN dengan Lapisan Tersembunyi
Output Layer untuk Q-values
Fungsi untuk Memilih Tindakan menggunakan
Strategi e-greedy
Fungsi untuk Memperbarui Model dengan
Pengalaman dari Replay Buffer
Gambar 3.1 Diagram Alir Penelitian


3.2 Pengumpulan Data
     Langkah awal adalah mengumpulkan dataset yang akurat dan relevan. Dataset didapatkan dari data sekunder, dataset ini merupakan hal yang penting dari simulasi dan eksperimen, mencakup koordinat lokasi yang mungkin meliputi lokasi depot dan titik pengiriman, jendela waktu untuk setiap pengiriman yang menentukan batas awal dan akhir kapan pengiriman harus dilakukan, serta jumlah kendaraan. Data ini harus mencerminkan situasi dunia nyata untuk memastikan bahwa model yang dikembangkan dapat diaplikasikan secara praktis. Normalisasi merupakan proses penting untuk menyamakan skala data, memastikan bahwa model dapat memprosesnya dengan efisien. 3.4 Desain model
     Implementasi Deep Q-Network (DQN) dengan mekanisme attention untuk Dynamic Vehicle Routing Problem with Time Windows (DVRPTW) melibatkan beberapa langkah utama, mulai dari pemilihan kerangka kerja hingga pembuatan lingkungan simulasi. Informasi ini digunakan untuk memperbarui kebijakan model dengan cara mengoptimalkan parameter jaringan sehingga meningkatkan estimasi nilai Q, yang merepresentasikan hadiah kumulatif yang diharapkan. Untuk meningkatkan stabilitas dan efisiensi pelatihan, teknik seperti experience replay dan target networks digunakan. Selain itu pada tahap pelatihan model ini juga dilakukan penyetelan hyperparameter untuk menemukan nilai optimal hyperparameter guna meningkatkan kinerja model. Ini merupakan langkah penting dalam machine learning karena dapat menghasilkan peningkatan akurasi, efisiensi, dan generalizability model. 3.6 Evaluasi Model
     Setelah fase pelatihan model Deep Q-Network (DQN) dengan multi-header attention untuk Dynamic Vehicle Routing Problem with Time Windows (DVRPTW) selesai, langkah evaluasi menjadi penting untuk memahami seberapa efektif model dalam menyelesaikan masalah yang ditargetkan. Dalam konteks DVRPTW, metrik yang relevan seperti total jarak tempuh oleh semua kendaraan dan kepatuhan terhadap jendela waktu pengiriman menjadi fokus utama. Total jarak tempuh mencerminkan efisiensi rute yang dihasilkan, sementara kepatuhan terhadap jendela waktu mencerminkan kualitas layanan yang dapat dijamin oleh model. 3.7 Analisis dan Penyempurnaan
     Langkah terakhir yaitu analisis secara mendalam kinerja model pada dataset pengujian. Penyempurnaan dilakukan untuk mengatasi kelemahan yang telah dianalisis sebelumnya seperti penyempurnaan pada tuning hyperparameter untuk peningkatan kinerja, modifikasi arsitektur dan pelatihan ulang model. 3.8 Jadwal Penelitian
     Jadwal penelitian digunakan untuk meningkatkan efektivitas dalam proses penelitian. Adanya jadwal penelitian ini setiap proses penelitian sudah terjadwal dalam tabel 3.1 sehingga penelitian lebih efektif dan optimal. Tabel 3.1 Jadwal Penelitian"
Armando Tirta Dwilaga_Kualifikasi.txt,"3.1 Gambaran Umum Penelitian
     Penelitian ini digunakan untuk mengatasi sensitivitas terhadap cacat pada gambar ban dengan melibatkan penggunaan jaringan syaraf menggunakan algoritma Convolutional Neural Network (CNN) dan membangun model atau kerangka kerja menggunakan Keras. Berikut adalah Gambar 3.1 Blok Diagram Gambaran Umum Penelitian. Berdasarkan Gambar 3.1 Blok Diagram Gambaran Umum Penelitian maka dapat dijelaskan di blok tersebut terbagi menjadi 3 bagian yaitu bagian pertama adalah unit 
masukan berisikan data preparation di mana gambar ban dimuat, diubah menjadi format yang sesuai dipersiapkan untuk pelatihan model Convolutional Neural Network (CNN) 
seperti pemrosesan gambar ban, selanjutnya data augmentation di mana data dibuat lebih ber variasi dari training data yang ada sehingga dapat meningkatkan keberagaman 
training data tanpa harus mengambil data baru, mencakup (rotasi, pergeseran horizontal/vertikal, perbesar gambar, perubahan kecerahan gambar, sampai mengubah nilai pixel), selanjutnya data di mana dataset yang telah di augmentasi dan disiapkan dibagi menjadi subset yang berbeda untuk training untuk melatih model, validation untuk menyempurnakan model serta memvalidasi performanya selama pelatihan, dan testing untuk mengevaluasi kinerja model akhir. Dataset dibagi menjadi training data, validation data, dan testing data dalam proporsi tertentu. Bagian kedua adalah unit pemrosesan yang bertindak adalah model training (forward Pass, tahap di mana input diproses melalui model untuk menghasilkan prediksi), tujuannya melatih model Convolutional Neural Network (CNN) menggunakan dataset pelatihan di mana data dari unit masukan diteruskan melalui jaringan neural di lakukan transformasi linier (konvulasi) dan non-linier (fungsi aktivasi) dilakukan pada data di setiap lapisan untuk menghasilkan output prediksi yang melibatkan komputasi di setiap neuron dan lapisan jaringan, yang merupakan inti dari proses pembelajaran dalam jaringan saraf. Selanjutnya unit pemrosesan Fine-tuning tujuannya dilakukan untuk 
menyempurnakan model lebih lanjut setelah pelatihan awal dengan dataset yang lebih kecil atau lebih spesifik nantinya. Proses di dalam Fine-tuning menyesuaikan bobot 
(menggunakan kumpulan data yang lebih kecil untuk menyesuaikan bobot model untuk performa yang lebih baik), pelatihan khusus (fokus pada fitur data yang lebih relevan 
dengan objek). Bagian ketiga adalah unit keluaran yang bertindak ada proses model evaluatioan (backwardpass, tahap di mana gradien (memperbarui parameter model dalam arah yang 
akan mengurangi fungsi loss) dari fungsi loss (metrik yang mengukur seberapa baik atau buruk model melakukan prediksi dibandingkan nilai aktualnya) dihitung dan digunakan 
untuk memperbarui parameter model selama pelatihan) tujuannya mengevaluasi performa model yang dilatih dan model dievaluasi menggunakan metrik yang relevan (accuracy, 
precision, recall, dan F1- score) berdasarkan prediksi yang dihasilkan dari model terhadap validasi atau uji data. Output dari proses ini adalah tentang hasil evaluasi model, yang memberikan informasi kinerja model. Selanjutnya ada dua alur pilihan yang bisa dilakukan, alur pertama jika hasil prediksi sudah sesuai dengan keinginan maka bisa langsung masuk ke model deployment (inference), dan alur kedua jika hasil prediksi masih perlu diperbaiki pada bagian unit pemrosesan terlebih dahulu fine tuning untuk penggunaan data set lebih kecil (jika menunjukan model belum mencapai performa yang diharapkan) baru masuk ke model deployment (inference) tujuannya menerapkan model 
terlatih untuk membuat prediksi pada data baru yang belum terlihat. Model deployment (inference) yang telah dilatih digunakan untuk membuat prediksi pada data baru atau dalam situasi dunia nyata, tahap di mana model menerima input baru dan menghasilkan output berdasarkan pada pembelajaran yang dilakukan selama proses pelatihan dan merupakan output akhir dari keseluruhan proses, di mana model ""mengambil keputusan"" atau ""membuat prediksi"" berdasarkan pada pengalaman yang telah diperoleh selama pelatihan. 3.2. Tahapan Penelitian
      Penelitian ini di dalamnya terdapat tahapan-tahapan yang dilakukan untuk membentuk satu kesatuan yang utuh dari awal sampai akhir dan membentuk kerangka penelitian mengenai klasifikasi pada produk ban menggunakan algoritma Convolutional Neural Network (CNN). Berikut Gambar 3.2 Tahapan penelitian. 3.2.1 Studi Literatur
      Tahap pertama adalah studi literatur di mana studi yang dilakukan berasal dari artikel ilmiah dan buku yang menunjang dalam menganalisis terkait dengan metode 
pengukuran kualitas, mengenai klasifikasi produk ban, meninjau penggunaan pembelajaran mesin algoritma Convolutional Neural Network (CNN) dari beberapa tahun ke belakang dalam konteks pengukuran kualitas untuk klasifikasi terhadap kondisi-kondisi produk ban. 3.2.2 Data Aquisition
     Tahap kedua adalah data aquisition dengan mengumpulkan kumpulan data sesuai tujuan penelitian dengan target untuk kumpulan data gambar ban untuk training data, 
validation data, dan testing data, memastikan bahwa kumpulan data tersebut memiliki varian yang secara akurat memang mewakili kondisi produk ban dan diperoleh dari sumber-sumber terpercaya. 3.2.4 Data Augmentation
     Tahap keempat adalah data augmentation meningkatkan variasi dalam dataset dengan teknik augmentasi data, menggunakan operasi seperti rotasi, pergeserarn 
horizontal/vertikal, perbesar gambar, perubahan kecerahan gambar, sampai mengubah nilai pixel untuk memperkaya dataset dan mengurangi overfitting (Saat disajikan dengan 
data baru yang belum pernah dilihat sebelumnya, performa model akan menurun drastis karena model tersebut dapat menyesuaikan diri dengan kumpulan training data dengan 
sangat efektif). Sebaliknya, pada saat runtime hanya menghasilkan variasi dari gambar yang sudah ada dibuat secara dinamis dan cukup bagi model untuk berlatih dari berbagai kondisi gambar ban yang ada pada kenyataaanya. Data asli pada dataset berjumlah 1.028 data gambar ban setelah dilakukan augmentasi secara fisik menggunakan website roboflow ada data yang tidak dapat diidentifikasi ada 3 gambar sehingga total gambar asli yang berhasil di upload dan dijadikan data asli yang tetap berjumlah 1.025 data gambar dan setelah di augmentasi bertambah menjadi 2.050 data gambar ban. 3.2.6 Model Building
      Tahap keenam adalah model building (membangun model Convolutional Neural Network (CNN) dengan Keras) membangun arsitektur model Convolutional Neural Network (CNN) menggunakan Keras, mengatur lapisan-lapisan seperti convolutional, MaxPooling2D, Flatten, dan Dense untuk membangun model. learning rate dalam penggunaan algoritma optimasi menggunakan Adaptive Momentum (Adam) untuk menghasilkan pembelajaran yang adaptif, pemilihan penggunaan Adaptive Momentum (Adam) jika dibandingkan dengan learning rate lain seperti Stochastic Gradient Descent (SGD) karena kecepatan pembelajaran adaptif untuk Adaptive Momentum (Adam) bisa secara otomatis menyesuaikan learning rate untuk setiap parameter dalam model klasifikasi ban sedangkan Stochastic Gradient Descent (SGD) memiliki learning rate tetap selama pelatihan model klasifikasi ban yang penentuannya dari user dan tidak bisa menyesuaikan learning rate secara otomatis berdasarkan kondisi aktual dari setiap parameter. Selanjutnya secara kestabilan dan konvergensi Adaptive Momentum (Adam) menyambung dari awal dapat mengubah kecepatan pembelajaran secara adaptif, sehingga membuatnya lebih stabil dan kecil kemungkinannya terjebak pada tingkat minimum lokal (nilai yang dianggap sebagai titik terendah dari loss function dalam model) sehingga Adaptive Momentum (Adam) cenderung mencapai konvergensi (tingkat kinerja yang diharapkan) lebih cepat dan andal dalam berbagai keadaan, sedangkan Stochastic Gradient Descent (SGD) mungkin lebih stuck pada nilai minimum atau terjebak pada nilai minimum lokal yang disebabkan oleh kemungkinan bergantung pada seberapa tepat kecepatan pemelajaran dipilih, kecepatan pemelajaran yang tetap dapat membuat model mencapai konvergensi terlalu cepat atau terlalu lambat. 3.3 Arsitektur Convolutional Neural Network (CNN)
     Convolutional Neural Network (CNN) yang dibangun menggunakan model atau kerangka kerja yang pada dasarnya menggunakan Keras dan juga tensorflow dengan menambahkan beberapa model lapisan-lapisan seperti lapisan convolutional (Conv2D), laposan pooling (MaxPooling2D), Flatten, dan lapisan fully connected (Dense). Terakhir evaluation di mana performa model pada testing data dinilai menggunakan hasil klasifikasi dan confusion matrix untuk memahami kinerjanya testing data. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. Fourth Cov2D
Total Neuron = (Ukuran Filter x Jumlah Channel Input + 1) x Filter = (3 x 3 x 32 + 1)x 16 = (289)x 16 = 4624
Jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. Selanjutnya adalah dalam penentuan ukuran spasialnya setiap filter diubah menjadi setengah dari ukuran input nya (5x5) menjadi (2x2) sebagai berikut. Setiap filter diubah menjadi setengah dari ukuran inputnya (2x2) menjadi (1x1) dan jumlah neuronnya 4624 mengikuti lapisan konvolusi keempat. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 9. Flatten
Tidak mengubah parameter yang ada karena fungsi flatten hanya mengubah matriks multidimensi menjadi vektor tunggal berdasarkan hasil dari jumlah filter pada lapisan 
lima cov2d yaitu 8 dan maxpooling2D dengan ukuran inputnya (1x1) sehingga menjadi matriks multidimensi (1, 1, 16) diubah menjadi nilai vektor tunggal dengan panjang 16 atau menjadi jumlah neuron sebanyak 16. 10. Dense Layer 1
Total Neuron = (Jumlah Neuron Input + 1) x Jumlah Neuron Output = (16+ 1)x 128 = (17)x 128 = 2176
11. Dropout Layer 1
Menggunakan 0.2 yang artinya sebanyak 20% dari neuron dalam dense layer 1 akan dinonaktifkan secara acak. 12. Dense Layer 2
Total Neuron = (Jumlah Neuron /nput + 1) x Jumlah Neuron Output = (128 + 1)x 64 = (129)x 64 = 8256
13. Dropout Layer 2
Menggunakan 0.2 yang artinya sebanyak 20% dari neuron dalam dense layer 2 akan dinonaktifkan secara acak. 14. Dense Layer 2
Total Neuron = (Jumlah Neuron /nput + 1) x Jumlah Neuron Output = (64 + 1)x 1 = (65)x 1 = 65
Ketika dimensi spasial (tinggi dan lebar) dikurangi menggunakan operasi lapisan pooling seperti maxpooling, jumlah neuron di setiap lapisan pooling akan menurun. Misalnya, dimensi spasial setiap filter (tinggi dan lebar) di lapisan maxpooling disesuaikan menjadi setengah dari dimensi masukannya. Karena hanya separuh dari masukan yang diproses lebih lanjut, hal ini juga menyebabkan berkurangnya jumlah neuron pada lapisan tersebut. Sedangkan penurunan pada dense terjadi karena penentuan jumlah neuron."
Devi Resviani_KUALIFIKASI.txt,"3.1 Tahapan Penelitian
Tahapan penelitian merupakan serangkaian langkah-langkah yang dilakukan dalam penelitian. Gambaran mengenai tahapan penelitian ini dapat dilihat pada Gambar 3.1. Tahapan penelitian ini terdiri dari studi literatur untuk memahami keadaan yang terfokus terhadap tentang mesin kompresor reciprocating, metode prediksi pemeliharaan mesin, predictive maintenance, dan machine learning. Preprocessing data melibatkan pembersihan data, normalisasi, dan transformasi data agar siap digunakan dalam model machine learning. Pengembangan dan melatih model machine learning menggunakan algoritma yang telah dipilih dengan data yang telah diperoses, tahap ini melibatkan pembagian data menjadi set pelatihan dan set pengujian, serta termasuk mengatur parameter model untuk mencapai kinerja terbaik. Tahap keempat analisis prediksi untuk memperkirakan kapan dan bagaimana kegagalan akan terjadi, sehingga pemeliharaan dapat direncanakan dengan tepat. Tahap kelima alert atau peringatan untuk memberikan informasi kepada tim pemeliharaan agar dapat segera mengambil tindakan. Tahap terakhir menjadwalkan tindakan pemeliharaan yang diperlukan berdasarkan peringatan untuk menghindari kegagalan mendadak dan meminimalkan downtime."
Erfiana Wahyuningsih_UK.txt,"Berikut flowchart penelitian untuk rangkaian SRAM 6T Low power dan High read stability dengan metode m- GDI. Dalam memulai desain SRAM 6T dengan menggunakan metode m-GDI, diperlukan studi literatur terkait beberapa penelitian dengan metode atau hasil serupa. Setelah mempelajari seluruh penelitian terkait, maka dilakukan desain
rangkaian SRAM dengan metode konvensional sebagai referensi untuk dilakukan proses m-GDI. Referensi rangkaian diperlukan untuk melihat hasil sebagai pembanding dengan rangkaian baru yang didesain dengan metode m-GDI."
Fitriana Indah Pramitasari_Kualifikasi.txt,"3.1 Alur Penelitian
       Alur penelitian menggambarkan alur dari awal hingga akhir penelitian dilaksanakan. Alur penelitian ini diuraikan pada Gambar 3.1 di bawah ini. 3.2 Identifikasi Masalah
       Identifikasi masalah adalah salah satu langkah pertama yang dilakukan sebelum melakukan penelitian. Identifikasi masalah merupakan suatu proses mencari dan mengetahui masalah yang ingin diselesaikan. Identifikasi masalah ini membantu penelitian untuk memahami tantangan yang dihadapi oleh petani kentang skala nasional dan 
merancang solusi yang tepat sesuai dengan kebutuhan mereka. Identifikasi masalah pada penelitian ini berfokus pada mengidentifikasi proses perancangan model koperasi petani, mengidentifikasi metode prediksi permintaan dengan ANN di dalam blockchain yang digunakan untuk mengoptimalkan permintaan pelanggan di masa depan selama periode 
tertentu, dan mengidentifikasi metode safety stock di dalam blockchain yang digunakan agar dapat mengoptimalkan stok dan permintaan. Identifikasi masalah pada penelitian ini, peneliti dapat lebih memahami kendala dan kebutuhan petani kentang skala nasional. Perancangan model platform koperasi untuk meningkatkan efisiensi dan kerjasama antarpetani dengan koperasi sebagai mitranya. Sementara itu, metode prediksi permintaan dengan menggunakan Artificial Neural 
Network (ANN) diharapkan dapat membantu petani mengelola produksi secara lebih tepat sesuai dengan kebutuhan pasar dan koperasi dapat menyesuaikan persediaan stok dan 
permintaan secara dinamis dari hasil prediksi permintaan. Selain itu, identifikasi masalah juga mencakup penerapan metode safety stock untuk mengoptimalkan manajemen stok, 
memastikan ketersediaan barang, dan meningkatkan responsibilitas terhadap fluktuasi permintaan pasar. Dengan penerapan ANN dan metode safety stock di dalam blockchain, 
semua prediksi dan manajemen stok dapat dicatat di dlaam buku besar yang tidak dapat diubah, sehingga meningkatkan transparansi dan keamanan data dalam rantai pasok. Sehingga koperasi ini dapat melakukan perencanaan yang lebih akurat, meminimalkan pemborosan, dan meningkatkan ketersediaan kentang sesuai dengan kebutuhan pelanggan. Dengan demikian, platform koperasi menjadi responsif terhadap perubahan permintaan pasar, mendukung pertumbuhan ekonomi para petani, memperkuat kolaborasi antar 
anggota koperasi serta memiliki transparansi dan keamanan pada rantai pasok. 3.3 Studi Literatur
       Studi literatur yang dilakukan pada penelitian engembangan platform koperasi petani ini dimulai dari pencarian dan review literatur-literatur terbaru dan relevan yang telah diterbitkan. Studi literatur juga membantu dalam mengetahui tantangan dan peluang yang mungkin dihadapi dalam pengembangan platform koperasi petani kentang. Sehingga penelitian ini akan menghasilkan data yang sesuai dengan tujuan penelitian. Proses ini 
memungkinkan peneliti untuk memahami konteks yang telah ada sebelumnya dan memanfaatkan pengetahuan serta data yang telah dihasilkan sebelumnya. Beberapa wilayah 
Indonesia berhasil dalam produksi kentang dan beberapa wilayah Indonesia yang tidak dapat memproduksi kentang. Data tersebut memberikan gambaran lengkap mengenai 
kegiatan pertanian kentang di berbagai wilayah Indonesia pada tahun 2022. Data primer yang akan digunakan pada penelitian ini adalah kebutuhan pengguna, aliran data dari petani dengan koperasi sebagai mitranya, data musim, data historis penjualan, data produksi kentang dan data harga kentang. Berdasarkan informasi yang didapatkan dari salah satu petani di Wonosobo, Jawa Tengah di sana terdapat banyak petani kentang dan sayuran lainnya. 3.5 Blockchain
       Pada penelitian ini, untuk meningkatkan keamanan dan transparansi maka menggunakan teknologi blockchain untuk rantai pasok kentang. Berikut flowchart kecerdasan buatan, safety stock yang dikombinasikan di dalam blockchain. Data rantai pasok yang telah dikumpulkan, kemudian dimasukkan ke 
dalam database. Data tersebut diverifikasi dalam blockchain dengan proses pembuatan blok baru yang melibatkan perhitungan hash blok sebelumnya, menyusun blok baru, 
menghitung hash blok baru, dan mencapai konsensus untuk menambahkan blok ke rantai. Hasil prediksi permintaan disimpan dalam blockchain dengan proses pembuatan blok baru yang sama seperti langkah sebelumnya. Hasil perhitungan safety stock disimpan dalam database dan dicatat dalam blockchain dengan pembuatan blok baru. 3.6 Design Sistem dengan UML
       Pengembangan platform koperasi petani kentang menggunakan metode Unified Modeling Language (UML) untuk menggambarkan struktur, fungsi dan interaksi komponen sistem secara visual. Pengguna platform ini terdiri dari consumers yang dapat mengakses 
produk pertanian secara langsung, farmers yang memanfaatkan platform untuk memasarkan hasil panen, companies yang terlibat dalam dukungan pengembangan teknologi, dan partner cooperatives yang menjadi bagian dari kolaborasi kerjasama antar koperasi untuk meningkatkan kesejahteraan bersama. Keterlibatan seluruh pihak ini, diharapkan platform koperasi petani menciptakan lingkungan yang saling mendukung dan berkelanjutan, memperkuat konektivitas antar anggota untuk mencapai tujuan bersama dalam dunia pertanian. Proses verifikasi produk kentang yang dihasilkan oleh petani. Website koperasi mengubah status transaksi berdasarkan hasil verifikasi. Dengan menerapkan metode ANN pada prediksi permintaan ini, penelitian dapat memberikan prediksi yang lebih tepat terkait kebutuhan pasar di masa mendatang sehingga dapat meningkatnya efektivitas rantai pasok. Kemudian, dilakukan data cleaning, dinormalisasi, dan di-transformasi untuk memastikan bahwa ANN yang akan dibangun dapat bekerja dengan efektif dan menghasilkan prediksi yang akurat. Berdasarkan hasil evaluasi, model prediksi yang akurat dari model ANN ini berguna untuk perusahan dalam membuat keputusan strategis seperti inventory management. Proses prediksi permintaan dengan ANN akan menghasilkan data permintaan yang diharapkan, informasi tersebut digunakan untuk proses inventory management. Tahapan pertama, penelitian ini memerlukan analisis data historis permintaan kentang, fluktuasi pasokan, dan waktu panen, sehingga dapat mengidentifikasi kebutuhan pasokan dan resiko keterlambatan. Penerapan metode safety stock pada penelitian ini akan menentukan tingkat persediaan tambahan yang diperlukan untuk mengatasi ketidakpastian dalam permintaan atau keterlambatan pasokan. Metode safety stock dalam pengembangan platform koperasi petani kentang pada penelitian ini untuk meningkatkan efisiensi manajemen persediaan. Selain itu, integrasi ini melibatkan penggunaan Artificial Neural Network (ANN) dan metode safety stock yang terintegrasi dalam blockchain untuk rantai pasok. Website koperasi akan terintegrasi dengan blockchain, untuk memastikan efisiensi dan transparansi dalam seluruh proses manajemen rantai pasok. 3.10 Pengujian Sistem
       Tahapan pengujian sistem dalam penelitian merupakan langkah untuk mengevaluasi kinerja atau fungsionalitas sistem yang dikembangkan atau diuji pada penelitian. Proses pengujian sistem mencakup implementasi prototipe atau model sistem, hingga serangkaian uji coba. Pada penelitian ini, sistem platform koperasi petani diharapkan dapat berjalan sesuai dengan tujuan dan persyaratan perkoperasian serta sesuai dengan model platform economic sharing. Platform koperasi petani kentang pada penelitian ini akan berbasis website dan dilengkapi dengan kecerdasan buatan yang dikombinasikan dengan blockchain. 3.11 Evaluasi
       Tahapan selanjutnya adalah evaluasi. Evaluasi dilakukan untuk memastikan bahwa semua komponen sistem berfungsi sesuai rencana. Evaluasi melibatkan penilaian kinerja pada sistem secara keseluruhan, dan memeriksa apakah integrasi berjalan tanpa hambatan. Tahapan evaluasi juga dapat mengidentifikasi apakah hasil pengujian sistem sesuai dengan tujuan awal dan menentukan area yang mungkin memerlukan peningkatan. Hasil dari tahap evaluasi menjadi petunjuk penting untuk membuat perubahan dan peningkatan, sehingga sistem dapat bekerja lebih baik lagi. 3.12 Analisis Hasil
       Analisis merupakan tahapan penelitian, dimana menyimpulkan serta menguraikan informasi dari hasil data yang telah diolah dan diuji sebelumnya. Tahapan analisis dapat memberikan makna dari temuan-temuan tersebut. Tahapan ini memberikan identifikasi faktor-faktor yang dapat mempengaruhi kinerja sistem dan memberikan rekomendasi untuk peningkatan di masa yang akan datang. 3.13 Jadwal Penelitian
       Penelitian ini diuraikan pada Tabel Jadwal Penelitian yang merupakan uraian manajemen waktu dalam perencanaan dan pelaksanaan suatu penelitian agar penelitian dapat memenuhi target waktu yang telah ditetapkan. Tabel ini menjelaskan tahapan-tahapan penelitian beserta waktu penelitian. Berikut uraian rencana jadwal penelitian program Doktor Teknologi Informasi di Universitas Gunadarma."
KUALIFIKASI_Riya Widayanti.txt,"Bab ini menyajikan desain yang digunakan dalam penelitian ini. Hal ini menentukan sumber dari mana data akan dikumpulkan dan bagaimana mengumpulkan dan menganalisis data ini. Pada bab ini akan dibahas mengenai filosofi keilmuan dari data governance, konsep teknolgi BLockchain dan penerapan data governance dalam teknologi blockchain di bidang pendidikan, yang akan memberikan pandangan utama saat melakukan penelitian. Skema Penelitian
Untuk menyelesaikan penelitian dirancang kerangka pikir yang menggambarkan langkah-langkah yang harus ditempuh, dapat dilihat penjelasan dan urutannya sebagai berikut:

3.2.1 Mendefinisikan Tata Kelola Data untuk Organisasi
Upaya Tata Kelola Data harus mendukung strategi dan tujuan bisnis. Strategi dan sasaran bisnis organisasi menginformasikan strategi data perusahaan dan bagaimana tata kelola data dan aktivitas manajemen data perlu dioperasionalkan dalam organisasi. Tata kelola data memungkinkan tanggung jawab bersama untuk keputusan terkait data. Kegiatan tata kelola data melintasi batasbatas organisasi dan sistem untuk mendukung tampilan data yang terintegrasi. Fokusnya adalah pada kesan yang dimiliki personel bisnis tentang seberapa baik perusahaan mengelola data dan menggunakan data untuk keuntungannya, serta pada kriteria objektif, seperti penggunaan alat, tingkat pelaporan, dll. Hasilnya ada bagaimana kerangka komunikasi dijelaskan dalam gambar 3.3. Kebijakan Penggunaan Data: Kebijakan tersebut menentukan tindakan tata kelola data termasuk hak, izin, dan kondisi. Penyimpanan Data Off-chain: Data pribadi harus disimpan off-chain untuk skalabilitas yang lebih baik dan efisiensi yang lebih tinggi. Selain itu, menyimpan data pribadi langsung ke clockchain, bahkan dalam bentuk terenkripsi, dapat menimbulkan potensi kebocoran privasi dan mengakibatkan ketidakpatuhan terhadap GDPR. Tergantung pada skenario tertentu, DBMS konvensional (misalnya, Oracle atau MongoDB), layanan penyimpanan awan (misalnya, S3, AWS atau Azure), atau sistem penyimpanan dapat digunakan untuk penyimpanan data. Hanya referensi ke data yang disimpan secara on-chain (yaitu, disimpan dalam buku besar terdistribusi). Referensi disebut penunjuk data itu bisa menjadi hash, string koneksi, jalur absolut, atau pengidentifikasi yang merujuk ke kumpulan data; tergantung pada sistem penyimpanan off-chain tertentu yang digunakan dalam platform."
Kualifikasi Witta Listiya Ningrum.txt,"3.1 Tahapan Penelitian
Penelitian ini melakukan pengembangan model klasifikasi toksisitas pada platform sosial media. Selain itu juga untuk menentukan dan membandingkan metode serta algoritma yang sudah digunakan pada penelitian sebelumnya, yang nantinya akan mengembangkan atau menciptakan suatu metode atau algoritma terbaru. Data tersebut harus mencakup berbagai jenis media, seperti teks, gambar dan video, untuk memungkinkan model mengenali toksisitas dari berbagai jenis konten yang ada pada platform sosial media. Large Language Model (LLM)
Large Language Model merupakan jenis model kecerdasan buatan (Artificial Intelligence) yang dilatih untuk memahami, menghasilkan dan memproses bahasa alami (Natural Languange) dalam skala besar. Generate Caption
Pada tahapan ini menggunakan model LLM, seperti BLIP atau Flamingo untuk menggabungkan kemampuan visual dan bahasa dalam menghasilkan teks/captioning dari representasi gambar dan video. Model Klasifikasi Toksisitas
Pada tahapan ini dilakukan pengembangan model dari hasil penggabungan ketiga representasi tersebut, dengan menggunakan teknik fusion, seperti concatenation atau attention mechanism untuk menghasilkan hasil klasifikasi akhir. Evaluasi Model
Pada tahapan ini dilakukan evaluasi untuk mengetahui kinerja terhadap model yang dikembangkan dengan menggunakan pengukuran akurasi, seperti precision, recall dan juga F1-score untuk klasifikasi teks, dan mengukur akurasi dengan confusion matrix untuk gambar dan video. Hasil
Tahapan ini menghasilkan klasifikasi sesuai dengan label yang sudah dikategorikan ke dalam 3 kategori toksisitas yaitu toxic, non-toxic, dan netral."
Kualifikasi_Andi Asnur Pranata M. H. (99219024).txt,"3.3.2 Data Sekunder
     Data sekunder dikumpulkan dari hasil studi literatur, review penelitian terdahulu, pencarian perangkat lunak dan beberapa template perangkat lunak yang banyak tersedia di internet. Data sekunder ini juga akan mengumpulkan mengenai informasi proyek, sumber dana, jenis laporan baik dari pemilik (pengguna jasa), konsultan dan kontraktor, serta jadwal pelaksanaan dan kemajuan fisik, termin pembayaran serta informasi orang-orang yang berkepentingan dalam proyek konstruksi. 3.4 Pengembangan Sistem
     Untuk pengembangan sistem, sesuai yang telah disampaikan pada latar belakang bahwa metode yang digunakan dalam memodelkan sistem informasi manajemen proyek konstruksi adalah Rapid Application Development (RAD). Metode RAD digunakan karena modul yang terlalu banyak sehingga untuk fleksibilitas dalam pengembangan sistem dapat dikendalikan serta jika ada perubahan pada setiap modul, maka pengembang secara fleksibel dapat merubah modul tersebut dan modul yang berkaitan. b. RAD Design Workshop
     Pada tahap ini, tahap perancangan proses sistem, basis data dan user interface yang akan dikerjakan untuk prototype sistem, kemudian menganalisis dan mengembangkan modul-modul yang dirancang. Tahap ini akan terdiri dari dua tahap, yaitu :
     1)	Tahap Pembangunan Sistem
     Jika perancangan siap dan sudah disetujui, maka proses sistem akan dibangun dengan menggunakan bahasa Laravel Framework sesuai dengan rancangan yang sudah dibuat. 3.4.4 Analisis Sistem Usulan
     Berdasarkan hasil pengumpulan informasi mengenai kelemahan sistem yang berjalan dan hasil identifikasi masalah, maka untuk menyelesaikan permasalahan tersebut akan dianalisis untuk keperluan sistem usulan guna untuk melakukan pengembangan sistem informasi manajemen pada proyek konstruksi. Dari hasil tersebut, akan terbentuk daftar usulan objek pada sistem tersebut, kemudian akan berlanjut pada tahap pembuatan class diagram pada sistem tersebut. 3.4.6.2 Spesifikasi Database
        Pada spesifikasi database ini akan merancangan desain tabel sistem manajemen informasi untuk proyek konstruksi. 3.4.7 Perancangan Antar Muka
     Pada perancangan antar muka ini akan dirancang antar muka sistem informasi manajemen untuk proyek konstruksi yang nanti akan dibagi berdasarkan aktor-aktor pada case diagram. 3.5 Implementasi
3.5.1 Pembangunan Sistem
      Pada pembangunan sistem ini akan dibangun menggunakan hardware dan software sesuai dengan kebutuhan spesifikasi yang akan dibutuhkan. Pengujian ini dilakukan untuk mengetahui apakah semua modul yang sudah dibentuk berjalan sesuai rancangan atau tidak, serta mengetahui apakah ada kesalahan-kesalahan terhadap proses pada sistem informasi manajemen untuk proyek konstruksi. 3.6 Kesimpulan dan Saran
      Setelah tahap penelitian selesai, maka pada tahap akhir ini peneliti akan memberikan kesimpulan mengenai hasil penelitian yang sudah didapatkan. Pada tahap akhir ini juga, peneliti akan memberikan saran kepada peneliti berikut yang akan melakukan penelitian dengan tema yang sama, untuk memberikan gambaran dalam mengembangkan penelitian yang sudah dilakukan sebelumnya."
Kualifikasi_Aris Gunaryati.txt,"3.1 Gambaran Umum Penelitian
     Motivasi dari Metodologi yang diusulkan adalah membuat suatu metode peramalan yang sesuai dengan data runtun waktu yang ada serta meningkatkan akurasinya dengan tetap memperhatikan efisiensi waktu komputasinya. Langkah-langkah yang dilakukan dalam penelitian ini adalah menganalisis data jumlah kasus harian Covid 19 di Jakarta berdasarkan dataset dari situs https://corona.jakarta.go.id tanggal 6 Maret 2020 sampai 30 Juni 2021 sebagai data training dan nanti akan diprediksi untuk tanggal 1 Juli 2021 sampai dengan 31 Juli 2021 sebagai data uji dengan tahapan sebagai berikut :
1. Mempersiapkan data runtun waktu yang akan dianalisis
2. Menganalisis data runtun waktu yang ada menggunakan metode statistika ARIMA
3. Menganalisis data runtun waktu yang ada menggunakan metode Quantum Neural Network
4. Mengembangkan model Hybrid ARIMA-Quantum Neural Network
5. Melakukan perbandingan tingkat akurasi hasil peramalan dengan tiap model

Untuk mendapatkan model peramalan yang diharapkan sesuai dengan data runtun waktu yang ada, maka perlu dilakukan pendekatan ilmiah yaitu dengan melihat pola data runtun waktu yang ada terlebih dahulu. Pendekatan lainnya adalah menggunakan tools untuk menentukan secara otomatis Bentuk model statistik ARIMA yang sesuai dengan runtun waktu yang ada, lalu model tersebut dilatih menggunakan quantum neural network agar diketahui pola-pola data yang	sudah	ada	dan	dapat	diuji	akurasinya. Tahapan Analisis Time Series (ARIMA)
a. Membuat Plot Time Series
Identifikasi asumsi stasioneritas data runtun waktu. Suatu deret pengamatan dikatakan stasioner apabila proses tidak berubah seiring dengan perubahan waktu
Tidak stasioner dalam mean : jika trend tidak datar (tidak sejajar smbu waktu)
Tidak stasioner dalam varian : jika trend datar atau hampir datar, tetapi data tersebar membangun pola melebar atau menyempit (pola terompet)
Tidak stasioner dalam mean & varians : jika trend tidak datar dan data membentuk pola terompet. Semua sinyal yang diberi pengali bobot ini kemudian dijumlahkan satu sama lain untuk menghasilkan unit aktivasi. Unit aktivasi ini kemudian dibandingkan dengan sebuah nilai ambang, dan hasilnya dimasukkan kedalam fungsi transfer (fungsi non-linier) yang akan menghasilkan sebuah keluaran. Untuk fungsi hyperbolic tangent,

3.4 MODEL HYBRID ARIMA NEURAL NETWORK
      Berdasarkan hasil peramalan model ARIMA, akan dilakukan proses analisis runtun waktu menggunakan metode jaringan syaraf tiruan. Dengan kata lain, output dari peramalan model ARIMA akan menjadi input pada proses pengolahan data menggunakan metode jaringan syaraf tiruan. Kemudian akan ditentukan model jaringan syaraf tiruan yang sesuai dan cocok untuk data runtun waktu tersebut. Secara matematis, hasil ramalan secara keseluruhan yang diperoleh adalah sebagai berikut :
Zt merupakan hasil peramalan yang merupakan gabungan nilai ramalan dari model ARIMA atau Exponential Smoothing dan nilai ramalan dari model JST. 3.5 MODEL QUANTUM HYBRID ARIMA NEURAL NETWORK
      Ada banyak pendekatan untuk pengembangan model Quantum Arima NN. Mirip dengan bit klasik, gerbang dasar dapat membentuk gerbang kuantum bemacam-macam dan menyelesaikan keadaan kuantum dari beberapa logika transformasi. berbasis elemen pada gerbang pergeseran fasa 1 bit dan gerbang kontrol-Tidak 2 bit dalam dinamika kuantum diambil sebagai fungsi aktivasi dalam Jaringan saraf. Untuk memudahkan aplikasi, formulir berikut:
Fungsi kompleks diberikan untuk menyatakan keadaan kuantum:
adalah bilangan imaginer adalah kuantum fase

3.6 Pengukuran Kinerja
3.6.1 Mean Squared Error
      Dalam statistik, Mean Squared Error (MSE) sebuah estimator adalah nilai yang diharapkan dari kuadrat error. Error yang ada menunjukkan seberapa besar perbedaan hasil estimasi dengan nilai yang akan diestimasi. Perbedaan itu terjadi karena adanya keacakan pada data atau karena estimator tidak mengandung informasi yang dapat menghasilkan estimasi yang lebih akurat
3.6.2 Komparasi Hasil Peramalan
      Setelah nilai Mean Squared Error dari kedua metode didapatkan, maka akan dilakukan komparasi terhadap nilai MSE yang didapatkan pada periode testing (out- sample)
Jika nilai MSESTATISTIKA < MSEANN maka metode Statistika memiliki performa lebih baik dibandingkan metode ANN karena memiliki tingkat kesalahan relatif lebih kecil. Sebaliknya, jika MSESTATISTIKA > MSEANN maka metode Statistika memilki performa lebih buruk dibandingkan metode ANN karena tingkat kesalahan yang dihasilkan relatif lebih besar."
Kualifikasi_Rama Dian Syah.txt,"3.1   Tahapan Penelitian
      Tahapan penelitian dibagi atas beberapa tahapan yang dilakukan dari awal sampai akhir. Pada penelitian ini mengajukan pengembangan algoritma kriptografi citra digital dengan mengkombinasi teknik konfusi dengan algoritma Cat Map dan Henon Map serta teknik difusi dengan algoritma Logistic Map. Pengembangan pada algoritma ini diharapkan dapat memiliki keamanan yang lebih tinggi dengan melalui beberapa parameter pengujian. Nilai PSNR = 30 dB membuktikan kualitas yang baik pada citra asli atau citra terdekripsi (Lone et al., 2021)."
Kualifikasi_Remigius.txt,"Dalam proses pengembangan sistem pembelajaran arsitektur berbasis metaverse ini, peneliti juga ingin menunjukkan perlunya keterlibatan komunitas dan persepsi pengguna bidang arsitektur agar sistem pembelajaran yang dihasilkan sesuai dengan kebutuhan dan harapan mereka dalam meningkatkan efektivitas pembelajaran arsitektur itu sendiri. Diharapkan, sistem pembelajaran arsitektur berbasis metaverse ini dapat memberi kemudahan kepada komunitas dosen dan mahasiswa dalam mempelajari berbagai sisi arsitektur dengan memasuki dunia virtual dan mereka dapat memahami materi yang diajarkan serta memecahkan permasalahan arsitektur yang dihadapi secara interaktif, kolaboratif, dan imersif dengan solusi tepat tanpa harus mencari berbagai referensi wujud nyata arsitektur di dunia fisik atau dunia nyata. Dengan konsep pembelajaran matakuliah Perkembangan Arsitektur 1 yang dilakukan dengan metode metaverse, pembelajaran secara online ini dapat dilakukan dengan lebih interaktif. 3.2 Kerangka Penelitian
      Penelitian ini dilakukan dalam mencapai tujuan utama, yaitu pengembangan sistem pembelajaran arsitektur berbasis metaverse, terutama terkait perkembangan arsitektur. Penelitian ini dilakukan melalui beberapa tahap, antara lain:
      Tahapan Pengembangan Sistem Pembelajaran Perkembangan Arsitektur 1 Berbasis Metaverse

4. Efektivitas Pembelajaran Kolaboratif
- Presensi
- Imersi
- Kehadiran dalam realitas yang disimulasi
- Kapabilitas metaverse dalam membentuk lingkungan pengguna untuk memahami
realitas


- Pemahaman materi pembelajaran
- Kemampuan memahami materi pembelajaran Perkembangan Arsitektur berbasis
metaverse
Penelitian mengenai pengembangan sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse ini dilakukan dengan melibatkan komunitas, yang terdiri dari dosen dan mahasiswa, di Program Studi S1 Arsitektur, Jurusan Teknik Arsitektur. Dalam mewujudkan sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse ini, peneliti juga melakukan konstruksi avatar dan konten pembelajaran Perkembangan Arsitektur 1 di dalam dunia digital, sehingga terbentuk dunia virtual berbasis konten pembelajaran Perkembangan Arsitektur. Dalam hal ini, komunitas dosen dan mahasiswa ini melakukan koneksi ke dunia virtual berupa sistem pembelajaran Perkembangan Arsitektur berbasis metaverse dan semua jenis kegiatan yang dilakukan dalam menyelesaikan masalah dan menyediakan solusi yang diperlukan dapat tersimpan dalam basis data server, sehingga dapat diambil kembali setiap kali mereka masuk dan terlibat kembali dalam sistem pembelajaran virtual kolaboratif ini. Dalam sistem pembelajaran virtual kolaboratif ini, komunitas dosen dan mahasiswa dapat berinteraksi satu sama lain dalam penyelesaian masalah yang ada dan mencari solusi yang diperlukan, sehingga mereka benar-benar dapat hadir dan terlibat di dalam sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse secara intensif, interaktif, dan imersif. Efektivitas Pembelajaran Kolaboratif
      Pada tahap ini, peneliti menguji efektivitas pembelajaran kolaboratif yang didasarkan pada persepsi pengguna mengenai sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse. Keempat, pada aspek outcome, persepsi pengguna juga dieksplorasi dan dievaluasi tentang ketercapaian tujuan dari pembelajaran Perkembangan Arsitektur 1 berbasis metaverse sesuai dengan kriteria dan indikator yang ditetapkan dosen pengampu. 3.3 Pendekatan Penelitian
      Penelitian ini dilakukan menggunakan pendekatan kuantitatif eksperimental terhadap sistem pembelajaran Perkembangan Arsitektur berbasis metaverse yang dikembangkan dalam komunitas dosen dan mahasiswa Jurusan Teknik Arsitektur, Program Studi S1 Arsitektur Universitas Gunadarma. Sistem pembelajaran berbasis metaverse ini dikembangkan sesuai dengan materi pembelajaran Perkembangan Arsitektur 1 di kalangan mahasiswa Jurusan Teknik Arsitektur semester 3. Apabila pengembangan sistem pembelajaran ini sudah selesai, model pembelajaran berbasis metaverse tersebut diuji validitas dan reliabilitasnya dengan melibatkan penilaian objektif dan otoritatif dari para ahli,
baik ahli materi maupun media pembelajaran. Jika model sistem pembelajaran berbasis metaverse ini sudah dinyatakan valid dan reliabel, model tersebut diujicobakan kepada komunitas pengguna yang terdiri dari dosen dan mahasiswa dari Jurusan Teknik Arsitektur semester 3, sehingga akhirnya dapat diketahui efektivitas pembelajaran kolaboratif berbasis metaverse tersebut sesuai dengan kriteria dan indikator yang ditetapkan oleh dosen pengampu. Efektivitas pembelajaran kolaboratif berbasis metaverse dalam penelitian ini dievaluasi dengan melihat peningkatan pemahaman mahasiswa mengenai materi pembelajaran Perkembangan Arsitektur sesuai dengan kriteria dan indikator yang ditetapkan oleh dosen pengampu. Dari hasil uji efektivitas sistem pembelajaran ini, diharapkan dapat diketahui sejumlah kelebihan dan kekurangannya, sehingga dapat dijadikan sebagai bahan pertimbangan rekomendasi dalam meningkatkan kualitas sistem pembelajaran Perkembangan Arsitektur berbasis metaverse tersebut."
Miftakhul Zaen_KUALIFIKASI.txt,"3.1 Tahapan Penelitian
    Dalam penelitian mengenai pengembangan algoritma DBSCAN dengan kuantum terdapat langkah-langkah yang dilakukan, seperti pada gambar 3.1. Langkah-langkah yang dilaukan diantaranya yaitu, pengumpulan data, definisi qubits kriteria, inisialisasi sistem kuantum, hingga evaluasi klaster. 1. Data
Tahap awal dalam penelitian di awali dengan pembuatan data, dimana data yang digunakan pada penelitian ini adalah data sintetik. Data sintetik digunakan untuk mendapatkan jumlah data yang besar, selain itu data sintetik juga bersifat fleksibel karena jumlah data yang digunakan dapat ditentukan sesuai dengan kebutuhan pengujian algoritma yang dikembangkan. Data sintetik yang dibuat berisikan nama supplier, harga, kualitas, dan waktu pengiriman. Definisi Qubits Kriteria
Pada tahap ini kriteria yang digunakan untuk pengelompokan supplier diubah menjadi representasi kuantum menggunakan qubits. Formasi Kluster Supplier dengan Quantum Measurement
Pada tahapan ini membentukan klaster supplier dengan mengukur state kuantum yang telah diubah melalui interaksi antar qubits yang mewakili supplier. 3.2 Rangkuman Langkah-Langkah Penelitian
      Setelah mengembangkan algoritma kuantum DBSCAN selanjutnya membandingkannya dengan algoritma DBSCAN untuk mengetahui seberapa baik algoritma DBSCAN jika dibandingkan dengan algoritma klasiknya. Langkah-langkah tersebut dapat dilihat pada Gambar 3.2 Rangkuman Langkah-Langkah Prosedur Penelitian. 3.3 Jadwal Penelitian
    Jadwal Penelitian digunakan untuk merencanakan, mengatur, dan mengelola waktu, sumber daya, dan tugas-tugas dalam rangka mencapai tujuan yang telah ditetapkan. Berikut merupakan rencana kegiatan pada penelitian ini."
Ragmar Faikar Eka_Kualifikasi.txt,"Tahapan penelitian dijelaskan dalam bentuk flowchart sehingga dapat menjelaskan proses yang dilakukan mulai dari Studi Literatur sampai dengan Kesimpulan, Jadwal dan Estimasi Penelitian digambarkan dalam bentuk Time Table untuk menjadwalkan dan melakukan estimasi waktu dari tiap tahap yang dilakukan. 3.1.2 Pengumpulan Data
       Tahap kedua yaitu pengumpulan data, data yang digunakan pada penelitian ini adalah data citra digital Kelapa Sawit dengan Tingkat kematangan Belum matang, Setengah matang, Matang, Terlalu matang dan Tandan Buah yang kosong. Gambar asli akan dilakukan resize menjadi ukuran 224x224 piksel lalu data tersebut akan di augmentasi untuk memperbanyak dan memvariasi data agar dan hasil augmentasi akan dijadikan sebagai data latih untuk model yang dibuat. 3.1.4 Pembuatan Model
       Tahap keempat yaitu pembuatan model machine learning menggunakan MobileNetV3 (Small/Large) dan menggabungkannya dengan attention module CBAM (Convolutional Block Attention Module). Proses pembuatannya meliputi pembuatan tampilan user, memasukan tflite ke dalam aplikasi sehingga aplikasi dapat menggunakan Model Machine Learning untuk mengklasifikasi kematangan kelapa sawit menggunakan Kamera smartphone. 3.1.9 Pengujian Aplikasi
       Tahap kesembilan yaitu pengujian aplikasi yang sudah dibuat, aplikasi akan diuji fungsi utamanya yaitu klasifikasi kematangan kelapa sawit. Kegiatan yang dilakukan pada tahun pertama yaitu melakukan studi literaur untuk pembuatan proposal penelitian (BAB 1 sampai BAB 3) lalu dilanjutkan dengan pengumpulan dan preprocessing data, setelah mendapatkan data kegiatan pembuatan dan pelatihan model dapat dilakukan. Pada tahun kedua, dilakukan evaluasi model dan saat hasil evaluasi model sudah cukup baik, model akan di deploy untuk dapat diimplementasi ke dalam aplikasi yang sudah dibuat. Aplikasi akan dievaluasi dan diuji kinerjanya sehingga mendapatkan Kesimpulan dari penelitian untuk ditulis dalam BAB 4 sampai BAB 5. Pada Akhir tahun kedua, setelah mendapatkan Kesimpulan penelitian, dilakukan pembuatan jurnal pertama dan dilanjutkan pada tahun ketiga untuk pembuatan jurnal kedua."
Reviana Siti Mardiah_Kualifikasi.txt,3.1 tahapan penelitian tahapan penelitian merupakan gambaran dari langkah langkah atau proses yang akan dilakukan dalam suatu penelitian. tahap an kedua adalah membuat model manajemen persediaan beras perum bulog berdasarkan hasil wawancara awal dengan pihak terkait. tahap ketiga adalah melakukan analisis terhadap model manajemen persediaan beras perum bulog untuk mengidentifikasi area yang perlu ditingkatkan . hasil analisis ini akan digunakan untuk merumuskan solusi terhadap permasalahan yang ada . tahap keempat adalah pengembangan solusi berbasis teknologi yang terdiri dari pengembangan be rbagai model dan prototype sistem yang akan diuji . usulan yang pertama adalah model generative ai untuk menghasilkan data sintetis yang realistis yang dapat digunakan sebagai data pelatihan untuk model prediksi . model ini kemudian diintegrasikan ke dalam model ml prediksi produksi hasil panen . usulan yang kedua adalah model prediksi permintaan beras yang merupakan model yang mirip dengan model prediksi produksi hasil panen beras dengan beberapa penyesuaian agar sesuai dengan karakteristik data untuk prediksi permintaan beras. usulan yang ketiga adalah pengembangan prototype decision support system yang mengintegrasikan model prediksi dan optimasi untuk mendukung kebijakan terkait pengadaan cadangan beras . tahap kelima adalah uji coba terhadap prototype decision support system . 1 tahapan penelitian 3.2 pemodelan manajemen persediaan beras perum bulog manajemen persediaan cadangan beras nasional telah menjadi perhatian penting dalam beberapa tahun terakhir karena meningkatnya permintaan pangan global perubahan iklim dan ketidakstabilan ekonomi. cadangan ini merupakan stok strategis yang diawasi oleh pemerintah untuk menstabilkan persediaan dan harga beras memberikan bantuan saat terjadi kekurangan pangan dan mendukung tujuan ketahanan pangan nasional . manajemen persed iaan cadangan beras yang efektif sangat penting untuk memitigasi risiko yang terkait dengan gangguan persediaan dan fluktuasi harga beras yulianis rachman 2021 yang pada akhirnya akan menjamin ketahanan pangan dan stabilitas ekonomi octania 2021 usdianto setiyowati 2023 . 60 para pemangku kepentingan yang terlibat dalam manajemen cadangan beras ini termasuk kementerian pertanian kementerian perdagangan kem enteri an keuangan dan kem enteri badan usaha milik negara sebagai regulator serta perum bulog yang bertanggung jawab untuk mengelola persediaan beras pemerintah dan stabilisasi harga di tingkat produsen dan konsumen octania 2021 . perum bulog bertanggung jawab untuk menjaga stabilitas harga dengan membeli gabah dan beras dari petani dengan harga yang ditentukan pemerintah ketika harga beli gabah turun sehingga melindungi petani dari kerugian dan menjual beras dengan harga yang lebih rendah daripada harga pasar ketika terjadi kenaikan harga beras untuk memastikan keterjangkauan harga beras bagi masyarakat octania 2021 . lembaga ini bertanggung jawab atas manajemen salah satu komponen cadangan beras nasional yaitu cadangan beras pemerintah cbp termasuk pada pengadaan dalam negeri dan impor penyimpanan dan penyaluran beras untuk kebutuhan stabilisasi harga bantuan pangan dan keadaan darurat fang chen zhang pei gao wang 2020 octania 2021 . beberapa penelitian telah menekankan peran penting perum bulog dalam manajemen persediaan cadangan beras di indonesia . melalui manajemen cbp perum bulog memainkan peran penting dalam menjaga ketahanan pangan nasional terutama saat terjadi fluktuasi harga atau gangguan p ersediaan . keberadaan cbp yang dikelola perum bulog tidak hanya menstabilkan harga beras di pasar tetapi juga menjamin ketersediaan beras bagi masyarakat sehingga berkontribusi terhadap stabilitas ekonomi nasional octania 2021 putro purwaningsih sensuse suryono 2022 silalahi et al. mengingat peran penting ini penerapan teknologi ai dapat membantu perum bulog dalam mengoptimalkan berbagai aspek dalam manajemen cadangan beras pemerintah seperti prediksi permintaan dan produksi hasil panen optimasi cadangan beras dan pengambilan keputusan yang lebih baik. ai dapat digunakan untuk menganalisis data historis dan realtime guna menghasilkan prediksi yang akurat mengenai permintaan dan produksi hasil panen beras mehmood et al. 2021 sehingga memungkinkan 61 perum bulog untuk mengoptimalkan cadangan beras menghindari kelebihan atau kekurangan cadangan beras dan mengambil keputusan yang lebih baik dalam manajemen cadangan beras pemerintah h. qin 2023 . berdasarkan kajian model manajemen persediaan beras perum bulog maka penelitian ini berfokus pada pemanfaatan teknologi ai untuk efektivitas manajemen cadangan beras pemerintah terutama pada proses pengadaan . gambar 3.1 menggambarkan model manajemen persediaan beras perum bulog . 2 model manajemen persediaan perum bulog 3.3. analisis analisis ini bertujuan untuk mengungkap kelemahan dan proses yang kompleks dalam manajemen persediaan cadangan beras pemerintah di perum bulog . penelitian ini menggunakan metode analisis swot untuk mengidentifikasi titik titik lemah yang krusial dalam manajemen persediaan cadangan beras pemerintah dan mengembangkan strategi untuk meningkatkan efisiensi dan efektivitas pengelolaan persediaan cadangan beras di indonesia analisis swot bertujuan untuk mengetahui kekuatan kelemahan peluang dan ancaman bisnis. 62 1. analisis swot 1. strengths kekuatan s1 dukungan pemerintah perum bulog didukung oleh berbagai kebijakan pemerintah yang bertujuan menjaga stabilitas harga dan ketersediaan beras seperti yang diatur dalam uu no. s2 infrastruktur logistik yang memadai perum bulog memiliki infrastruktur logistik yang cukup baik termasuk gudang penyimpanan yang tersebar di berbagai daerah yang berperan penting dalam menjaga kecukupan persediaan cadangan beras octania 2021 utomo 2020 s3 pengalaman dan keahlian perum bulog memiliki pengalaman puluhan tahun dalam manajemen persediaan beras mulai dari pengadaan hingga distribusi yang menjadi keunggulan dalam menjaga stabilitas harga dan persediaan beras anggraini et al. gambar 3.3 menggambarkan perbedaan yang signifikan antara harga gabah di tingkat petani dengan harga beli yang ditetapkan pemerintah. kon disi ini menyebabkan petani lebih memilih menjual hasil panennya ke sektor swasta. 1 jumlah impor beras bps 2024 negara asal 2017 2018 2019 2020 2021 2022 2023 berat bersih ton india 32209.7 337999 7973.3 10594.4 215386.46 178533.57 69715.7 thailand 108944.8 795600.1 53278 88593.1 69360.037 80182.506 1381921.2 vietnam 16599.9 767180.9 33133.1 88716.4 65692.874 81828.039 1147705.3 pakistan 87500 310990 182564.9 110516.5 52479.011 84407 309309.7 myanmar 57475 41820 166700.6 57841.4 3790 3830 141204 jepang 72.1 0.2 90 0.3 230.291 56.087 61.5 tiongkok 2419 227.7 24.3 23.8 42.601 6 7 lainnya 54.3 6.5ssss 744.6 0.3 760.146 364.065 12933.3 total 305274.8 2253824.4 444508.8 356286.2 407741.42 429207.27 3062857.6 02000400060008000 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024perbandingan harga rata rata gkp di tingkat petani dan harga pembelian pemerintah tingkat petani kelompok kualitas gkp gabah kering panen tingkat petani hpp harga pembelian pemerintah gkp gkp gabah kering panen64 gambar 3. 3. opportunities peluang o1 memanfaatkan teknologi canggih integrasi teknologi canggih seperti blockchain dan ai berpotensi meningkatkan presisi transparansi dan efisiensi dalam mengelola persediaan cbp anggraini et al. 0500000100000015000002000000250000030000003500000 2017 2018 2019 2020 2021 2022 2023berat bersih tonimpor beras tahun 2017 2023 impor beras65 o2 pengembangan sistem yang terintegrasi terdapat peluang untuk mengembangkan sistem yang lebih terintegrasi dan canggih yang dapat memfasilitasi pengelolaan cbp yang lebih baik anggraini et al. 2. strategi setelah dilakukan analisis swot maka dapat dirumuskan strategi yang dapat digunakan perum bulog untuk meningkatkan kekuatan mengatasi kelemahan memanfaatkan peluang dan meminimalkan potensi ancaman . a. strategi s o leveraging strengths to optimize opportunities so1 pemanfaatan teknologi blockchain untuk transparansi dan efisiensi memanfaatkan dukungan pemerintah untuk mengadopsi teknologi blockchain dalam manajemen pangan yang dapat meningkatkan transparansi efisiensi dan keamanan dalam transaksi dan pencatatan. 66 so2 pengembangan sistem terintegrasi meningkatkan infrastruktur yang sudah ada dengan mengembangkan sistem manajemen data yang terintegrasi dan canggih untuk memperkuat pengelolaan cbp secara lebih efektif s2 o2. so3 pengembangan model prediksi permintaan dan produksi hasil panen memanfaatkan ai untuk mengembangkan model prediksi permintaan dan produksi hasil panen beras yang akurat sehingga memungkinkan pengambilan keputusan pengadaan yang lebih tepat untuk menghindari kelebihan atau kekurangan persediaan s3 o1 o2. so4 optimalisasi program peningkatan produksi pangan memanfaatkan tenaga ahli dan infrastruktur yang ada untuk mendukung program pemerintah dalam meningkatkan produksi pangan lokal mengurangi ketergantungan impor serta memperkuat stabilitas harga dan persediaan beras s1 s3 o3. b. strategi s t using strengths to counter threats st1 pengembangan alat pendukung keputusan untuk pengadaan impor mengembangkan decision support tool dst dengan menggunakan input dari model prediksi untuk membantu menentukan kebijakan impo r beras. st2 optimalisasi manajemen krisis dengan prediksi produksi hasil panen menerapkan model prediksi hasil panen untuk mempersiapkan dan merespons secara efektif dampak perubahan iklim pada produksi beras . model prediksi ini dimanfaatkan untuk meningkatkan ketahanan pangan dan kesiapan dalam menghadapi fluktuasi hasil panen yang tidak terduga s3 t2. wo2 peningkatan koordinasi antar lembaga melalui sistem terintegrasi membangun sistem terintegrasi yang melibatkan semua lembaga terkait untuk mengatasi masalah kurangnya koordinasi dan perbedaan data serta memfasilitasi pengambilan keputusan yang lebih cepat dan akurat w5 o2. wt2 peningkatan manajemen persediaan melalui analisis tingkat lanjut meningkatkan sistem manajemen persediaan dengan model prediksi agar lebih responsif terhadap perubahan permintaan dan kondisi darurat serta mengurangi risiko kekurangan persediaan dan mengatasi fluktuasi harga w3 t2. strategi tersebut meliputi pengembangan model prediksi permintaan dan produksi optimasi manajemen krisis dengan prediksi produksi pembuatan alat pendukung keputusan untuk pengadaan optimasi kebijakan impor dengan model prediksi dan peningkatan manajemen persediaan melalui analisis lanjutan. strategi strategi ini bertujuan untuk meningkatkan kinerja perum bulog dalam manajemen cadangan beras pemerintah cbp dan dapat memberikan rekomendasi berbasis data untuk mendukung kebijakan pengadaan cadangan beras . 3.4. pengembangan model prediksi produksi hasil panen beras gambar 3. 6 menggambarkan delapan tahapan yang dijalani dalam pengembangan model prediksi untuk produksi hasil panen beras pada penelitian ini. setiap tahap dirancang untuk memastikan keakuratan dan efektivitas model dalam memprediksi hasil panen . 3.4.1 pengumpulan data pengembangan model prediksi produksi hasil panen ini akan menggunakan data yang dikumpulkan dari website badan pusat statistik bps dan bmkg . eda memungkinkan prediksi produksi hasil panen yang tepat dengan membangun fondasi yang kuat untuk pemodelan tingkat lanjut . tahap ini sangat penting untuk menghasilkan model prediksi berbasis machine learning yang akurat dan efektif. proses untuk memilih hyperparameter terbaik dalam model machine learning untuk meningkatkan kinerja dikenal sebagai penyetelan hyperparameter tuning soleymani mohammadzadeh 2023 . meskipun memerlukan upaya yang cukup besar penyetelan ulang dapat menghasilkan peningkatan signifikan dalam performa prediksi. rmse mengukur besarnya tingkat kesalahan prediksi dimana semakin rendah nilainya mendekati nol maka hasil prediksi akan semakin akurat. 13 flowchart evaluasi model pada tahap evaluasi model juga diuji dengan dua rancangan yang berbeda yaitu model tanpa features engineering serta pada model dengan features engineering. 6 adalah hasil evaluasi model pada penelitian ini. 6 hasil evaluasi model dataset kondisi tanpa feature engineering kondisi dengan feature engineering r2 rmse mae r2 rmse mae 7030 0773 0441 0176 0933 0209 0115 8020 0795 0418 0155 0956 0163 0099 9010 0976 0111 0079 0976 0109 0076 84 hasil evaluasi menunjukkan pengaruh positif dari penggunaan feature engineering fe terhadap kinerja model machine learning dalam skema pembagian dataset yang berbeda 7030 8020 dan 9010. khususnya penggunaan feature engineering membantu meningkatkan nilai r yang menandakan peningkatan kemampuan model dalam menjelaskan variabilitas data yang diamati. analisis lebih lanjut pada perbedaan skema pembagian data menunjukkan bahwa model dengan proporsi data pelatihan yang lebih besar 9010 menunjukkan hasil yang paling stabil dan akurat. hal i ni menyoroti pentingnya fe dalam meningkatkan efektivitas model dan menunjukkan keuntungan dari alokasi yang lebih besar pada data pelatihan dalam pengembangan model prediksi berbasis machine learning . nilai rmse terendah yang diperoleh adalah 0109 yang me nunjukkan potensi model untuk menghasilkan prediksi produksi yang sangat akurat jika terus dikembangkan. 2021 mendapatkan nilai rmse 33.575.59574 untuk model prediksi nya hal ini menunjukkan bahwa model prediksi yang diusulkan memiliki potensi untuk menyaingi model yang ada dalam hal keakuratan hasil prediksi . meskipun hal ini tidak mutlak karena model ini dibangun dengan algoritma dan data yang berbeda. namun potensi model ini menghasilkan prediksi yang sangat akurat masih terlihat. berdasarkan hasil ini penelitian ini akan fokus pada penambahan data pelatihan menggunakan generative adversarial network gan dan pengembangan lebih lanjut pada teknik feature engineering untuk meningkatkan kinerja prediksi. 3.5 pengembangan model prediksi permintaan model prediksi yang digunakan untuk prediksi permintaan adalah model yang sama dengan yang digunakan untuk memprediksi produksi atau hasil panen . gambar 85 3.14 adalah tahapan pengembangan model prediksi permintaan pada penelitian ini. 14 tahapan penelitian prediksi permintaan yang diusulkan 86 pengembangan model prediksi permintaan beras ini terdiri dari delapan tahap. tahap kedelapan adalah evaluasi hasil yang meliputi pengukuran performa model yang dapat dilihat dari nilai r2 root mean squared error rmse dan mean absolute error mae. jika performa model perlu ditingkatkan hyperparameter dapat disesuaikan kembali di tahap k etujuh . setelah performa terbaik tercapai model dapat diintegrasikan ke dalam model decision support system . 3.6 prototype decision support system dss untuk pengadaan cadangan beras pemerintah decision support system dss pada penelitian ini akan digunakan untuk mendukung proses pengambilan keputusan internal terkait kebijakan pengadaan cadangan beras pemerintah . dss ini akan mengintegrasikan data hasil prediksi produksi hasil panen permintaan dan persediaan aktual untuk menentukan variabel keputusan pengadaan yang optimal. alat ini memungkinkan perum 87 bulog untuk merencanakan dan mengelola cadangan beras pemerintah cbp secara efisien sesuai dengan kebutuhan masyarakat . proses ini dimulai dengan perum bulog memeriksa tingkat persediaan cbp. proses ini dimulai dengan penerimaan data tentang prediksi produksi hasil panen dalam negeri dan kebutuhan beras mendatang. selanjutnya evaluasi kondisi pasar beras internasional kebijakan pemerintah dan perubahan iklim global . lalu dilakukan perhitungan terhadap jumlah cadangan beras pemerintah cbp yang optimal yang menentukan jumlah beras yang harus disimpan untuk kebutuhan darurat. berbeda dengan metode heuristik drl lebih kuat dengan hasil konvergensi yang stabil dan lebih cocok untuk masalah pengambilan keputusan z. zhang zhang qiu 2019 . 16 diagram alir skenario dasar pengambilan keputusan 89 berdasarkan data persediaan aktual dilakukan evaluasi apakah jumlah tersebut sudah mencukupi untuk memenuhi prediksi permintaan atau kebutuhan beras mendatang . sebaliknya jika persediaan dinilai belum mencukupi kebutuhan maka akan dilakukan penyerapan atau pengadaan dari produksi hasil panen dalam negeri . pada kondisi di mana produksi hasil panen dalam negeri yang ada pada petani atau mitra kerja perum bulog sangat minim maka akan dilakukan pengadaan dari sumber dalam negeri dan impor untuk mengisi kekurangan tersebut. jika kondisi di mana produksi hasil panen dalam negeri kosong. output dari dss ini adalah sebuah rekomendasi untuk strategi dan jumlah pengadaan beras. rekomendasi ini dapat digunakan oleh perum bulog untuk membuat kebijakan pengadaan beras khususnya dalam menentukan kebutuhan akan impor beras. dss yang diusulkan adalah sebuah platform yang mengintegrasikan dua model prediksi dan proses optimasi untuk cadangan persediaan beras pemerintah . sistem ini membantu perum bulog dan lembaga terkait dalam me ngambil kebij akan strategis terkait pengadaan cadangan beras pemerintah . hal ini untuk memastikan ketersediaan cadangan beras pemerintah cbp sehingga mendukung stabilitas harga dan ketahanan pangan n asional. 17 prototype dss yang diusulkan 3.7 uji coba uji coba sistem bertujuan untuk memvalidasi apakah sistem yang dikembangkan sesuai dengan tujuan awal dan layak untuk digunakan. hal ini termasuk fungsi yang salah atau hilang kesalahan interface masalah dengan struktur data atau akses database eksternal kesalahan kinerja serta kesalahan yang terkait dengan operasi dan shutdown sistem corso moss koren lee kochenderfer 2021 .
Reza Al Husna_Kualifikasi.txt,"3.1 Tahapan Penelitian
      Secara garis besar penelitian ini terdiri dari beberapa tahapan, yaitu Akuisisi data, pre-processing data, pengembangan dan pelatihan model, pengujian dan evaluasi model, serta pengembangan system deteksi penyakit daun kakao, ditunjukkan pada Gambar 3.1. 3.2 Akuisisi Data Penyakit Daun Tanaman Kakao
      Pengumpulan citra Penyakit Daun tanaman kakao dikumpulkan secara langsung oleh peneliti (data primer) dan juga menggunakan data yang dikumpulkan oleh peneliti lain (data sekunder). Dataset primer akan dilakukan pengambilan foto penyakit daun tanaman kakao yang terdapat pada kebun kakao di daerah Kabupaten Solok, Provinsi Sumatra Barat. Dataset sekunder menggunakan dataset yang telah digunakan umum oleh para peneliti lain terkait penyakit daun tanaman kakao. 3.4 Pengembangan dan Pelatihan Model
        Data citra daun kakao yang telah melalui preprocessing dan ekstraksi fitur, kemudian digunakan untuk pelatihan dan pembuatan model deep learning menggunakan pendekatan feature fusion berbasis attention yaitu fitur ekstraksi HoG dan LBP digabungkan ke dalam vision transformer yang menggunakan attention mechanism. Attention mechanism dalam vision transformer memberikan fokus yang berbeda pada fitur HoG dan LBP. Penggunaan attention mechanism dapat meningkatkan akurasi model dengan mengurangi pengaruh noise atau informasi yang tidak relevan dalam gambar. 3.5 Pengujian dan Evaluasi Model
       Pengujian dan Evaluasi model dilakukan untuk melihat akurasi model saat mengidentifikasi penyakit daun tanaman kakao. 3.6 Pengembangan Sistem Deteksi Penyakit Daun Kakao
       Setelah melakukan pelatihan dan pengembangan model, serta tahap pengujian dan evaluasi model, system deteksi untuk penyakit daun kakao diimplemetasikan dengan melibatkan pengintegrasian model ke dalam aplikasi atau perangkat keras. Pengembangan system menciptakan Solusi yang efektif dan efisien dalam mengidentifikasi penyakit daun kakao."
Robert_Kualifikasi.txt,"3.1 Alur Penelitian
     Gambar 3.1 menunjukkan metode penelitian. Terdapat 5 tahap utama yang akan dilakukan, yang pertama adalah studi literatur untuk menyusun bab 1 dan bab
2. Tahap kedua adalah pengumpulan citra ekspresi wajah (data citra berupa data primer dan data sekunder). Tahap ketiga adalah pembentukan dataset untuk tiap model (SVM, CNN, dan MNN), skenario pembentukan dataset dilakukan berdasarkan pada penelitian (Robert, 2023). Pada tahap keempat dilakukan pembentukan model, khusus untuk SVM dan CNN menggunakan model pada penelitian (Robert, 2023), sedangkan MNN menggunakan usulan pada penelitian ini. Tahap terakhir adalah pelatihan dan pengujian untuk semua model (SVM, CNN, MNN), terdapat tahap parameter tuning untuk tiap model. Kemudian semua performa dari tiap model akan dibandingkan satu sama lain, dan juga dianalisis pada bab 4. Gambar 3.1. Metode penelitian
3.2 Pengumpulan Citra Ekspresi Wajah
Citra ekspresi wajah dikumpulkan secara langsung oleh peneliti (data primer) dan juga menggunakan data yang dikumpulkan oleh peneliti lain (data sekunder). Terdapat 7 ekspresi wajah yang akan digunakan dalam penelitian ini, yaitu: marah, jijik, menghina, senang, sedih, kaget, dan netral (tanpa ekspresi). Gambar 3.2 menunjukkan contoh 7 ekspresi wajah manusia yang digunakan penelitian ini. Gambar 3.2. Contoh 7 jenis ekspresi wajah
Dataset primer akan dilakukan pengambilan citra ekspresi wajah mahasiswa Universitas Gunadarma baik pria maupun wanita. Pengambilan akan dilakukan dari beberapa sudut pandang guna menambah variasi dataset. Gambar 3.3 menunjukan contoh dataset primer dari berbagai sudut pandang. Gambar 3.3. Contoh dataset primer
Dataset sekunder digunakan dataset yang telah digunakan umum oleh peneliti lain terkait pengenalan ekspresi wajah. Terdapat beberapa dataset yang umum digunakan dalam penelitian ekspresi wajah. Pertama, Extended Cohn- Kanade (CK+) yang berisi citra ekspresi wajah pria dan wanita dari berbagai etnis dengan resolusi tinggi (Kanade, Cohn, & Tian, 2000; Lucey et al., 2010). Kedua, Taiwanese Facial Expression Image Dataset (TFEID) yang berisi citra ekspresi wajah pria dan wanita dari etnis Taiwan (Chen & Yen, 2007). Ketiga, Japanese Female Facial Expression (JAFFE) yang terdiri dari citra ekspresi wajah wanita etnis Jepang (Lyons, 2021; Lyons, Kamachi, & Gyoba, 2020). Gambar 3.4 menunjukan contoh citra dataset CK+ (a), JAFFE (b), dan TFEID (c). Gambar 3.4. Contoh dataset sekunder
Tabel 3.1 menunjukkan detail dari tiap dataset, mulai dari jumlah citra dari tiap kelas serta ruang warna dan ukuran citra. CK+ memiliki jumlah yang tidak seimbang pada kelas neutral dan memiliki ruang warna campur antara RGB dan Gray, dengan ukuran citra dikisaran 640 490. JAFFE dataset memiliki jumlah citra pada tiap kelas yang seimbang dengan perbedaan diantara 0 hingga 2 citra, ukuran citra 256 256, dan ruang warna grayscale. TFEID juga memiliki jumlah citra yang seimbang ditiap kelas, ukuran citra dikisaran 481 600, ruang warna RGB. 3.3 Pembentukan Dataset
Secara garis besar, dalam pembuatan model AI (khususnya ML dan DL) terdapat proses yang berperan penting, yaitu preprocessing dataset seperti ekstrasi fitur, penyesuaian ukuran citra, dan augmentasi (Deshmukh et al., 2016; Franchi et al., 2020; Mohammad & Ali, 2011; Ravi et al., 2020; Sawardekar & Naik, 2018; Shan et al., 2009). Pada penelitian (Robert, 2023), dilakukan sebuah skenario pembentukan dataset menggunakan beberapa metode pengolahan citra (seperti konversi warna ke grayscale, deteksi wajah, dan ekstrasi fitur), di mana preprocessing mempengaruhi performa dari model ML dan DL. Selain itu, pada penelitian (Alam & Yao, 2019) juga dilakukan penelitian yang serupa, di mana preprocessing mempengaruhi performa model machine learning. Pada tahap ketiga, dilakukan pembentukan dataset. Gambar 3.5 menunjukkan alur pembentukan dataset untuk SVM. Pertama, dilakukan pendeteksian wajah menggunakan VJA, proses ini berguna untuk mengurangi noise pada citra. Hasil VJA membuat ukuran citra bervariasi, oleh karena itu dilakukan resizing citra untuk menyamakan semua ukuran citra dan juga menyesuaikan dengan dimensi input model. Selanjutnya dilakukan konversi warna citra dari RGB ke Grayscale dikarena fitur warna tidak dibutuhkan dan agar dapat diekstrasi fiturnya menggunakan LMP. Terakhir, terdapat dua proses ekstrasi fitur berbeda. Proses ekstrasi fitur pertama menggunakan LMP (Robert, 2023). Proses ekstrasi fitur kedua adalah usulan dari penelitian ini, di mana pertama diaplikasikan Gabor Filter terlebih dahulu kemudian diekstrasi menggunakan LMP. Gambar 3.5. Pembentukan dataset untuk SVM
Gambar 3.6 menunjukkan alur pembentukan dataset untuk CNN dan MNN. Terdapat 3 proses yang akan dilakukan. Pertama, dilakukan deteksi wajah menggunakan VJA guna mengurangi noise. Kemudian dilakukan konversi warna dari RGB ke Grayscale karena fitur warna tidak dibutuhkan untuk mengenali ekspresi wajah. Terakhir, mengubah ukuran citra untuk menyamakan semua ukuran citra dan sesuai dengan dimensi input model. Skema pembentukan dataset ini berdasarkan performa terbaik dari penelitian (Robert, 2023). Gambar 3.6. Pembentukan dataset untuk CNN dan MNN
Dataset yang sudah melalui pembentukan dataset, dilakukan augmentasi dari sisi geometris seperti membalikan flipping secara horizontal dan vertikal, dan rotasi dari 0  hingga 45 . SVM training dataset digunakan untuk melatih model, sedangkan testing dataset digunakan untuk menguji dataset. CNN dan MNN terdapat pembagian lagi pada training, di mana 90% dari training dataset digunakan untuk melatih model dan 10% dari training dataset digunakan untuk validasi, dan testing dataset digunakan untuk menguji model. Visualisasi pembagian dataset
3.3.1 Deteksi wajah
Proses deteksi wajah menggunakan VJA, VJA memanfaatkan dua komponen yaitu integral image dan Haar Basis Function. Parameter ketiga adalah nilai threshold untuk masing-masing Haar yang digunakan untuk menentukan apakah Haar tersebut merupakan fitur wajah atau bukan. Output dari algoritma ini adalah berupa koordinat wajah yang dimulai pada koordinat [? Selain itu, jika semua nilai fitur lebih besar dari threshold maka detection window tersebut merupakan wajah, dan algoritma akan memberikan empat nilai yaitu ? Gambar 3.10 menunjukkan hasil dari implementasi algoritma deteksi wajah pada sebuah citra. Baris 2 dilakukan pembuatan matriks yang digunakan untuk menyimpan hasil. Image Resize (menggunakan Bicubic Interpolation)
Gambar 3.11 menunjukkan hasil perubahan ukuran citra wajah menggunakan Algoritma 3.3. Gambar 3.15 menunjukkan contoh hasil implementasi algoritma LMP pada citra wajah. 3.4 Pembentukan Model
3.4.1 Pembentukan Model SVM
Dalam penelitian sebelumnya (Robert, 2023), telah dilakukan pengenalan ekspresi wajah menggunakan SVM dengan menggunakan 4 kernel yaitu: Linear, Polynomial, Sigmoid, dan RBF. Berdasarkan dari penelitian (Robert, 2023), didapatkan model terbaik untuk mengenal ekspresi wajah adalah menggunakan kernel Sigmoid. Operasi pertama adalah operasi morgologi opening pada citra original, kemudian dilakukan pengurangan nilai piksel antara citra original dengan hasil opening, operasi opening itu sendiri adalah operasi erosi yang dilanjutkan operasi dilasi. Operasi kedua adalah erosi pada citra original, kemudian dilakukan pengurangan citra original terhadap hasil erosi. Ketiga adalah dilasi pada citra original, kemudian dilakukan pengurangan hasil dilasi terhadap citra original. Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? Baris 12 dilakukan pengambilan nilai terkecil yang digunakan untuk sebagai hasil fitur citra berdasarkan persamaan. Baris 25 dilakukan pengambilan nilai terbesar yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.14). Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? Baris 11 dilakukan pengambilan nilai terkecil yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.10). Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? Baris 11 dilakukan pengambilan nilai terbesar yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.8). 3.4.3.1.3 Percobaan dan Perbandingan Hasil Operasi Morfologi
Tabel 3.2 menunjukkan hasil operasi morfologi menggunakan SE berdasarkan 3.4.3.1 dan 3.4.3.2. Operasi Morfologi dan Structure Element pada Persegi Panjang
Berdasarkan dari Tabel 3.3 dapat dilihat operasi GMO dengan SE disk 3 3 menghasilkan corner pada persegi panjang. Operasi GMD dengan SE disk 3 3 memiliki hasil garis berbentuk persegi panjang dengan nilai tiap corner terdapat hilang. Operasi GMD dengan SE kotak 3 3 menghasilkan garis yang berbentuk persegi panjang. GME dengan SE kotak 3 3 memiliki hasil yang mirip dengan GMD, namun memiliki luas yang berbeda. Dari hasil yang didapatkan, tiap ukuran memiliki hasil yang mirip dengan operasi yang digunakan. Operasi GMD dan GME memiliki hasil yang mirip dengan perbedaan tepi pada GMD menebal kearah luar persegi, sedangkan tepi pada GME menebal kearah dalam. Tabel 3.4 merupakan hasil morfologi menggunakan citra wajah dengan warna grayscale. Structure Element dan Operasi Morfologi pada Wajah
Berdasarkan Tabel 3.4, dapat dilihat hasil ekstrasi fitur menggunakan berbagai kombinasi dua bentuk dan lima ukuran. SE kotak 5 5 juga memiliki hasil yang sama dengan disk 5 5 dengan perbedaan yang tidak dapat dilihat kasat mata. SE kotak 7 7 juga memiliki hasil yang sama dengan disk 7 7 dengan perbedaan yang tidak dapat dilihat kasat mata. SE disk 9 9 dengan operasi GMO fitur yang didapatkan tidak akurat karena tidak mengekstrasi tepi (bentuk) dari ekspresi wajah. Selain itu luas pada lubang hidung dan mulut juga lebih luas dibandingkan dengan SE 5 5, dan memiliki garis tepi yang lebih tebal. SE kotak 9 9 dengan operasi GMO, fitur yang didapatkan mirip dengan disk 9 9 dengan operasi GMO namun memiliki fitur yang lebih jelas terlihat. SE kotak 9 9 dengan operasi 9 9 GME fitur yang didapatkan terlihat dengan jelas dan mirip dengan hasil yang menggunakan SE disk 9 9 dengan operasi GME. Namun terdapat perbedaan fitur yang signifikan pada bagian hidung, di mana bentuk dari hidung sedikit berubah menjadi sedikit kotak. SE kotak 9 9 dengan operasi GMD, hasil yang didapatkan hampir sama dengan hasil yang menggunakan SE disk 9 9 dengan operasi GMD, hanya terdapat perbedaan pada tebal garis pada hidung. Operasi menggunakan SE 15 15, hasil yang didapatkan memiliki pola yang sama dengan operasi yang menggunakan SE 9 9. SE disk atau kotak 15 15 dengan operasi GMO fitur lebih terlihat jelas dibandingkan dengan ukuran 9 9. SE disk atau kotak 15 15 dengan operasi GME fitur lebih tebal, namun untuk SE kotak bagian hidung menjadi lebih kotak dari ukuran sebelumnya. Berdasarkan dari hasil yang didapatkan dan analisis. SE disk 5 5 atau kotak 5 5 dengan operasi GMD memiliki hasil terbaik. Tepi pada bagian mata tergabung, bagian mulut tidak menjadi lebih luas. Namun pada bagian hidung menjadi lebih kecil dibandingkan citra original. Selain itu operasi menggunakan GME dengan SE disk 5 5 atau kotak 5 5 juga memberikan hasil yang baik. Tepi dari tiap komponen wajah terlihat namun tepi pada mata dan bola mata tergabung, namun memberikan hasil ekstrasi fitur bagian hidung lebih baik dibandingkan GMD. Karena luas hidung lebih sesuai pada GME dibandingkan GMD. 3.4.3.2 Pembentukan dan Pelatihan Model
Gambar 3.18 menunjukkan arsitektur dari MNN secara garis besar. Di mana terdapat 5 layer utama yang memiliki tugas masing-masing. Di mana terdapat beberapa variasi arsitektur Variasi pertama terdapat pada morphology layer, di mana akan digunakan dua jenis ekstrasi fitur berdasarkan pada hasil subbab 3.4.3.3. Variasi kedua terdaoat pada hidden layer, di mana akan digunakan beberapa kombinasi fully-connected layer. Gambar 3.18. Model MNN yang diusulkan
Input Layer adalah layer pertama dari model MNN yang bertugas untuk menerima input berupa citra. Di mana dimensi dari intput layer itu sendiri sesuai dengan ukuran citra pada dataset yaitu 160 160. Kedua adalah morphology layer. Menerima masukan dari input layer kemudian dilakukan proses morfologi. Terdapat dua jenis morfologi layer yang akan digunakan. Morfology layer pertama adalah dilation layer kemudian subtraction layer. Di mana lapisan morfologi jenis pertama menggunakan hasil terbaik dari operasi morfologi dilasi pada subbab 3.4.3.3. Morphology layer jenis kedua adalah erosion layer dilanjutkan subtraction layer. Di mana lapisan morfologi jenis kedua ini menggunakan hasil terbaik dari operasi morfologi erosi pada subab 3.4.3.3. Lapisan ketiga adalah flatten layer, lapisan yang bertugas mengubah citra menajadi feature vector. Masukan dari lapisan ini adalah hasil dari subtraction layer pada lapisan morfologi. Di mana hasil dari subtraction layer adalah citra dengan ukuran 160 160 yang kemudian diubah mnejadi satu dimensi yaitu 25600. Lapisan keempat adalah Fully Connected Layer (FC Layer) yang bertugas untuk mempelajari dan menganalisa nilai feature vector dari flatten layer. Pada lapisan ini dilakukan beberapa konfigurasi FC Layer mulai dari jumlah FC Layer, dan jumlah Neuron pada FC Layer. Konfigurasi pertama akan diuji coba 2 FC Layer dengan masing-masing neuron adalah 512, dan 256. Konfigurasi kedua akan dicoba 1024, dan 512. Kemudian dari situ akan dicoba analisis mana yang lebih baik sehingga dapat dikonfigurasi lebih lanjut. Jika konfigurasi pertama memiliki hasil lebih baik artinya memungkinkan FC Layer untuk dibuat lebih sederhana dengan mengurangi jumlah neuron. Jika konfigurasi kedua lebih baik artinya terdapat kemungkinan untuk meninkatkan performa dari model karena dataset memiliki kompleksitas tinggi. Beberapa lapisan akan dilakukan tuning paramter. Tuning pertama terdapat pada bentuk SE, ukuran SE, jenis operasi pada morphological layer. Tuning kedua terdapat pada FC-Layer yaitu fungsi aktivasi (ReLu/Sigmoid/Tanh), dan jumlah neuron pada tiap hidden layer. Selain arsitektur, pada proses pelatihan juga dilakukan tuning pada learning rate, jumlah epoch, dan batch size. Terakhir adalah Output Layer, merupakan penentuan dari ekspresi berdasarkan dari bobot hidden layer. Di mana fungsi aktivasi yang digunakan untuk output layer adalah softmax yang artinya output berupa probabilitas dari tiap ekspresi. Kemudian untuk loss function yang akan digunakan adalah categorical crossentropy, dimana fungsi loss ini digunakan jika model memprediksi multi- kelas (multi-class prediction)."
Tatya Atyanti Paramastri_Kualifikasi.txt,"3.1. Alur Penelitian
1. Permasalahan yang diangkat pada penelitian ini adalah motif batik Indonesia sangat beragam dan memiliki maknanya masing-masing. Namun tidak banyak masyarakat yang masih mengetahui nama, makna dan pemakaian dari masing-masing motif batik. Menurut Dewan Ahli PPBI (Paguyuban Pecinta Batik Indonesia) Sekar Jagad, Ibu Mari S. Condronegoro saat ini sering kali ditemukan kesalahan dalam pemakaian motif batik, seperti mengenakan kain yang seharusnya digunakan pada upacara kematian ketika menghadiri acara pernikahan. Solusi yang diusulkan adalah melakukan klasifikasi motif batik. Fokus studi literatur terbagi menjadi tiga topik, yaitu klasifikasi motif batik, komputasi kuantum, dan deteksi tepi. Sehingga hasil yang didapatkan memuaskan dan akurat. Dataset yang akan dikumpulkan merupakan citra motif ""batik daur hidup"" Yogyakarta dari kain batik tradisional yang merupakan batik cap maupun batik tulis (bukan printing). Motif yang akan digunakan dalam penelitian akan didiskusikan terlebih dahulu dengan narasumber supaya dapat mewakili ""batik daur hidup"" Yogyakarta yang sangat penting untuk diketahui dalam bersosialdi masyarakat. Hal ini dilakukan karena ""batik daur hidup"" Yogyakarta tercatat memiliki ratusan motif hingga tahun 2006 (Sekar Jagad, 2015). Selain itu ukuran citra yang lebih kecil dapat mempercepat proses segmentasi dan klasifikasi tanpa kehilangan informasi penting. Proses selanjutnya adalah mengubah ruang warna menjadi grayscale, perubahan warna ini dilakukan karena dapat meningkatkan kontras, meningkatkan efisiensi komputasi, dan meningkatkan ketahanan terhadap variasi pencahayaan. Selanjutnya proses augmentasi digunakan untuk meningkatkan ukuran dataset dengan menambahkan data baru tanpa perlu melakukan pengumpulan data baru. Komputasi kuantum diterapkan mulai dari proses segmentasi karena berdasarkan penelitian terdahulu, deteksi tepi berbasis kuantum dapat mendeteksi lebih banyak tepi dibandingkan deteksi tepi klasik berdasarkan jumlah tepi yang dihasilkan (Sundani, dkk., 2019). Hal serupa juga berlaku pada metode QCNN yang memiliki hasil lebih baik dan akurat dibandingkan dengan metode CNN klasik. Sehingga hipotesisnya, hasil dari ekstraksi fitur dan klasifikasi akan menjadi lebih optimal. Pengolahan data ini dilakukan sebagai pembanding performa model segmentasi dan klasifikasi berbasis kuantum. Performa akan dibandingkan dari akurasi yang dihasilkan. Penelitian ini menggunakan pendekatan kualitatif dan kuantitatif. Penelitian kualitatif dilakukan dalam proses mengkaji studi literatur dan melakukan wawancara dengan Dewan Ahli PPBI Sekar Jagad, Ibu Mari S. Condronegoro untuk mempelajari batik daur hidup Yogyakarta."
Tia Haryanti_Kualifikasi.txt,"3.1 Kerangka Umum
     Penelitian ini bertujuan untuk mengembangkan sistem deteksi dini kantuk sebelum berkendara dengan menggunakan kombinasi data visual berupa data citra wajah dan data fisiologis. Kondisi pre-driving mengacu pada kondisi sebelum pengemudi memulai perjalanan, sehingga sistem ini sangat penting untuk mencegah risiko kecelakaan di jalan. Sistem ini mengintegrasikan teknologi pengenalan wajah dan analisis data fisiologis untuk memberikan deteksi yang lebih akurat. Blok diagram secara umum yang digunakan pada penelitian ini dapat dilihat pada Gambar 3.1 Blok Diagram. Model ini terdiri dari tiga tahapan yaitu input, proses, dan output. Penelitian deteksi dini kantuk untuk kondisi pre-driving menggabungkan data visual yaitu pengumpulan data citra wajah pengemudi yang diambil menggunakan kamera, serta data fisiologis yang diukur berupa data EKG menggunakan perangkat wearable yaitu smartwatch dan pulse oximeter untuk mengukur saturasi oksigen (SpO2). Tahapanpre-processing dan ekstraksi fitur dilakukan pada kedua j enis data yaitu data citra gambar dan data fisiologis. Model Convolutional Neural Network (CNN) digunakan untuk mengekstraksi fitur dari data citra wajah yang merupakan data visual, sementara Long Short-Term Memory (LSTM) digunakan untuk memproses data fisiologis yang bersifat time-series. Fitur-fitur yang diekstraksi dari kedua model ini digabungkan untuk menghasilkan vector fitur gabungan. Vektor fitur ini kemudian digunakan sebagai input untuk model Support Vector Machine (SVM) yang melakukan klasifikasi akhir untuk mendeteksi kantuk. Hasil deteksi kemudian digunakan untuk memberikan peringatan kepada pengemudi layak tidak nya pengemudi untuk berkendara. 3.2 Tahapan Peneletian
     Tahapan penelitian merupakan urutan atau langkah-langkah yang dilakukan secara terstruktur dan sistematis pada penelitian ini, secara garis besar terbagi menjadi empat tahapan. Berikut adalah Gambar 3.2 Tahapan Penelitian yang dilakukan pada penelitian ini. Pemilihan dan Persiapan Dataset
     Tahapan ini merupakan tahapan identifikasi awal dari penelitian meliputi identifikasi masalah penelitian yang berfokus pada masalah utama yaitu mendeteksi kantuk pada pengemudi menggunakan pemrosesan citra dan fisiologis. Pemilihan dataset memastikan bahwa dataset yang dikumpulkan relevan dengan tujuan penelitian, yaitu hanya menggunakan data yang berkaitan dengan kondisi pre-driving, serta memastikan bahwa data visual dan data fisiologis diambil pada waktu yang sama. Dataset visual berupa citra wajah yang berfokus pada wajah pengemudi yang diambil menggunakan kamera dengan spesifikasi 12 MP. Data visual dan fisiologis berupa data yang diambil dari partisipan dalam kondisi terjaga dan mengantuk. 3.3.2 Pre-Processing Data
      Melakukan analisis eksploratif data untuk memahami karakteristik dataset sehingga meningkatkan kualitas deteksi. Augmentasi gambar dilakukan untuk meningkatkan variasi data, seperti rotasi, flipping horizontal atau vertikal, zooming, dan perubahan cahaya
7. Langkah dari pembuatan model yaitu penulisan kode untuk membangun model sesuai dengan desain arsitektur yaitu CNN, LSTM, dan SVM. Ekstraksi fitur dengan LSTM melibatkan beberapa langkah penting:
1. Membangun Model LSTM: Membangun model LSTM dengan lapisan LSTM dan Dense untuk ekstraksi fitur. 3.4.4 Desain Arsitektur
      Desain arsitektur merupakan proses menentukan struktur dan komponen model yang akan dibangun, yang terdiri dari jenis model, jumlah dan jenis layer, fungsi aktivasi, teknik regularisasi, dan konfigurasi model. Hasil dari kedua model digabungkan dan diklasifikasikan menggunakan Support Vector Machine (SVM). Model ini terdiri dari tiga tahapan yaitu akuisisi data, preprocessing data, ekstraksi fitur, penggabungan fitur, dan klasifikasi dengan SVM, dan output sistem. Digunakan untuk mengolah data visual, seperti mengenali mata tertutup atau mulut menguap sebagai indikator kantuk. Menggabungkan fitur yang diekstrak dari CNN dan LSTM untuk mendapatkan representasi data yang komprehensif, memastikan bahwa model dapat mengidentifikasi kantuk berdasarkan kombinasi indikator visual dan fisiologis. Selanjutnya yaitu menggunakan Support Vector Machines (SVM) untuk mengklasifikasikan data sebagai ""kantuk"" atau ""tidak kantuk"". SVM dipilih karena kemampuannya dalam mengklasifikasikan data yang kompleks dan memberikan batas keputusan yang jelas ""layak"" atau ""tidak layak"" pengemudi untuk berkendara. Jika pengklasifikasi mendeteksi keadaan mengantuk, maka pengklasifikasi menghasilkan alarm atau notifikasi pemberitahuan untuk memberi tahu bahwa pengemudi tidak layak untuk berkendara atau kembali ke fase pertama dan memulai ulang prosedur. 3.4.5 Pelatihan Model dengan Dataset
     Pelatihan model dilakukan dengan menggunakan training set, dengan tuning hyperparamaters berdasarkan kinerja pada validation set. Pelatihan model dilakukan dengan model SVM menggunakan training set. 3.5 Evaluasi
     Model gabungan ini dievaluasi menggunakan metrik seperti akurasi, presisi, recall, dan F1-score untuk memastikan performa dan keandalannya. Implementasi sistem ini diharapkan dapat memberikan notifikasi atau peringatan kepada pengemudi jika tanda-tanda kantuk terdeteksi selama kondisi pre-driving, sehingga dapat meningkatkan keselamatan berkendara secara signifikan. Berdasarkan hasil validasi, model dapat di-tune atau dioptimalkan untuk meningkatkan performa, misalnya dengan mengubah arsitektur, parameter, atau teknik training. 3.6 Implementasi
     Setelah penyempurnaan, model dianggap siap untuk digunakan. Model ini harus dapat secara akurat mendeteksi kantuk pengemudi dalam berbagai kondisi dengan minimal kesalahan. Langkah selanjutnya yaitu penerapan model dalam sistem nyata dan pemantauan efektivitasnya dalam kondisi pengemudi pada lingkungan pre-driving. Model yang telah dioptimalkan diintegrasikan ke dalam sistem deteksi dini kantuk untuk pengujian awal. Selanjutnya yaitu melakukan uji coba lapangan untuk mengevaluasi efektivitas sistem dalam kondisi nyata, memungkinkan pengumpulan feedback untuk perbaikan lebih lanjut."
Utami Lestari_Kualifikasi.txt,"Penelitian ini bertujuan untuk mengembangkan aplikasi berbasis Large Language Model (LLM) dengan arsitektur GPT-4 yang mampu melakukan telaah sejawat(peer review) secara otomatis pada artikel ilmiah dari jurnal komputer. Data utama yang digunakan adalah artikel ilmiah berbahasa Indonesia dalam bidang ilmu komputer dari berbagai jurnal akademik. Sebelum digunakan, data akan diperiksa untuk menghilangkan informasi pribadi yang dapat mengidentifikasi penulis atau reviewer. Aplikasi ini diharapkan dapat membantu para peneliti dan editor jurnal dalam menganalisis dan memperoleh wawasan dari artikel yang seringkali bersifat kompleks dan teknis. Proses ini melibatkan beberapa tahap penting yang bertujuan untuk membersihkan dan menyiapkan data teks agar sesuai dengan kebutuhan model serta meningkatkan kualitas dan konsistensi representasi teks. 2 Tahapan Preprocessing
Tahap pertama adalah tokenisasi, di mana teks dipecah menjadi unit-unit yang lebih kecil yang dikenal sebagai token, memungkinkan model untuk menganalisis teks pada tingkat yang lebih granular. 3.1.4 Evaluasi Model LLM
Evaluasi model merupakan langkah yang penting dalam pengembangan sistem kecerdasan buatan, karena memungkinkan untuk menilai kinerja dan efektivitas model dalam menyelesaikan tugas tertentu. Tanpa evaluasi yang tepat, model yang dikembangkan dapat menghasilkan prediksi yang tidak akurat atau tidak dapat diandalkan, yang berpotensi menyebabkan kinerja sistem yang buruk secara keseluruhan. Recall memberikan informasi tentang seberapa banyak instance positif yang berhasil diidentifikasi oleh model dari semua instance positif yang
4. sebenarnya dalam dataset. 3.1.5 Validasi Ahli
Proses validasi ahli ini memastikan bahwa model GPT-4 yang digunakan untuk telaah sejawat mampu memberikan evaluasi yang akurat, relevan, dan sesuai dengan standar akademik, dengan masukan berharga dari para ahli di bidangnya. 3.2 Jadwal Penelitian
Jadwal penelitian bertujuan untuk mengatasi target waktu penelitian, memastikan bahwa penelitian ini dapat diselesaikan sesuai dengan batas waktu yang telah ditetapkan. Adanya jadwal penelitian, diharapkan penelitian dapat berjalan secara efisien dan sesuai rencana, sehingga memberikan kepastian bahwa semua tahapan penelitian dapat diselesaikan tepat pada waktunya. Table jadwal penelitian dapat dilihat pada table 3.1"
Yoga Panji Perdana Nugraha_Kualifikasi.txt,"3.1 Motivasi
       Industri manufaktur memiliki berbagai macam produk yang ada di dalamnya. Dalam upaya pemenuhan kualitas yang tinggi serta menjaga kepuasan pelanggan dan reputasi perusahaan maka mendeteksi produk yang cacat sedini mungkin merupakan aspek yang penting. Pengembangan aplikasi pendeteksi cacat pada produk ini didasari keinginan peneliti untuk meningkatkan kinerja pengendalian kualitas pada industri manufaktur sehingga dapat membantu menjaga kualitas produk serta efisiensi dalam kegiatan pengendalian kualitas. Untuk meminimalisir pemborosan waktu, bahan baku, biaya dan sumber daya lainnya karena deteksi cacat pada produk dilakukan sedini dan secepat mungkin. Meningkatkan efisiensi pada kegiatan inspeksi produk dengan menerapkan otomatisasi melalui aplikasi yang dikembangkan. Mengintegrasikan teknologi yang sedang berkembang seperti artificial intelligence dengan industri manufaktur sehingga tercipta manufaktur cerdas yang akan berakibat pendapatan profit perusahaan yang optimal. Memberikan kontribusi pemahaman dan pengembangan teknologi baru dalam deteksi objek sehingga bisa menjadi referensi untuk pembaca serta penelitian selanjutnya. Tahap Awal
Kegiatan yang dilakukan pada tahap awal ini adalah merancang dan membuat prototype alat deteksi cacat dan pengumpulan data cacat objek. Gambar 3.2 di atas menggambarkan rancangan alat yang akan dikembangkan. Tahap ini terdapat kegiatan yaitu evaluasi dan penyempurnaan model deteksi cacat objek. Evaluasi dan penyempurnaan dilakukan agar ftur yang ada pada aplikasi yang akan dikembangkan dapat ditampilkan dengan maksimal. Fitur yang akan ditambahkan pada model pendeteksi objek berupa kemampuan komputer untuk secara otomatis menyimpan hasil deteksi menjadi sebuah basis data. Pengujian data dilakukan untuk menguji model sejauh mana dapat mendeteksi cacat dari suatu produk. Pada akhirnya akan menampilkan output model dalam mendeteksi cacat pada produk. Setelah itu maka dibangun aplikasi yang mampu mendeteksi cacat produk pada industri secara real time. Aplikasi ini nantinya akan menampilkan hasil deteksi dari produk yang bergerak. Informasi yang disampaikan antara lain kondisi dari produk cacat atau tidak serta bagian mana yang cacat akan ditandai oleh bounding box. Selain itu diterapkan juga pengukuran evaluasi seperti precision, recall, dan mean average precision (MAP) untuk memastikan model yang dikembangkan dapat digunakan dengan optimal. Nantinya akan dikembangkan sebuah aplikasi yang kemungkinan berbasis web untuk mempermudah pengguna untuk mengambil gambar (bergerak maupun tak bergerak) yang kemudian mengirimnya ke sistem pendeteksi cacat dan menerima hasil deteksi secara real time. Hasil deteksi secara real time dikehendaki agar produk dapat diperiksa selama proses produksi berlangsung sehingga cacat dapat dideteksi secepat dan seakurat mungkin. Hal ini akan membantu operator untuk melakukan kegiatan inspeksi produk dengan efisien."
