nama_dokumen,summary
Alifurrohman_Kualifikasi.txt,3.2 pengumpulan data langkah awal adalah mengumpulkan dataset yang akurat dan relevan. dataset didapatkan dari data sekunder dataset ini merupakan hal yang penting dari simulasi dan eksperimen mencakup koordinat lokasi yang mungkin meliputi lokasi depot dan titik pengiriman jendela waktu untuk setiap pengiriman yang menentukan batas awal dan akhir kapan pengiriman harus dilakukan serta jumlah kendaraan. data ini harus mencerminkan situasi dunia nyata untuk memastikan bahwa model yang dikembangkan dapat diaplikasikan secara praktis. n ormalisasi merupakan proses penting untuk menyamakan skala data memastikan bahwa model dapat memprosesnya dengan efisien. korelasi membantu mengidentifikasi fitur fitur yang saling terkait dan memberikan wawasan tentang bagaimana setiap fitur dapat mempengaruhi model prediksi rute. 3.4 desain model implementasi deep q network dqn dengan mekanisme attention untuk dynamic vehicle routing problem with time windows dvrptw melibatkan beberapa langkah utama mulai dari pemilihan kerangka kerja hingga pembuatan lingkungan simulasi. informasi ini digunakan untuk memperbarui kebijakan model dengan cara mengoptimalkan parameter jaringan sehingga meningkatkan estimasi nilai q yang merepresentasikan hadiah kumulatif yang diharapkan. untuk meningkatkan stabilitas dan efisiensi pelatihan teknik seperti experience replay dan target networks digunakan. selain itu pada tahap pelatihan model ini juga dilakukan penyetelan hyperparameter untuk menemukan nilai optimal hyperparameter guna meningkatkan kinerja model. ini merupakan langkah penting dalam machine learning karena dapat menghasilkan peningkatan akurasi efisiensi dan generalizability model. 3.6 evaluasi model setelah fase pelatihan model deep q network dqn dengan multi header attention untuk dynamic vehicle routing problem with time windows dvrptw selesai langkah evaluasi menjadi penting untuk memahami seberapa efektif model dalam menyelesaikan masalah yang ditargetkan. evaluasi dilakukan dengan menguji model terhadap kumpulan data pengujian yang tidak terlibat selama proses pelatihan me mberikan masukan penting tentang kemampuan generalisasi model terhadap skenario baru dan belum pernah dilihat. total jarak tempuh mencerminkan efisiensi rute yang dihasilkan sementara kepatuhan terhadap jendela waktu mencerminkan kualitas layanan yang dapat dijamin oleh model. 3.7 analisis dan penyempurnaan langkah terakhir yaitu analisis secara mendalam kinerja model pada dataset pengujian. penyempurnaan dilakukan untuk mengatasi kelemahan yang telah dianalisis sebelumnya seperti penyempurnaan pada tuning hyperparameter untuk peningkatan kinerja modifikasi arsitektur dan pelatihan ulang model. 3.8 jadwal penelitian jadwal penelitian digunakan untuk meningkatkan efektivitas dalam proses penelitian. adanya jadwal penelitian ini setiap proses penelitian sudah terjadwal dalam tabel 3.1 sehingga penelitian lebih efektif dan optimal.
Armando Tirta Dwilaga_Kualifikasi.txt,3.1 gambaran umum penelitian penelitian ini digunakan untuk mengatasi sensitivitas terhadap cacat pada gambar ban dengan melibatkan penggunaan jaringan syaraf menggunakan algoritma convolutional neural network cnn dan membangun model atau kerangka kerja menggunakan keras. berikut adalah gambar 3.1 blok diagram gambaran umum penelitian. data preparation data augmentasi data splitting model training forward pass model evaluation backward passfinetuning if neededinput unit processing unit output unit model deployment inference12 gambar 3.1 blok diagram gambaran umum penelitian berdasarkan gambar 3.1 blok diagram gambaran umum penelitian maka dapat dijelaskan di blok tersebut terbagi menjadi 3 bagian yaitu bagian pertama adalah unit masukan berisikan data preparation di mana gambar ban dimuat diubah menjadi format yang sesuai dipersiapkan untuk pelatihan model convolutional neural network cnn seperti pemrosesan gambar ban selanjutnya data augmentation di mana data dibuat lebih ber variasi dari training data yang ada sehingga dapat meningkatkan keberagaman training data tanpa h arus mengambil data baru mencakup rotasi pergeseran horizontalvertikal perbesar gambar perubahan kecerahan gambar sampai mengubah nilai pixel selanjutnya data di mana dataset yang telah di augmentasi dan disiapkan dibagi menjadi subset yang berbeda untuk training untuk melatih model validation untuk menyempurnakan model serta me mvalidasi performanya selama pelatihan dan testing untuk mengevaluasi kinerja model akhir . data set dibagi menjadi training data validation data dan testing data dalam proporsi tertentu. bagian kedua adalah unit pemrosesan yang bertindak adalah model training forward pass tahap di mana input diproses melalui model untuk menghasilkan prediksi tujuannya melatih model convolutional neural network cnn menggunakan dataset pelatihan di mana data dari unit masukan diteruskan melalui jaringan neural di lakukan transformasi linier konvulasi dan non linier fungsi aktivasi dilakukan pada data di setiap lapisan untuk menghasilkan output prediksi yang melibatkan komputasi di setiap neuron dan lapisan jaringan yang merupakan inti dari proses pembelajaran dalam jaringan saraf. selanjutnya unit pemrosesan finetuning tujuannya dilakukan untuk menyempurnakan model lebih lanjut setelah pelatihan awal dengan dataset yang lebih kecil atau lebih spesifik nantinya. proses di dalam finetuning menyesuaikan bobot menggunakan kumpulan data yang lebih kecil untuk menyesuaikan bobot model untuk performa yang lebih baik pelatihan khusus fokus pada fitur data yang lebih relevan dengan objek . bagian ketiga adalah unit keluaran yang bertindak ada proses model evaluatioan backward pass tahap di mana gradien memperbarui parameter model dalam arah yang akan mengurangi fungsi loss dari fungsi loss metrik yang mengukur seberapa baik atau buruk model melakukan prediksi dibandingkan nilai aktualnya dihitung dan digunakan untuk memperbarui parameter model selama pelatihan tujuannya mengevaluasi performa model yang dilatih dan model dievaluasi menggunakan metrik yang relevan accuracy precision recall dan f1 score berdasarkan prediksi yang dihasilkan dari model terhadap validasi atau uji data. output dari proses ini adalah tentang hasil evaluasi model yang memberikan informasi kinerja model. selanjutnya ada dua alur pilihan yang bisa dilakukan alur pertama jika hasil prediksi sudah sesuai dengan keinginan maka bisa langsung masuk ke model deployment inference dan alu r kedua jika hasil prediksi masih perlu diperbaiki pada bagian unit pemrosesan terlebih dahulu fine tuning untuk penggunaan data set lebih kecil jika menunjukan model belum mencapai performa yang diharapkan baru masuk ke model deployment inference tujuannya menerapkan model terlatih untuk membuat prediksi pada data baru yang belum terlihat. model deployment inference yang telah dilatih digunakan untuk membuat prediksi pada data baru atau dalam situasi dunia nyata tahap di mana model menerima input baru dan menghasilkan output berdasarkan pada pembelajaran yang dilakukan selama proses pelatihan dan merupakan output akhir dari keseluruhan proses di mana model mengambil keputusan atau membuat prediksi berdasarkan pada pengalaman yang telah diperoleh selama pelatihan. 3.2. tahapan penelitian penelitian ini di dalamnya terdapat tahapan tahapan yang dilakukan untuk membentuk satu kesatuan yang utuh dari awal sampai akhir dan membentuk kerangka penelitian mengenai klasifikasi pada produk ban menggunakan algoritma convolutional neural network cnn . 3.2.1 studi literatur tahap pertama adalah studi literatur di mana studi yang dilakukan berasal dari artikel ilmiah dan buku yang menunjang dalam menganalisis terkait dengan metode pen gukuran kualitas mengenai klasifikasi produk ban meninjau penggunaan pembelajaran mesin algoritma convolutional neural network cnn dari beberapa tahun ke belakang dalam konteks peng ukuran kualitas untuk klasifikasi terhadap kondisi kondisi produk ban . 5 tahapan data preprocessing 3.2.4 data augmentation tahap keempat adalah data augmentation meningkatkan variasi dalam dataset dengan teknik augmentasi data menggunakan operasi seperti rotasi pergeserarn horizontalvertikal perbesar gambar perubahan kecerahan gambar sampai mengubah nilai pixel untuk memperkaya dataset dan mengurangi overfitting saat disajikan dengan data baru yang belum pernah dilihat sebelumnya performa model akan menurun drastis karena model tersebut dapat menyesuaikan diri dengan kumpulan training data dengan sangat efektif. sebaliknya pada saat runtime hanya menghasilkan variasi dari gambar yang sudah ada dibuat secara dinami s dan cukup bagi model untuk berlatih dari berbagai kondisi gambar ban yang ada pada kenyataaanya. secara lebih jelas nilai teknik augmentasi pertama dilakukan dengan manual menggunakan bantuan dari website roboflow dengan resize gambar menjadi 640 x 640 pada augmentasinya menggunakan model flip horizontal dan vertikal 90 pemutaran searah jarum jam berlawanan arah jarum jam dan terbalik rotasi 45 dan 45 shear 5 horizontal dan 5 vertikal brightness 20 sampai 20. data asli pada dataset berjumlah 1 .028 data gambar ban setelah dilakukan augmentasi secara fisik menggunakan website roboflow ada data yang tidak dapat diidentifikasi ada 3 gambar sehingga total gambar asli yang berhasil di upload dan dijadikan data asli yang tetap berjumlah 1.025 data ga mbar dan setelah di augmentasi bertambah menjadi 2 .050 data gambar ban. 7 tahapan splitting data 3.2.6 model building tahap ke enam adalah model building membangun model convolutional neural network cnn dengan keras membangun arsitektur model convolutional neural network cnn menggunakan keras mengatur lapisan lapisan seperti convolutional maxpooling2d flatten dan dense untuk membangun model. selanjutnya secara kestabilan dan konvergensi adaptive momentum adam menyambung dari awal dapat mengubah kecepatan pembelajaran secara adaptif sehingga membuatnya lebih stabil dan kecil kemungkinannya terjebak pada tingkat minimum lokal nilai yang dianggap sebagai titik terendah dari loss function dalam model sehingga adaptive momentum adam cenderung mencapai konvergensi tingkat kinerja yang diharapkan lebih cepat dan andal dalam berbagai keadaan sedangkan stochastic gradient descent sgd mungkin lebih stuck pada nilai minimum atau terjebak pada nilai minimum lokal ya ng disebabkan oleh kemungkinan bergantung pada seberapa tepat kecepatan pemelajaran dipilih kecepatan pemelajaran yang tetap dapat membuat model mencapai konvergensi terlalu cepat atau terlalu lambat. 9 tahapan model evaluation testing 3.3 arsitektur convolutional neural network cnn convolutional neural network cnn yang dibangun menggunakan model atau kerangka kerja yang pada dasarnya menggunakan keras dan juga tensorflow dengan menambahkan beberapa model lapisan lapisan seperti lapisan convolutional conv2d laposan pooling maxpooling 2d flatten dan lapisan fully connected dense . 10 tahapan convolutional neural network cnn dengan model keras. filter convolutional layer pertama yang berfungsi untuk mengekstrak fitur fitur visual diterapkan pada gambar untuk menghasilkan fitur fitur yang lebih abstrak formula untuk mengetahui jumlah training datanya dengan . max pooling memilih nilai maksimum di dalam jendela pooling untuk m engurangi ukuran fitur dan mempertahankan informasi penting. selanjutnya dense layers lapisan dense digunakan sebagai lapisan output dalam model klasifikasi di mana jumlah neuron dalam lapisan output sesuai dengan jumlah kelas yang harus diprediksi di mana ada tiga lapisan dense ditambahkan dengan fungsi pertama dan kedua menggunakan relu sebagai 0 f x max x yang artinya menunjukkan bahwa keluarannya nol jika masukannya negatif atau nol dan output x jika masukannya positif dengan 128 unit neuron dan pada dense kedua 64 unit neuron karena tugasnya mengurangi dimensi representasi pada lapisan dense pertama maka model dapat mempelajari pola yang lebih rumit dan mendalam dari data dengan menambahkan lapisan yang lebih padat yang dapat meningkatkan performa model dalam tugas klasifikasi gambar. terakhir evaluation di mana performa model pada testing data dinilai menggunakan hasil klasifikasi dan confusion matrix untuk memahami kinerjanya testing data. bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 23x23 menjadi 11x11 sebagai berikut. setiap filter diubah menjadi setengah dari ukuran inputnya 11x11 menjadi 5x5 dan jumlah neuronnya 18464 mengikuti lapisan konvolusi ketiga . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 112 21 9 21 5 7. fourth cov2d totalneuronukuranfilterxjumlah𝐶ℎ𝑎𝑛𝑛𝑒𝑙𝐼𝑛𝑝𝑢𝑡1xfilter 3x3x321x16 289x16 4624 jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. selanjutnya adalah dalam penentuan ukuran spasialnya s etiap filter diubah menjadi setengah dari ukuran input nya 5x5 menjadi 2x2 sebagai berikut. setiap filter diubah menjadi setengah dari ukuran inputnya 2x2 menjadi 1x1 dan jumlah neuronnya 4624 mengikuti lapisan konvolusi keempat . bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 𝐼𝑛𝑝𝑢𝑡𝑆𝑖𝑧𝑒𝑃𝑜𝑜𝑙𝑆𝑖𝑧𝑒 𝑆𝑡𝑟𝑖𝑑𝑒1 12 21 1 21 0.51 9. flatten tidak mengubah parameter yang ada karena fungsi flatten hanya mengubah matriks multidimensi menjadi vektor tunggal berdasarkan hasil dari jumlah filter pada lapisan lima cov2d yaitu 8 dan maxpooling2d dengan ukuran inputnya 1x1 sehingga menjadi matriks multidimensi 1 1 16 diubah menjadi nilai vektor tunggal dengan panjang 16 atau menjadi jumlah neuron sebanyak 16. 10. dense layer 1 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 161x128 17x128 2176 11. dropout layer 1 menggunakan 0.2 yang artinya sebanyak 20 dari neuron dalam dense layer 1 akan dinonaktifkan secara acak. 12. dense layer 2 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 1281x64 129x64 8256 13. dropout layer 2 menggunakan 0.2 yang artinya sebanyak 20 dari neuron dalam dense layer 2 akan dinonaktifkan secara acak. 14. dense layer 2 totalneuronjumlahneuron𝐼𝑛𝑝𝑢𝑡1xjumlahneuron𝑂𝑢𝑡𝑝𝑢𝑡 641x1 65x1 65 ketika dimensi spasial tinggi dan lebar dikurangi menggunakan operasi lapisan pooling seperti maxpooling jumlah neuron di setiap lapisan pooling akan menurun. misalnya dimensi spasial setiap filter tinggi dan lebar di lapisan maxpooling disesuaikan menjadi setengah dari dimensi masukannya. karena hanya separuh dari masukan yang diproses lebih lanjut hal ini juga menyebabkan berkurangnya jumlah neuron pada lapisan tersebut. sedangkan penurunan pada dense terjadi karena penentuan jumlah neuron.
Bayu Kumoro.txt,"3.1. Obyek penelitian
       Obyek penelitian ini adalah gambar RGB sebagai cover (diambil dari internet Google Image) atau gambar grayscale yang akan disisipkan pesan berupa teks. 3.2. Tahapan Penelitian
       Penelitian ini berusaha mengembangkan algoritma dan prototipe sebagai solusi dari masalah dan kekurangan dari teknik yang pernah dilakukan peneliti terdahulu yang dapat mengatasi optimasi hardware FPGA. Rencana penelitian mencoba menggabungkan mengembangkan algoritma dan mengembangkan metode yang dapat mengoptimalkan performa FPGA. Sehingga, metode untuk hasil yang diinginkan yaitu penggunaan komponen efisien, power yang digunakan lebih kecil dibandingkan penelitian sebelumnya. Rencana yang akan dilakukan menggunakan metode dan modifikasi pada Penelitian (Abdullah AlWatyan, 2017) dengan melakukan tahapan penyediaan pesan rahasia dan Cover Image. Dengan pesan rahasia akan dissipkan pada cover Image dengan key yang sudah dibuat pada hardware FPGA hingga menjadi Stego Image. Pada tahapan ekstraksi Stego Image akan mengambil pesa rahasia yang sudah disisipkan dengan key yang sudah dibuat. Kemudian, Stego Image akan menjadi pesan rahasia dan Cover Image. Dan untuk mengoptimalkan hardware FPGA akan menggunakan perbandingan metode yang dilakukan peneliti (E. A. Elshazly, 2018) dengan menggunakan metode algoritma steganografi gambar GEMD. Tahap pertama dimulai dengan simulasi metode pada MATLAB. Pada MATLAB, difokuskan pada metode algoritma yang diajukan sampai dengan pengecekan kualitas stego image dan cover image. Setelah tahap MATLAB sudah selesai, dilanjutkan pada hardware FPGA. Pada FPGA, diimplementasikan algoritma yang sudah dibuat dan pengecekan kualitas gambar dilanjutkan dengan analisa energi yang digunakan FPGA. 3.3 Proses Algoritma Pemasukan Dan Ekstraksi Pesan Tersembunyi
      Pseudo-random number generator (PRNG) dapat digunakan untuk memilih piksel secara acak dan menyematkan pesan. Data dapat disembunyikan dalam LSB bidang warna tertentu (bidang merah) dari piksel yang dipilih secara acak dalam ruang warna RGB. Generator angka acak pseudo menghitung dan memilih urutan piksel yang akan dipilih untuk penyematan data berdasarkan kunci. As gambar terdiri dari kontribusi piksel dari komponen merah, hijau dan biru dan setiap piksel memiliki angka dari komponen warna (untuk gambar bitmap 24-bit masing-masing merah, hijau dan pixel biru memiliki 8 bit). Pada 8 bit dari jumlah warna, jika kita mengubah bit yang paling tidak signifikan, sistem penglihatan kita dapat mendeteksi perubahan dalam pixel dan dengan demikian dimungkinkan untuk mengganti bit pesan dengan bit pixel gambar. Jika kami mengubah LSB dalam satu byte gambar, kami dapat menambah atau mengurangi satu dari nilai yang diwakilinya. Untuk menyembunyikan pesan, data terlebih dahulu dikonversi ke dalam format byte dan disimpan dalam array byte. Pesan tertanam di setiap bit ke posisi LSB dari bidang merah setiap piksel. Masukkan bit pesan rahasia ke LSB dari Pixel bidang merah. Dalam proses ekstraksi ini, proses pertama-tama mengambil kunci dan kemudian kunci acak. Kunci-kunci ini mengambil poin dari LSB di mana pesan rahasia didistribusikan secara acak. Kemudian penerima dapat mengekstraksi pesan yang tertanam persis menggunakan stego-key. Inisialisasi kunci acak yang memberikan posisi tombol bit pesan dalam piksel merah yang disematkan secara acak. Untuk mendekode, pilih piksel dan Ekstrak nilai LSB piksel merah. 5. Baca masing-masing piksel kemudian isi array dikonversi menjadi nilai desimal yang sebenarnya adalah nilai ASCII dari karakter tersembunyi. 6. Nilai ASCII yang didapat dari atas adalah XOR dengan stego-key dan memberikan file pesan, yang disembunyikan di dalam gambar sampul. 3.4 Pengecekan Kualitas Cover Image Dengan Stego Image
       Structural Similarity Index Metrics (SSIM) dikenal sebagai kualitas metric yang digunakan untuk mengukur kemiripan diantara 2 buah citra dan dipercaya berkorelasi dengan kualitas persepsi Human Visual System (HVS). Nilai SSIM berada pada rentang -1 hingga 1. Semakin tinggi nilai SSIM, maka semakin tinggi tingkat kemiripan dari 2 buah citra."
Devi Resviani_KUALIFIKASI.txt,3.1 tahapan penelitian tahapan penelitian merupakan serangkaian langkah langkah yang dilakukan dalam penelitian. gambaran mengenai tahapan penelitian ini dapat dilihat pada gambar 3.1. studi literatur pengumpulan dan analisis data preprocessing data pemilihan algoritma yang efektif pengembangan model machine learning sistem peringatan pemeliharaan prediktif evaluasi identifikasi permasalahan integrasi model dengan sistem peringatan pemeliharaan prediktif gambar 3.1 tahapan penelitian gambar 3.1 menunjukkan tahapan penelitian sebagai dasar untuk pengembangan sistem pemeliharaan prediktif menggunakan teknik machine 45 learning. tahapan penelitian ini terdiri dari studi literatur untuk memahami keadaan yang terfokus terhadap tentang mesin kompresor reciprocating metode prediksi pemeliharaan mesin predictive maintenance dan machine learning. preprocessing data melibatkan pembersihan data normalisasi dan transformasi data agar siap digunakan dalam model machine learning. pemilihan algoritma machine learning yang efektif untuk membantu mencapai akurasi yang lebih tinggi dan efisiensi dalam prediksi. pengembangan dan melatih model machine learning menggunakan algoritma yang telah dipilih dengan data yang telah diperoses tahap ini melibatkan pembagian data menjadi set pelatihan dan set pengujian serta termasuk mengatur parameter model untuk mencapai kinerja terbaik. s istem peringatan pemeliharaan prediktif menggunakan model machine learning untuk memprediksi kegagalan mesin sistem ini bertujuan untuk memberikan peringatan dini sebelum terjadinya kerusakan atau kegagalan mesin. 46 tabel 3.1 deskripsi kolom dataset mesin kompresor reciprocating no nama kolom deskripsi 1. id identifikasi unik untuk setiap entri 2. rpm kecepatan putaran mesin dalam rotasi per menit 3. motor power daya motor dalam watt 4. torque torsi yang dihasilkan oleh mesin 5. outlet pressure bar tekanan keluaran dalam bar 6. air flow aliran udara yang diukur 7. noise db tingkat kebisingan dalam desibel 8. outlet temperature suhu keluaran 9. water pump outlet pressure tekanan keluaran pompa air 10. water inlet temperature suhu air masuk 11. water outlet temperature suhu air keluar 12. water pump power daya pompa air 13. water flow aliran air 14. oil pump power daya pompa oli 15. oil tank temperature suhu tangki oli 16. gacceleration x gaccx akselerasi gravitasi pada sumbu x 17. gacceleration y gaccy akselerasi gravitasi pada sumbu y 18. gacceleration z gaccz akselerasi gravitasi pada sumbu z 19. hacceleration x haccx akselerasi horizontal pada sumbu x 20. hacceleration y haccy akselerasi horizontal pada sumbu y 21. hacceleration z haccz akselerasi horizontal pada sumbu z 22. bearings kondisi bearing misalnya ok 23. water pump kondisi pompa air misalnya ok 24. radiator kondisi radiator misalnya clean 25. exvalve kondisi katup eksvalve misalnya clean 26. ac motor kondisi motor ac misalnya stable analisis data penelitian ini menggunakan metode exploratory data analysis eda bertujuan untuk memberikan gambaran umum tentang data dan mengidentifikasi pola atau anomali yang mungkin tidak terlihat dengan metode lain. 3.4 sistem peringatan pemeliharaan prediktif sistem peringatan pemeliharaan prediktif adalah sistem yang menggunakan teknik teknik analisis data terutama machine learning untuk memantau kondisi mesin dan memprediksi kegagalan yang mungkin terjadi. gambaran alur kerja usulan sistem peringatan pemeliharaan prediktif dapat dilihat pada gambar 3.4. monitoring data realtime input data ke model prediksideteksi anomalianalisis prediksialert jadwal pemeliharaan gambar 3.4 alur kerja sistem peringatan pemeliharaan prediktif gambar 3.4 menggambarkan alur kerja dari sistem pemeliharaan prediktif pada mesin kompresor reciprocating berbasis machine learning. tahap kelima alert atau peringatan untuk memberikan informasi kepada tim pemeliharaan agar dapat segera mengambil tindakan. tahap terakhir menjadwalkan tindakan pemeliharaan yang diperlukan berdasarkan peringatan untuk menghindari kegagalan mendadak dan meminimalkan downtime.
Erfiana Wahyuningsih_UK.txt,berikut flowchart penelitian untuk rangkaian sram 6t low power dan high read stability dengan metode m gdi. gambar 10. alur penelitian sram 6t dengan metode m gdi dalam me mulai desain sram 6t dengan menggunakan metode m gdi diperlukan studi literatur terkait bebera pa penelitian dengan metode atau hasil serupa . referensi rangkaian diperlukan untuk melihat hasil sebagai pembanding dengan rangkaian baru yang didesain dengan metode mgdi. dipilih desain berdasarkan penelitian sebelumnya yang dilakukan oleh ebrahim abiri dan abdolreza darabi 2015 sram 8t dengan low power dan high read stability menggunakan metode m gdi. hasil yang dihar apkan tetap mengacu pada low power dan mempertaha nkan pula kemampuan high read stability.
Fitriana Indah Pramitasari_Kualifikasi.txt,3.1 alur penelitian alur penelitian menggambarkan alur dari awal hingga akhir penelitian dilaksanakan. alur penelitian ini diuraikan pada gambar 3.1 di bawah ini. gambar 3.1 alur penelitian 27 3.2 identifikasi masalah identifikasi masalah adalah salah satu langkah pertama yang dilakukan sebelum melakukan penelitian. identifikasi masalah merupakan suatu proses mencari dan mengetahui masalah yang ingin diselesaikan. identifikasi masalah ini membantu penelitian untuk memah ami tantangan yang dihadapi oleh petani kentang skala nasional dan merancang solusi yang tepat sesuai dengan kebutuhan mereka. identifikasi masalah pada penelitian ini berfokus pada mengidentifikasi proses perancangan model koperasi petani mengidentifikas i metode prediksi permintaan dengan ann di dalam blockchain yang digunakan untuk mengoptimalkan permintaan pelanggan di masa depan selama periode tertentu dan mengidentifikasi metode safety stock di dalam blockchain yang digunakan agar dapat mengoptimalkan stok dan permintaan. identifikasi masalah pada penelitian ini peneliti dapat lebih memahami kendala dan kebutuhan petani kentang skala nasional. perancangan model platform koperasi untuk meningkatkan efisiensi dan kerjasama antarpetani dengan koperasi sebagai mitranya. sement ara itu metode prediksi permintaan dengan menggunakan artificial neural network ann diharapkan dapat membantu petani mengelola produksi secara lebih tepat sesuai dengan kebutuhan pasar dan koperasi dapat menyesuaikan persediaan stok dan permintaan secar a dinamis dari hasil prediksi permintaan. selain itu identifikasi masalah juga mencakup penerapan metode safety stock untuk mengoptimalkan manajemen stok memastikan ketersediaan barang dan meningkatkan responsibilitas terhadap fluktuasi permintaan pasar. dengan penerapan ann dan metode safety stock di dalam blockchain semua prediksi dan manajemen stok dapat dicatat di dlaam buku besar yang tidak dapat diubah sehingga meningkatkan transparansi dan keamanan data dalam rantai pasok. sehingga koperasi ini dapat melakukan perencanaan yang lebih akurat meminimalkan pemborosan dan meningkatkan ketersediaan kentang sesuai dengan kebutuhan pelanggan. dengan demikian platform koperasi menjadi responsif terhadap perubahan permintaan pasar mendukung pertumbuhan ekonomi para petani memperkuat kolaborasi antar anggota koperasi serta memiliki transparansi dan keamanan pada rantai pasok. 28 3.3 studi literatur studi literatur yang dilakukan pada penelitian engembangan platform koperasi petani ini dimulai dari pencarian dan review literatur literatur terbaru dan relevan yang telah diterbitkan. studi literatur juga membantu dalam mengetahui tantangan dan peluang yang mungkin dihadapi dalam pengembangan platform koperasi petani kentang. sehingga penelitian ini akan menghasilkan data yang sesuai dengan tujuan penelitian. proses ini memungkinkan peneliti untuk memahami konteks yang telah ada sebelumnya dan memanfaatkan pengetahua n serta data yang telah dihasilkan sebelumnya. beberapa wilayah indonesia berhasil dalam produksi kentang dan beberapa wilayah indonesia yang tidak dapat mempro duksi kentang. data tersebut memberikan gambaran lengkap mengenai kegiatan pertanian kentang di berbagai wilayah indonesia pada tahun 2022. berikut data bps tahun 2022 produksi kentang di berbagai wilayah indonesia. data primer yang akan digunakan pada penelitian ini adalah kebutuhan pengguna aliran data dari petani dengan koperasi sebagai mitranya data musim data historis penjualan data produksi kentang dan data harga kentang. berdasarkan informasi yang didapatkan dari salah satu petani di wonosobo jawa tengah disana terdapat banyak petani kentang dan sayuran lainnya. 30 gambar 3.2 pola distribusi kentang 3.5 blockchain pada penelitian ini untuk meningkatkan keamanan dan transparansi maka menggunakan teknologi blockchain untuk rantai pasok kentang. berikut flowchart kecerdasan buatan safety stock yang dikombinasikan di dalam blockchain. data rantai pasok yang telah dikumpulkan kemudian 31 dimasukkan ke dalam database. data tersebut diverifikasi dalam blockchain dengan proses pembuatan blok baru yang melibatkan perhitungan hash blok sebelumnya menyusun blok baru menghitung hash blok baru dan mencapai konsensus untuk menambahkan blok ke ra ntai. hasil prediksi permintaan disimpan dalam blockchain dengan proses pembuatan blok baru yang sama seperti langkah sebelumnya. hasil perhitungan safety stock disimpan dalam database dan dicatat dalam blockchain dengan pembuatan blok baru. 3.6 design sistem dengan uml pengembangan platform koperasi petani kentang menggunakan metode unified modeling language uml untuk menggambarkan struktur fungsi dan interaksi komponen sistem secara visual. analisis kebutuhan sistem dapat menentukan arah dan ruang lingkup proyek pengembang sistem serta memastikan bahwa produk akhir akan memenuhi harapan dan memecahkan masalah yang dihadapi oleh pengguna. pengguna platform ini terdiri dari consumers yang dapat mengakses produk pertanian secara langsung farmers yang memanfaatkan platform untuk memasarkan hasil panen companies yang terlibat dalam dukungan pengembangan teknologi dan partner cooperatives yang menjadi bagian dari 33 kolaborasi kerjasama antar koperasi untuk meningkatkan kesejahteraan bersama. keterlibatan seluruh pihak ini diharapkan platform koperasi petani menciptakan lingkungan yang saling mendukung dan berkelanjutan memperkuat konektivitas antar anggota untuk me ncapai tujuan bersama dalam dunia pertanian. proses verifi kasi produk kentang yang dihasilkan oleh petani. website koperasi akan mengirimkan informasi kepada konsumen bahwa transaksi telah berhasil dicatat. website koperasi mengubah status transaksi berdasarkan hasil verifikasi. input data yang akan digunakan a dalah data kuantitatif dan kualitatif yang dapat mempengaruhi permintaan di masa depan sehingga agar hasil prediksi permintaan dapat lebih akurat. dengan menerapkan metode ann pada prediksi permintaan ini penelitian dapat memberikan prediksi yang lebih tepat terkait kebutuhan pasar di masa mendatang sehingga dapat meningkatnya efektivitas rantai pasok. kemudian dilakukan data cleaning di normalisasi dan di transformasi untuk memastikan bahwa ann yang akan dibangun dapat bekerja dengan efektif dan menghasilkan prediksi yang akurat. berdasarkan hasil evaluasi model prediksi yang akurat dari model ann ini berguna untuk perusahan dalam membuat keputusan strategis seperti inventory management . proses prediksi permintaan dengan ann akan menghasilkan data permintaan yang diharapkan informasi tersebut digunakan untuk proses inventory management . 3.8 inventory management proses inventory management menggunakan metode safety stock merupakan proses untuk menjaga ketersediaan persediaan dalam platform secara efektif. tahapan pertama penelitian ini memerlukan analisis data historis 42 permintaan kentang fluktuasi pasokan dan waktu panen sehingga dapat mengidentifikasi kebutuhan pasokan dan resiko keterlambatan. penerapan metode safety stock pada penelitian ini akan menentukan tingkat persediaan tambahan yang diperlukan untuk mengatasi ketidakpastian dalam permintaan atau keterlambatan pasokan. hal ini bertujuan untuk memberikan keandalan dan menghindari kekurangan persediaan yang dapat mengha mbat operasional koperasi. metode safety stock dalam pengembangan platform koperasi petani kentang pada penelitian ini untuk meningkatkan efisiensi manajemen persediaan. selain itu integrasi ini melibatkan penggunaan artificial neural network ann dan metode safety stock yang terintegrasi dalam blockchain untuk rantai pasok. website koperasi akan terintegrasi dengan blockchain untuk memastikan efisiensi dan transparansi dalam seluruh proses manajemen rantai pasok. 3.10 pengujian sistem tahapan pengujian sistem dalam penelitian merupakan langkah untuk mengevaluasi kinerja atau fungsionalitas sistem yang dikembangkan atau diuji pada penelitian. proses pengujian sistem mencakup implementasi prototipe atau model sistem hingga serangkaian u ji coba. tujuan dari tahapan pengujian sistem adalah mengidentifikasi adanya kegagalan mengukur efektivitas sebuah sistem serta 43 memastikan sistem berjalan sesuai dengan tujuan dan persyaratan yang telah ditetapkan sebelumnya. pada penelitian ini sistem platform koperasi petani diharapkan dapat berjalan sesuai dengan tujuan dan persyaratan perkoperasian serta sesuai dengan model platform economic sharing . platform koperasi petani kentang pada penelitian ini akan berbasis website dan dilengkapi dengan kecerdasan buatan yang dikombinasikan dengan blockchain. 3.11 evaluasi tahapan selanjutnya adalah evaluasi. evaluasi dilakukan untuk memastikan bahwa semua komponen sistem berfungsi sesuai rencana. evaluasi melibatkan penilaian kinerja pada sistem secara keseluruhan dan memeriksa apakah integrasi berjalan tanpa hambatan. ta hapan evaluasi juga dapat mengidentifikasi apakah hasil pengujian sistem sesuai dengan tujuan awal dan menentukan area yang mungkin memerlukan peningkatan. hasil dari tahap evaluasi menjadi petunjuk penting untuk membuat perubahan dan peningkatan sehingga sistem dapat bekerja lebih baik lagi. 3.12 analisis hasil analisis merupakan tahapan penelitian dimana menyimpulkan serta menguraikan informasi dari hasil data yang telah diolah dan diuji sebelumnya. tahapan analisis dapat memberikan makna dari temuan temuan tersebut. tahapan ini memberikan identifikasi faktor faktor yang dapat mempengaruhi kinerja sistem dan memberikan rekomendasi untuk peningkatan di masa yang akan datang.
Ike Putri Kusumawijaya (99216004).txt,"Penelitian ini melakukan pengembangan metode untuk deteksi pergerakan anomali pada kerumunan menggunakan Algoritma Generative Adversarial Network. Metodologi yang digunakan adalah sebagai berikut. Pada tahap ini dilakukan studi terhadap beberapa artikel dan buku yang menguraikan mengenai pemrosesan video, deteksi pergerakan anomali pada kerumunan dan Algoritma Generative Adversarial Network
2. Merancang algoritma Generative Adversarial Network secara Real time untuk mengklasifikasi antara pergerakan normal dan anomali pada kerumunan
3. Untuk klasifikasi, dan filter median diimplementasikan pada hasil klasifikasi. Pada proses segmentasi juga akan dianalisis pola yang memiliki kerumunan normal dan tidak normal/anomali, pola ini dijadikan formation block berfungsi untuk mendapatkan karakterisitik yang didapat dari kerumunan, karakteristik yang diambil adalah nilai kepadatan kerumunan, tekanan pada kerumunan, serta pergerakan kerumunan yang terjadi, hasil dari karakteristik ini menjadi point of interest atau hal utama dalam pengambilan pola, sehingga dari nilai ini akan diketahui analisis perilaku yang terekam dalam video. Perhitungan generation error maps menggunakan pre-trained Conditional Generative Adversarial Networks (CGAN) untuk menghitung binary detection maps untuk setiap tingkat representasional. Hasil deteksi akhir ditentukan berdasarkan penggabungan peta deteksi yang diekstraksi. Pengembangan dari proses training dan testing ini adalah dengan menggunakan input video seperti pada gambar 3.4. Hasil proses training dan testing disimpan dalam server, sehingga saat kamera menangkap video maka akan membandingkan dengan server yang sudah terisi oleh konsep atau pola dari normal dan anomali. Sehingga penelitian yang akan diteliti memiliki konsep sebagai berikut. Pada gambar 3.5 konsep penelitian berawal dari kamera pengawas yang membandingkan masukkan dari train GAN yang disimpan di dalam server dengan video yang diambil dari kamera secara real time, hasil dari perbandingan video tersebut akan menghasilkan behaviour analysis atau pola gerakan anomali pada kerumunan sehingga outputnya akan menjadi suatu dataset untuk identifikasi gerakan anomali pada kerumunan."
KUALIFIKASI_Riya Widayanti.txt,hal ini menentukan sumber dari mana data akan dikumpulkan dan bagaimana mengumpulkan dan menganalisis data ini. ini menunjukkan bahwa peneliti telah memikirkan elemenelemen desain penelitian tertentu saunders lewis thornhill 2011. pada bab ini akan dibahas mengenai filosofi keilmuan dari data governance konsep teknolgi blockchain dan penerapan data governance dalam teknologi blockchain di bidang pendidikan yang akan memberikan pandangan utama saat melakukan penelitian. selanjutnya akan dijelaskan pendekatan yang digunakan penelitian dalam pengumpulan data menganalisis data yang digunakan serta etika lain yang akan dipatuhi terutama terkait kerahasiaan data yang digunakan. 3.2. skema penelitian untuk menyelesaikan penelitian dirancang kerangka pikir yang menggambarkan langkahlangkah yang harus ditempuh dapat dilihat penjelasan dan urutannya sebagai berikut 24 gambar 3.1 kerangka perancangan tata kelola sumber dama 2017 25 3.2.1 mendefinisikan tata kelola data untuk organisasi upaya tata kelola data harus mendukung strategi dan tujuan bisnis. strategi dan sasaran bisnis organisasi menginformasikan strategi data perusahaan dan bagaimana tata kelola data dan aktivitas manajemen data perlu dioperasionalkan dalam organisasi. tata kelola data memungkinkan tanggung jawab bersama untuk keputusan terkait data. kegiatan tata kelola data melintasi batasbatas organisasi dan sistem untuk mendukung tampilan data yang terintegrasi. tata kelola data membutuhkan pemahaman yang jelas tentang apa yang diatur dan siapa yang diatur serta siapa yang mengatur uraian lebih detil tentang proses transformasi warna ruang warna yang digunakan algoritma trasnformasinya. fokusnya adalah pada kesan yang dimiliki personel bisnis tentang seberapa baik perusahaan mengelola data dan menggunakan data untuk keuntungannya serta pada kriteria objektif seperti penggunaan alat tingkat pelaporan dll. tahap selanjutnya adalah mengolah data yang terkumpul dengan kerangka tata kelola data yang dijelaskan dalam dmbok. tahap selanjutnya adalah merancang struktur tata kelola data sesuai dengan struktur pengelolan pengejaran merdeka belajar. dalam merancang struktur tata kelola data juga dilakukan penentuan peran area keputusan dan tanggung jawab yang dilakukan. perancangan peran dilakukan dengan menggunakan metode wawancara dan mengadaptasi kajian pada data subject tim pengelola merdeka belajar. struktur tata kelola data yang telah dirancang akan dikonfirmasikan dengan menggunakan kuesioner sehingga akan dihasilkan struktur yang dapat dipertanggungjawabkan. selanjutnya kesimpulan dibuat sebagai tahap terakhir dari penelitian ini.
Kualifikasi Witta Listiya Ningrum.txt,3.1 tahapan penelitian penelitian ini melakukan pengembangan model klasifikasi toksisitas pada platform sosial media. selain itu juga untuk menentukan dan membandingkan metode serta algoritma yang sudah digunakan pada penelitian sebelumnya yang nantinya akan mengembangkan atau menciptakan suatu metode atau algoritma terbaru. 6. model klasifikasi toksisitas pada tahapan ini dilakukan pengembangan model dari hasil penggabungan ketiga representasi tersebut dengan menggunakan teknik fusion seperti concatenation atau attention mechanism untuk menghasilkan hasil klasifikasi akhir. 7. evaluasi model pada tahapan ini dilakukan evaluasi untuk mengetahui kinerja terhadap model yang dikembangkan dengan menggunakan pengukuran akurasi seperti precision recall dan juga f1score untuk klasifikasi teks dan mengukur akurasi dengan confusion matrix untuk gambar dan video. 8. hasil tahap an ini menghasilkan klasifikasi sesuai dengan label yang sudah dikategorikan ke dalam 3 kategori toksisitas yaitu toxic nontoxic dan netral.
Kualifikasi_Andi Asnur Pranata M. H. (99219024).txt,"3.3.2 Data Sekunder
     Data sekunder dikumpulkan dari hasil studi literatur, review penelitian terdahulu, pencarian perangkat lunak dan beberapa template perangkat lunak yang banyak tersedia di internet. Data sekunder ini juga akan mengumpulkan mengenai informasi proyek, sumber dana, jenis laporan baik dari pemilik (pengguna jasa), konsultan dan kontraktor, serta jadwal pelaksanaan dan kemajuan fisik, termin pembayaran serta informasi orang-orang yang berkepentingan dalam proyek konstruksi. 3.4 Pengembangan Sistem
     Untuk pengembangan sistem, sesuai yang telah disampaikan pada latar belakang bahwa metode yang digunakan dalam memodelkan sistem informasi manajemen proyek konstruksi adalah Rapid Application Development (RAD). Metode RAD digunakan karena modul yang terlalu banyak sehingga untuk fleksibilitas dalam pengembangan sistem dapat dikendalikan serta jika ada perubahan pada setiap modul, maka pengembang secara fleksibel dapat merubah modul tersebut dan modul yang berkaitan. b. RAD Design Workshop
     Pada tahap ini, tahap perancangan proses sistem, basis data dan user interface yang akan dikerjakan untuk prototype sistem, kemudian menganalisis dan mengembangkan modul-modul yang dirancang. Tahap ini akan terdiri dari dua tahap, yaitu :
     1)	Tahap Pembangunan Sistem
     Jika perancangan siap dan sudah disetujui, maka proses sistem akan dibangun dengan menggunakan bahasa Laravel Framework sesuai dengan rancangan yang sudah dibuat. 3.4.4 Analisis Sistem Usulan
     Berdasarkan hasil pengumpulan informasi mengenai kelemahan sistem yang berjalan dan hasil identifikasi masalah, maka untuk menyelesaikan permasalahan tersebut akan dianalisis untuk keperluan sistem usulan guna untuk melakukan pengembangan sistem informasi manajemen pada proyek konstruksi. Dari hasil tersebut, akan terbentuk daftar usulan objek pada sistem tersebut, kemudian akan berlanjut pada tahap pembuatan class diagram pada sistem tersebut. 3.4.6.2 Spesifikasi Database
        Pada spesifikasi database ini akan merancangan desain tabel sistem manajemen informasi untuk proyek konstruksi. 3.4.7 Perancangan Antar Muka
     Pada perancangan antar muka ini akan dirancang antar muka sistem informasi manajemen untuk proyek konstruksi yang nanti akan dibagi berdasarkan aktor-aktor pada case diagram. 3.5 Implementasi
3.5.1 Pembangunan Sistem
      Pada pembangunan sistem ini akan dibangun menggunakan hardware dan software sesuai dengan kebutuhan spesifikasi yang akan dibutuhkan. Pengujian ini dilakukan untuk mengetahui apakah semua modul yang sudah dibentuk berjalan sesuai rancangan atau tidak, serta mengetahui apakah ada kesalahan-kesalahan terhadap proses pada sistem informasi manajemen untuk proyek konstruksi. 3.6 Kesimpulan dan Saran
      Setelah tahap penelitian selesai, maka pada tahap akhir ini peneliti akan memberikan kesimpulan mengenai hasil penelitian yang sudah didapatkan. Pada tahap akhir ini juga, peneliti akan memberikan saran kepada peneliti berikut yang akan melakukan penelitian dengan tema yang sama, untuk memberikan gambaran dalam mengembangkan penelitian yang sudah dilakukan sebelumnya."
Kualifikasi_Aris Gunaryati.txt,3.1 gambaran umum penelitian motivasi dari metodologi yang diusulkan adalah membu at suatu metode peramalan yang sesuai dengan data runtun waktu yang ada serta meni ngkatkan akurasinya dengan tetap memp erhatikan efisiensi w aktu ko mputasi nya. langkahlangkah yang dilakuk an dalam p enelitian ini adalah m enganalisis data jum lah kasus h arian covid 19 di jakarta berdasarkan dataset dari situs ht tpscorona.jakarta.go.id tangg al 6 maret 2020 sampai 30 juni 2021 sebagai data training dan nanti akan diprediksi untuk tanggal 1 juli 2021 sampai dengan 31 juli 2021 sebagai data uji dengan tahapan sebagai berikut 1. mempersiapk an data runtun w aktu yang akan dia nalisis 2. menganalisis data runtun waktu yang ada meng gunakan metode statistika arima 3. menganalisis data runtun waktu yang ada menggunakan metode quantum neural network 4. mengembangkan model hybrid arimaquantum neural network 5. menentukan mod el yang cocok untuk s etiap variabel 6. menguji kecocokan masingmasing model 7. melakuk an peramalan dengan menggunakan mo del yang cocok 8. melakuk an perbandingan tingkat aku rasi hasil peramalan dengan tiap model untuk mend apatkan model peramalan yang diharapkan sesuai dengan data runtun waktu yang ada maka perlu dilakukan pendekatan ilmiah yaitu dengan melihat pola d ata runtun waktu yang ada terlebih dahulu. dengan melihat pola data awal yang di miliki maka akan memud ahkan dalam memi lih model yang sesuai untuk data tersebut. pendekatan lainnya adalah me nggunakan too ls untuk m enentukan secara otomatis bentuk model statistik arima yang sesuai dengan runtun waktu yang ada lalu model tersebut dilatih menggunakan quantum neural network agar diket ahui polapola d ata yang sudah ada d an d apat d iuji akurasinya.17 tipe model pola tipikal acf pola tipikal pacf ar p menurun secara ekspon ensial sinusoidal terputus s etelah lag p ma q terputus s etelah lag q menurun secara ekspon ensial sinusoidal arma p q menurun secara ekspon ensial sinusoidal menurun secara ekspon ensial sinusoidal 3.2 model arima bentuk u mum model ar ima dapat dinyatakan dalam p ersamaan berikut ............................... ................................ ................................. 1 operator ar adalah ............................................................2 operator ma adalah ............................................................ ............. .3 1. autore gressive integrated moving average arima not asi model arima p d q p orde untuk pros es autoregressive ar d orde yang menyatakan banyaknya proses dife rensi d ilakuk an pada data time series yang tidak stasione r q orde yang menyatakan proses moving a verage ma. pola teoretis acf dan pacf dari proses yang stasio ner sumber aswi dan sukarna 2006 2. tahapan analisis time series arima a. membuat plot time series identifikasi asumsi s tasione ritas data runtun waktu. suatu de ret pengamatan dikat akan stasioner apabila proses tidak berubah seiring dengan perubahan waktu tidak stasioner dalam mean jika trend tidak datar tidak sejajar smbu waktu tidak stasioner dalam varian jika trend datar atau hampir datar tetapi data tersebar membangun pola m elebar atau m enyempit pola t eromp et18 tidak stasioner dalam mean varians j ika trend tidak datar dan data memb entuk po la terompet. augmented di ckey fuller uji formal untuk stasion eritas hipotesis h0 terdapat akar unit dan data tidak st asioner 0 h1 tid ak terdapat akar unit dan data stasioner 0 span taraf signifik ansi α statistik uji ............................................................... ...4 ............................................................................................... .5 ............................................................................................... .......... .6 kriteria uji h0 ditolak jika nilai mu tlak dari augmented di ckey fuller nilai kritis mackinnon atau nilai prob . α. b. menghitung membu at plot acf dan pacf mengidentifikasi model runtun w aktu yang mungkin mengestimasi p arameter model c. uji signifik ansi parameter hipotesis h0 danatau parameter tidak signifik an terhadap model h1 danatau parameter signifik an terhadap model taraf signifik ansi α statistik uji danatau 19 kriteria uji tolak h 0 jika atau p value alpha d. verifikasi mo del independensi residual hipotesis h0 tidak ada korelasi antarlag h1 paling sedikit ada satu dengan k12 24 36 48 ada ko relasi antarlag statistik uji kriteria uji tolak h 0 jika atau p value alpha dengan m l ag maksim um s jumlah p arameter yang diesti masi dan taraf signifik ans normalitas residual hipotesis h0 residual berdistriusi norm al h1 residual t idak berdistribusi norm al statistik uji fungsi peluang kumulatif r esidual distribusi ku mulatif yang diobs ervasi dari suatu sampel acak sebanyak n o servasi kriteria uji tolak h 0 jika atau p value alpha ukuran ketepatan ra malan mod el dengan uku ran ketepatan p eramalan yang baik ad alah model yang menghasilkan error yang kecil. nilai teng ah kesalahan kuadrat mean square er ror 20 berikut flo wchart langkahlangkah membu at model arima gambar 1. flowch art analisis runtun waktu arima 3.3 model neural network dalam buku jaringan syaraf tiruan dan pemrogramannya menggunakan matlab drs. semua sinyal yang diberi pengali bobo t ini kemudian dijumlahkan satu sama lain untuk menghasilkan unit aktivasi. keduanya memiliki range antara 1 sampai 1. untuk fungsi hyperbolic tangent 3.4 model hybrid arima neural network berdasarkan hasil peramalan model arima akan dilakukan proses analisis runtun waktu menggunakan metode jaringan syaraf tiruan. dengan kata lain output dari peramalan model arima akan menjadi input pada proses pengolahan data menggunakan metode jaringan syaraf tiruan. kemudian akan ditentukan model jaringan syaraf tiruan yang sesuai dan cocok untuk data runtun waktu tersebut. secara matemat is hasil ramalan secara keselu ruhan yang diperoleh adalah sebagai berikut zt merupakan hasil pe ramalan yang merupakan gabungan nilai ramalan dari model arima atau exponential smoothing dan nilai ramalan dari model jst. berikut ini adalah arsitektur model peramal an hybrid arima jst dan es jst gambar 5 model hybrid arima jst dan hybrid es jst 3.5 model quantum hybrid arima neural network ada b anyak pendekatan untuk pengembangan model quantum arima nn. untuk memud ahkan aplikasi fo rmulir berikut fungsi ko mpleks dib erikan untuk menyatakan keadaan kuantum ............................................................... .................. ................ .8 adalah bilangan imaginer adalah k uantum fase 3.6 pengukuran kinerja 3.6.1 mean squared error dalam statistik mean squared error mse sebuah estimator adalah nilai yang diharapkan dari kuadrat error . error yang ada menunjukkan seberapa besar perbedaan hasil estimasi dengan nilai yang akan diestimasi. perbedaan itu terjadi karena adanya keacakan pada data atau karena estimator tida k mengandung informasi yang dapat menghasilkan estimasi yang lebih akurat 3.6.2 komparasi hasil peramalan setelah nilai mean squared error dari kedua metode didapatkan maka akan dilakukan komparasi terhadap nilai mse yang didapatkan pada periode testing out sample jika nilai mse statistika mse ann maka metode statistika memiliki performa lebih baik dibandingkan metode ann karena memiliki tingkat kesalahan relatif lebih kecil. sebaliknya jika mse statistika mse ann maka metode statistika memilki perform a lebih buruk dibandingkan metode ann karena tingkat kesalahan yang dihasilkan relatif lebih besar.
Kualifikasi_I Komang Sugiartha.txt,"3.1 Tahapan Penelitian
Penelitian ini memiliki fokus pengembangan model dalam melakukan prediksi terjadinya stunting pada suatu wilayah berbasis Generative Adversarial Networks. Gambar 3.1 adalah tahapan penelitian dalam membangun model prediksi terjadinya stunting pada suatu wilayah berbasis Generative Adversarial Networks. Pengembangan model pada penelitian ini dibagi menjadi beberapa tahapan, antara lain:
(a) Akusisi Data memperoleh informasi yang dibutuhkan dalam rangka mencapai tujuan penelitian. (c) Proses Pengembangan Model Prediksi Stunting berbasis Generative Adversarial Networks untuk memprediksi keadaan stunting pada suatu wilayah. Yaitu perbandingan kinerja pemodelan yang telah divalidasi sebelumnya kemudian dengan data uji, lalu mengaplikasikan data train dengan menciptakan prediksi berdasarkan data baru. 3.2 Faktor Indikator Penyebab Stunting
Faktor-faktor yang mempengaruhi terjadinya stunting pada balita terdiri dari beberapa faktor, diantaranya:
Hasil penelitian yang dilakukan oleh Umiyah and Hamidiyah (2021)menunjukkan bahwa ada hubungan antara berat badan lahir dengan kejadian stunting dengan nilai Pvalue = 0,009 (P < 0,05). Hasil penelitian yang dilakukan oleh Apriluana and Fikawati (2018)menunjukkan faktor status gizi dengan berat badan lahir < 2.500 gram memiliki pengaruh secara bermakna terhadap kejadian stunting pada anak dan memiliki risiko mengalami stunting sebesar 3,82 kali. Hasil penelitian yang dilakukan oleh Romadoniyah et al. Hasil penelitian yang dilakukan oleh Ariati (2019)menunjukkan prevalensi stunting sebesar 32,5 % dan balita Normal 67,5%. Hasil penelitian yang dilakukan oleh Al-Rahmad et al. (2013)diperoleh kejadian stunting pada balita disebabkan rendahnya pendapatan kelu- arga (p=0,026; OR=3,1), pemberian ASI tidak eksklusif (p=0,002; OR=4,2), pemberian MP-ASI kurang baik (p=0,007; OR=3,4), serta imunisasi tidak lengkap (p=0,040; OR=3,5).Dari hasil penelitian tersebut, penulis melakukan keterhubungan antara faktor-faktor penyebab stunting dengan indikator PIS-PK. Dari hasil keterhubungan faktor penyebab stunting dengan indikator PIS-PK, indikator yang digunakan ada 4 indikator. Indikator 11 (Keluarga mempunyai akses sarana air bersih)

3.3 Pengembangan Model
Beberapa tahap yang dilakukan dalam pengembangan model stunting diantaranya akuisisi data, preprocessing dan proses pengembangan model. Gambar menunjukkan proses pengembangan model yang dilakukan dalam penelitian. Proses Pengembangan Model Prediksi Stunting berbasis Generative Adversarial Networks untuk memprediksi keadaan stunting pada suatu wilayah. 3.3.3 Proses Pengembangan Model Prediksi Stunting
Pada tahap ini menjelaskan metode yang diusulkan menggunakan pendekatan kerangka Generative Adversarial Networks (GAN) untuk prediksi terjadi- nya stunting pada suatu wilayah. (2014) sebagai kerangka model deep learning untuk menangkap distribusi data pelatihan dengan menghasilkan data baru dari distribusi yang sama menggunakan model generator dan diskriminator. GAN akan menghasilkan lebih banyak ruang fitur yang dapat dimanfaatkan, sehingga mengurangi potensi kelebihan fitur selama pelatihan. Gambar 3.6 Metode arsitektur yang diusulkan Model G (Generator) dilatih untuk menghasilkan data yang terlihat seperti data persebaran data indeks keluarga sehat di setiap wilayah, sedangkan model D (Discriminator) dilatih untuk membedakan antara data dari Generator dan data nyata. Kesalahan dari D digunakan untuk melatih G untuk mengalahkan D. Persaingan antara G dan D memaksa D untuk membedakan secara acak dari variabilitas nyata, secara formal GAN menyelesaikan permainan min-max dengan persamaan berikut:
   Diskriminator mengeluarkan nilai D(x) yang menunjukkan kemungkinan bahwa x adalah data nyata dengan tujuan memaksimalkan peluang untuk mengenali data nyata sebagai data nyata yang dihasilkan sebagai data palsu. Di sisi generator, fungsi tujuan menggunakan model untuk menghasilkan D setinggi mungkin (x) nilai untuk membalikkan perbedaan. (2014) menampilkan sebuah generator dan diskriminator; generator G dilatih untuk menghasilkan sampel palsu yang dapat menipu pembeda D, sedangkan yang terakhir dilatih untuk membedakan antara sampel asli dan palsu. Diskriminator secara bersamaan berusaha meningkatkan kemampuannya untuk mengenali sampel nyata dengan memaksimalkan logD(x) ke 1, dan sampel palsu dengan memaksimalkan logD(1 - D(G(z))) ke 0. 3.4 Pengujian
Tujuan dari penelitian ini adalah membangun model prediksi berbasis Generative Adversarial Networks untuk memprediksi terjadinya stunting pada suatu wilayah yang dibutuhkan untuk mencegah terjadinya stunting dan mengoptimalkan perencanaan program pemerintah dalam penurunan terjadinya stunting."
Kualifikasi_Nur Azizah.txt,"3.1. Tahapan Penelitian
Penelitian ini berusaha mengembangkan model diagnosis Smear Negative Pulmonary Tuberculosis dengan metode Deep Learning menggunakan algoritma Faster R-CNN sebagai solusi dari masalah dan kekurangan dari teknik yang pernah dilakukan peneliti terdahulu yang dapat menghasilkan sebuah model diagnosis awal dan diagnosis akhir bagi pasien TB Negatif dengan menghasilkan tingkat akurasi tinggi. Rencana penelitian mencoba menggabungkan untuk mengembangkan deep learning dan mengembangkan metode yang dapat mengoptimalkan tingkat akurasi diagnosis. Sistem yang dibagun
Setelah mengumpulkan dataset, langkah selanjutnya adalah membangun sistem untuk pelatihan dan mengevaluasi jaringan yang dibuat. Sedangkan F1 score adalah rataan harmonic antara precision dan recall. Persamaan 2, 3, 4, dan 5 merupakan rumus untuk precision, recall, Fl-score, dan akurasi."
Kualifikasi_Rama Dian Syah.txt,3.1 tahapan penelitian tahapan penelitian dibagi atas beberapa tahapan yang dilakukan dari awal sampai akhir. tahapan dimulai dari studi literatur sampai analisis yang membentuk alur secara sistematis. tahapan penelitian ini terpada pada gambar 3.1 gambar 3.1 tahapan penelitian tahapan penelitian pada gambar 3.1 menjelaskan tahapan yang dilakukan pada penelitian ini. pada penelitian ini mengajukan pengembangan algoritma kriptografi citra digital dengan mengkombinasi teknik konfusi dengan algoritma cat map dan henon map serta teknik difusi dengan algoritma logistic map . pengembangan pada algoritma ini diharapkan dapat memiliki keamanan yang lebih tinggi dengan melalui beberapa parameter pengujian. citra asli dan kunci enkripsi menjadi input pada proses enkripsi. langkah pertama yaitu p engacakan piksel dilakukan dengan algoritma cat map menggu nakan persamaan 2.1 dan algoritma henon map menggunakan persamaan 2.5 dan 2.6. kemudian pembangkitan keystream dengan algoritma logistic map menggunakan persamaan 2.9. keystream yang dibangkitkan akan dilakukan operasi xor dengan piksel citra asli sehingga menghasilkan citra terenkripsi. pengembalian posisi piksel dengan algoritma henon map menggunakan persamaan 2.7 dan 2.8 serta algoritma cat map menggunakan persamaan 2.2 sehingga menghasilkan citra asli kembali. 3.3 pengujian tahapan pengujian dilakukan unt uk mengetahui hasil pada proses enkripsi dan dekripsi beberapa pengujian yang dilakukan yaitu 1. histogram histogram merupakan analisis statistik yang menunjukkan penyebaran atau distribusi piksel pada citra. kriptografi pada citra digital yang ideal memiliki distribusi nilai p iksel yang beragam benlashram et al. c dan c merupakan dua citra terenkripsi dengan dua kunci yang berbeda. 5. entropi entropi digunakan untuk mengukur keacakan pada citra. nilai entropi akan menunjukkan keacakan piksel pada citra terenkripsi elghandour et al. 21 𝐻𝑚 𝑃𝑚𝑖𝑙𝑜𝑔 21 𝑃𝑚𝑖 255 𝑖0 3.6 pada persamaan 3.6 terdapat m yang merupakan citra yang digunakan. n merupakan nilai piksel pada citra dan p merupakan probabilitas yang terjadi pada citra. citra terenkrpsi dengan nilai entropi yang men dekati 8 membuktikan keamanan yang baik pada citra te renkripsi lone et al. 2021 .
Kualifikasi_Remigius.txt,dalam proses pengembangan sistem pembelajaran arsitektur berbasis metaverse ini peneliti juga ingin menunjukkan perlunya keterlibatan komunitas dan persepsi pengguna bidang arsitektur agar si stem pembelajaran yang dihasilkan sesuai dengan kebutuhan dan harapan mereka dalam meningkatkan efektivitas pembelajaran arsitektur itu sendiri. diharapkan sistem pembelajaran arsitektur berbasis metaverse ini dapat memberi kemudahan kepada komunitas dose n dan mahasiswa dalam mempelajari berbagai sisi arsitektur dengan memasuki dunia virtual dan mereka dapat memahami materi yang diajarkan serta memecahkan permasalahan arsitektur yang dihadapi secara interaktif kolaboratif dan imersif dengan solusi tepat tanpa harus mencari berbagai referensi wujud nyata arsitektur di dunia fisik atau dunia nyata. dengan konsep pembelajaran matakuliah perkembangan arsitektur 1 yang dilakukan dengan metode metaverse p embelajaran secara online ini dapat dilakukan dengan lebih interaktif. 3.2 kerangka penelitian penelitian ini dilakukan dalam mencapai tujuan utama yaitu pengembangan sistem pembelajaran arsitektur berbasis metaverse terutama terkait perkembangan arsitektur. tahapan proses hasil 1. identifikasi topik pembelajaran menghimpun materi pembelajaran perkembangan arsitektur selama satu semester himpunan materi pembelajaran perkembangan arsitektur selama satu semester 2. konstruksi dunia visual melakukan konstruksi visual konstruksi fisik dan desain dunia fisik dan desain visual menyiapkan latar arsitektur dan tata letak dunia digital konstruksi avatar dan konten pembelajaran perkembangan arsitektur di dunia digital dunia virtual berbasis konten pembelajaran perkembangan arsitektur 3. penggunaan dunia nyata koneksi dunia virtual persistensi interaksi interaksi dunia virtual keterlibatan komunitas 31 4. efektivitas pembelajaran kolaboratif presensi imersi kehadiran dalam realitas yang disimulasi kapabilitas metaverse dalam membentuk lingkungan pengguna untuk memahami realitas pemahaman materi pembelajaran kemampuan memahami materi pembelajaran perkembangan arsitektur berbasis metaverse penelitian mengenai pengembangan sistem pembelajaran perkembangan arsitektur 1 berbasis metaverse ini dilakukan dengan melibatkan komunitas yang terdiri dari dosen dan mahasiswa di program studi s1 arsitektur jurusan teknik arsitektur. dalam mewujudkan sistem pembelajaran perkembangan arsitektur 1 berbasis metaverse ini peneliti juga melakukan konstruksi avatar dan konten pembelajaran perkembangan arsitektur 1 di dalam dunia digital sehingga terbentuk dunia virtual berbasis konten pembelajaran perkembangan arsitektur. dalam hal ini komunitas dosen dan mahasiswa ini melakukan koneksi ke dunia virtual berupa sistem pembelajaran perkembangan arsitektur berbasis metaverse dan semua jenis kegiatan yang dilakukan dalam menyelesaikan masalah dan menyediakan solusi yang diperlukan dapat tersimpan dalam basis data server sehingga dapat diambil kembali setiap kali mereka masuk dan terlibat kembali dalam sistem pembelajaran virtual kolaboratif ini. dalam sistem pembelajaran virtual kolaboratif ini komunitas dosen dan mahasiswa dapat berinteraksi satu sama lain dalam penyelesaian masalah yang ada dan mencari solusi yang diperlukan sehingga mereka benar benar dapat hadir dan terlibat di dalam sistem pembelajaran perkembangan arsitektur 1 berbasis metaverse secara intens if interaktif dan imersif. keempat pada aspek outcome persepsi pengguna juga dieksplorasi dan dievaluasi tentang ketercapaian tujuan dari pembelajaran perkembangan arsitektur 1 berbasis metaverse sesuai dengan kriteria dan indikator yang ditetapkan dosen pengampu. 3.3 pendekatan penelitian penelitian ini dilakukan menggunakan pendekatan kuantitatif eksperimental terhadap sistem pembelajaran perkembangan arsitektur berbasis metaverse yang dikembangkan dalam komunitas dosen dan mahasiswa jurusan teknik arsitektur program studi s1 arsitektur universitas gunadarma. sistem pembelajaran berbasis metaverse ini dikembangkan sesuai dengan materi pembelajaran perkembangan arsitektur 1 di kalangan mahasiswa jurusan teknik arsitektur semester 3 . apabila pengembangan sistem pembelajaran ini s udah selesai model pembelajaran berbasis metaverse tersebut diuji validitas dan reliabilitasnya dengan melibatkan penilaian objektif dan otoritatif dari para ahli 35 baik ahli materi maupun media pembelajaran. efektivitas pembelajaran kolaboratif berbasis metaverse dalam penelitian ini dievaluasi dengan melihat peningkatan pemahaman mahasiswa mengenai materi pembelaja ran perkembangan arsitektur sesuai dengan kriteria dan indikator yang ditetapkan oleh dosen pengampu. dari hasil uji efektivitas sistem pembelajaran ini diharapkan dapat diketahui sejumlah kelebihan dan kekurangannya sehingga dapat dijadikan sebagai baha n pertimbangan rekomendasi dalam meningkatkan kualitas sistem pembelajaran perkembangan arsitektur berbasis metaverse tersebut.
MetaMeysawati_KUALIFIKASI(99216026).txt,"Penelitian ini berusaha untuk mendapatkan klasifikasi genre film di Indonesia berdasarkan dari penayangan film atau dikenal dengan istilah cuplikan (thriller) yang dapat membantu para peminat film agar dapat membedakan genre Komedi, Drama/Romantis, Aksi, Horor dan Thriller. Rencana penelitian mencoba untuk menggabungkan teknik dan mengembangkan algoritma yang dapat menghasilkan klasifikasi film ke dalam lima genre Komedi, Drama/Romantis, Aksi, Horor dan Thriller sehingga didapat metode yang menghasilkan nilai akurasi yang maksimal. Rencana yang akan dilakukan menggunakan modifikasi dengan menggunakan metode yang sama pada penelitian (Shafirra, 2010) untuk mengklasifikasikan data teks yang telah berhasil di dapat menggunakan metode Support Vector Mechine (SVM). Kelima metode tersebut dapat dijadikan kombinasi untuk klasifikasi genre dari film."
Miftakhul Zaen_KUALIFIKASI.txt,3.1 tahapan penel itian dalam penelitian mengenai pengembangan algoritma dbscan dengan kuantum terdapat langkahlangkah yang dilakukan seperti pada gambar 3.1. langkah langkah yang dilaukan d iantaranya yaitu pengumpulan data definisi qubits kriteria inisialis asi sistem kuantum hingga evaluasi klaster. data definisi qubits kriteria inisialisasi sistem kuantum penentuan eps dan minpts kuantum identifikasi core supplier dengan kuantum sirkuitidentifikasi noise supplier dengan kuantum sirkuit penanganan noise dengan kuantum stateformasi klaster supplier dengan kuantum measurementimplementasi quantum distance measure identifikasi core supplier dengan kuantum sirkuit evaluasi klaster1 2 3 4 5 6 9 10 117 8 gamb ar 3.1 tahapan penel itian 1. data tahap awal dalam penelitian di awali dengan pembuatan data dimana data yang digunakan pada penel itian ini adalah data s intetik. data sintetik digunakan untuk mendapatkan jumlah data yang besar sela in itu data sintetik juga b ersifat fleksibel kar ena ju mlah data yang digunakan dapat ditentukan sesuai dengan kebutuhan pengujian algo ritma yang dikembang kan. data sintetik yang dibuat berisikan nama supplier harga kualitas dan waktu pengiriman. 2. definis i qubits kriteria pada taha p ini kriteria yang digunak an untuk pengelompokan supplier diubah menjadi representasi kuantum menggunakan qubits. 10. formasi kluster supplier dengan quantum measurement pada tahapan ini m embentukan klaster supplier dengan mengukur state kuantum yang telah diubah melalui interaksi antar qubits yang mewakili supplier . tahapan ini bertujuan untuk menilai seberapa baik kluster yang terbentuk mengguna kan. 3.2 rangkuman langkah langk ah penelitian setelah mengembangkan algoritma kuantum dbscan selanjutnya membandingk annya dengan algo ritma dbscan untuk mengetahui seberapa baik algoritma dbscan jika dibandingkan dengan algorit ma klasiknya . data definisi qubits kriteria inisialisasi sistem kuantum penentuan eps dan minpts kuantum identifikasi core supplier dengan kuantum sirkuitidentifikasi noise supplier dengan kuantum sirkuit penanganan noise dengan kuantum stateformasi klaster supplier dengan kuantum measurementimplementasi quantum distance measure identifikasi core supplier dengan kuantum sirkuitnormalisasi data penentuan epsilon dan minpts hitung jarak antar supplier identifikasi core supplier identifikasi core supplieridentifikasi noise supplier supplier tidak termasuk dalam klasterformasi klaster supplier evaluasi klasterusulan algoritma gambar 3.2 rangkuman langkah langkah prosedur penelitian
Octaviani Hutapea_UK.txt,"3.1 Tahapan Penelitian
      Penelitian ini berusahan mengembangkan sistem Identifikasi Jenis dan Tingkat Kerusakan Jalan Serta Sebarannya Menggunakan Model Convolutional Neural Network dengan tahapan penelitian awal adalah analisis kebutuhan dari Direktorat Jenderal Bina Marga mengenai jenis kerusakan jalan serta tingkat kerusakannya. Akuisisi dan Analisis data dalam pengambilan citra kerusakan perkerasan jalan dilakukan mengikuti prosedur pedoman survei yang dikeluarkan Direktorat Jenderal Bina Marga. Pembentukan data set dilakukan dengan melabeli setiap citra yang dilakukan oleh pakar berdasarkan jenis dan tingkat kerusakan. Setelahnya dilakukan augmentasi data terhadap data set citra untuk mengurangi overfitting dengan cara meningkatkan dataset. Pembentukan model identifikasi jenis kerusakan dan tingkat kerusakan dibuat menggunakan model CNN dengan data latih yang diambil 90% dari dataset yang sudah dibuat. Pengujian dan validasi sistem dilakukan dengan memanfaatkan matrik konfusi untuk menghitung akurasi dari model yang sudah dibuat. Penentuan Koordinat Sebaran Kerusakan Pada Jalan akan dilakukan dengan penetapan titik koordinat setiap kerusakan pada peta lokasi. 3.2 Analisis kebutuhan
      Jenis kerusakan yang dilakukan pencatatanya pada tabel komponen perkerasan berdasarkan pedoman survei pengumpulan data kondisi jaringan jalan yang dikeluarkan oleh Direktorat Jenderal Bina Marga sebagai berikut:
1) Retak Permukaan:
a) Retak Kulit Buaya
b) Retak Tepi
c) Retak Refleksi Sambungan
d) Retak Selip
2) Lubang
3) Alur
      Berdasarkan Indeks Kondisi Perkerasan tingkat kerusakan perkerasan jalan dalam Pedoman Bahan Konstruksi Bangunan Dan Rekayasa Sipil dibagi menjadi tiga tingkatan yaitu Rendah, Sedang, dan Tinggi untuk setiap masing-masing jenis kerusakan dijelaskan dalam tabel 3.1 (Nono & Hamdani, 2016). 3.3 Akuisisi dan Analisis Data
      Akuisisi data citra kerusakan dilakukan dengan menggunakan gambar video atau gambar digital yang berkoordinat, berikut merupakan beberapa syarat yang harus dipenuhi dalam proses akuisisi data citra berdasarkan pedoman survei pengumpulan data kondisi jaringan jalan yang dikeluarkan oleh Direktorat Jenderal Bina Marga:
1) Kamera yang digunakan harus dapat menghasilkan gambar digital dengan resolusi kamera minimum 1280*1920 pixel (setara dengan full HD video). 3.4 Pembentukan Dataset
      Pembentukan dataset diawali dengan pelabelan data oleh pakar ke dalam tiga jenis yang akan diidentifikasi berdasarkan tingkat kerusakannya. 3.7 Pengukuran Tingkat Kerusakan
      Perkerasan pada ruas yang telah dipilih dibagi menjadi beberapa unit perkerasan. Survei dilakukan secara visual dan data yang dinilai dan dicatat pada saat suvei tiap unit sampel adalah jenis, tingkat keparahan, dan kuantitas kerusakan perkerasan. Formulir survei atau alat yang dapat merekam sekurang-kurangnya informasi sebagai berikut: tanggal, lokasi, ruas, seksi, ukuran unit sampel, jumlah dan ukuran panel, jenis, tingkat keparahan, dan kuantitas kerusakan, dan nama-nama petugas survei. Berdasarkan Indeks Kondisi Perkerasan cara pengukuran tingkat kerusakan perkerasan jalan dalam Pedoman Bahan Konstruksi Bangunan Dan Rekayasa Sipil Penilaian kondisi perkerasan dilakukan untuk setiap lajur jalan, dengan arah pengukuran 2 (dua) arah. 3.8 Penentuan Koordinat Sebaran Kerusakan Pada Jalan
      Penentuan Koordinat Sebaran Kerusakan Pada Jalan akan dilakukan dengan penetapan titik koordinat setiap kerusakan pada peta lokasi. Digambarkan dalam diagram alur di bawah ini. 3.9 Analisis dan Evaluasi Hasil
      Hasil pengukuran yang dihasilkan dari citra jenis kerusakan jalan perkerasan akan dianalisis dan evaluasi dengan pengolahan data secara manual yang dikerjakan oleh binamarga serta dihitung kembali akurasi dari pengukuran tingkat kerusakan tersebut."
Prameswari Rizcha Julianda_Kualifikasi.txt,"3.1 Tahapan Penelitian
       Tahapan yang dilakukan pada penelitian ini secara garis besar terdapat tiga kelompok tahapan yang mana luaran akhirnya adalah membangun sebuah model. Berikut beberapa tahapan tersebut yang dapat dilihat pada Gambar 3.1. Penjelasan lengkap mengenai tahapan penelitian secara lengkap dapat dilihat pada subbab berikutnya. Pustaka atau referensi yang terkait dengan penelitian ini meliputi teori tentang stres, manajemen stres, analisis SWOT atau dalam hal ini fokus pada personal SWOT analysis, kemudian tentang balanced scorecard yang dalam hal ini fokus pada personal balanced scorecard, hingga Multi-criteria decision making (MCDM) yang mana dalam hal ini adalah Analytical Hierarchy Process (AHP). 3.3 Menentukan Objek Penelitian
       Penelitian yang dilakukan merupakan penelitian kuantitatif dengan pendekatan fenomenologis yang mana fenomena yang menjadi fokus penelitian ini adalah pada saat mahasiswa tingkat akhir dihadapkan pada penyusunan Capstone Design Project dan Skripsi. Objek penelitian yang diteliti dalam penelitian ini adalah mahasiswa tingkat akhir Program Studi Teknik Industri Universitas Gunadarma. Terdapat 315 populasi mahasiswa tingkat akhir Program Studi Teknik Industri Gunadarma yang mana semuanya akan menjadi objek dalam penelitian ini. Penelitian ini menggunakan instrumen pengumpulan data berupa kuesioner, yaitu kuesioner tingkat stres dan kuesioner stresor mahasiswa dengan skala Likert 1 - 5. Kuesioner dibuat berdasarkan kebutuhan data yang akan dieksplorasikan dalam penelitian, yaitu mengidentifikasi tingkat stres mahasiswa dan mengungkap penyebab stres pada mahasiswa. K-Nearest Neighbor (K-NN) adalah suatu metode yang menggunakan algoritma super-vised dimana hasil dari query instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori pada K-NN. Pada mahasiswa tingkat akhir, peluang ini dapat diidentifikasikan seperti pemanfaatan teknologi dan informasi sehingga dapat mencapai tujuan yaitu lulus tepat waktu. Pada mahasiswa tingkat akhir, ancaman ini dapat diidentifikasikan seperti persaingan teman seangkatan yang akan meneliti tema yang sama, dan lain sebagainya. Pada mahasiswa tingkat akhir, contoh perspektif internal ini adalah kejujuran dari seorang mahasiswa dalam kaitannya dengan menyontek pekerjaan orang lain
2. Contoh: Tingkat komunikasi seorang pegawai dengan atasan, rekan kerja atau customer. Pada mahasiswa tingkat akhir, contoh perspektif eksternal adalah tingkat komunikasi mereka dengan orang tua atau wali, dosen, senior atau junior, dan teman seangkatan atau sepermainan. Pada mahasiswa tingkat akhir, contoh perspektif pengetahuan dan pembelajaran adalah pengetahuan dan penguasaan perangkat lunak sebagai alat bantu dalam mengerjakan tugas kuliah
4. Contoh: Penghematan Penggunaan Dana Perusahaan Dalam Setiap Proyek. Pada mahasiswa tingkat akhir, contoh perspektif keuangan adalah pengalokasian penggunaan uang yang diberikan oleh orang tua. 3.8 Penentuan Key Performance Indicator (KPI) dan Analisis Output
       Hasil luaran dari Personal Balanced Scorecard kemudian selanjutnya akan ditentukan Key Performance Indicator yang mana mempertimbangkan hasil dari luaran Personal SWOT Analysis dan hasil dari identifikasi stres dan stressor. KPI ini selanjutnya akan dilakukan pembobotan dengan menggunakan Analitycal Hierarchy Process (AHP) dengan perangkat lunak Expert Choice yang nantinya akan menghasilkan model baru dari manajemen mahasiswa tingkat akhir dalam kaitannya dengan keahlian menyelesaikan masalah, atau dalam hal ini adalah stressor."
Proposal Disertasi Kualifikasi Adam Huda Nugraha.txt,"3.1 Kerangka Metode Penelitian yang diusulkan
       Penelitian ini bertujuan untuk mendapatkan model arsitektur LSTM untuk suara digit desimal berbahasa Indonesia sehingga tingkat akurasi pengenalan dapat lebih tinggi dibandingkan dengan penelitian sebelumnya. Akurasi atau tingkat akurasi pengenalan adalah jumlah ketepatan suara digit desimal yang diucapkan dibagi dengan jumlah data suara yang akan diterjemahkan menjadi teks, sehingga semakin tinggi akurasi pengenalan menunjukkan semakin tingginya hasil ketepatan sistem menterjemahkan sinyal-sinyal suara ini menjadi teks. Model LSTM telah diterapkan untuk pengenalan suara berbahasa Bengali (Nahid, et al. 2016) (LSTM Daneshvar), kedua model ini diteliti untuk dibandingkan tingkat akurasi pengenalan dengan model yang diusulkan untuk mengenali digit desimal bahasa Indonesia. 3.2 Pengumpulan Data Latih Dan Data Uji
       Tahapan penelitian dilakukan seperti yang terlihat pada gambar 3.1, meliputi tahap pengumpulan data, tahap pra-pengolahan data yang mengektraksi fitur suara menggunakan spektrogram atau MFCC, tahap pembelajaran untuk mencari model, tahap klasifikasi untuk menguji model yang dihasilkan dan evaluasi dengan menghitung akurasi pengenalan. Pada tahap pengumpulan data, dilakukan perekaman data suara digit desimal berbahasa Indonesia mulai dari digit 0 hingga digit 9, kemudian dilakukan pemotongan data suara sesuai dengan digit yang diucapkan dan dilanjutkan dengan pemberian label untuk setiap file digit yang disimpan. Untuk melihat tingkat akurasi, pengujian kemudian dilakukan terhadap DTW, HMM, RNN, model LSTM Daneshvar, model LSTM Nahid dan model LSTM yang diusulkan. Data digit suara 0-9 yang digunakan dalam penelitian ini diambil di dalam ruang kelas ujian komputer Universitas Gunadarma. Pada minggu pertama perekaman, didapatkan 100 pembicara masing-masing mengucapkan digit 0 sampai 9 sehingga terdapat 1000 data suara. 1000 data ini kemudian dilakukan proses klasifikasi LSTM menggunakan fitur spektrogram dengan 900 data latih dan 100 data uji, akurasi yang dihasilkan masih rendah sekitar 60%. Klasifikasi LSTM yang dihasilkan dari 3000 data ini meningkat menjadi 70%. Perekaman dilanjutkan dengan mendapatkan 5000 data suara dari 500 pembicara, hasil klasifikasi LSTM meningkat menjadi 80%. Algoritma untuk mengekstraksi ciri dari nilai-nilai amplitudo ini kemudian dikembangkan agar mempermudah proses pengenalan sinyal suara. 2014) untuk pemodelan dengan LSTM dengan harapan agar LSTM dapat menemukan sendiri model akustik dari pembicara tanpa perlu fitur ekstraksi ciri seperti MFCC, sehingga spektrogram juga digunakan pada penelitian pengenalan suara digit desimal berbahasa Indonesia ini saat proses pra-pengolahan data. Jumlah sample didapatkan dari formula berikut:
Sample = sample-rate x time (detik)
=8000 x 0.02 =160
Pada gambar 3.3 diperlihatkan hasil pemotongan dari 20 milidetik atau 160 sample sinyal suara setelah dilakukan proses windowing dengan menggunakan hamming. 3.3.2 Mel-Frequency Ceptral Ceptrum (MFCC)
       Ekstraksi ciri pada dasarnya adalah mengubah sinyal-sinyal amplitudo atau spektrogram menjadi hanya beberapa vektor koefisien yang diperkirakan mengandung informasi yang penting. Pada gambar 3.5, menunjukkan plot hasil 64 koefisien FFT kemudian dilakukan perhitungan mel-frekuensi dengan 12 parameter. 3.4 Dynamic Time Warping (DTW)
       Untuk melakukan pengenalan suara dengan algoritma DTW, sebelumnya dilakukan pengambilan fitur ciri spektrogram atau MFCC dari 7990 data latih, begitu juga dengan 790 data uji. Koefisien spektrogram atau MFCC tersebut kemudian yang dijadikan data matriks untuk pengenalan dengan DTW. Terdapat 12 koefisien yang dihasilkan jika menggunakan MFCC, dan 64 koefisien jika menggunakan spektrogram. Setiap sel distance matriks (D) berisi kombinasi antara jarak euclidean dan bobot 
antara sel A dan sel B. Proses ini dihitung hingga semua sel matriks D terisi. Jarak DTW antara matriks A dan matriks B kemudian didapatkan dari sel matriks D yang berada di sel terakhir dari distance matriks atau di sel pada baris 83 dan kolom 52."
Ragiel.txt,"Berdasarkan dari penelitian tersebut, peneliti mengusulkan model paralelisasi antara proses transformasi subbyte dan shiftrows, sehingga diharapkan penggunaan resources dapat lebih efisien, mengoptimalkan kecepatan serta memberikan tingkat keamanan yang tinggi. Diagram alur pada gambar 3.1 menunjukkan usulan peneliti untuk melakukan pengembangan algoritma AES dengan model paralelisasi antara transformasi subbytes dan shiftrows. Adapun proses untuk algoritma AES sebagai berikut :
1. Proses ini terjadi pada putaran terakhir dalam algoritma AES. Setelah proses ini selesai maka akan menghasilkan sebuah cipher text."
Ragmar Faikar Eka_Kualifikasi.txt,tahapan penelitian dijelaskan dalam bentuk flowchart sehingga dapat menjelaskan proses yang dilakukan mulai dari studi literatur sampai dengan kesimpulan jadwal dan estimasi penelitian digambarkan dalam bentuk time table untuk menjadwalkan dan melakukan estimasi waktu dari tiap tahap yang dilakuk an. 3.1.2 pengumpulan data tahap kedua yaitu pengumpulan data data yang digunakan pada penelitian ini adalah data citra digital kelapa sawit dengan tingkat kematangan belum matang setengah matang matang terlalu matang dan tandan buah yang kosong . gambar asli akan dilakukan resize menjadi ukuran 224x224 piksel lalu data tersebut akan di augmentasi untuk memperbanyak dan m emvariasi data agar dan hasil augmentasi akan dijadikan sebagai data latih untuk model yang dibuat. 32 3.1.4 pembuatan model tahap keempat yaitu pembuatan model machine learning menggunakan mobilenetv3 smalllarge dan menggabungkannya dengan attention module cbam convolutional block attention module. proses pembuatannya meliputi pembuatan tampilan user memasukan tflite ke dalam aplikasi sehingga aplikasi dapat menggunakan model machine learning untuk mengklasifikasi kematangan kelapa sawit menggunakan kamera smartphone. tahun pertama tahun kedua tahun ketiga studi literatur evaluasi model submit jurnal pertama pembuatan proposal bab 1 sampai bab 3 deploy dimplementasi model pembuatan jurnal kedua pengumpulan dataset pembua tan aplikasi submit jurnal kedua preprocessing data menulis hasil penelitian bab 4 pembuatan model pengujian dan evaluasi aplikasi melatih model menulis hasil penelitian bab 4 dan bab 5 pembuatan jurnal pertama tabel 2. kegiatan penelitian kegiatan yang dilakukan pada tahun pertama yaitu melakukan studi literaur untuk pembuatan proposal penelitian bab 1 sam pai bab 3 lalu dilanjutkan dengan pengumpulan dan preprocessing data setelah mendapatkan data kegiatan pembuatan dan pelatihan m odel dapat dilakukan. pada tahun kedua dilakukan evaluasi model dan saat hasil evaluasi model sudah cukup baik model akan di deploy untuk dapat diimp lementasi ke dalam aplikasi yang sudah dibuat. aplikasi akan dievaluasi dan diuji kinerjanya sehingga mendapatkan kesimpulan dari peneli tian untuk ditulis da lam b ab 4 sampai bab 5 . pada akhir tahun kedu a setelah mendapatkan kesimpulan penelitian dilakukan pembuatan jurnal pertama dan dilanjutkan pada tahun ketiga untuk pembuatan jurnal kedua.
Reza Al Husna_Kualifikasi.txt,3.1 tahapan penelitian secara garis besar penelitian ini terdiri dari beberapa tahapan yaitu akuisisi data preprocessing data pengembangan dan pelatihan model pengujian dan evaluasi model serta pengembangan system deteksi penyakit daun kakao ditunjukkan pada gambar 3.1. gambar 3.1 tahapan penelitian 3.2 akuisisi data penyakit daun tanaman kakao pengumpulan citra penyakit daun tanaman kakao dikumpulkan secara langsung oleh peneliti data primer dan juga menggunakan data yang dikumpulkan oleh peneliti lain data sekunder. terdapat 4 kelas penyakit dan satu kelas daun sehat yang akan digunakan dalam penelitian ini yaitu daun sehat penyakit antraknosa colletotrichum gloeosporioides penyakit vascular streak dieback vsd penyakit leaf blotch dan penyakit cocoa swollen shoot virus disease cssvd. 46 gambar 3.2 contoh 4 jenis penyakit daun tanaman kakao dataset primer akan dilakukan pengambilan foto penyakit daun tanaman kakao yang terdapat pada kebun kakao di daerah kabupaten solok provinsi sumatra barat. dataset sekunder meng gunakan dataset yang telah digunakan u mum oleh para peneliti lain terkait penyakit daun tanaman kakao. 3.4 pengembangan dan pelatihan model data citra daun kakao yang telah melalui preprocessing dan ekstraksi fitur kemudian digunakan untuk pelatihan dan pembuatan model deep learning menggunakan pendekatan feature fusion berbasis attention yaitu fitur ekstraksi hog dan lbp digabungkan ke dalam vision transformer yang menggunakan attention mechanism . penggunaan attention mechanism dapat meningkatkan akurasi model dengan mengurangi pengaruh noise atau informasi yang tidak relevan dalam gambar. gambar 3.3 pengembangan dan pelatihan model 3.5 pengujian dan evaluasi model pengujian dan evaluasi model dilakukan untuk me lihat akurasi model saat mengidentifikasi penyakit daun tanaman kakao. 4 pengujian dan evaluasi model 3.6 pengembangan sistem deteksi penyakit daun kakao setelah melakukan pelatihan dan pengembangan model serta tahap pengujian dan evaluasi model system deteksi untuk penyakit daun kakao diimplemetasikan dengan melibatkan pengintegrasian model ke dalam aplikasi atau perang kat keras. pengembangan system menciptakan solusi yang efektif dan efisien dalam mengidentifikasi penyakit daun kakao. 5 alur identifikasi penyakit tanaman kakao
Riezka Yunistika Fajriatifah_Kualifikasi.txt,"Dalam disertasi ini diusulkan metode klasifikasi anomali paru akibat Covid- 19 pada citra sinar-X paru yang terbagi ke dalam tiga tahap utama, seperti yang ditunjukkan pada Gambar 3.1. Gambar 3,1 menggambarkan peta penelitian yang terdiri dari tiga tahap utama: pelatihan model tunggal, ensemble learning dengan metode stacking dan implementasi model. Pada tahap preprocessing dilakukan augmentasi citra untuk meningkatkan variasi data pada dataset. Tahapan selanjutnya adalah pembuatan dan pelatihan model. Proses pembuatan dan pelatihan model memanfaatkan teknik transfer learning pada beberapa arsitektur (ConvNext, DenseNet, Vgg19, ResnetV2_50 dan Xception). Teknik transfer learning bertujuan untuk mempercepat proses pelatihan dengan mentransfer bobot dari model yang sudah dilatih sebelumnya. Proses pelatihan juga memanfaatkan teknik penjadwalan learning rate, agar model dapat mencapai nilai gradient optimal sehingga mendapatkan performa model terbaik dari proses pelatihan. Tahap berikutnya adalah mengevaluasi model yang telah dilatih menggunakan dataset uji yang tidak digunakan selama proses pelatihan. Evaluasi model dilakukan dengan mengukur performanya menggunakan classification metric seperti accuracy, precision, recall, danfl-score. Hasil dari proses ini berupa beberapa model terbaik. Hasil prediksi dari beberapa model ini disimpan untuk digunakan pada tahap kedua. Pada tahap ini hasil prediksi dari berbagai model tunggal yang telah dilatih sebelumnya dikumpulkan. Hasil prediksi ini mencakup probabilitas atau kelas yang diprediksi oleh masing-masing model tunggal untuk setiap sampel dalam dataset. Hasil prediksi yang dikumpulkan digunakan untuk membentuk dataset baru. Dataset baru ini terdiri dari fitur-fitur yang merupakan prediksi dari model-model tunggal sebelumnya, serta label asli dari dataset. Dataset baru ini bertujuan untuk merepresentasikan pola dan kesalahan yang dihasilkan oleh model- model tunggal. Meta-model bertujuan untuk belajar dari kesalahan dan pola prediksi yang dihasilkan oleh model-model tunggal. Meta-model akan menggabungkan prediksi dari berbagai model tunggal untuk menghasilkan prediksi akhir yang lebih 
akurat. 3.1 Dataset COVID-19 Radiography
       Dataset COVID-19 Radiography adalah kumpulan data citra sinar-X paru- paru. Dataset ini terdiri dari 21165 citra sinar-X paru yang dikategorikan menjadi 4 kelas seperti: 3616 citra sinar-X positif Covid-19, 10119 citra normal, 6012 citra lung opacity dan 1345 citra pneumonia virus. Gambar 3.2 menunjukkan beberapa citra sinar-X paru COVID-19, viral pneumonia, lung opacity dan normal yang disediakan oleh database Radiografi COVID-19. Gambar 3.2 di atas merupakan contoh sampel citra dari dataset Radiograph COVID-19. Citra sinar-X Normal memiliki paru-paru yang bersih tanpa bintik- bintik putih, yang menandakan bahwa paru-paru tidak mengalami peradangan atau infeksi. Citra sinar-X COVID-19 memiliki ciri khas berupa bintik-bintik putih, yang menunjukkan adanya cairan dan menandakan infeksi pada paru-paru. Citra sinar-X Lung Opacity menunjukkan ciri khas berupa bintik-bintik putih keabu- abuan. Pada citra sinar-X Viral Pneumonia, terdapat bintik-bintik putih pada bagian atas paru-paru yang menunjukkan infeksi pada saluran pernapasan atas. Tahap selanjutnya adalah mengubah ukuran (resize) semua citra dalam dataset agar sesuai dengan kebutuhan model. Pada tahap preprocessing dataset, dilakukan augmentasi citra dengan memutar citra-citra dalam dataset untuk menghasilkan variasi gambar yang terotasi. Hasil augmentasi citra ini kemudian diubah ukurannya menjadi dimensi yang sama. Tahap selanjutnya adalah normalisasi citra menggunakan nilai rata-rata dan standar deviasi dari dataset ImageNet, penggunaan nilai tersebut memiliki tujuan agar hasil normalisasi sama dengan proses pelatihan bobot untuk transfer learning model. Pembagian ini dilakukan untuk mengurangi beban pelatihan yang berat dan validasi hasil pelatihan dapat dilakukan secara perbatch. 3.4 Pembuatan dan Pelatihan Model
3.4.1 Model ConvNeXt
       Model ConvNeXt adalah salah satu variasi dari arsitektur Convolutional Neural Network (CNN). Tahap ini berperan penting dalam proses pengklasifikasian suatu hasil feature extraction menjadi suatu output prediksi. Pada tahap fine tuning, nilai learning rate yang kecil digunakan agar bobot pra-pelatihan model tidak mengalami perubahan signifikan yang dapat mengakibatkan hasil pelatihan model kurang baik. Gambar 3.8 merupakan rancangan alur pelatihan dalam penelitian ini. Alur pelatihan model terbagi menjadi 3 tahapan yakni proses inisialisasi, proses pelatihan dan proses log hasil pelatihan. Proses validasi performa model merupakan proses dimana hasil pelatihan model tiap peoch dilakukan validasi pada data validasi yang telah disiapkan sebelumnya dengan cara mengukur nilai loss dan juga performa metric yang meliputi accuracy, recall, precision, dan f1-score. Hal ini bertujuan agar model dapat memperbaharui nilai bobot pengetahuan dengan cara melakukan backpropagation berdasarkan nilai loss dan akan menyimpan file model terbaik berdasarkan hasil validasi metrik performa pada data validasi. Proses ketiga adalah log hasil pelatihan model yang meliputi menyimpan hasil output model dan juga log pelatihan menggunakan Comet logger. Comet logger akan menyimpan nilai hasil pelatihan seperti loss, learning rate, dan metrik performa pada platform CometML untuk visualisasi grafik pelatihan model. Proses penyimpanan log ini juga bertujuan agar dapat membandingkan berbagai hasil pelatihan dengan konfigurasi berbeda sehingga mendapatkan model dengan performa terbaik berdasarkan metrik performa pada data validasi. Hasil pengujian model akan diukur menggunakan beberapa metrik performa, seperti akurasi, presisi, recall, dan fl-score. Penggunaan metrik accuracy memiliki tujuan untuk mengetahui tingkat keakuratan prediksi yang dibuat oleh model terhadap data uji. Recall dihitung dengan membandingkan jumlah prediksi yang benar-benar positif yang berhasil diidentifikasi oleh model dengan jumlah total kasus positif sebenarnya dalam kelas tersebut. Penggunaan F1-score bertujuan untuk menilai keseimbangan antara precision dan recall dalam performa model klasifikasi. 3.8 Metode Stacking
      Stacking atau Stacked generalization merupakan teknik ensemble learning yang mengintegrasikan prediksi dari beberapa model untuk menghasilkan prediksi yang lebih akurat. Model dasar adalah beberapa model tunggal yang masing masing dilatih menggunakan dataset yang sama. Output dari setiap model dasar, yang merupakan prediksi digunakan sebagai input untuk meta-model. Meta-model adalah model yang dilatih pada prediksi yang dihasilkan oleh model dasar. Model ini berfungsi untuk mempelajari bagaimana prediksi dari model dasar dapat digabungkan untuk menghasilkan prediksi akhir yang lebih akurat. Tahap pertama, hasil prediksi dari beberapa model dasar dikumpulkan. Hasil prediksi yang dikumpulkan dari model-model dasar kemudian digunakan untuk membentuk dataset baru. Setelah dataset baru dibuat, yang terdiri dari prediksi dari model-model dasar sebagai fitur, langkah selanjutnya adalah pelatihan meta-model. 3.9 Evaluasi Model Stacking
       Pengujian model stacking dalam penelitian ini dilakukan seperti pengujian pada model tunggal. 3.10 Inference dan Deployment
       Inference dan Deployment adalah tahap di mana model terbaik yang telah melalui proses pengujian digunakan untuk membuat prediksi pada data baru. Inference dilakukan melalui sebuah API yang dibuat menggunakan framework Flask untuk menjalankan model. API yang dibuat menerima file citra X-ray dada sebagai input. Output dari API ini adalah hasil prediksi yang mencakup nama kelas dan confidence score untuk citra tersebut. confidence score ini mengindikasikan tingkat keyakinan model terhadap prediksinya."
RizqiaCahyaningtyas_Kualifikasi.txt,"3.1. pengamatan yang dilakukan adalah tentang kerusakan pada modul 
surya, yang dilanjutkan dengan akuisisi dan analisis dataset dimana data dataset didapatkan 
dari website Kaggle (www.kaggle.com) yang berjumlah 559 data yang berformat .jpg. Dataset kemudian 
dibagi menjadi 3 yaitu data latih untuk kebutuhan pelatihan model, data validasi untuk proses 
validasi dan penyetelan model, data uji untuk evaluasi model yang akan menghitung 
confusion matrix guna mencari nilai akurasi, presisi, recall, dan hasil F1-score. 3.2 Identifikasi Masalah
Institut Teknologi PLN memiliki laboratorium Pembangkit Listrik Tenaga Surya (PLTS) yang memiliki kapasitas 12.4 kWp, yang dalam sehari energi yang dihasilkan sekitar 40 - 60 kWh. Untuk memudahkan dalam pemeliharaan modul surya perlu dibuatkan sistem untuk deteksi, klasifikasi dan identifikasi kerusakan modul surya yang sering terjadi, diantaranya karena faktor lingkungan, kerusan produksi, korosi dan penuaan alami. kerusakan pada modul surya diklasifikasikan menjadi 4 kelas yaitu Clean, dusty , electrical damage, Physical damage. Arsitektur dan Model CNN (Convolutional Neural Network)
       Proses pemodelan dilakukan dalam beberapa kali percobaan dengan ukuran batch size yang berbeda sehingga diperoleh model klasifikasi jenis kerusakan modul surya dengan hasil akurasi terbaik. Evaluasi Model
     Evaluasi model dilakukan dengan menggunakan confusion matrix guna mencari nilai akurasi, presisi, recall, dan hasil F1-score. Hasilnya akan digunakan sebagai landasan pertimbangan perlu tidaknya dilakukan pelatihan ulang pada model. Akurasi menentukan seberapa baik model mampu melakukan prediksi jenis kerusakan modul surya dengan benar. Presisi adalah akurasi atau kecocokan antara data yang diminta dengan hasil prediksi yang diberikan oleh model yang sudah disusun oleh arsitektur CNN. Klasifikasi dan Identifikasi kerusakan Modul Surya
       Pada tahapan terakhir dibangun sebuah prototype perangkat lunak aplikasi yang dapat mengklasifikasikan jenis kerusakan yang dilihat dari bentuk, ukuran serta pelebaran kerusakan pada modul surya sehingga dapat teridentifikasi tingkat kerusakan dan penanganannya. Rencana Penelitian
     Untuk mencapai target penelitian, maka penulis menyusun rencana Penelitian berupa jadwal kegiatan yang berguna untuk memastikan agar capaian yang ditetapkan dapat dipenuhi sesuai waktu yang telah ditetapkan termasuk target luaran berupa 2 (dua) buah publikasi."
Robert_Kualifikasi.txt,3.1 alur penelitian gambar 3.1 menunjuk kan metode penelitian. terdapat 5 tahap utama yang akan dilakukan yang pertama adalah studi literatur untuk menyusun bab 1 dan bab 2. tahap kedua adalah pengumpulan citra ekspresi wajah data citra berupa data primer dan data sekunder. tahap ketiga adalah pembentukan dataset untuk tiap model svm cnn dan mnn skenario pembentukan dataset dilakukan berdasarkan pada penelitian robert 2023 . pada tahap keempat dilakukan pembentukan model khusus untuk svm dan cnn menggunakan model pada penelitian robert 2023 sedangkan mnn menggunakan usulan pada penelitian ini. tahap terakhir adalah pelatihan dan pengujian untuk semua model svm cnn mnn terdapat tahap parameter tuning untuk tiap model . kemudian semua performa dari tiap model akan dibandingkan satu sama lain dan juga dianalisis pada bab 4. gambar 3.1. metode penelitian 48 3.2 pengumpulan citra ekspresi wajah citra ekspresi wajah dikumpulkan secara langsung oleh peneliti data primer dan juga menggunakan data yang dikumpulkan oleh peneliti lain data sekunder. terdapat 7 ekspresi wajah yang akan digunakan dalam penelitian ini yaitu marah jijik menghina senang sedih kaget dan netral tanpa ekspresi. gambar 3.2 menunjukkan contoh 7 ekspresi wajah manusia yang digunakan penelitian ini. gambar 3.2. contoh 7 jenis ekspresi wajah dataset primer akan dilakukan pengambilan citra ekspresi wajah mahasiswa universitas gunadarma baik pria maupun wanita. pengambilan akan dilakukan dari beberapa sudut pandang guna menambah variasi dataset . gambar 3.3 menunjukan contoh dataset primer dari berbagai sudut pandang. gambar 3.3. contoh dataset primer 49 dataset sekunder digunakan dataset yang telah digunakan umum oleh peneliti lain terkait pengenalan ekspresi wajah. terdapat beberapa dataset yang umum digunakan dalam penelitian ekspresi wajah . pertama extended cohn kanade ck yang berisi citra ekspresi wajah pria dan wanita dari berbagai etnis dengan resolusi tinggi kanade cohn tian 2000 lucey et al. kedua taiwanese facial expression image dataset tfeid yang berisi citra ekspresi wajah pria dan wanita dari etnis taiwan chen yen 2007 . ketiga japanese female facial expression jaffe yang terdiri dari citra ekspresi wajah wanita etnis j epang lyons 2021 lyons kamachi gyoba 2020 . tabel 3.1. detail dataset sekunder ekspresi dataset ck dataset jaffe dataset tfeid total anger 45 30 34 109 disgust 59 29 40 128 fear 25 32 40 97 happy 69 31 40 140 neutral 107 30 39 176 sad 28 31 39 98 surprise 83 30 36 149 ukuran citra 640490 256256 481600 warna citra rgb gray gray rgb 51 3.3 pembentukan dataset secara garis besar dalam pembuatan model ai khususnya ml dan dl terdapat proses yang berperan penting yaitu preprocessing dataset seperti ekstrasi fitur penyesuaian ukuran citra dan augmentasi deshmukh et al. pada penelitian robert 2023 dilakukan sebuah skenario pembentukan dataset menggunakan beberapa metode pengolahan citra seperti konversi warna ke grayscale deteksi wajah dan e kstrasi fitur di mana preprocessing mempengaruhi performa dari model ml dan dl. selain itu pada penelitian alam yao 2019 juga dilakukan penelitian yang serupa di mana preprocessing mempengaruhi performa model machine learning . pertama dilakukan pendeteksian wajah menggunakan vja proses ini berguna untuk mengurangi noise pada citra . hasil vja membuat ukuran citra bervariasi oleh karena itu dilakukan resizing citra untuk menyamakan semua ukuran citra dan juga menyesuaikan dengan dimensi input model . pertama dilakukan deteksi wajah menggunakan vja guna mengurangi noise . kemudian dilakukan konversi warna dari rgb ke grayscale karena fitur warna tidak dibutuhkan untuk mengenali ekspresi wajah. t erakhir mengubah ukuran citra untuk menyamakan semua ukuran citra dan sesuai dengan dimensi input model . gambar 3.10 menunjukkan hasil dari implementasi algoritma deteksi wajah pada sebuah citra. baris 2 dilakukan pembuatan matri ks yang digunakan untuk menyimpan hasil. berdasarkan dari penelitian robert 2023 didapatkan model terbaik untuk mengenal ekspresi wajah adalah menggunakan kernel sigmoid. algoritma 3.9. gradien morfologi dilasi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 input image grayscale image w h structure element se n n output feature image output w2 h2 for i 0 to w do for j 0 to h do create empty list arr for h 0 to z do for w 0 to z do if sehw 1 do arr appendimageih spacejh space end if end for end for outputispacejspace maxarr end for end for for i to w do for j to h do outputij imageij outputij end for end for 73 pada algoritma 3.9. terdapat dua input yang digunakan pertama adalah citra original dengan ukuran 𝑊 𝐻. kedua adalah structure element dengan ukuran 𝑁 𝑁. hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran 𝑊 𝐻. baris 1 dan 2 dilakukan perulangan baris dan kolom citra yang digunakan untuk menentukan koordinat operasi dilasi. baris 3 digunakan untuk menyimpan kandidat nilai piksel pada baris 4 hingga 10. baris 11 dilakukan pengambilan nilai terbesar yang digunakan untuk sebagai hasil fitur citra berdasarkan persamaan 2.8. kemudian pada baris 14 dan 15 dilakukan perulangan kembali pada baris dan kolom citra untuk menentukan koordinat perhitungan. 74 3.4.3.1.3 percobaan dan perbandingan hasil operasi morfologi tabel 3.2 menunjukkan hasil operasi morfologi menggunakan se berdasarkan 3.4.3.1 dan 3.4.3.2. pada operasi morfologi tabel 3.2. digunakan citra yang memiliki objek persegi panjang berwarna putih . di mana putih pada citra bernilai 1 sedangkan hitam adalah 0. tabel 3.3. operasi morfologi dan structure element pada persegi panjang original image se operasi operasi dan hasil disk original opening 33 55 77 99 1515 75 disk original erosi 33 55 7x7 99 1515 disk dilasi original 33 55 77 99 1515 kotak original opening 33 55 77 99 1515 76 kotak original erosi 33 55 77 99 1515 kotak dilasi original 33 55 77 99 1515 berdasarkan dari tabel 3.3 dapat dilihat operasi gmo dengan se disk 33 menghasilkan corner pada persegi panjang. operasi gmd dengan se disk 33 memiliki hasil garis berbentuk persegi panjang dengan nilai tiap corner terdapat hilang . operasi gmd dengan se kotak 33 menghasilkan garis yang berbentuk persegi panja ng. gme dengan se kotak 33 memiliki hasil yang mirip dengan gmd namun memiliki luas yang berbeda. 77 kemudian dilakukan operasi menggunakan 3 ukuran se lain yaitu 5 5 77 99 dan 1515. dari hasil yang didapatkan tiap ukuran memiliki hasil yang mirip dengan operasi yang digunakan. operasi gmd dan gme memiliki hasil yang mirip dengan perbedaan tepi pada gmd menebal kearah luar persegi sedangkan tepi pada gme menebal kearah dalam. 78 tabel 3.4 merupakan hasil morfologi menggunakan citra wajah dengan warna grayscale . nilai intensitas piksel pada citra berkisaran antara 0 hingga 255. operasi morfologi dan se yang digunakan berdasarkan usulan pada subbab 3.4.3.1 dan 3.4.3.2. tabel 3.4. structure element dan operasi morfologi pada wajah citra original se operasi operasi dan hasil disk gmo 33 55 77 99 1515 79 disk gme 33 55 77 99 1515 disk gmd 33 55 77 99 1515 80 kotak gmo 33 55 77 99 1515 kotak gme 33 55 77 99 1515 berdasarkan tabel 3.4 dapat dilihat hasil ekstrasi fitur menggunakan berbagai kom binasi dua bentuk dan lima ukuran . se kotak 5 5 juga memiliki hasil yang sama dengan disk 55 dengan perbedaan yang tid ak dapat dilihat kasat mata. se kotak 7 7 juga memiliki hasil yang sama dengan disk 77 dengan perbedaan yang tidak dapat dilihat kasat mata. pada operasi menggunakan se ukuran 9 9. se disk 99 dengan operasi gmo fitur yang didapatkan tidak akurat karena tidak mengekstrasi tepi bentuk dari ekspresi wajah. se kotak 9 9 dengan operasi 9 9 gme fitur yang didapatkan terlihat dengan jelas dan mirip dengan hasil yang menggunakan se disk 99 dengan operasi gme. se kotak 9 9 dengan operasi gmd hasil yang didapatkan hampir sama dengan hasil yang menggunakan se disk 99 dengan operasi gmd hanya terdapat perbedaan pada tebal garis pada hidung. 82 operasi menggunakan se 15 15 hasil yang didapatkan memiliki pola yang sama dengan operasi yang menggunakan se 9 9. se disk atau kotak 15 15 dengan operasi gmo fitur lebih terlihat jelas dibandingkan dengan ukuran 9 9. se disk atau kotak 15 15 dengan operasi gme fitur lebih tebal namun untuk se kotak bagian hidung menjadi lebih kotak dari ukuran sebelumnya. berdasarkan dari hasil yang didapatkan dan analisis. se disk 55 atau kotak 55 dengan operasi gmd memiliki hasil terbaik. selain itu operasi menggunakan gme dengan se disk 55 atau kotak 5 5 juga memberikan hasil yang baik. tepi dari tiap komponen wajah terlihat namun tepi pada mata dan bola mata tergabung namun memberikan hasil ekstrasi fitur bagian hidung lebih baik dibandingkan gmd. 83 3.4.3.2 pembentukan dan pelatihan model gambar 3.18 menunjukkan arsitektur dari mnn secara garis besar. di mana terdapat beberapa variasi arsitektur variasi pertama terdapat pada morphology layer di mana akan digunakan dua jenis ekstrasi fitur berdasarkan pada hasil subbab 3.4.3.3. variasi kedua terdaoat pada hidden layer di mana akan digunakan beberapa kombinasi fullyconnected layer . gambar 3.18. model mnn yang diusulkan input layer adalah layer pertama dari model mnn yang bertugas untuk menerima input berupa citra. di mana lapisan morfologi jenis pertama menggunakan hasil terbaik dari operasi morfologi dilasi pada subbab 3.4.3.3. morphology layer jenis kedua adalah erosion layer dilanjutkan subtraction layer . di mana lapisan morfologi jenis kedua ini menggunakan hasil terbaik dari operasi m orfologi erosi pada subab 3.4.3.3. 84 lapisan ketiga adalah flatten layer lapisan yang bertugas mengubah citra menajadi feature vector. masukan dari lapisan ini adalah hasil dari subtraction layer pada lapisan morfologi. di mana hasil dari subtraction layer adalah citra dengan ukuran 160160 yang kemudian diubah mnejadi satu dimensi yaitu 25600. lapisan keempat adalah fully connected layer fc layer yang bertugas untuk mempelajari dan menganalisa nilai feature vector dari flatten layer . pada lapisan ini dilakukan beberapa konfigurasi fc layer mulai dari jumlah fc layer dan jumlah neuron pada fc layer . konfigurasi pertama akan diuji coba 2 fc layer dengan masing masing neuron adalah 512 dan 25 6. konfigurasi kedua akan dicoba 1024 dan 512. kemudian dari situ akan dicoba analisis mana yang lebih baik sehingga dapat dikonfigurasi lebih lanjut. jika konfigurasi pertama memiliki hasil lebih baik artinya memungkinkan fc layer untuk dibuat lebih sederhana dengan mengurangi jumlah neuron . jika konfigurasi kedua lebih baik artinya terdapat kemungkinan untuk meninkatkan performa dari model karena dataset memiliki kompleksitas tinggi . beberapa lapisan akan dilakukan tuning paramter. tuning pertama terdapat pada bentuk se ukuran se jenis operasi pada morphological layer . tuning kedua terdapat pada fclayer yaitu fungsi aktivasi relusigmoidtanh dan jumlah neuron pada tiap hidden layer. selain arsitektur pada proses pelatihan juga dilakukan tuning pada learning rate jumlah epoch dan batch size. terakhir adalah output layer merupakan penentuan dari ekspresi berdasarkan dari bobot hidden layer . di mana fungsi aktivasi yang digunakan untuk output layer adalah softmax yang artinya output berupa probabilitas dari tiap ekspresi. kemudian untuk loss function yang akan digunakan adalah categorical crossentropy di mana fungsi loss ini digunakan jika model memprediksi multi kelas multi class prediction .
Tatya Atyanti Paramastri_Kualifikasi.txt,permasalahan yang diangkat pada penelitian ini adalah motif batik indonesia sangat beragam dan memiliki maknanya masingmasing. namun tidak banyak masyarakat yang masih mengetahui nama makna dan pemakaian dari masingmasing motif batik. menurut dewan ahli ppbi paguyuban pecinta batik indonesia sekar jagad ibu mari s. condronegoro saat ini sering kali ditemukan kesalahan dalam pemakaian motif batik seperti mengenakan kain yang seharusnya digunakan pada upacara kematian ketika menghadiri acara pernikahan. solusi yang diusulkan adalah melakukan klasifikasi motif batik. fokus studi literatur terbagi menjadi tiga topik yaitu klasifikasi motif batik komputasi kuantum dan deteksi tepi. sehingga hasil yang didapatkan memuaskan dan akurat. dataset yang akan dikumpulkan merupakan citra motif batik daur hidup yogyakarta dari kain batik tradisional yang merupakan batik cap maupun batik tulis bukan printing. motif yang akan digunakan dalam penelitian akan didiskusikan terlebih dahulu dengan narasumber supaya dapat mewakili batik daur hidup yogyakarta yang sangat penting untuk diketahui dalam bersosialdi masyarakat. selain itu ukuran citra yang lebih kecil dapat mempercepat proses segmentasi dan klasifikasi tanpa kehilangan informasi penting. proses selanjutnya adalah mengubah ruang warna menjadi grayscale perubahan warna ini dilakukan karena dapat meningkatkan kontras meningkatkan efisiensi komputasi dan meningkatkan ketahanan terhadap variasi pencahayaan. selanjutnya proses augmentasi digunakan untuk meningkatkan ukuran dataset dengan menambahkan data baru tanpa perlu melakukan pengumpulan data baru. komputasi kuantum diterapkan mulai dari proses segmentasi karena berdasarkan penelitian terdahulu deteksi tepi berbasis kuantum dapat mendeteksi lebih banyak tepi dibandingkan deteksi tepi klasik berdasarkan jumlah tepi yang dihasilkan sundani dkk. 2019. hal serupa juga berlaku pada metode qcnn yang memiliki hasil lebih baik dan akurat dibandingkan dengan metode cnn klasik. sehingga hipotesisnya hasil dari ekstraksi fitur dan klasifikasi akan menjadi lebih optimal. gambar 3.2 perbandingan hasil deteksi tepi berbasis kuantum dan klasik sundani dkk. pengolahan data ini dilakukan sebagai pembanding performa model segmentasi dan klasifikasi berbasis kuantum. pengolahan data kedua model komputasi kuantum dan komputasi klasik akan dilakukan dengan menggunakan komputer yang sama yaitu komputer klasik. performa akan dibandingkan dari akurasi yang dihasilkan.
Tia Haryanti_Kualifikasi.txt,3.1 kerangka umum penelitian ini bertujuan untuk mengembangkan sistem deteksi dini kantuk sebelum berkendara dengan menggunakan kombinasi data visual berupa data citra wajah dan data fisiologis. kondisi predriving mengacu pada kondisi sebelum pengemudi memulai perjalanan sehingga sistem ini sangat penting untuk mencegah risiko kecelakaan di jalan . sistem ini mengintegrasikan teknologi pengenalan wajah dan analisis data fisiologis untuk memberikan deteksi yang lebih akurat. blok d iagram secara umum yang digunakan pada penelitian ini dapat dilihat pada gambar 3.1 blok diagram. objek preprocessing data fisiologis ekstrasi fiturpenggabungan fitur klasifikasi data image data visual kantuk ya tidak gambar 3.1 blok diagram model ini terdiri dari tiga tahapan yaitu input proses dan output . penelitian deteksi dini kantuk untuk kondisi predriving menggabungkan data visual yaitu pengumpulan data citra wajah pengemudi yang diambil menggunakan kamera serta data fisiologis yang diukur berupa data ekg menggunakan perangkat wearable yaitu smartwatch dan pulse oximeter untuk mengukur saturasi oksigen spo2 . tahapan preprocessing dan ekstraksi fitur dilakukan pada kedua jenis data yaitu data citra gambar dan data fisiologis. model convolutional neural network cnn digunakan untuk mengekst raksi fitur dari data citra wajah yang merupakan data visual sementara long short term memory lstm digunakan untuk memproses data fisiologis yang bersifat timeseries. fitur fitur yang diekstraksi dari kedua model ini digabungkan untuk menghasilkan vect or fitur gabungan. vektor fitur ini kemudian digunakan sebagai input untuk model support vector machine 43 svm yang melakukan klasifikasi akhir untuk mendeteksi kantuk. hasil deteksi kemudian digunakan untuk memberikan peringatan kepada pengemudi layak tidak nya pengemudi untuk berkendara. pengumpulan data data visualdata fisiologis pemilihan dan persiapan dataset preprocessing data pembuatan modelekstraksi fitur penggabungan fitur evaluasi pemisahan dataset pembangunan model pelatihan model evaluasi model implementasi gambar 3. 2 tahapan penelitian 44 3.3. pemilihan dan persiapan dataset tahapan ini merupakan tahapan identifikasi awal dari penelitian meliputi identifikasi masalah penelitian yang berfokus pada masalah utama yaitu mendeteksi kantuk pada pengemudi menggunakan pemrosesan citra dan fisiologis . pemilihan dataset memastikan bahwa dataset yang dikumpulkan relevan dengan tujuan penelitian yaitu hanya menggunakan data yang berkaitan dengan kondisi predriving serta memastikan bahwa data visual dan data fisiologis diambil pada waktu yang sama. 3.3.2 preprocessing data melakukan analisis eksploratif data untuk memahami karakteristik dataset sehingga meningkatkan kualitas deteksi . langkah dari pembuatan model yaitu penulisan kode untuk membangun model sesuai dengan desain arsitektur yaitu cnn lstm dan svm. 2. membangun model lstm membangun model lstm dengan lapisan lstm dan dense untuk ekstraksi fitur. 3.4.4 desain arsitektur desain a rsitektur merupakan proses menentukan struktur dan komponen model yang akan dibangun yang terdiri dari jenis model jumlah dan jenis layer fungsi aktivasi teknik regularisasi dan konfigurasi model. hasil dari kedua model digabungkan dan diklasifikasikan menggunakan support vector machine svm. 50 digunakan untuk mengolah data visual seperti mengenali mata tertutup atau mulut menguap sebagai indikator kantuk. menggabungkan fitur yang diekstrak dari cnn dan lstm untuk mendapatkan representasi data yang komprehensif memastikan bahwa model dapat mengidentifikasi kantuk berdasarkan kombinasi indikator visual dan fis iologis. selanjutnya yaitu menggunakan support vector machines svm untuk mengklasifikasikan data sebagai kantuk atau tidak kantuk. svm dipilih karena kemampuannya dalam mengklasifikasikan data yang kompleks dan memberikan batas keputusan yang jelas layak atau tidak layak pengemudi untuk berkendara. jika pengklasifikasi mendeteksi keadaan mengantuk maka pengklasifikasi menghasilkan alarm atau notifikasi pemberitahuan untuk memberi tahu bahwa pengemudi tidak layak untuk berkendara atau kembali ke f ase pertama dan memulai ulang prosedur. 3.5 evaluasi model gabungan ini dievaluasi menggunakan metrik seperti akurasi presisi recall dan f1score untuk memastikan performa dan keandalannya. implementasi sistem ini diharapkan dapat memberikan notifikasi atau peringatan kepada pengemudi jika tanda tanda kantuk terdeteksi selama kondisi predriving sehingga dapat meningkatkan keselamatan berkendara secara signifikan. berdasarkan hasil validasi model dapat ditune atau dioptimalkan untuk meningkatkan performa misalnya dengan mengubah arsitektur parameter atau teknik training . 3.6 implementasi setelah penyempurnaan model dianggap siap untuk digunakan. model ini harus dapat secara akurat mendeteksi kantuk pengemudi dalam berbagai kondisi dengan minimal kesalahan. langkah selanjutnya yaitu penerapan model dalam sistem nyata dan pemantauan efektivitasnya dalam kondisi pengemudi pada 51 lingkungan predriving. model yang telah dioptimalkan diintegrasikan ke dalam sistem deteksi dini kantuk untuk pengujian awal. selanjutn ya yaitu m elakukan uji coba lapangan untuk mengevaluasi efektivitas sistem dalam kondisi nyata memungkinkan pengumpulan feedback untuk perbaikan lebih lanjut. 3.7 rencana kegiatan tabel 3.1 rencana kegiatan no nama kegiatan bulan 1 2 3 4 5 6 7 8 9 10 11 12 1 kajian literatur 2 perencanaan penelitian 3. pengumpulan data 4. prapemrosesan data 5. pembuatan model 6. pelatihan dan evaluasi model 7. penyusunan laporan akhir 8. presentasi laporan akhir 9. publikasi jurnal ilmiah internasional 10. pengajuan hki
Utami Lestari_Kualifikasi.txt,3.1 gambaran umum penelitian ini bertujuan untuk mengembangkan aplikasi berbasis large language model llm dengan arsitektur gpt 4 yang mampu melakukan telaah sejawatpeer review secara otomatis pada artikel ilmiah dari jurnal komputer. data utama yang digunakan adalah ar tikel ilmiah berbahasa indonesia dalam bidang ilmu komputer dari berbagai jurnal akademik. sebelum digunakan data akan diperiksa untuk menghilangkan informasi pribadi yang dapat mengidentifikasi penulis atau reviewer. aplikasi ini diharapkan dapat membant u para peneliti dan editor jurnal dalam menganalisis dan memperoleh wawasan dari artikel yang seringkali bersifat kompleks dan teknis. 2 tahapan preprocessing tahap pertama adalah tokenisasi di mana teks dipecah menjadi unit unit yang lebih kecil yang dikenal sebagai token memungkinkan model untuk menganalisis teks pada tingkat yang lebih granular. hal ini tidak hanya meningkatkan produktivitas tetapi juga memastikan bahwa artikel yang dipublikasikan memenuhi standar ilmiah yang tinggi. 3.1.4 evaluasi model llm evaluasi model merupakan langkah yang penting dalam pengembangan sistem kecerdasan buatan karena memungkinkan untuk menilai kinerja dan efektivitas model dalam menyelesaikan tugas tertentu. tanpa evaluasi yang tepat model yang dikembangkan dapat menghasilkan prediksi yang tidak akurat atau tidak dapat diandalkan yang berpotensi menyebabkan kinerja sistem yang buruk secara keseluruhan. 3. recall memberikan informasi tentang seberapa banyak instance positif yang berhasil diidentifikasi oleh model dari semua instance positif yang 4. sebenarnya dalam dataset. adanya jadwal penelitian diharapkan penelitian dapat berjalan secara efisien dan sesuai rencana sehingga memberikan kepastian bahwa semua tahapan penelitian dapat diselesaikan tepat pada waktunya. table jadwal penelitian dapat dilihat pada table 3.1 table 3. 1 jadwal penelitian no uraian kegiatan 2023 2024 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12 1 penyusunan proposal 2 uji kualifikasi 3 evaluasi progres pertama 4 paper pertama 5 evaluasi progres kedua no uraian kegiatan 2025 2026 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 1 paper ke dua 2 evaluasi rkp 3 sidang tertutup 4 sidang terbuka
Yoga Panji Perdana Nugraha_Kualifikasi.txt,3.1 motivasi industri manufaktur memiliki berbagai macam produk yang ada di dalamnya. dalam upaya pemenuhan kualitas yang tinggi serta menjaga kepuasan pelanggan dan reputasi perusahaan maka mendeteksi produk yang cacat sedini mungkin merupakan aspek yang penting. 1. pengembangan aplikasi pendeteksi cacat pada produk ini didasari keinginan peneliti untuk meningkatkan kinerja pengendalian kualitas pada industri manufaktur sehingga dapat membantu menjaga kualitas produk serta efisien si dalam kegiatan pengendalian kualitas. 2. untuk meminimalisir pemborosan waktu bahan baku biaya dan sumber daya lainnya karena deteksi cacat pada produk dilakukan sedini dan secepat mungkin. 3. meningkatkan efisiensi pada kegiatan inspeksi produk d engan mene rapkan otomatisasi mel alui aplikasi yang dikembangkan. 4. mengintegrasikan teknologi yang sedang berkembang seperti artificial intelligence dengan industri manufaktur sehingga tercipta manufaktur cerdas yang akan berakibat pendapatan profit perusahaan yang op timal. 5. memberikan kontribusi pemahaman dan pengembangan teknologi baru dalam deteksi objek sehingga bisa menjadi referensi untuk pembaca serta penelitian selanjutnya. tahap awal tahap pengembanganperancangan dan pembuatan prototype alat deteksi cacatpengumpulan data cacat objek uji coba prototype alat deketsi cacat objekperancangan model deteksi cacat objek menggunakan deep learning implementasi dan pelatihan model deteksi cacat objek evaluasi dan penyempurnaan model deteksi cacat objek pengujian model deteksi objek menggunakan deep learning pembuatan aplikasi pendeteksi objek cacattahap optimasi pengajuan hki dan jurnal internasional q 1 gambar 3. 1. tahap awal kegiatan yang dilakukan pada tahap awal ini adalah merancang dan membuat prototype alat deteksi cacat dan pengumpulan data cacat objek. 2 rancangan prototipe alat gambar 3.2 di atas menggambarkan rancangan alat yang akan dikembangkan. tahap ini terdapat kegiatan yaitu eval uasi dan penyempurnaan model deteksi cacat objek. evaluasi dan penyempurnaan dilakukan agar fitur yang ada pada aplikasi yang akan dikembangkan dapat ditampilkan dengan maksimal. p engujian data dilakukan untuk menguji model sejauh mana dapat mendeteksi cacat dari suatu produk. pada akhirnya akan menampilkan output model dalam mendeteksi cacat pada produk. setelah itu maka dibangun aplikasi yang mampu mendeteksi cacat produk pada industri secara real time. aplikasi ini nantinya akan menampilkan hasil deteksi dari produk yang bergerak. selain itu diterapkan juga pengukuran evaluasi seperti precision recall dan mean average precision map untuk memastikan model yang dikembangkan dapat digunakan dengan optimal. nantinya akan dikembangkan sebuah aplikasi yang kemungkinan berbasis web untuk mempermudah pengguna untuk mengambil gambar bergerak maupun tak bergerak yang kemudian mengirimnya ke sistem pendeteksi cacat dan menerima hasil deteksi secara real time. hasil deteksi secara real time dikehendaki agar produk dapat diperiksa selama proses produksi berlangsung sehingga cacat dapat dideteksi secepat dan seakurat mungkin.
