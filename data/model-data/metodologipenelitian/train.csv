kalimat,summary
"3.1 Alur Penelitian
       Alur penelitian menggambarkan alur dari awal hingga akhir penelitian dilaksanakan. Alur penelitian ini diuraikan pada Gambar 3.1 di bawah ini. 3.2 Identifikasi Masalah
       Identifikasi masalah adalah salah satu langkah pertama yang dilakukan sebelum melakukan penelitian. Identifikasi masalah merupakan suatu proses mencari dan mengetahui masalah yang ingin diselesaikan. Identifikasi masalah ini membantu penelitian untuk memahami tantangan yang dihadapi oleh petani kentang skala nasional dan 
merancang solusi yang tepat sesuai dengan kebutuhan mereka. Identifikasi masalah pada penelitian ini berfokus pada mengidentifikasi proses perancangan model koperasi petani, mengidentifikasi metode prediksi permintaan dengan ANN di dalam blockchain yang digunakan untuk mengoptimalkan permintaan pelanggan di masa depan selama periode 
tertentu, dan mengidentifikasi metode safety stock di dalam blockchain yang digunakan agar dapat mengoptimalkan stok dan permintaan. Identifikasi masalah pada penelitian ini, peneliti dapat lebih memahami kendala dan kebutuhan petani kentang skala nasional. Perancangan model platform koperasi untuk meningkatkan efisiensi dan kerjasama antarpetani dengan koperasi sebagai mitranya. Sementara itu, metode prediksi permintaan dengan menggunakan Artificial Neural 
Network (ANN) diharapkan dapat membantu petani mengelola produksi secara lebih tepat sesuai dengan kebutuhan pasar dan koperasi dapat menyesuaikan persediaan stok dan 
permintaan secara dinamis dari hasil prediksi permintaan. Selain itu, identifikasi masalah juga mencakup penerapan metode safety stock untuk mengoptimalkan manajemen stok, 
memastikan ketersediaan barang, dan meningkatkan responsibilitas terhadap fluktuasi permintaan pasar. Dengan penerapan ANN dan metode safety stock di dalam blockchain, 
semua prediksi dan manajemen stok dapat dicatat di dlaam buku besar yang tidak dapat diubah, sehingga meningkatkan transparansi dan keamanan data dalam rantai pasok. Sehingga koperasi ini dapat melakukan perencanaan yang lebih akurat, meminimalkan pemborosan, dan meningkatkan ketersediaan kentang sesuai dengan kebutuhan pelanggan. Dengan demikian, platform koperasi menjadi responsif terhadap perubahan permintaan pasar, mendukung pertumbuhan ekonomi para petani, memperkuat kolaborasi antar 
anggota koperasi serta memiliki transparansi dan keamanan pada rantai pasok. 3.3 Studi Literatur
       Studi literatur yang dilakukan pada penelitian engembangan platform koperasi petani ini dimulai dari pencarian dan review literatur-literatur terbaru dan relevan yang telah diterbitkan. Studi literatur juga dapat dari teori-teori buku yang relevan dengan metode yang digunakan. Analisis literatur membantu untuk mengidentifikasi kerangka kerja, metode, dan teknologi yang telah digunakan pada penelitian sebelumnya. Studi literatur dapat digunakan sebagai mencari solusi dan menganalisa penelitian yang dilakukan. Studi literatur juga membantu dalam mengetahui tantangan dan peluang yang mungkin dihadapi dalam pengembangan platform koperasi petani kentang. Sehingga informasi tersebut dapat memberikan sebuah wawasan terkait dengan penelitian yang dilakukan. 3.4 Pengumpulan Data
       Pengumpulan data yang digunakan sebagai bahan dalam mengolah data. Sehingga penelitian ini akan menghasilkan data yang sesuai dengan tujuan penelitian. Penelitian ini mengumpulkan data sekunder dan data primer. Pengumpulan data pada penelitian ini terdiri dari beberapa proses sebagai berikut. Pengumpulan data sekunder memanfaatkan sumber informasi yang sudah ada, seperti literatur ilmiah, dokumen resmi, dan data statistik yang relevan. Proses ini 
memungkinkan peneliti untuk memahami konteks yang telah ada sebelumnya dan memanfaatkan pengetahuan serta data yang telah dihasilkan sebelumnya. Berdasarkan data yang diperoleh dari Badan Pusat Statistik (BPS) tahun 2022, menjelaskan data produksi kentang di berbagai wilayah Indonesia. Beberapa wilayah 
Indonesia berhasil dalam produksi kentang dan beberapa wilayah Indonesia yang tidak dapat memproduksi kentang. Data tersebut memberikan gambaran lengkap mengenai 
kegiatan pertanian kentang di berbagai wilayah Indonesia pada tahun 2022. Berikut data BPS tahun 2022 produksi kentang di berbagai wilayah Indonesia. Pengumpulan data primer yaitu melakukan pencarian secara langsung untuk mengumpulkan data serta informasi baru sesuai dengan tujuan penelitian. Metode ini 
seperti pengambilan data survei, wawancara, observasi, atau eksperimen, dengan tujuan untuk kebutuhan penelitian. Data primer yang akan digunakan pada penelitian ini adalah kebutuhan pengguna, aliran data dari petani dengan koperasi sebagai mitranya, data musim, data historis penjualan, data produksi kentang dan data harga kentang. Pengambilan data primer dilakukan di Wonosobo, Jawa Tengah. Berdasarkan informasi yang didapatkan dari salah satu petani di Wonosobo, Jawa Tengah di sana terdapat banyak petani kentang dan sayuran lainnya. Menurut BPS, Kabupaten Wonosobo, Jawa Tengah adalah wilayah yang terbanyak memproduksi kentang di provinsi Jawa 
Tengah. Pola distribusi kentang di Wonosobo, Jawa Tengah terdiri dari 3 pola sebagai berikut (Zaenuri, et al, 2023). 3.5 Blockchain
       Pada penelitian ini, untuk meningkatkan keamanan dan transparansi maka menggunakan teknologi blockchain untuk rantai pasok kentang. Berikut flowchart kecerdasan buatan, safety stock yang dikombinasikan di dalam blockchain. Berdasarkan gambar di atas menggambarkan kombinasi ANN dan safety stock di dalam blockchain. Data rantai pasok yang telah dikumpulkan, kemudian dimasukkan ke 
dalam database. Data tersebut diverifikasi dalam blockchain dengan proses pembuatan blok baru yang melibatkan perhitungan hash blok sebelumnya, menyusun blok baru, 
menghitung hash blok baru, dan mencapai konsensus untuk menambahkan blok ke rantai. Data yang diverifikasi kemudian diproses menggunakan model Artificial Neural Network 
(ANN). Tahapan dalam ANN meliputi praproses data, inisialisasi model, pelatihan model, validasi model, dan evaluasi kinerja. Hasil prediksi permintaan disimpan dalam blockchain dengan proses pembuatan blok baru yang sama seperti langkah sebelumnya. Selanjutnya permintaan data diverifikasi, dan jika valid, safety stock dihitung menggunakan rumus safety stock yang sudah ada. Hasil perhitungan safety stock disimpan dalam database dan dicatat dalam blockchain dengan pembuatan blok baru. Semua data dari proses tersebut dicatat dalam blockchain untuk memastikan transparansi dan keamanan. Proses validasi memastikan bahwa data permintaan dan pengelolaan stok selalu diperbarui dan valid sebelum digunakan untuk pengambilan keputusan. 3.6 Design Sistem dengan UML
       Pengembangan platform koperasi petani kentang menggunakan metode Unified Modeling Language (UML) untuk menggambarkan struktur, fungsi dan interaksi komponen sistem secara visual. Dimana proses metode UML ini diawali dengan identifikasi kebutuhan sistem dan pemahaman terhadap fungsionalitas yang terkait dengan economic sharing dan prinsip-prinsip perkoperasian. 3.6.1 Analisis Kebutuhan Sistem
       Analisis kebutuhan sistem bertujuan untuk mengidentifikasi kebutuhan dari pengguna dan stakeholder sistem. Pengumpulan informasi pada proses ini mengenai detail 
cara kerja sistem dan batasan-batasan yang ada. Pengumpulan data dilakukan melalui wawancara, atau survei. Analisis kebutuhan sistem dapat menentukan arah dan ruang 
lingkup proyek pengembang sistem, serta memastikan bahwa produk akhir akan memenuhi harapan dan memecahkan masalah yang dihadapi oleh pengguna. Gambar 3.4 menjelaskan proses Multi-Stakeholder Cooperative, dimana anggota koperasi termasuk dari workers, community, producers, dan consumers. Mereka memilih 
Board of Director dari para anggotanya. Board of Director merupakan struktur organisasi yang bertanggung jawab dalam mengawasi manajemen yang dijalankan oleh 
koperasi, dengan setiap anggotanya memiliki tugas khusus sesuai dengan tujuan koperasi. Dewan Direksi berperan penting dalam menjaga keberlanjutan dan keseimbangan antara 
berbagai kepentingan dalam konteks Multi-Stakeholder Cooperative. Gambar 3.5 mendeskripsikan pengguna platform koperasi petani yang melibatkan sejumlah pihak. Pengguna platform ini terdiri dari consumers yang dapat mengakses 
produk pertanian secara langsung, farmers yang memanfaatkan platform untuk memasarkan hasil panen, companies yang terlibat dalam dukungan pengembangan teknologi, dan partner cooperatives yang menjadi bagian dari kolaborasi kerjasama antar koperasi untuk meningkatkan kesejahteraan bersama. Keterlibatan seluruh pihak ini, diharapkan platform koperasi petani menciptakan lingkungan yang saling mendukung dan berkelanjutan, memperkuat konektivitas antar anggota untuk mencapai tujuan bersama dalam dunia pertanian. 3.6.2 Use Case Diagram
       Model pertama UML adalah pemodelan use case diagram, dimana menggambarkan skenario-skenario utama pengguna platform koperasi. Use case diagram digunakan untuk menunjukkan hubungan dan struktur kelas-kelas yang terlibat dalam sistem, termasuk entitas-entitas seperti data permintaan, stok kentang, dan pengguna. Gambar 3.6 adalah diagram use case untuk Platform Koperasi Petani yang menunjukkan berbagai interaksi antara pengguna dan sistem. Pada diagram use case, terdapat aktor yang terdiri dari petani, konsumen, anggota koperasi dan admin. Registrasi dilakukan oleh petani, anggota koperasi dan konsumen. Proses login untuk mengakses 
fitur yang terdapat pada website koperasi dapat dilakukan oleh petani, anggota koperasi, konsumen dan admin. Proses mencatat produksi hanya dilakukan oleh petani, dimana 
petani mencatat data produksi kentang mereka yang kemudian akan dicatat di blockchain. Proses melacak produksi dan distribusi menggunakan blockchain. Proses verifikasi produk kentang yang dihasilkan oleh petani. Proses verifikasi dilakukan oleh admin dan anggota koperasi. Proses melakukan pembelian melalui website dilakukan oleh konsumen. Proses mengelola transaksi yang terjadi di dalam sistem, memastikan semua transaksi tercatat di blockchain dilakukan oleh admin. Proses melihat laporan dan statistik dari data produksi, distribusi, dan transaksi yang terjadi di dalam sistem dilakukan oleh admin, anggota koperasi dan petani. 3.6.3	Activity Diagram
       Model kedua adalah activity diagram untuk menggambarkan alur kerja atau proses-proses yang terjadi dalam platform koperasi. Activity diagram dapat membantu 
dalam menguraikan langkah-langkah yang diperlukan dari pemesanan kentang hingga manajemen stok. Diagram pada Gambar 3.7 adalah proses mencatat produksi yang diawali dengan login seorang petani ke dalam platform koperasi petani. Petani memulai dengan membuka 
platform dan memilih opsi untuk login. Setelah login berhasil, petani memasukkan data produksi kentang. Sistem akan memverifikasi data. Selanjutnya sistem mencatat data di 
blockchain dan sistem memberikan notifikasi ke petani. Diagram pada Gambar 3.8 merupakan alur proses melacak produksi dan distribusi. Proses ini dilakukan dari anggota melakukan login ke sistem. Selanjutnya anggota koperasi memilih menu pelacak produksi dan distribusi. Sistem akan menampilkan tampilan menu pelacak produksi dan distribusi dan anggota koperasi memasukkan ID produk. Sistem akan mengambil data dari blockchain dan jika data telah dikirim oleh blockchain, selanjutnya sistem akan menampilkan informasi pelacak. Diagram pada Gambar 3.9 menggambarkan proses pembelian yang dilakukan oleh konsumen. Konsumen melakukan login di sistem koperasi petani. Selanjutnya, konsumen mencari produk yang ingin dibeli dan memilih produk serta memasukkan jumlah pembelian. Sistem akan memverifikasi stok produk. Konsumen selanjutnya memasukkan informasi pembayaran. Sistem akan mencatat transaksi di blockchain dan sistem akan mengirimkan notifikasi pembelian. Diagram pada Gambar 3.10 menggambarkan alur proses mengelola transaksi. Pertama, admin melakukan login pada sistem website koperasi. Admin memilih menu manajemen transaksi dan sistem menampilkan daftar transaksi yang terjadi. Admin memverifikasi transaksi yang belum diverifikasi lalu mengubah status transaksi sesuai hasil verifikasi. Sistem memperbarui data di blockchain. Sistem akan menampilkan laporan transaksi yang berhasil diperbarui di blockchain. 3.6.4	Sequence Diagram
       Model ketiga adalah sequence diagram, dimana diagram ini dapat membantu dalam menggambarkan urutan peristiwa atau interaksi antar komponen dalam sistem, seperti bagaimana data pemesanan diteruskan dan diproses. Diagram pada Gambar 3.11 menggambarkan proses mencatat produksi kentang petani di website koperasi yang terintegrasi dengan blockchain. Pertama, petani memilih 
menu login dan mengisi form login ke website koperasi. Website memverifikasi dan mengirimkan notifikasi login sukses. Petani selanjutnya memilih menu data produksi dan 
melakukan input data produksi seperti nama produk, kuantitas, tanggal produksi, dan sebagainya. Website akan memverifikasi data yang di masukkan oleh petani. Setelah data 
diverifikasi, website koperasi mengirimkan data produksi ke blockchain. Blockchain akan memverifikasi data tersebut melalui node-node yang ada. Data produksi yang telah 
diverifikasi oleh node blockchain selanjutnya dicatat di dalam blockchain. Kemudian website koperasi mengirimkan notifikasi ke petani bahwa data produksi telah berhasil tercatat. Diagram pada Gambar 3.12 menunjukkan tahapan yang dilalui oleh seorang petani untuk melacak produksi dan distribusi kentang melalui website koperasi yang 
terintegrasi dengan blockchain. Pertama, anggota koperasi. Mengirimkan permintaan login ke website koperasi. Website akan memverifikasi dan mengirimkan notifikasi login 
sukses. Anggota koperasi memilih menu pelacak produksi dan distribusi pada website. Anggota koperasi memasukkan ID produk yang ingin dilacak. Website koperasi mengirimkan permintaan untuk mengambil data pelacakan dari blockchain. Blockchain mengirimkan data pelacakan yang diminta. Selanjutnya, website koperasi akan menampilkan informasi pelacakan yang diperoleh dari blockchain kepada anggota koperasi. Diagram pada Gambar 3.13 menunjukkan alur proses pembelian yang dilakukan oleh konsumen untuk membeli produk kentang melalui website koperasi yang terintegrasi 
dengan blockchain. Konsumen mengirimkan permintaan login ke website koperasi. Website koperasi akan memverifikasi data login yang diinput oleh konsumen dan mengirimkan notifikasi login sukses. Konsumen memilih menu produk dan input nama produk yang akan dicari di sistem website. Website koperasi akan menampilkan daftar produk yang diinginkan. Konsumen memilih produk dan memasukkan jumlah pembelian. Website akan memverifikasi stok produk yang tersedia. Setelah stok produk diverifikasi, website akan mengirimkan informasi stok tersedia dan konsumen memasukkan informasi pembayaran. Website koperasi akan mengirimkan data transaksi ke blockchain untuk dicatat. Blockchain memverifikasi dan mencatat transaksi. Website koperasi akan mengirimkan informasi kepada konsumen bahwa transaksi telah berhasil dicatat. Diagram yang ditampilkan pada Gambar 3.14 menggambarkan langkah- langkah dalam mengelola transaksi di website koperasi. Admin mengirimkan permintaan login ke 
website koperasi. Website koperasi akan memverifikasi dan mengirimkan notifikasi login sukses. Setelah berhasil login, admin memilih menu manajemen transaksi. Admin melihat daftar transaksi yang terjadi. Selanjutnya admin memilih transaksi yang belum diverifikasi dan melakukan verifikasi. Website koperasi mengubah status transaksi berdasarkan hasil verifikasi. Website mengirimkan permintaan untuk memperbarui status transaksi di blockchain. Blockchain memverifikasi dan memperbarui status transaksi. Admin melihat laporan transaksi yang telah diperbarui. Website mengambil data laporan dari blockchain dan menampilkan kepada admin. 3.7 Prediksi Permintaan dengan ANN
       Metode prediksi permintaan dalam penelitian ini bertujuan untuk memprediksi jumlah kentang yang diminta oleh pasar atau konsumen pada periode waktu tertentu. Metode yang digunakan pada prediksi permintaan adalah metode ANN. Input data yang akan digunakan adalah data kuantitatif dan kualitatif yang dapat mempengaruhi permintaan di masa depan, sehingga agar hasil prediksi permintaan dapat lebih akurat. Dengan menerapkan metode ANN pada prediksi permintaan ini, penelitian dapat memberikan prediksi yang lebih tepat terkait kebutuhan pasar di masa mendatang sehingga dapat meningkatnya efektivitas rantai pasok. Prediksi ini juga dapat memberikan petani wawasan yang berharga terkait potensi pasar dan membantu mereka mengoptimalkan produksi serta mitra koperasi dapat merencanakan strategi pemasaran yang lebih efektif. Dengan menerapkan metode prediksi permintaan kentang menggunakan metode ANN, penelitian ini dapat memberikan kontribusi signifikan dalam mendukung keberlanjutan dan efisiensi dalam pertanian kentang. Langkah pra-pemrosesan data melibatkan pengumpulan data historis relevan seperti data penjualan sebelumnya, data harga, data produksi kentang serta data musim sebagai faktor eksternal yang dapat memengaruhi permintaan. Kemudian, dilakukan data cleaning, dinormalisasi, dan di-transformasi untuk memastikan bahwa ANN yang akan dibangun dapat bekerja dengan efektif dan menghasilkan prediksi yang akurat. Langkah selanjutnya adalah desain dan pelatihan model ANN. Tahapan ini melibatkan proses pemilihan arsitektur jaringan yang tepat, termasuk jumlah lapisan tersembunyi, jumlah neuron per lapisan, fungsi aktivasi, dan algoritma pembelajaran. Pelatihan model merupakan proses ANN menyesuaikan bobotnya berdasarkan kesalahan prediksi melalui metode seperti backpropagation. Pada tahapan ini, penyesuaian parameter seperti kecepatan belajar dan momentum dilakukan untuk memperbaiki proses pembelajaran model. Setelah dilatih dengan baik, model akan mampu mengenali pola kompleks dan hubungan non-linear dalam data. Tahapan evaluasi model ANN adalah tahapan dimana model yang telah dilatih dan diuji menggunakan dataset yang belum pernah dilihat sebelumnya untuk menilai kemampuannya dalam memprediksi permintaan dengan akurat. Matrik evaluasi seperti Mean Square Error (MSE) atau Mean Absolute Percentage Error (MAPE) merupakan matrik yang sering digunakan untuk mengukur kinerja model. Berdasarkan hasil evaluasi, model prediksi yang akurat dari model ANN ini berguna untuk perusahan dalam membuat keputusan strategis seperti inventory management. Proses prediksi permintaan dengan ANN akan menghasilkan data permintaan yang diharapkan, informasi tersebut digunakan untuk proses inventory management. Data prediksi permintaan tersebut akan digunakan sebagai dasar perhitungan safety stock. Sehingga perusahaan dapat merencanakan dan menyesuaikan kuantitas inventory yang cukup untuk memenuhi permintaan, dimana akan meminimalisir biaya penyimpanan dan mengurangi risiko kekurangan stok. Tujuannya agar operasi bisnis dapat berjalan dengan lancar dan efisien, serta mengoptimalkan ketersediaan produk. 3.8 Inventory Management
       Proses inventory management menggunakan metode safety stock merupakan proses untuk menjaga ketersediaan persediaan dalam platform secara efektif. Tahapan pertama, penelitian ini memerlukan analisis data historis permintaan kentang, fluktuasi pasokan, dan waktu panen, sehingga dapat mengidentifikasi kebutuhan pasokan dan resiko keterlambatan. Penerapan metode safety stock pada penelitian ini akan menentukan tingkat persediaan tambahan yang diperlukan untuk mengatasi ketidakpastian dalam permintaan atau keterlambatan pasokan. Hal ini bertujuan untuk memberikan keandalan dan menghindari kekurangan persediaan yang dapat menghambat operasional koperasi. Metode safety stock dalam pengembangan platform koperasi petani kentang pada penelitian ini untuk meningkatkan efisiensi manajemen persediaan. 3.9 Integrasi
       Tahapan integrasi merupakan proses menggabungkan sistem, aplikasi, atau teknologi yang berbeda menjadi satu kesatuan yang berfungsi secara harmonis. Pada proses ini, berbagai komponen yang sebelumnya beroperasi secara terpisah agar dapat berinteraksi satu sama lain dalam mencapai tujuan bersama. Integrasi bertugas menyatukan aspek desain sistem dengan UML, prediksi permintaan dan inventory management. Proses integrasi menjamin bahwa data yang diolah sebelumnya dapat digunakan dengan baik untuk mendukung pengambilan keputusan. Selain itu, integrasi ini melibatkan penggunaan Artificial Neural Network (ANN) dan metode safety stock yang terintegrasi dalam blockchain untuk rantai pasok. Data dari prediksi permintaan dan pengelolaan stok dicatat secara transparan dan aman dalam blockchain. Website koperasi akan terintegrasi dengan blockchain, untuk memastikan efisiensi dan transparansi dalam seluruh proses manajemen rantai pasok. 3.10 Pengujian Sistem
       Tahapan pengujian sistem dalam penelitian merupakan langkah untuk mengevaluasi kinerja atau fungsionalitas sistem yang dikembangkan atau diuji pada penelitian. Proses pengujian sistem mencakup implementasi prototipe atau model sistem, hingga serangkaian uji coba. Tujuan dari tahapan pengujian sistem adalah mengidentifikasi adanya kegagalan, mengukur efektivitas sebuah sistem serta memastikan sistem berjalan sesuai dengan tujuan dan persyaratan yang telah ditetapkan sebelumnya. Pada penelitian ini, sistem platform koperasi petani diharapkan dapat berjalan sesuai dengan tujuan dan persyaratan perkoperasian serta sesuai dengan model platform economic sharing. Platform koperasi petani kentang pada penelitian ini akan berbasis website dan dilengkapi dengan kecerdasan buatan yang dikombinasikan dengan blockchain. 3.11 Evaluasi
       Tahapan selanjutnya adalah evaluasi. Evaluasi dilakukan untuk memastikan bahwa semua komponen sistem berfungsi sesuai rencana. Evaluasi melibatkan penilaian kinerja pada sistem secara keseluruhan, dan memeriksa apakah integrasi berjalan tanpa hambatan. Tahapan evaluasi juga dapat mengidentifikasi apakah hasil pengujian sistem sesuai dengan tujuan awal dan menentukan area yang mungkin memerlukan peningkatan. Hasil dari tahap evaluasi menjadi petunjuk penting untuk membuat perubahan dan peningkatan, sehingga sistem dapat bekerja lebih baik lagi. 3.12 Analisis Hasil
       Analisis merupakan tahapan penelitian, dimana menyimpulkan serta menguraikan informasi dari hasil data yang telah diolah dan diuji sebelumnya. Tahapan analisis dapat memberikan makna dari temuan-temuan tersebut. Tahapan ini memberikan identifikasi faktor-faktor yang dapat mempengaruhi kinerja sistem dan memberikan rekomendasi untuk peningkatan di masa yang akan datang. 3.13 Jadwal Penelitian
       Penelitian ini diuraikan pada Tabel Jadwal Penelitian yang merupakan uraian manajemen waktu dalam perencanaan dan pelaksanaan suatu penelitian agar penelitian dapat memenuhi target waktu yang telah ditetapkan. Tabel ini menjelaskan tahapan-tahapan penelitian beserta waktu penelitian. Berikut uraian rencana jadwal penelitian program Doktor Teknologi Informasi di Universitas Gunadarma.","3.1 Alur Penelitian
       Alur penelitian menggambarkan alur dari awal hingga akhir penelitian dilaksanakan. Alur penelitian ini diuraikan pada Gambar 3.1 di bawah ini. 3.2 Identifikasi Masalah
       Identifikasi masalah adalah salah satu langkah pertama yang dilakukan sebelum melakukan penelitian. Identifikasi masalah merupakan suatu proses mencari dan mengetahui masalah yang ingin diselesaikan. Identifikasi masalah ini membantu penelitian untuk memahami tantangan yang dihadapi oleh petani kentang skala nasional dan 
merancang solusi yang tepat sesuai dengan kebutuhan mereka. Identifikasi masalah pada penelitian ini berfokus pada mengidentifikasi proses perancangan model koperasi petani, mengidentifikasi metode prediksi permintaan dengan ANN di dalam blockchain yang digunakan untuk mengoptimalkan permintaan pelanggan di masa depan selama periode 
tertentu, dan mengidentifikasi metode safety stock di dalam blockchain yang digunakan agar dapat mengoptimalkan stok dan permintaan. Identifikasi masalah pada penelitian ini, peneliti dapat lebih memahami kendala dan kebutuhan petani kentang skala nasional. Perancangan model platform koperasi untuk meningkatkan efisiensi dan kerjasama antarpetani dengan koperasi sebagai mitranya. Sementara itu, metode prediksi permintaan dengan menggunakan Artificial Neural 
Network (ANN) diharapkan dapat membantu petani mengelola produksi secara lebih tepat sesuai dengan kebutuhan pasar dan koperasi dapat menyesuaikan persediaan stok dan 
permintaan secara dinamis dari hasil prediksi permintaan. Selain itu, identifikasi masalah juga mencakup penerapan metode safety stock untuk mengoptimalkan manajemen stok, 
memastikan ketersediaan barang, dan meningkatkan responsibilitas terhadap fluktuasi permintaan pasar. Dengan penerapan ANN dan metode safety stock di dalam blockchain, 
semua prediksi dan manajemen stok dapat dicatat di dlaam buku besar yang tidak dapat diubah, sehingga meningkatkan transparansi dan keamanan data dalam rantai pasok. Sehingga koperasi ini dapat melakukan perencanaan yang lebih akurat, meminimalkan pemborosan, dan meningkatkan ketersediaan kentang sesuai dengan kebutuhan pelanggan. Dengan demikian, platform koperasi menjadi responsif terhadap perubahan permintaan pasar, mendukung pertumbuhan ekonomi para petani, memperkuat kolaborasi antar 
anggota koperasi serta memiliki transparansi dan keamanan pada rantai pasok. 3.3 Studi Literatur
       Studi literatur yang dilakukan pada penelitian engembangan platform koperasi petani ini dimulai dari pencarian dan review literatur-literatur terbaru dan relevan yang telah diterbitkan. Studi literatur juga membantu dalam mengetahui tantangan dan peluang yang mungkin dihadapi dalam pengembangan platform koperasi petani kentang. Sehingga penelitian ini akan menghasilkan data yang sesuai dengan tujuan penelitian. Proses ini 
memungkinkan peneliti untuk memahami konteks yang telah ada sebelumnya dan memanfaatkan pengetahuan serta data yang telah dihasilkan sebelumnya. Beberapa wilayah 
Indonesia berhasil dalam produksi kentang dan beberapa wilayah Indonesia yang tidak dapat memproduksi kentang. Data tersebut memberikan gambaran lengkap mengenai 
kegiatan pertanian kentang di berbagai wilayah Indonesia pada tahun 2022. Data primer yang akan digunakan pada penelitian ini adalah kebutuhan pengguna, aliran data dari petani dengan koperasi sebagai mitranya, data musim, data historis penjualan, data produksi kentang dan data harga kentang. Berdasarkan informasi yang didapatkan dari salah satu petani di Wonosobo, Jawa Tengah di sana terdapat banyak petani kentang dan sayuran lainnya. 3.5 Blockchain
       Pada penelitian ini, untuk meningkatkan keamanan dan transparansi maka menggunakan teknologi blockchain untuk rantai pasok kentang. Berikut flowchart kecerdasan buatan, safety stock yang dikombinasikan di dalam blockchain. Data rantai pasok yang telah dikumpulkan, kemudian dimasukkan ke 
dalam database. Data tersebut diverifikasi dalam blockchain dengan proses pembuatan blok baru yang melibatkan perhitungan hash blok sebelumnya, menyusun blok baru, 
menghitung hash blok baru, dan mencapai konsensus untuk menambahkan blok ke rantai. Hasil prediksi permintaan disimpan dalam blockchain dengan proses pembuatan blok baru yang sama seperti langkah sebelumnya. Hasil perhitungan safety stock disimpan dalam database dan dicatat dalam blockchain dengan pembuatan blok baru. 3.6 Design Sistem dengan UML
       Pengembangan platform koperasi petani kentang menggunakan metode Unified Modeling Language (UML) untuk menggambarkan struktur, fungsi dan interaksi komponen sistem secara visual. Pengguna platform ini terdiri dari consumers yang dapat mengakses 
produk pertanian secara langsung, farmers yang memanfaatkan platform untuk memasarkan hasil panen, companies yang terlibat dalam dukungan pengembangan teknologi, dan partner cooperatives yang menjadi bagian dari kolaborasi kerjasama antar koperasi untuk meningkatkan kesejahteraan bersama. Keterlibatan seluruh pihak ini, diharapkan platform koperasi petani menciptakan lingkungan yang saling mendukung dan berkelanjutan, memperkuat konektivitas antar anggota untuk mencapai tujuan bersama dalam dunia pertanian. Proses verifikasi produk kentang yang dihasilkan oleh petani. Website koperasi mengubah status transaksi berdasarkan hasil verifikasi. Dengan menerapkan metode ANN pada prediksi permintaan ini, penelitian dapat memberikan prediksi yang lebih tepat terkait kebutuhan pasar di masa mendatang sehingga dapat meningkatnya efektivitas rantai pasok. Kemudian, dilakukan data cleaning, dinormalisasi, dan di-transformasi untuk memastikan bahwa ANN yang akan dibangun dapat bekerja dengan efektif dan menghasilkan prediksi yang akurat. Berdasarkan hasil evaluasi, model prediksi yang akurat dari model ANN ini berguna untuk perusahan dalam membuat keputusan strategis seperti inventory management. Proses prediksi permintaan dengan ANN akan menghasilkan data permintaan yang diharapkan, informasi tersebut digunakan untuk proses inventory management. Tahapan pertama, penelitian ini memerlukan analisis data historis permintaan kentang, fluktuasi pasokan, dan waktu panen, sehingga dapat mengidentifikasi kebutuhan pasokan dan resiko keterlambatan. Penerapan metode safety stock pada penelitian ini akan menentukan tingkat persediaan tambahan yang diperlukan untuk mengatasi ketidakpastian dalam permintaan atau keterlambatan pasokan. Metode safety stock dalam pengembangan platform koperasi petani kentang pada penelitian ini untuk meningkatkan efisiensi manajemen persediaan. Selain itu, integrasi ini melibatkan penggunaan Artificial Neural Network (ANN) dan metode safety stock yang terintegrasi dalam blockchain untuk rantai pasok. Website koperasi akan terintegrasi dengan blockchain, untuk memastikan efisiensi dan transparansi dalam seluruh proses manajemen rantai pasok. 3.10 Pengujian Sistem
       Tahapan pengujian sistem dalam penelitian merupakan langkah untuk mengevaluasi kinerja atau fungsionalitas sistem yang dikembangkan atau diuji pada penelitian. Proses pengujian sistem mencakup implementasi prototipe atau model sistem, hingga serangkaian uji coba. Pada penelitian ini, sistem platform koperasi petani diharapkan dapat berjalan sesuai dengan tujuan dan persyaratan perkoperasian serta sesuai dengan model platform economic sharing. Platform koperasi petani kentang pada penelitian ini akan berbasis website dan dilengkapi dengan kecerdasan buatan yang dikombinasikan dengan blockchain. 3.11 Evaluasi
       Tahapan selanjutnya adalah evaluasi. Evaluasi dilakukan untuk memastikan bahwa semua komponen sistem berfungsi sesuai rencana. Evaluasi melibatkan penilaian kinerja pada sistem secara keseluruhan, dan memeriksa apakah integrasi berjalan tanpa hambatan. Tahapan evaluasi juga dapat mengidentifikasi apakah hasil pengujian sistem sesuai dengan tujuan awal dan menentukan area yang mungkin memerlukan peningkatan. Hasil dari tahap evaluasi menjadi petunjuk penting untuk membuat perubahan dan peningkatan, sehingga sistem dapat bekerja lebih baik lagi. 3.12 Analisis Hasil
       Analisis merupakan tahapan penelitian, dimana menyimpulkan serta menguraikan informasi dari hasil data yang telah diolah dan diuji sebelumnya. Tahapan analisis dapat memberikan makna dari temuan-temuan tersebut. Tahapan ini memberikan identifikasi faktor-faktor yang dapat mempengaruhi kinerja sistem dan memberikan rekomendasi untuk peningkatan di masa yang akan datang. 3.13 Jadwal Penelitian
       Penelitian ini diuraikan pada Tabel Jadwal Penelitian yang merupakan uraian manajemen waktu dalam perencanaan dan pelaksanaan suatu penelitian agar penelitian dapat memenuhi target waktu yang telah ditetapkan. Tabel ini menjelaskan tahapan-tahapan penelitian beserta waktu penelitian. Berikut uraian rencana jadwal penelitian program Doktor Teknologi Informasi di Universitas Gunadarma."
"3.1 Motivasi Penelitian
     Penelitian ini dilakukan dengan motivasi mengenalkan pentingnya sistem pembelajaran bidang arsitektur menggunakan teknologi metaverse. Dalam proses pengembangan sistem pembelajaran arsitektur berbasis metaverse ini, peneliti juga ingin menunjukkan perlunya keterlibatan komunitas dan persepsi pengguna bidang arsitektur agar sistem pembelajaran yang dihasilkan sesuai dengan kebutuhan dan harapan mereka dalam meningkatkan efektivitas pembelajaran arsitektur itu sendiri. Diharapkan, sistem pembelajaran arsitektur berbasis metaverse ini dapat memberi kemudahan kepada komunitas dosen dan mahasiswa dalam mempelajari berbagai sisi arsitektur dengan memasuki dunia virtual dan mereka dapat memahami materi yang diajarkan serta memecahkan permasalahan arsitektur yang dihadapi secara interaktif, kolaboratif, dan imersif dengan solusi tepat tanpa harus mencari berbagai referensi wujud nyata arsitektur di dunia fisik atau dunia nyata. Studi literatur Yose Indarta, Ambiyar, Agariadne Dwinggo Samala, Ronal Watrianthos (2022) berjudul ""Metaverse: Tantangan dan Peluang dalam Pendidikan"" menyimpulkan bahwa implementasi Metaverse di dunia pendidikan memiliki peluang besar dalam menunjang proses pelaksanaan pendidikan menjadi lebih baik. Pendidikan berbasis audiovisual merupakan aplikasi Metaverse paling popular dan banyak digunakan dalam pembelajaran. Berdasar penelitian, pendidikan berbasis pengalaman menjadi lebih baik, apakah melalui belajar secara langsung maupun simulasi didukung teknologi. Dengan konsep pembelajaran matakuliah Perkembangan Arsitektur 1 yang dilakukan dengan metode metaverse, pembelajaran secara online ini dapat dilakukan dengan lebih interaktif. Dalam proses metaverse menyediakan banyak dukungan-dukungan pada proses pembelajaran online dengan tidak menghilangkan pengalaman belajar di kampus. Metode belajar di mana saja dan kapan saja menjadi konsep menarik yang disenangi banyak pihak. Seharusnya
     waktu, ruang dan biaya dan lainnya dapat dipangkas dengan kehadiran teknologi metaverse. 3.2 Kerangka Penelitian
      Penelitian ini dilakukan dalam mencapai tujuan utama, yaitu pengembangan sistem pembelajaran arsitektur berbasis metaverse, terutama terkait perkembangan arsitektur. Penelitian ini dilakukan melalui beberapa tahap, antara lain:
      Tahapan Pengembangan Sistem Pembelajaran Perkembangan Arsitektur 1 Berbasis Metaverse

4. Efektivitas Pembelajaran Kolaboratif
- Presensi
- Imersi
- Kehadiran dalam realitas yang disimulasi
- Kapabilitas metaverse dalam membentuk lingkungan pengguna untuk memahami
realitas


- Pemahaman materi pembelajaran
- Kemampuan memahami materi pembelajaran Perkembangan Arsitektur berbasis
metaverse
Penelitian mengenai pengembangan sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse ini dilakukan dengan melibatkan komunitas, yang terdiri dari dosen dan mahasiswa, di Program Studi S1 Arsitektur, Jurusan Teknik Arsitektur. Peneliti melibatkan dosen matakuliah Perkembangan Arsitektur 1 dan 200 mahasiswa semester 3 yang mengambil matakuliah Perkembangan Arsitektur, yaitu sekitar 200 mahasiswa, yang terdiri dari 40 mahasiswa per kelas dari empat kelas. 1. Identifikasi Topik Pembelajaran
      Peneliti melakukan identifikasi topik pembelajaran tentang arsitektur dalam matakuliah Perkembangan Arsitektur. Pembelajaran ini dilakukan pada mahasiswa dari semester III dalam Program Studi S1 Arsitektur, Jurusan Teknik Arsitektur, Fakultas Teknik Sipil dan Perencanaan, Universitas Gunadarma. Pada tahap ini, peneliti menghimpun materi pembelajaran Perkembangan Arsitektur 1 selama satu semester, sehingga terbentuk suatu himpunan materi pembelajaran Perkembangan Arsitektur selama satu semester yang siap dimasukkan dalam
sistem pembelajaran Perkembangan Arsitektur berbasis metaverse. Dalam penelitian ini, beberapa topik pembelajaran ditemukan dalam matakuliah Perkembangan Arsitektur 1.
a. Perkembangan arsitektur di Indonesia dan latar belakang pembentukannya. b. Pengaruh kebudayaan Hindu, Budha, Islam, serta kolonial belanda dan arsitek-arsitek yang terlibat aktif di dalamnya. c. Arsitektur tradisional di Daerah Sumatera yang diwakili rumah tradisional di Aceh, Riau, Minangkabau, Batak. d. Arsitektur tradisional di Kalimantan yang diwakili rumah Panjang/ Lamin. e. Arsitektur tradisional di Daerah Sulawesi yang diwakili rumah Makasar/ Minahasa/ Toraja. f. Arsitektur tradisional di Daerah Jawa dan Bali yang diwakili rumah Jawa/ Sunda dan Bali. g. Arsitektur tradisional di Daerah Bali yang diwakili rumah tradisional Bali. h. Perkembangan arsitektur di Indonesia setelah masa kemerdekaan sampai sekarang termasuk gejala-gejala yang mendasarinya. 3. Arsitektur tradisional di Daerah Sumatera yang diwakili rumah tradisional di Aceh, Riau, Minangkabau, Batak
- Arsitektur tradisional Aceh, Riau, Minangkabau, Batak
Mahasiswa dapat menguraikan secara umum
- Pengaruh sistem kekerabatan dan kepercayaan masyarakat pada arsitektur tradisional
- Pola-pola kampung tradisional secara konseptual
beserta konsep ruang dalam arsitektur tradisional
4. Arsitektur tradisional di Daerah Kalimantan yang diwakili rumah Panjang/ Lamin. - Arsitektur tradisional dayak
- Pengenalan sistem kekerabatan dan kepercayaan masyarakat tradisional daerah setempat
- Pengenalan pola kampung tradisional, letak dan orientasinya
- Pengenalan rumah tradisional Rumah
Panjang/Lamin sebagai rumah tradisional dayak
Mahasiswa dapat menguraikan secara umum
- Pengaruh sistem kekerabatan dan kepercayaan masyarakat pada arsitektur tradisional
- Pola-pola kampung tradisional secara konseptual beserta konsep ruang dalam arsitektur tradisional
5. Arsitektur tradisional di Daerah Sulawesi yang diwakili rumah Makasar/ Minahasa/ Toraja
- Arsitektur tradisional Makasar, Minahasa, Toraja
- Pengenalan sistem kekerabatan dan kepercayaan masyarakat tradisional daerah setempat
- Pengenalan pola kampung tradisional, letak dan orientasinya
- Pengenalan rumah tradisional
Mahasiswa dapat menguraikan secara umum
- Pengaruh sistem kekerabatan dan kepercayaan masyarakat pada arsitektur tradisional


6. Arsitektur tradisional di Daerah Jawa dan Bali yang diwakili rumah Jawa/Sunda dan Bali
- Arsitektur tradisional Jawa, Sunda. - Pola orientasi alam pada arsitektur tradisional
- Fungsi ruang dalam rumah tradisional Jawa, Sunda
- Keterkaitan antara sistem kekerabatan dan sistem religi setempat
Mahasiswa dapat menguraikan
- Pola desa dan bentuk arsitektur yang lahir (umum) Mahasiswa dapat menerapkan
- Konsep dan pola orientasi arsitektur tradisional terhadap fungsi-fungsi baru yang ada sekarang
7. Arsitektur tradisional di Daerah Bali yang diwakili rumah tradisonal Bali
- Arsitektur tradisional Bali
- Fungsi Undagi dalam arsitektur tradisional Bali
- Konsep Nawa Sangah
- Pola orientasi alam pada arsitektur tradisional
- Fungsi ruang dalam rumah tradisional Bali
- Keterkaitan antara sistem kekerabatan dan sistem religi setempat
Mahasiswa dapat menguraikan
- Pola desa dan bentuk arsitektur yang lahir (umum) Mahasiswa dapat menerapkan
- Konsep dan pola orientasi arsitektur tradisional terhadap fungsi-fungsi baru yang ada sekarang
8. Perkembangan arsitektur di Indonesia setelah masa kemerdekaan sampai sekarang termasuk gejala-gejala yang
mendasarinya
- Arsitektur di Indonesia setelah masa kemerdekaan sampai sekarang
Mahasiswa dapat menjelaskan
- Perkembangan arsitektur di Indonesia setelah masa kemerdekaan sampai sekarang termasuk gejala- gejala yang mendasarinya

2. Konstruksi Dunia Visual
      Pada tahap ini, peneliti melakukan konstruksi dunia visual atas dasar materi pembelajaran Perkembangan Arsitektur, baik konstruksi fisik maupun konstruksi desain. Tujuannya adalah membentuk dunia fisik dan desain visual sebagai dasar terbentuknya dunia digital menuju dunia virtual terkait Perkembangan Arsitektur. Setelah itu, peneliti menyiapkan latar arsitektur dan tata-letak sesuai dengan materi pembelajaran yang sudah dipersiapkan terkait Perkembangan Arsitektur. Dalam mewujudkan sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse ini, peneliti juga melakukan konstruksi avatar dan konten pembelajaran Perkembangan Arsitektur 1 di dalam dunia digital, sehingga terbentuk dunia virtual berbasis konten pembelajaran Perkembangan Arsitektur. 3. Penggunaan Dunia Nyata
      Pada tahap ini, peneliti menerapkan sistem pembelajaran Perkembangan Arsitektur berbasis metaverse yang sudah dikembangkan untuk digunakan dalam dunia nyata. Penerapan sistem pembelajaran berbasis metaverse ini dilakukan pada komunitas dosen dan mahasiswa semester 3 Jurusan Teknik Arsitektur, Program Studi S1 Fakultas Teknik Sipil dan Perencanaan Universitas Gunadarma. Dalam hal ini, komunitas dosen dan mahasiswa ini melakukan koneksi ke dunia virtual berupa sistem pembelajaran Perkembangan Arsitektur berbasis metaverse dan semua jenis kegiatan yang dilakukan dalam menyelesaikan masalah dan menyediakan solusi yang diperlukan dapat tersimpan dalam basis data server, sehingga dapat diambil kembali setiap kali mereka masuk dan terlibat kembali dalam sistem pembelajaran virtual kolaboratif ini. Dalam sistem pembelajaran virtual kolaboratif ini, komunitas dosen dan mahasiswa dapat berinteraksi satu sama lain dalam penyelesaian masalah yang ada dan mencari solusi yang diperlukan, sehingga mereka benar-benar dapat hadir dan terlibat di dalam sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse secara intensif, interaktif, dan imersif. 4. Efektivitas Pembelajaran Kolaboratif
      Pada tahap ini, peneliti menguji efektivitas pembelajaran kolaboratif yang didasarkan pada persepsi pengguna mengenai sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse. Persepsi pengguna ini berkaitan dengan aspek input, proses, output, dan outcome dari sistem pembelajaran berbasis metaverse tersebut. Pertama, pada aspek input, persepsi pengguna dieksplorasi dan dievaluasi tentang kelengkapan dan kememadaian dari materi pembelajaran Perkembangan Arsitektur selama satu semester, baik narasi, grafik, interaksi maupun teknik visualisasinya. Kedua, pada aspek proses, persepsi pengguna dieksplorasi dan dievaluasi tentang ketepatan konstruksi fisik dan konstruksi desain, ketepatan latar arsitektur dan tata letak dunia digital, serta ketepatan karakter avatar yang merepresentasi pengguna dalam dunia virtual beserta konten pembelajaran yang dimasukkan ke dalam sistem pembelajaran Perkembangan Arsitektur berbasis metaverse. Ketiga, pada aspek output, persepsi pengguna dieksplorasi dan dievaluasi mengenai konektivitas dan persistensi pengguna ke dunia virtual, interaktivitas dan keterlibatan pengguna di dunia virtual, dan presensi serta imersi pengguna dalam komunitas dunia virtual. Keempat, pada aspek outcome, persepsi pengguna juga dieksplorasi dan dievaluasi tentang ketercapaian tujuan dari pembelajaran Perkembangan Arsitektur 1 berbasis metaverse sesuai dengan kriteria dan indikator yang ditetapkan dosen pengampu. 3.3 Pendekatan Penelitian
      Penelitian ini dilakukan menggunakan pendekatan kuantitatif eksperimental terhadap sistem pembelajaran Perkembangan Arsitektur berbasis metaverse yang dikembangkan dalam komunitas dosen dan mahasiswa Jurusan Teknik Arsitektur, Program Studi S1 Arsitektur Universitas Gunadarma. Sistem pembelajaran berbasis metaverse ini dikembangkan sesuai dengan materi pembelajaran Perkembangan Arsitektur 1 di kalangan mahasiswa Jurusan Teknik Arsitektur semester 3. Apabila pengembangan sistem pembelajaran ini sudah selesai, model pembelajaran berbasis metaverse tersebut diuji validitas dan reliabilitasnya dengan melibatkan penilaian objektif dan otoritatif dari para ahli,
baik ahli materi maupun media pembelajaran. Jika model sistem pembelajaran berbasis metaverse ini sudah dinyatakan valid dan reliabel, model tersebut diujicobakan kepada komunitas pengguna yang terdiri dari dosen dan mahasiswa dari Jurusan Teknik Arsitektur semester 3, sehingga akhirnya dapat diketahui efektivitas pembelajaran kolaboratif berbasis metaverse tersebut sesuai dengan kriteria dan indikator yang ditetapkan oleh dosen pengampu. Efektivitas pembelajaran kolaboratif berbasis metaverse dalam penelitian ini dievaluasi dengan melihat peningkatan pemahaman mahasiswa mengenai materi pembelajaran Perkembangan Arsitektur sesuai dengan kriteria dan indikator yang ditetapkan oleh dosen pengampu. Dari hasil uji efektivitas sistem pembelajaran ini, diharapkan dapat diketahui sejumlah kelebihan dan kekurangannya, sehingga dapat dijadikan sebagai bahan pertimbangan rekomendasi dalam meningkatkan kualitas sistem pembelajaran Perkembangan Arsitektur berbasis metaverse tersebut.","Dalam proses pengembangan sistem pembelajaran arsitektur berbasis metaverse ini, peneliti juga ingin menunjukkan perlunya keterlibatan komunitas dan persepsi pengguna bidang arsitektur agar sistem pembelajaran yang dihasilkan sesuai dengan kebutuhan dan harapan mereka dalam meningkatkan efektivitas pembelajaran arsitektur itu sendiri. Diharapkan, sistem pembelajaran arsitektur berbasis metaverse ini dapat memberi kemudahan kepada komunitas dosen dan mahasiswa dalam mempelajari berbagai sisi arsitektur dengan memasuki dunia virtual dan mereka dapat memahami materi yang diajarkan serta memecahkan permasalahan arsitektur yang dihadapi secara interaktif, kolaboratif, dan imersif dengan solusi tepat tanpa harus mencari berbagai referensi wujud nyata arsitektur di dunia fisik atau dunia nyata. Dengan konsep pembelajaran matakuliah Perkembangan Arsitektur 1 yang dilakukan dengan metode metaverse, pembelajaran secara online ini dapat dilakukan dengan lebih interaktif. 3.2 Kerangka Penelitian
      Penelitian ini dilakukan dalam mencapai tujuan utama, yaitu pengembangan sistem pembelajaran arsitektur berbasis metaverse, terutama terkait perkembangan arsitektur. Penelitian ini dilakukan melalui beberapa tahap, antara lain:
      Tahapan Pengembangan Sistem Pembelajaran Perkembangan Arsitektur 1 Berbasis Metaverse

4. Efektivitas Pembelajaran Kolaboratif
- Presensi
- Imersi
- Kehadiran dalam realitas yang disimulasi
- Kapabilitas metaverse dalam membentuk lingkungan pengguna untuk memahami
realitas


- Pemahaman materi pembelajaran
- Kemampuan memahami materi pembelajaran Perkembangan Arsitektur berbasis
metaverse
Penelitian mengenai pengembangan sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse ini dilakukan dengan melibatkan komunitas, yang terdiri dari dosen dan mahasiswa, di Program Studi S1 Arsitektur, Jurusan Teknik Arsitektur. Dalam mewujudkan sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse ini, peneliti juga melakukan konstruksi avatar dan konten pembelajaran Perkembangan Arsitektur 1 di dalam dunia digital, sehingga terbentuk dunia virtual berbasis konten pembelajaran Perkembangan Arsitektur. Dalam hal ini, komunitas dosen dan mahasiswa ini melakukan koneksi ke dunia virtual berupa sistem pembelajaran Perkembangan Arsitektur berbasis metaverse dan semua jenis kegiatan yang dilakukan dalam menyelesaikan masalah dan menyediakan solusi yang diperlukan dapat tersimpan dalam basis data server, sehingga dapat diambil kembali setiap kali mereka masuk dan terlibat kembali dalam sistem pembelajaran virtual kolaboratif ini. Dalam sistem pembelajaran virtual kolaboratif ini, komunitas dosen dan mahasiswa dapat berinteraksi satu sama lain dalam penyelesaian masalah yang ada dan mencari solusi yang diperlukan, sehingga mereka benar-benar dapat hadir dan terlibat di dalam sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse secara intensif, interaktif, dan imersif. Efektivitas Pembelajaran Kolaboratif
      Pada tahap ini, peneliti menguji efektivitas pembelajaran kolaboratif yang didasarkan pada persepsi pengguna mengenai sistem pembelajaran Perkembangan Arsitektur 1 berbasis metaverse. Keempat, pada aspek outcome, persepsi pengguna juga dieksplorasi dan dievaluasi tentang ketercapaian tujuan dari pembelajaran Perkembangan Arsitektur 1 berbasis metaverse sesuai dengan kriteria dan indikator yang ditetapkan dosen pengampu. 3.3 Pendekatan Penelitian
      Penelitian ini dilakukan menggunakan pendekatan kuantitatif eksperimental terhadap sistem pembelajaran Perkembangan Arsitektur berbasis metaverse yang dikembangkan dalam komunitas dosen dan mahasiswa Jurusan Teknik Arsitektur, Program Studi S1 Arsitektur Universitas Gunadarma. Sistem pembelajaran berbasis metaverse ini dikembangkan sesuai dengan materi pembelajaran Perkembangan Arsitektur 1 di kalangan mahasiswa Jurusan Teknik Arsitektur semester 3. Apabila pengembangan sistem pembelajaran ini sudah selesai, model pembelajaran berbasis metaverse tersebut diuji validitas dan reliabilitasnya dengan melibatkan penilaian objektif dan otoritatif dari para ahli,
baik ahli materi maupun media pembelajaran. Jika model sistem pembelajaran berbasis metaverse ini sudah dinyatakan valid dan reliabel, model tersebut diujicobakan kepada komunitas pengguna yang terdiri dari dosen dan mahasiswa dari Jurusan Teknik Arsitektur semester 3, sehingga akhirnya dapat diketahui efektivitas pembelajaran kolaboratif berbasis metaverse tersebut sesuai dengan kriteria dan indikator yang ditetapkan oleh dosen pengampu. Efektivitas pembelajaran kolaboratif berbasis metaverse dalam penelitian ini dievaluasi dengan melihat peningkatan pemahaman mahasiswa mengenai materi pembelajaran Perkembangan Arsitektur sesuai dengan kriteria dan indikator yang ditetapkan oleh dosen pengampu. Dari hasil uji efektivitas sistem pembelajaran ini, diharapkan dapat diketahui sejumlah kelebihan dan kekurangannya, sehingga dapat dijadikan sebagai bahan pertimbangan rekomendasi dalam meningkatkan kualitas sistem pembelajaran Perkembangan Arsitektur berbasis metaverse tersebut."
"3.1 Tahapan Penelitian
Tahapan penelitian merupakan serangkaian langkah-langkah yang dilakukan dalam penelitian. Gambaran mengenai tahapan penelitian ini dapat dilihat pada Gambar 3.1. Tahapan penelitian ini terdiri dari studi literatur untuk memahami keadaan yang terfokus terhadap tentang mesin kompresor reciprocating, metode prediksi pemeliharaan mesin, predictive maintenance, dan machine learning. Identifikasi permasalahan secara spesifik yang akan diatasi oleh penelitian, termasuk mendefinisikan ruang lingkup serta tujuan dari penelitian. Pengumpulan data yang relevan dan melakukan analisis awal untuk memahami karakteristik dan pola dalam data. Preprocessing data melibatkan pembersihan data, normalisasi, dan transformasi data agar siap digunakan dalam model machine learning. Tahap ini bertujuan untuk mengatasi masalah data yang hilang, outliers, dan memastikan data berada dalam format yang sesuai. Pemilihan algoritma machine learning yang efektif untuk membantu mencapai akurasi yang lebih tinggi dan efisiensi dalam prediksi. Pengembangan dan melatih model machine learning menggunakan algoritma yang telah dipilih dengan data yang telah diperoses, tahap ini melibatkan pembagian data menjadi set pelatihan dan set pengujian, serta termasuk mengatur parameter model untuk mencapai kinerja terbaik. Sistem peringatan pemeliharaan prediktif menggunakan model machine learning untuk memprediksi kegagalan mesin, sistem ini bertujuan untuk memberikan peringatan dini sebelum terjadinya kerusakan atau kegagalan mesin. Integrasi model dengan sistem peringatan pemeliharaan prediktif, tahap ini memastikan bahwa model dapat bekerja secara real-time dan memberikan peringatan yang akurat kepada pengguna. Serta tahapan terakhir adalah evaluasi kinerja sistem secara keseluruhan untuk memastikan bahwa sistem peringatan pemeliharaan prediktif berfungsi dengan baik dan mencapai tujuan yang diinginkan. 3.2 Pengumpulan dan Analisis Data
Observasi data dilakukan pada platform terpercaya yang menyediakan berbagai dataset publik yaitu Kaggle. Data yang digunakan yaitu dataset mesin kompresor reciprocating yang divisualisasikan oleh akun kaggle bernama Ahmet Okudan. Dataset tersebut berisi data operasional dari mesin kompresor reciprocating dengan total 1000 sampel. Setiap sampel dalam dataset tersebut mencakup 26 kolom yang berisi informasi dapat dilihat pada Tabel 3.1. Analisis data penelitian ini menggunakan metode Exploratory Data Analysis (EDA) bertujuan untuk memberikan gambaran umum tentang data dan mengidentifikasi pola atau anomali yang mungkin tidak terlihat dengan metode lain. Penerapan metode EDA dalam dataset mesin kompresor reciprocating menggunakan software python open access di notebook jupyter, gambaran mengenai alur proses analisis data dapat dilihat pada Gambar 3.2. 3.3 Preprocessing Data
      Preprocessing data sangat penting untuk memastikan bahwa data yang digunakan untuk pelatihan model machine learning memiliki kualitas yang tinggi, relevan, dan sesuai. Alur proses data preprocessing pada penelitian ini dapat dilihat pada Gambar 3.3. 3.4 Sistem Peringatan Pemeliharaan Prediktif
      Sistem peringatan pemeliharaan prediktif adalah sistem yang menggunakan teknik-teknik analisis data, terutama machine learning untuk memantau kondisi mesin dan memprediksi kegagalan yang mungkin terjadi. Penelitian ini mengusulkan sistem untuk memberikan peringatan dini kepada operator atau tim pemeliharaan sehingga tindakan preventif dapat dilakukan sebelum kegagalan terjadi, mengurangi downtime, dan biaya pemeliharaan. Gambaran alur kerja usulan sistem peringatan pemeliharaan prediktif dapat dilihat pada Gambar 3.4. Tahap awal adalah data operasional mesin dikumpulkan secara menerus menggunakan sensor, bertujuan memastikan data terkini tersedia untuk dianalisis guna mendeteksi
tanda-tanda awal kegagalan mesin. Tahap kedua menginput data dari pemantauan real-time ke dalam model prediksi pemeliharaan yang telah dilatih. Tahap ketiga mendeteksi adanya anomali atau pola yang tidak biasa agar tindakan pencegahan dapat diambil sebelum kerusakan terjadi. Tahap keempat analisis prediksi untuk memperkirakan kapan dan bagaimana kegagalan akan terjadi, sehingga pemeliharaan dapat direncanakan dengan tepat. Tahap kelima alert atau peringatan untuk memberikan informasi kepada tim pemeliharaan agar dapat segera mengambil tindakan. Tahap terakhir menjadwalkan tindakan pemeliharaan yang diperlukan berdasarkan peringatan untuk menghindari kegagalan mendadak dan meminimalkan downtime.","3.1 Tahapan Penelitian
Tahapan penelitian merupakan serangkaian langkah-langkah yang dilakukan dalam penelitian. Gambaran mengenai tahapan penelitian ini dapat dilihat pada Gambar 3.1. Tahapan penelitian ini terdiri dari studi literatur untuk memahami keadaan yang terfokus terhadap tentang mesin kompresor reciprocating, metode prediksi pemeliharaan mesin, predictive maintenance, dan machine learning. Preprocessing data melibatkan pembersihan data, normalisasi, dan transformasi data agar siap digunakan dalam model machine learning. Pengembangan dan melatih model machine learning menggunakan algoritma yang telah dipilih dengan data yang telah diperoses, tahap ini melibatkan pembagian data menjadi set pelatihan dan set pengujian, serta termasuk mengatur parameter model untuk mencapai kinerja terbaik. Tahap keempat analisis prediksi untuk memperkirakan kapan dan bagaimana kegagalan akan terjadi, sehingga pemeliharaan dapat direncanakan dengan tepat. Tahap kelima alert atau peringatan untuk memberikan informasi kepada tim pemeliharaan agar dapat segera mengambil tindakan. Tahap terakhir menjadwalkan tindakan pemeliharaan yang diperlukan berdasarkan peringatan untuk menghindari kegagalan mendadak dan meminimalkan downtime."
"3.1 Kerangka Umum
     Penelitian ini bertujuan untuk mengembangkan sistem deteksi dini kantuk sebelum berkendara dengan menggunakan kombinasi data visual berupa data citra wajah dan data fisiologis. Kondisi pre-driving mengacu pada kondisi sebelum pengemudi memulai perjalanan, sehingga sistem ini sangat penting untuk mencegah risiko kecelakaan di jalan. Sistem ini mengintegrasikan teknologi pengenalan wajah dan analisis data fisiologis untuk memberikan deteksi yang lebih akurat. Blok diagram secara umum yang digunakan pada penelitian ini dapat dilihat pada Gambar 3.1 Blok Diagram. Model ini terdiri dari tiga tahapan yaitu input, proses, dan output. Penelitian deteksi dini kantuk untuk kondisi pre-driving menggabungkan data visual yaitu pengumpulan data citra wajah pengemudi yang diambil menggunakan kamera, serta data fisiologis yang diukur berupa data EKG menggunakan perangkat wearable yaitu smartwatch dan pulse oximeter untuk mengukur saturasi oksigen (SpO2). Tahapanpre-processing dan ekstraksi fitur dilakukan pada kedua j enis data yaitu data citra gambar dan data fisiologis. Model Convolutional Neural Network (CNN) digunakan untuk mengekstraksi fitur dari data citra wajah yang merupakan data visual, sementara Long Short-Term Memory (LSTM) digunakan untuk memproses data fisiologis yang bersifat time-series. Fitur-fitur yang diekstraksi dari kedua model ini digabungkan untuk menghasilkan vector fitur gabungan. Vektor fitur ini kemudian digunakan sebagai input untuk model Support Vector Machine (SVM) yang melakukan klasifikasi akhir untuk mendeteksi kantuk. Hasil deteksi kemudian digunakan untuk memberikan peringatan kepada pengemudi layak tidak nya pengemudi untuk berkendara. 3.2 Tahapan Peneletian
     Tahapan penelitian merupakan urutan atau langkah-langkah yang dilakukan secara terstruktur dan sistematis pada penelitian ini, secara garis besar terbagi menjadi empat tahapan. Berikut adalah Gambar 3.2 Tahapan Penelitian yang dilakukan pada penelitian ini. 3.3. Pemilihan dan Persiapan Dataset
     Tahapan ini merupakan tahapan identifikasi awal dari penelitian meliputi identifikasi masalah penelitian yang berfokus pada masalah utama yaitu mendeteksi kantuk pada pengemudi menggunakan pemrosesan citra dan fisiologis. Tahapan ini dilakukan untuk memasikan bahwa hanya data yang relevan, berkualitas tinggi, dan siap untuk diproses lebih lanjut yang digunakan. Pemilihan dataset memastikan bahwa dataset yang dikumpulkan relevan dengan tujuan penelitian, yaitu hanya menggunakan data yang berkaitan dengan kondisi pre-driving, serta memastikan bahwa data visual dan data fisiologis diambil pada waktu yang sama. Tahapan pengumpulan data dan pre-processing data merupakan tahap awal untuk mempersiapkan dataset yang akan digunakan. 3.3.1 Pengumpulan Data
     Data dibagi menjadi dua kategori utama yaitu data primer dan data sekunder. Data primer diperoleh berdasarkan pengumpulan dan pengamatan langsung oleh peneliti berdasarkan kondisi subjek penelitian dan rekaman aktivitas fisik atau ekspresi wajah menggunakan kamera, serta pengukuran fisiologis yang menggunakan perangkat wearable. Data primer ini berupa data objektif dengan mengumpulkan data citra wajah dan pengukuran fisiologis. Berikut merupakan Gambar 3.3 Pengumpulan Data. Dataset visual berupa citra wajah yang berfokus pada wajah pengemudi yang diambil menggunakan kamera dengan spesifikasi 12 MP. Data visual dan fisiologis berupa data yang diambil dari partisipan dalam kondisi terjaga dan mengantuk. Data fisiologis mencakup pengukuran langsung dari respons tubuh berupa sinyal EKG (Elektrokardiogram) yang merekam detak jantung (HR), variabilitas detak jantung atau Heart Rate Variability (HRV) menggunakan perangkat wearable dan pengukuran saturasi oksigen dalam darah (SpO2) yang diukur menggunakan pulse oximeter. 3.3.2 Pre-Processing Data
      Melakukan analisis eksploratif data untuk memahami karakteristik dataset sehingga meningkatkan kualitas deteksi. Pre-Processing yang dilakukan yaitupreprocessing citra dan pre-processing data fisiologis. Pre-processing citra yaitu dengan mendeteksi wajah dan mata, normalisasi pencahayaan, pemotongan area wajah yang relevan. Ektraksi frame dari video menggunakan OpenCV. Preprocessing data fisiologis yaitu dengan normalisasi data, dan segmentasi. Berikut merupakan Gambar 3.4 Pre-Processing Data. Dataset yang dikumpulkan kemudian diolah, yang meliputi normalisasi, 
penghilangan noise, dan teknik pra-pemrosesan lainnya untuk membuat data siap 
digunakan dalam ekstraksi fitur. Langkah ini melibatkan pembersihan dan penyiapan data 
untuk analisis. Proses pre-processing untuk data visual atau data gambar, yaitu:
1. Pengumpulan data visual dengan mengambil gambar wajah pengemudi menggunakan kamera berfokus pada mata. 2. Deteksi wajah dan deteksi mata, menggunakan algoritma deteksi wajah seperti Haar Cascades atau Dlib untuk mendetekasi dan melokalisasi wajah dalam gambar. 3. Deteksi mata, yaitu mendeteksi mata da;am area wajah yang terdeteksi. 4. Ekstraksi ROI (Region of Interest) dengan mengambil area mata dari gambar. 5. Teknik normalisasi untuk mengubah ukuran gambar mata menjadi dimensi yang konsisten missal nya 64x64 pixel, serta menormalisasi nilai pixel gambar dalam rentang [0, 1] atau [-1, 1]. 6. Augmentasi gambar dilakukan untuk meningkatkan variasi data, seperti rotasi, flipping horizontal atau vertikal, zooming, dan perubahan cahaya
7. Penyimpanan data yang diproses dengan menyimpan gambar yang telah diproses dan fitur yang diekstraski dalam format terstruktur (CSV atau database). Proses pre-processing untuk data fisiologis yaitu:
1. Pengumpulan data fisiologis menggunakan wearable untuk merekam detak jantung (HR), variabilitas detak jantung (HRV), dan saturasi oksigen (SpO2). 2. Pembersihan data dengan menghilangkan noise dengan menggunakan teknik filtering, dan imputasi data hilang dengan mengisi data yang hilang menggunakan metode seperti mean, median, atau interpolasi. 3. Normalisasi data dengan Min-Max sehingga menyesuaikan dengan skala data ke rentang yang konsisten [0, 1]. 4. Segmentasi data dilakukan dengan membagi data menjadi segmen dengan ukuran waktu tetap (yaitu 30 detik). 5. Normalisasi data untuk memastikan konsistensi skala antar subjek dan pengukuran. 6. Penyimpanan data yang diproses yaitu data fisiologis dalam format terstruktur. Langkah selanjutnya yaitu sinkronisasi data dengan menggabungkan data visual serta data fisiologis berdasarkan timestamp. Selanjutnya memastikan bahwa data visual dan fisiologis yang telah disinkronkan mencerminkan kondisi yang sama pada waktu yang sama. Selanjutnya yaitu menyimpan data yang telah disinkronkan dalam format yang mudah diakses untuk dianalisis lebih lanjut. 3.4. Pembuatan Model
     Pembuatan model merupakan proses implementasi dari desain arsitektur yang telah direncanakan. Langkah dari pembuatan model yaitu penulisan kode untuk membangun model sesuai dengan desain arsitektur yaitu CNN, LSTM, dan SVM. Selanjutnya, mengonfigurasi model dengan optimizer, fungsi loss, dan metrik evaluasi. Kemudian melakukan pelatihan model menggunakan dataset yang telah dibagi menjadi training set dan validation set pada tahapan pre-processing. Selanjutnya dilakukan validasi serta tuning hyperparameters untuk mengoptimalkan kinerja model. 3.4.1	Ekstraksi Fitur
     Ekstraksi fitur dilakukan untuk menangkap karakteristik penting dari data yang telah diproses. Fitur ini akan digunakan sebagai input untuk model pembelajaran mesin. Ektraksi fitur dilakukan pada data visual berupa data gambar, dan data fisiologis. 1. Data Visual
a. Eye Aspect Ratio (EAR), digunakan untuk mendeteksi apakah mata terbuka atau tertutup. b. Pupil Dilation, digunakan untuk mengukur perubahan ukuran pupil. c. Redness of Eyes, mengukur tingkat kemerahan pada mata. d. Eye Openess, mengukur bukaan mata berdasarkan jarak vertikal antara kelopak mata atas dan bawah. 2. Data Fisiologis
a. Heart Rate (HR), mengukur detak jantung per menit. b. Heart Rate Variability (HRV), mengukur variabilitas detak jantung. c. Respiratory Rate (RR), mengukur laju pernapasan. d. SpO2, saturasi oksigen dalam darah
     Ekstraksi fitur dengan Convolutional Neural Network (CNN) adalah proses yang menggunakan lapisan konvolusi dan pooling untuk menangkap fitur penting dari data gambar. Langkah-langkah ekstraksi fitur dengan CNN:
1. Convolutional Layer: Menggunakan filter untuk menangkap fitur spasial dari gambar. 2. Pooling Layer: Mengurangi dimensi peta fitur sambil mempertahankan fitur penting. 3. Fully Connected Layer: Menghubungkan peta fitur yang telah diratakan untuk melakukan klasifikasi atau ekstraksi fitur. 4. Pelatihan Model: Menyesuaikan bobot filter melalui backpropagation dengan data latih. 5. Ekstraksi Fitur: Menggunakan model yang telah dilatih untuk mengekstraksi fitur dari gambar baru. Ekstraksi fitur dengan Long Short-Term Memory (LSTM) adalah proses yang menggunakan jaringan LSTM untuk menangkap pola temporal dan hubungan jangka panjang dalam data sekuensial, seperti data fisiologis (EKG, HR, HRV, RR, dan SpO2). LSTM sangat efektif dalam menangani data yang memiliki ketergantungan waktu. Ekstraksi fitur dengan LSTM melibatkan beberapa langkah penting:
1. Menyiapkan Data: Menyiapkan data sekuensial dalam bentuk yang sesuai untuk input ke LSTM. 2. Membangun Model LSTM: Membangun model LSTM dengan lapisan LSTM dan Dense untuk ekstraksi fitur. 3. Melatih Model LSTM: Melatih model menggunakan data sekuensial untuk menyesuaikan bobot jaringan. 4. Ekstraksi Fitur: Menggunakan model yang telah dilatih untuk mengekstraksi fitur dari data sekuensial baru. 3.4.2 Penggabungan Fitur
      Fitur-fitur yang telah diekstraksi dari ekstraksi fitur dengan model CNN yaitu dari gambar visul dengan mengekstraksi bagian mata dan ektraksi fitur dari data sekuensial dengan menggunakan LSTM berupa data fisiologis. Selanjutnya, penggabungan fitur visual dan fitur sekuensial menggunakan metode penggabungan (concatenation) digabungkan membentuk satu set fitur komprehensif yang akan digunakan untuk pelatihan model yaitu klasifikasi akhir menggunakan model SVM. 3.4.3 Pemisahan Dataset
      Pembagian dataset merupakan langkah penting dalam proses pelatihan dan evaluasi model. Merujuk pada penelitian (Li, K., Gong, Y., & Ren, Z., 2020) untuk pembagian dataset dibagi menjadi tiga bagian yaitu training set (40%), validation set (10%), dan test set (50%), namun pada penelitian ini pembagian dataset yang terdiri dari data gambar dan data fisiologis dibagi menjadi berikut:
1. Training Set (75%), data yang digunakan untuk melatih melatih model. 2. Validation Set (15%), digunakan untuk tuning hyperparameters dan memilih model terbaik. 3. Test Set (15%), digunakan untuk mengevaluasi kinerja akhir model. 3.4.4 Desain Arsitektur
      Desain arsitektur merupakan proses menentukan struktur dan komponen model yang akan dibangun, yang terdiri dari jenis model, jumlah dan jenis layer, fungsi aktivasi, teknik regularisasi, dan konfigurasi model. Jenis model penelitian ini melibatkan dua model utama yaitu Convolutional Neural Network (CNN) untuk data visual dan Long Short-Term Memory (LSTM) untuk data fisiologis. Hasil dari kedua model digabungkan dan diklasifikasikan menggunakan Support Vector Machine (SVM). Model ini terdiri dari tiga tahapan yaitu akuisisi data, preprocessing data, ekstraksi fitur, penggabungan fitur, dan klasifikasi dengan SVM, dan output sistem. Berikut merupakan Gambar 3.5 Arsitektur Model. Tahap ini mencakup perancangan arsitektur CNN yang akan digunakan, termasuk pemilihan jumlah dan jenis layer, fungsi aktivasi, dan teknik regularisasi. Digunakan untuk mengolah data visual, seperti mengenali mata tertutup atau mulut menguap sebagai indikator kantuk. LSTM digunakan untuk menganalisis data fisiologis yang berurutan, seperti pola detak jantung yang menunjukkan kelelahan atau penurunan kewaspadaan. Menggabungkan fitur yang diekstrak dari CNN dan LSTM untuk mendapatkan representasi data yang komprehensif, memastikan bahwa model dapat mengidentifikasi kantuk berdasarkan kombinasi indikator visual dan fisiologis. Selanjutnya yaitu menggunakan Support Vector Machines (SVM) untuk mengklasifikasikan data sebagai ""kantuk"" atau ""tidak kantuk"". SVM dipilih karena kemampuannya dalam mengklasifikasikan data yang kompleks dan memberikan batas keputusan yang jelas ""layak"" atau ""tidak layak"" pengemudi untuk berkendara. Jika pengklasifikasi mendeteksi keadaan mengantuk, maka pengklasifikasi menghasilkan alarm atau notifikasi pemberitahuan untuk memberi tahu bahwa pengemudi tidak layak untuk berkendara atau kembali ke fase pertama dan memulai ulang prosedur. 3.4.5 Pelatihan Model dengan Dataset
     Pelatihan model dilakukan dengan menggunakan training set, dengan tuning hyperparamaters berdasarkan kinerja pada validation set. Pelatihan model dilakukan dengan model SVM menggunakan training set. 3.5 Evaluasi
     Model gabungan ini dievaluasi menggunakan metrik seperti akurasi, presisi, recall, dan F1-score untuk memastikan performa dan keandalannya. Implementasi sistem ini diharapkan dapat memberikan notifikasi atau peringatan kepada pengemudi jika tanda-tanda kantuk terdeteksi selama kondisi pre-driving, sehingga dapat meningkatkan keselamatan berkendara secara signifikan. Berdasarkan hasil validasi, model dapat di-tune atau dioptimalkan untuk meningkatkan performa, misalnya dengan mengubah arsitektur, parameter, atau teknik training. 3.6 Implementasi
     Setelah penyempurnaan, model dianggap siap untuk digunakan. Model ini harus dapat secara akurat mendeteksi kantuk pengemudi dalam berbagai kondisi dengan minimal kesalahan. Langkah selanjutnya yaitu penerapan model dalam sistem nyata dan pemantauan efektivitasnya dalam kondisi pengemudi pada lingkungan pre-driving. Model yang telah dioptimalkan diintegrasikan ke dalam sistem deteksi dini kantuk untuk pengujian awal. Selanjutnya yaitu melakukan uji coba lapangan untuk mengevaluasi efektivitas sistem dalam kondisi nyata, memungkinkan pengumpulan feedback untuk perbaikan lebih lanjut.","3.1 Kerangka Umum
     Penelitian ini bertujuan untuk mengembangkan sistem deteksi dini kantuk sebelum berkendara dengan menggunakan kombinasi data visual berupa data citra wajah dan data fisiologis. Kondisi pre-driving mengacu pada kondisi sebelum pengemudi memulai perjalanan, sehingga sistem ini sangat penting untuk mencegah risiko kecelakaan di jalan. Sistem ini mengintegrasikan teknologi pengenalan wajah dan analisis data fisiologis untuk memberikan deteksi yang lebih akurat. Blok diagram secara umum yang digunakan pada penelitian ini dapat dilihat pada Gambar 3.1 Blok Diagram. Model ini terdiri dari tiga tahapan yaitu input, proses, dan output. Penelitian deteksi dini kantuk untuk kondisi pre-driving menggabungkan data visual yaitu pengumpulan data citra wajah pengemudi yang diambil menggunakan kamera, serta data fisiologis yang diukur berupa data EKG menggunakan perangkat wearable yaitu smartwatch dan pulse oximeter untuk mengukur saturasi oksigen (SpO2). Tahapanpre-processing dan ekstraksi fitur dilakukan pada kedua j enis data yaitu data citra gambar dan data fisiologis. Model Convolutional Neural Network (CNN) digunakan untuk mengekstraksi fitur dari data citra wajah yang merupakan data visual, sementara Long Short-Term Memory (LSTM) digunakan untuk memproses data fisiologis yang bersifat time-series. Fitur-fitur yang diekstraksi dari kedua model ini digabungkan untuk menghasilkan vector fitur gabungan. Vektor fitur ini kemudian digunakan sebagai input untuk model Support Vector Machine (SVM) yang melakukan klasifikasi akhir untuk mendeteksi kantuk. Hasil deteksi kemudian digunakan untuk memberikan peringatan kepada pengemudi layak tidak nya pengemudi untuk berkendara. 3.2 Tahapan Peneletian
     Tahapan penelitian merupakan urutan atau langkah-langkah yang dilakukan secara terstruktur dan sistematis pada penelitian ini, secara garis besar terbagi menjadi empat tahapan. Berikut adalah Gambar 3.2 Tahapan Penelitian yang dilakukan pada penelitian ini. Pemilihan dan Persiapan Dataset
     Tahapan ini merupakan tahapan identifikasi awal dari penelitian meliputi identifikasi masalah penelitian yang berfokus pada masalah utama yaitu mendeteksi kantuk pada pengemudi menggunakan pemrosesan citra dan fisiologis. Pemilihan dataset memastikan bahwa dataset yang dikumpulkan relevan dengan tujuan penelitian, yaitu hanya menggunakan data yang berkaitan dengan kondisi pre-driving, serta memastikan bahwa data visual dan data fisiologis diambil pada waktu yang sama. Dataset visual berupa citra wajah yang berfokus pada wajah pengemudi yang diambil menggunakan kamera dengan spesifikasi 12 MP. Data visual dan fisiologis berupa data yang diambil dari partisipan dalam kondisi terjaga dan mengantuk. 3.3.2 Pre-Processing Data
      Melakukan analisis eksploratif data untuk memahami karakteristik dataset sehingga meningkatkan kualitas deteksi. Augmentasi gambar dilakukan untuk meningkatkan variasi data, seperti rotasi, flipping horizontal atau vertikal, zooming, dan perubahan cahaya
7. Langkah dari pembuatan model yaitu penulisan kode untuk membangun model sesuai dengan desain arsitektur yaitu CNN, LSTM, dan SVM. Ekstraksi fitur dengan LSTM melibatkan beberapa langkah penting:
1. Membangun Model LSTM: Membangun model LSTM dengan lapisan LSTM dan Dense untuk ekstraksi fitur. 3.4.4 Desain Arsitektur
      Desain arsitektur merupakan proses menentukan struktur dan komponen model yang akan dibangun, yang terdiri dari jenis model, jumlah dan jenis layer, fungsi aktivasi, teknik regularisasi, dan konfigurasi model. Hasil dari kedua model digabungkan dan diklasifikasikan menggunakan Support Vector Machine (SVM). Model ini terdiri dari tiga tahapan yaitu akuisisi data, preprocessing data, ekstraksi fitur, penggabungan fitur, dan klasifikasi dengan SVM, dan output sistem. Digunakan untuk mengolah data visual, seperti mengenali mata tertutup atau mulut menguap sebagai indikator kantuk. Menggabungkan fitur yang diekstrak dari CNN dan LSTM untuk mendapatkan representasi data yang komprehensif, memastikan bahwa model dapat mengidentifikasi kantuk berdasarkan kombinasi indikator visual dan fisiologis. Selanjutnya yaitu menggunakan Support Vector Machines (SVM) untuk mengklasifikasikan data sebagai ""kantuk"" atau ""tidak kantuk"". SVM dipilih karena kemampuannya dalam mengklasifikasikan data yang kompleks dan memberikan batas keputusan yang jelas ""layak"" atau ""tidak layak"" pengemudi untuk berkendara. Jika pengklasifikasi mendeteksi keadaan mengantuk, maka pengklasifikasi menghasilkan alarm atau notifikasi pemberitahuan untuk memberi tahu bahwa pengemudi tidak layak untuk berkendara atau kembali ke fase pertama dan memulai ulang prosedur. 3.4.5 Pelatihan Model dengan Dataset
     Pelatihan model dilakukan dengan menggunakan training set, dengan tuning hyperparamaters berdasarkan kinerja pada validation set. Pelatihan model dilakukan dengan model SVM menggunakan training set. 3.5 Evaluasi
     Model gabungan ini dievaluasi menggunakan metrik seperti akurasi, presisi, recall, dan F1-score untuk memastikan performa dan keandalannya. Implementasi sistem ini diharapkan dapat memberikan notifikasi atau peringatan kepada pengemudi jika tanda-tanda kantuk terdeteksi selama kondisi pre-driving, sehingga dapat meningkatkan keselamatan berkendara secara signifikan. Berdasarkan hasil validasi, model dapat di-tune atau dioptimalkan untuk meningkatkan performa, misalnya dengan mengubah arsitektur, parameter, atau teknik training. 3.6 Implementasi
     Setelah penyempurnaan, model dianggap siap untuk digunakan. Model ini harus dapat secara akurat mendeteksi kantuk pengemudi dalam berbagai kondisi dengan minimal kesalahan. Langkah selanjutnya yaitu penerapan model dalam sistem nyata dan pemantauan efektivitasnya dalam kondisi pengemudi pada lingkungan pre-driving. Model yang telah dioptimalkan diintegrasikan ke dalam sistem deteksi dini kantuk untuk pengujian awal. Selanjutnya yaitu melakukan uji coba lapangan untuk mengevaluasi efektivitas sistem dalam kondisi nyata, memungkinkan pengumpulan feedback untuk perbaikan lebih lanjut."
"3.1 Alur Penelitian
     Gambar 3.1 menunjukkan metode penelitian. Terdapat 5 tahap utama yang akan dilakukan, yang pertama adalah studi literatur untuk menyusun bab 1 dan bab
2. Tahap kedua adalah pengumpulan citra ekspresi wajah (data citra berupa data primer dan data sekunder). Tahap ketiga adalah pembentukan dataset untuk tiap model (SVM, CNN, dan MNN), skenario pembentukan dataset dilakukan berdasarkan pada penelitian (Robert, 2023). Pada tahap keempat dilakukan pembentukan model, khusus untuk SVM dan CNN menggunakan model pada penelitian (Robert, 2023), sedangkan MNN menggunakan usulan pada penelitian ini. Tahap terakhir adalah pelatihan dan pengujian untuk semua model (SVM, CNN, MNN), terdapat tahap parameter tuning untuk tiap model. Kemudian semua performa dari tiap model akan dibandingkan satu sama lain, dan juga dianalisis pada bab 4. Gambar 3.1. Metode penelitian
3.2 Pengumpulan Citra Ekspresi Wajah
Citra ekspresi wajah dikumpulkan secara langsung oleh peneliti (data primer) dan juga menggunakan data yang dikumpulkan oleh peneliti lain (data sekunder). Terdapat 7 ekspresi wajah yang akan digunakan dalam penelitian ini, yaitu: marah, jijik, menghina, senang, sedih, kaget, dan netral (tanpa ekspresi). Gambar 3.2 menunjukkan contoh 7 ekspresi wajah manusia yang digunakan penelitian ini. Gambar 3.2. Contoh 7 jenis ekspresi wajah
Dataset primer akan dilakukan pengambilan citra ekspresi wajah mahasiswa Universitas Gunadarma baik pria maupun wanita. Pengambilan akan dilakukan dari beberapa sudut pandang guna menambah variasi dataset. Gambar 3.3 menunjukan contoh dataset primer dari berbagai sudut pandang. Gambar 3.3. Contoh dataset primer
Dataset sekunder digunakan dataset yang telah digunakan umum oleh peneliti lain terkait pengenalan ekspresi wajah. Terdapat beberapa dataset yang umum digunakan dalam penelitian ekspresi wajah. Pertama, Extended Cohn- Kanade (CK+) yang berisi citra ekspresi wajah pria dan wanita dari berbagai etnis dengan resolusi tinggi (Kanade, Cohn, & Tian, 2000; Lucey et al., 2010). Kedua, Taiwanese Facial Expression Image Dataset (TFEID) yang berisi citra ekspresi wajah pria dan wanita dari etnis Taiwan (Chen & Yen, 2007). Ketiga, Japanese Female Facial Expression (JAFFE) yang terdiri dari citra ekspresi wajah wanita etnis Jepang (Lyons, 2021; Lyons, Kamachi, & Gyoba, 2020). Gambar 3.4 menunjukan contoh citra dataset CK+ (a), JAFFE (b), dan TFEID (c). Gambar 3.4. Contoh dataset sekunder
Tabel 3.1 menunjukkan detail dari tiap dataset, mulai dari jumlah citra dari tiap kelas serta ruang warna dan ukuran citra. CK+ memiliki jumlah yang tidak seimbang pada kelas neutral dan memiliki ruang warna campur antara RGB dan Gray, dengan ukuran citra dikisaran 640 490. JAFFE dataset memiliki jumlah citra pada tiap kelas yang seimbang dengan perbedaan diantara 0 hingga 2 citra, ukuran citra 256 256, dan ruang warna grayscale. TFEID juga memiliki jumlah citra yang seimbang ditiap kelas, ukuran citra dikisaran 481 600, ruang warna RGB. 3.3 Pembentukan Dataset
Secara garis besar, dalam pembuatan model AI (khususnya ML dan DL) terdapat proses yang berperan penting, yaitu preprocessing dataset seperti ekstrasi fitur, penyesuaian ukuran citra, dan augmentasi (Deshmukh et al., 2016; Franchi et al., 2020; Mohammad & Ali, 2011; Ravi et al., 2020; Sawardekar & Naik, 2018; Shan et al., 2009). Pada penelitian (Robert, 2023), dilakukan sebuah skenario pembentukan dataset menggunakan beberapa metode pengolahan citra (seperti konversi warna ke grayscale, deteksi wajah, dan ekstrasi fitur), di mana preprocessing mempengaruhi performa dari model ML dan DL. Selain itu, pada penelitian (Alam & Yao, 2019) juga dilakukan penelitian yang serupa, di mana preprocessing mempengaruhi performa model machine learning. Pada tahap ketiga, dilakukan pembentukan dataset. Gambar 3.5 menunjukkan alur pembentukan dataset untuk SVM. Pertama, dilakukan pendeteksian wajah menggunakan VJA, proses ini berguna untuk mengurangi noise pada citra. Hasil VJA membuat ukuran citra bervariasi, oleh karena itu dilakukan resizing citra untuk menyamakan semua ukuran citra dan juga menyesuaikan dengan dimensi input model. Selanjutnya dilakukan konversi warna citra dari RGB ke Grayscale dikarena fitur warna tidak dibutuhkan dan agar dapat diekstrasi fiturnya menggunakan LMP. Terakhir, terdapat dua proses ekstrasi fitur berbeda. Proses ekstrasi fitur pertama menggunakan LMP (Robert, 2023). Proses ekstrasi fitur kedua adalah usulan dari penelitian ini, di mana pertama diaplikasikan Gabor Filter terlebih dahulu kemudian diekstrasi menggunakan LMP. Gambar 3.5. Pembentukan dataset untuk SVM
Gambar 3.6 menunjukkan alur pembentukan dataset untuk CNN dan MNN. Terdapat 3 proses yang akan dilakukan. Pertama, dilakukan deteksi wajah menggunakan VJA guna mengurangi noise. Kemudian dilakukan konversi warna dari RGB ke Grayscale karena fitur warna tidak dibutuhkan untuk mengenali ekspresi wajah. Terakhir, mengubah ukuran citra untuk menyamakan semua ukuran citra dan sesuai dengan dimensi input model. Skema pembentukan dataset ini berdasarkan performa terbaik dari penelitian (Robert, 2023). Gambar 3.6. Pembentukan dataset untuk CNN dan MNN
Dataset yang sudah melalui pembentukan dataset, dilakukan augmentasi dari sisi geometris seperti membalikan flipping secara horizontal dan vertikal, dan rotasi dari 0  hingga 45 . Tujuan dari augmentasi dataset adalah memperbanyak dataset dan juga variasi dataset. Dataset yang telah dibentuk kemudian dibagi menjadi dua jenis dataset. Jenis pertama adalah training dataset dengan jumlah 90% dari total semua dataset. Kedua adalah testing dataset yang terdiri dari 10% dari total semua dataset. Gambar 3.7 menunjukkan visualisasi pembagian dataset. SVM training dataset digunakan untuk melatih model, sedangkan testing dataset digunakan untuk menguji dataset. CNN dan MNN terdapat pembagian lagi pada training, di mana 90% dari training dataset digunakan untuk melatih model dan 10% dari training dataset digunakan untuk validasi, dan testing dataset digunakan untuk menguji model. Proses pembentukan dataset untuk SVM, CNN, dan MNN menggunakan Algoritma 3.1 hingga Algoritma 3.6. Gambar 3.7. Visualisasi pembagian dataset
3.3.1 Deteksi wajah
Proses deteksi wajah menggunakan VJA, VJA memanfaatkan dua komponen yaitu integral image dan Haar Basis Function. Algoritma 3.1 menunjukkan cara menghitung dari integral image. Input merupakan citra dengan ruang warna grayscale (dengan nama variabel img) dengan ukruan ? ? h dan output berupa integral image (yang disimpan pada nama variabel itg_img). Algoritma integral image cukup sederhana, baris 1 dan 2 dilakukan perulangan terhadap baris dan kolom citra yang digunakan untuk menentukan koordinat integral image yang sedang dihitung. Baris ketiga dilakukan perhitungan integral image untuk koordinat (? ?, ? ?) yang menghitung total nilai piksel citra asli mulai dari koordinat (0,0) hingga (? ?, ? ?) menggunakan Persamaan (2.18). Algoritma 3.1. Integral image
Algoritma 3.2 menunjukkan cara kerja dari VJA. Input dari Algoritma 3.2 adalah citra integral dengan ukuran ? ? h yang diproses menggunakan Algoritma
3.1. Terdapat beberapa parameter yang digunakan pada Algoritma 3.2. Parameter pertama adalah detection window dengan ukuran ? ?2   h2 yang digunakan untuk perhitungan Haar. Parameter kedua adalah Haar yang menggunakan Gambar 2.14. Parameter ketiga adalah nilai threshold untuk masing-masing Haar yang digunakan untuk menentukan apakah Haar tersebut merupakan fitur wajah atau bukan. Output dari algoritma ini adalah berupa koordinat wajah yang dimulai pada koordinat [? ?1, ? ?1] dengan panjang dan lebar yaitu [? ?2, h2]. Baris pertama dan kedua dilakukan perulangan terhadap baris dan kolom citra yang digunakan untuk menentukan koordinat detction window, agar lebih mudah memahami dapat melihat Gambar 3.8. Setiap perulangan pada baris pertama dan kedua, dilakukan komputasi tiap area Haar (mulai dari jenis Haar (a) hingga (d)). Pada baris 4,9,13,17 terdapat tiga perhitungan. Perhitungan pertama adalah area putih, perhitungan kedua adalah area hitam, perhitungan ketiga adalah fitur (area putih - area hitam), untuk lebih mudah memahami perhitungan area putih, area hitam, dan fitur dapat melihat Gambar 3.9. Setiap perhitungan nilai fitur (a) hingga fitur (d), dilakukan pengecekan apakah nilai fitur yang dihitung merupakan fitur atau bukan dengan cara membandingkan nilai fitur dengan threshold masing- masing (TH_a hingga TH_d) seperti baris 6,10,14,18. Jika salah satu dari keempat nilai fitur lebih kecil dari threshold yang ditentukan maka Haar tersebut bukan merupakan fitur wajah dan algoritma akan melakukan pergeseran detection window ke koordinat selanjutnya. Selain itu, jika semua nilai fitur lebih besar dari threshold maka detection window tersebut merupakan wajah, dan algoritma akan memberikan empat nilai yaitu ? ?, ? ?, ? ?? ?, ? ?h. ? ?, ? ? merupakan koordinat dari deteksi terakhir. ? ?? ?, ? ?h merupakan luas dari detection window itu sendiri. Algoritma 3.2. Algoritma Viola-Jones
Input	: citra integral w h ==> itg_gry
Parameter : detection window dengan ukuran w2 h2 ==> dw
Gambar 3.8. Pergeseran Detection Window
Gambar 3.9. Contoh perhitungan Haar
Gambar 3.9 menunjukkan contoh perhitungan untuk fitur a dan b pada Algoritma 3.2 baris 4 dan 9. Pertama, dilakukan perhitungan pada area hitam dan putih. Untuk mendapatkan hanya luas ? ? dilakukan pengurangan dengan luas ? ? dan ? ? kemudian dilakukan pertambahan luas ? ?. Perhitungan dilakukan secara demikian dikarenakan pada integral image luas ? ? = ? ? + ? ? + ? ? + ? ?, luas ? ? = ? ? + ? ?, dan luas ? ? = ? ? + ? ?. Secara teknis, pengurangan dengan luas ? ? dilakukan dua kali
dikarenakan luas ? ? dan ? ?, oleh karena itu dilakukan pertambahan luas ? ? setelah pengurangan dengan ? ? dan ? ?. Perhitungan luas ? ? memiliki pola yang sama dengan perhitungan luas ? ?. Kemudian dilakukan pengurangan antara area putih dengan hitam untuk mendapatkan nilai fitur. Fitur kemudian dibandingkan dengan nilai thershold untuk menentukan apakah Haartersebut fitur wajah atau bukan. Pola untuk perhitungan area putih, area hitam, dan fitur untuk Haar (a), (b), (c), (d) memiliki pola yang sama. Gambar 3.10 menunjukkan hasil dari implementasi algoritma deteksi wajah pada sebuah citra. 3.3.2 Image Resize
Proses perubahan ukuran citra menggunakan metode bicubic interpolation. Algoritma 3.3 menunjukkan cara kerja dari bicubic interpolation. Input berupa citra (yang ingin diubah ukurannya) dan rasio (skala ukuran yang diinginkan). Parameter berupa koefisien a yang dapat mempengaruhi koordinat tetangga (umumnya nilai dikisaran -0.75 hingga -0.5). Hasil dari algoritma ini adalah citra dengan ukuran [(?? ? ? ), (?? ? ? ), ? ?]. Pada baris 1 dilakukan perhitungan ? ?? ?, ? ??? yang digunakan untuk menentukan ukuran citra setelah diubah ukurannya dan kemudian dilakukan perhitungan h yang digunakan untuk konstanta yang akan mempengaruhi koordinat tetangga pada piksel yang akan dihitung. Baris 2 dilakukan pembuatan matriks yang digunakan untuk menyimpan hasil. Baris 4,5,6 dilakukan perulangan pada channel,
? ?? ?, ? ??? (matriks yang dibuat pada baris 2), perulangan ini digunakan untuk menentukan koordinat matriks hasil yang akan dihitung. Baris 6 dan 7 dilakukan perhitungan ? ? dan ? ?, yang merupakan konstanta yang mempengaruhi koordinat tetangga (nilai berubah setiap perulangan berjalan pada baris 3, 4, 5). Baris 8 hingga 15 dilakukan perhitungan ? ?1 hingga ? ?4 dan ? ?1 hingga ? ?4, variabel tersebut digunakan untuk menentukan bobot (weight) tiap tetangga dan koordinat tetangga pada piksel yang sedang dihitung. Baris 16 dan 17 dilakukan perhitungan menggunakan persamaan (2.19), di mana baris 16 perhitungan untuk bobot horizontal, baris 17 untuk bobot vertikal yang masing-masing disimpan pada matrix_l dan matrix_r. Baris 19 dilakukan pembuatan matriks berukuran 4 4 yang digunakan untuk menyimpan nilai piksel tetangga. Baris 20 hingga 35 merupakan pengambilan nilai tetangga berdasarkan dari perhitungan sebelumnya. Baris 36 merupakan perkalian antara matriks tetangga 4 4 dengan bobot horizontal, kemudian dikalikan dengan bobot vertikal. Hasil yang didapatkan kemudian disimpan pada variabel output. Algoritma 3.3. Image Resize (menggunakan Bicubic Interpolation)
Gambar 3.11 menunjukkan hasil perubahan ukuran citra wajah menggunakan Algoritma 3.3. Berdasarkan Gambar 3.11 ukuran dari input citra adalah 600 480 dan ratio = 0,5. Ukuran menjadi 300 240 setelah melalui proses algoritma perubahan ukuran citra. 3.3.3 Color conversion
      Proses konversi warna dari RGB ke Grayscale menggunakan Persamaan (2.2). Algoritma 3.4 menujukan proses konversi ruang warna citra dari RGB ke Grayscale dengan cara mengambil nilai Y dari YCbCr. Input dari algoritma ini adalah citra RGB yang memiliki dimensi ? ? ? ? ? ? yang diberi nama variabel img_rgb. Output dari algoritma ini adalah citra grayscale yang disimpan pada variable img_gry. Baris 1 dan 2 dilakukan perulangan baris dan kolom citra yang digunakan untuk menentukan koordinat citra grayscale yang akan dihitung. Baris 3 dilakukan konversi citra grayscale menggunakan Persamaan (2.2), di mana dilakukan penjumlahan nilai dari ketiga channel citra RGB. Tiap channel memiliki bobot masing-masing, untuk channel R memiliki bobot 0.299, channel G memiliki bobot 0.587, dan channel B memiliki bobot 0.114. Gambar 3.12 menunjukkan contoh hasil implementasi algoritma konversi warna dari RGB ke Grayscale (dengan menggunakan nilai Y dari YCbCr). 3.3.4 Gabor Filter
Pada proses SVM terdapat proses Gabor filter sebelum diekstraksi menggunakan LMP. Pertama dilakukan pembuatan filter terlebih dahulu. Algoritma
3.5 menunjukkan cara pembuatan Gabor filter. Input dari algoritma ini adalah sebuah citra grayscale yang memiliki ukuran [? ?, ? ?]. Kemudian jumlah filter, luas kernel, lambda (?? ), psi (?? ), sigma ? ?, dan gamma ? ? yang akan digunakan untuk pembuatan Gabor filter. Terakhir adalah threshold bawah dan threshold atas yang digunakan untuk deteksi tepi. Pada baris 1 dilakukan perulangan untuk menentukan rotasi (nilai theta) Gabor filter berdasarkan jumlah filter yang digunakan. Baris 2 dilakukan pembuatan Gabor filter berdasarkan Persamaan (2.23) dan parameter yang di-input. Gambar 3.13 menunjukkan contoh Gabor filter dengan total 16 filter. Pada baris 4 dilakukan perulangan terhadap filter-filter, di mana filter-filter tersebut digunakan untuk konvolusi citra pada baris 5. Pada baris 7 dilakukan deteksi tepi menggunakan Canny edge detection dengan parameter lower threshold dan upper threshold. Algoritma 3.5. Gabor Filter
3.3.5 Ekstrasi fitur LMP
Proses ekstrasi fitur menggunakan metode LMP. Algoritma 3.6 menunjukkan proses dari ekstrasi fitur LMP. Input dari algoritma LMP adalah citra grayscale dengan ukuran ? ? h. Parameter yang digunakan adalah pattern yang memiliki dimensi 8   2 yang digunakan untuk menentukan arah perhitungan tetangga. Hasil dari Algoritma 3.6 adalah citra LMP dengan ukuran (?? - 4)   (h - 4), ukuran citra berkurang untuk menghindari perhitungan di luar dari ukuran citra. Baris 2 dan 3 dilakukan perulangan baris dan kolom citra yang digunakan untuk menentukan koordinat citra LMP yang akan dihitung. Baris 4 menyimpan nilai piksel berdasarkan koordinat baris 2 dan 3. Baris 5 dilakukan perulangan untuk mengambil arah dari ptn secara berurutan. Baris 6 dan 7 digunakan untuk megambil nilai piksel tetangga radius pertama (disimpan pada variable cur_val1) dan nilai piksel tetangga radius kedua (disimpan pada variable cur_val2). Baris 9 dilakukan perbandingan nilai tengah (ctr_val) dengan nilai tetangga radius pertama (cur_val1) berdasarkan (2.22). Baris 14 dilakukan perbandingan nilai tetangga radius pertama dengan nilai tetangga radius kedua (cur_val2) berdasarkan persamaan (2.21). Kemudian dilakukan perhitungan nilai LMP menggunakan persamaan (2.20). Gambar 3.15 menunjukkan contoh hasil implementasi algoritma LMP pada citra wajah. Algoritma 3.6. Local Monotonic Pattern. 3.4 Pembentukan Model
3.4.1 Pembentukan Model SVM
Dalam penelitian sebelumnya (Robert, 2023), telah dilakukan pengenalan ekspresi wajah menggunakan SVM dengan menggunakan 4 kernel yaitu: Linear, Polynomial, Sigmoid, dan RBF. Pada penelitian tersebut dataset TFEID yang digunakan untuk melatih dan menguji model. Dataset TFEID memiliki keterbatasan dalam jumlah dan variasi etnis, di mana hanya terdapat 1 etnis saja yaitu orang Taiwan. Berdasarkan dari penelitian (Robert, 2023), didapatkan model terbaik untuk mengenal ekspresi wajah adalah menggunakan kernel Sigmoid. Pada penelitian ini akan dilakukan pengujian ulang SVM dengan dataset gabungan berdasarkan usulan. Terdapat 4 kernel yang akan digunakan untuk model SVM dalam penelitian ini yaitu: Linear, Polynomial, Sigmoid, dan RBF. Persamaan (2.29), (2.30), (2.31), (2.32)menunjukkan persamaan dari keempat kernel yang digunakan secara berurutan. Selain kernel terdapat parameter yang juga akan dikonfigurasi yaitu C. Terdapat beberapa nilai C berada diantara 0 hingga 100. 3.4.2 Pembentukan Model CNN
Dalam penelitian sebelumnya (Robert, 2023), model CNN yang digunakan adalah MobileNetV2, yang merupakan pre-trained model. Penelitian tersebut menggunakan dataset TFEID yang memiliki keterbatasan baik dalam jumlah maupun variasi dari etnis ekspresi wajah manusia. Oleh karena itu, dalam penelitian ini akan dilakukan pelatihan dan pengujian ulang menggunakan dataset gabungan sesuai dengan usulan yang telah diuraikan sebelumnya. Tabel 3.2 menunjukkan arsitektur MobileNetV2 dari konvolusi layer pertama hingga output layer. Kolom pertama menunjukkan tipe dari layer dan juga stride yang digunakan. Kolom kedua menunjukan ukuran filter dan jumlah filter. Kolom terakhir menunjukkan ukuran masukan citra dari layer sebelumnya. Fungsi aktivasi (activation function) pada output layer adalah Softmax, selain itu model juga dilakukan parameter tuning pada Learning rate, batch size, dan fungsi aktivasi pada FC-Layer saat dilakukan proses pelatihan. Tabel 3.2. Arsitektur MobileNetV2
3.4.3 Pembentukan Model MNN
Gambar 3.16 menunjukan alur dari pembentukan MNN. Pertama dilakukan penentuan SE yang digunakan. Dalam penelitian ini akan digunakan bentuk SE berdasarkan penelitian lain (Shen et al., 2019), dan SE yang diusulkan pada penelitian ini. Selanjutnya dilakukan penentuan operasi morfologi, pertama akan digunakan operasi morfologi yang telah dilakukan peneliti lain dan juga operasi morfologi usulan pada penelitian ini. Kemudian dilakukan uji coba, analisis, dan dibandingkan pada semua operasi morfologi dan SE yang diusulkan pada penelitian ini. Terakhir dilakukan perancangan arsitektur MNN berdasarkan dari analisis dari tahap ketiga. Gambar 3.16. Proses pembentukan model
3.4.3.1 Ekstrasi Fitur
3.4.3.1.1 Penentuan Structure Element
Dalam penelitian ini digunakan dua SE dan tiga operasi morfologi yang dibandingkan satu dengan yang lain. Gambar 3.17 menunjukkan SE bentuk beserta ukuran yang digunakan untuk operasi morfologi. Terdapat dua bentuk SE yang digunakan, yaitu disk dan kotak. Terdapat empat ukuran SE yang digunakan, yaitu 3 3, 5 5, 9 9, dan 15 15, ukuran digunakan untuk kedua bentuk. Di mana kotak berwarna putih bernilai 1 dan hitam adalah 0. Gambar 3.17. Struktur element yang digunakan
3.4.3.1.2 Penentuan Operasi Morfologi
Tahap setelah penentuan bentuk dan juga ukuran SE yang akan digunakan adalah penentuan operasi morfologi yang digunakan. Terdapat tiga operasi yang digunakan dalam penelitian ini. Operasi pertama adalah operasi morgologi opening pada citra original, kemudian dilakukan pengurangan nilai piksel antara citra original dengan hasil opening, operasi opening itu sendiri adalah operasi erosi yang dilanjutkan operasi dilasi. Operasi kedua adalah erosi pada citra original, kemudian dilakukan pengurangan citra original terhadap hasil erosi. Ketiga adalah dilasi pada citra original, kemudian dilakukan pengurangan hasil dilasi terhadap citra original. Ketiga operasi tersebut menggunakan Algoritma 3.7, Algoritma 3.8, dan Algoritma 3.9 secara berurutan. Algoritma 3.7. Gradien Morfologi Opening
Pada Algoritma 3.7. terdapat dua input yang digunakan, pertama adalah citra original dengan ukuran ? ? ? ?. Kedua adalah structure element dengan ukuran ? ? ? ?. Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? ? ?). Pada baris pertama dilakukan perhitungan space yang digunakan untuk menentukan koordinat mulai operasi opening. Baris 2 dan 3 dilakukan perulangan baris dan kolom citra yang digunakan untuk menentukan koordinat operasi opening. Baris 4 digunakan untuk menyimpan kandidat nilai piksel pada baris 5 hingga 11. Baris 12 dilakukan pengambilan nilai terkecil yang digunakan untuk sebagai hasil fitur citra berdasarkan persamaan. Kemudian baris 15 dan 16 dilakukan perulangan baris dan kolom citra untuk menentukan koordinat operasi opening. Baris 17 digunakan untuk menyimpan kandidat nilai piksel pada baris 18 hingga 24. Baris 25 dilakukan pengambilan nilai terbesar yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.14). Kemudian pada baris 28 dan 29 dilakukan perulangan kembali pada baris dan kolom citra untuk menentukan koordinat perhitungan. Baris 30 dilakukan pengurangan antara citra original dengan citra opening. Algoritma 3.8. Gradien Morfologi Erosi
Pada Algoritma 3.8. terdapat dua input yang digunakan, pertama adalah citra original dengan ukuran ? ? ? ?. Kedua adalah structure element dengan ukuran ? ? ? ?. Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? ? ?). Pada baris pertama dilakukan perhitungan space yang digunakan untuk menentukan koordinat mulai operasi opening. Baris 1 dan 2 dilakukan perulangan baris dan kolom citra yang digunakan untuk menentukan koordinat operasi opening. Baris 3 digunakan untuk menyimpan kandidat nilai piksel pada baris 4 hingga 10. Baris 11 dilakukan pengambilan nilai terkecil yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.10). Kemudian pada baris 14 dan 15 dilakukan perulangan kembali pada baris dan kolom citra untuk menentukan koordinat perhitungan. Baris 16 dilakukan pengurangan antara citra original dengan citra opening. Algoritma 3.9. Gradien Morfologi Dilasi
Pada Algoritma 3.9. terdapat dua input yang digunakan, pertama adalah citra original dengan ukuran ? ? ? ?. Kedua adalah structure element dengan ukuran ? ? ? ?. Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? ? ?). Baris 1 dan 2 dilakukan perulangan baris dan kolom citra yang digunakan untuk menentukan koordinat operasi dilasi. Baris 3 digunakan untuk menyimpan kandidat nilai piksel pada baris 4 hingga 10. Baris 11 dilakukan pengambilan nilai terbesar yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.8). Kemudian pada baris 14 dan 15 dilakukan perulangan kembali pada baris dan kolom citra untuk menentukan koordinat perhitungan. Baris 16 dilakukan pengurangan antara citra dilasi dengan citra original. 3.4.3.1.3 Percobaan dan Perbandingan Hasil Operasi Morfologi
Tabel 3.2 menunjukkan hasil operasi morfologi menggunakan SE berdasarkan 3.4.3.1 dan 3.4.3.2. Pada operasi morfologi Tabel 3.2. digunakan citra yang memiliki objek persegi panjang berwarna putih. Di mana putih pada citra bernilai 1 sedangkan hitam adalah 0. Tabel 3.3. Operasi Morfologi dan Structure Element pada Persegi Panjang
Berdasarkan dari Tabel 3.3 dapat dilihat operasi GMO dengan SE disk 3 3 menghasilkan corner pada persegi panjang. Operasi GMD dengan SE disk 3 3 memiliki hasil garis berbentuk persegi panjang dengan nilai tiap corner terdapat hilang. Operasi GME dengan disk 3 3 mirip dengan GMD dengan SE disk 3 3 namun corner dari GME utuh. Operasi GMO menggunakan SE kotak 3 3 tidak dapat mengekstrasi fitur dari persegi. Operasi GMD dengan SE kotak 3 3 menghasilkan garis yang berbentuk persegi panjang. GME dengan SE kotak 3 3 memiliki hasil yang mirip dengan GMD, namun memiliki luas yang berbeda. Pada GMD garis terlelak pada luar objek sedangkan GME berada pada dalam objek. Kemudian dilakukan operasi menggunakan 3 ukuran SE lain yaitu 5 5, 7 7, 9 9, dan 15 15. Dari hasil yang didapatkan, tiap ukuran memiliki hasil yang mirip dengan operasi yang digunakan. GMO mengekstrasi tiap sudut pada persegi dengan perbedaan pada ukuran sudut, di mana semakin besar SE semakin besar sudut yang didapatkan. GMD dapat mengkestrasi tepi dari persegi, di mana semakin besar ukuran SE semakin tebal garis yang didapatkan. GME dapat mengekstrasi tepi dari persegi, di mana semakin besar ukuran SE semakin tebal garis yang didapatkan. Operasi GMD dan GME memiliki hasil yang mirip dengan perbedaan tepi pada GMD menebal kearah luar persegi, sedangkan tepi pada GME menebal kearah dalam. Tabel 3.4 merupakan hasil morfologi menggunakan citra wajah dengan warna grayscale. Nilai intensitas piksel pada citra berkisaran antara 0 hingga 255. Operasi morfologi dan SE yang digunakan berdasarkan usulan pada subbab 3.4.3.1 dan 3.4.3.2. Tabel 3.4. Structure Element dan Operasi Morfologi pada Wajah
Berdasarkan Tabel 3.4, dapat dilihat hasil ekstrasi fitur menggunakan berbagai kombinasi dua bentuk dan lima ukuran. Pertama dilakukan ekstrasi fitur menggunakan SE ukuran 3 3 terhadap semua bentuk SE dan operasi. SE disk 3 3 dengan operasi GMO, fitur tidak terlihat. SE disk 3 3 dengan operasi GME atau GMD, fitur terlihat namun tidak jelas. SE kotak 3 3 dengan operasi GMO, fitur juga tidak terlihat. SE kotak 3 3 dengan operasi GME atau GMD fitur sedikit terlihat dan tidak memiliki perbedaan secara kasat mata. Kemudian dilakukan menggunakan SE ukuran 5 5. SE disk 5 5 dengan operasi GMO terdapat sangat sedikit fitur yang didapatkan yaitu pada bagian mata. SE disk 5 5 dengan operasi GME atau GMD fitur lebih jelas terlihat dan terdapat perbedaan, di mana operasi GME tepi pada bagian mata dengan bola mata tergabung sedangkan GMD tidak. Selain itu luas lubang hidung dan mulut juga lebih besar pada operasi GME dibandingkan operasi GMD. SE kotak 5 5 juga memiliki hasil yang sama dengan disk 5 5 dengan perbedaan yang tidak dapat dilihat kasat mata. Kemudian dilakukan operasi menggunakan SE ukuran 7 7. SE disk 7 7 dengan operasi GMO terdapat sangat sedikit fitur yang didapatkan yaitu pada bagian mata. SE disk 7 7 dengan operasi GME atau GMD fitur lebih jelas terlihat dan terdapat perbedaan, di mana operasi GME tepi pada bagian mata dengan bola mata tergabung sedangkan GMD tidak. Selain itu luas lubang hidung dan mulut juga lebih besar pada operasi GME dibandingkan operasi GMD. SE kotak 7 7 juga memiliki hasil yang sama dengan disk 7 7 dengan perbedaan yang tidak dapat dilihat kasat mata. Pada operasi menggunakan SE ukuran 9 9. SE disk 9 9 dengan operasi GMO fitur yang didapatkan tidak akurat karena tidak mengekstrasi tepi (bentuk) dari ekspresi wajah. SE disk 9 9 dengan operasi GME dan GMD memiliki fitur yang mirip dengan SE 5 5, di mana operasi GME tepi pada bagian mata tergabung sedangkan GMD tidak. Selain itu luas pada lubang hidung dan mulut juga lebih luas dibandingkan dengan SE 5 5, dan memiliki garis tepi yang lebih tebal. SE kotak 9 9 dengan operasi GMO, fitur yang didapatkan mirip dengan disk 9 9 dengan operasi GMO namun memiliki fitur yang lebih jelas terlihat. SE kotak 9 9 dengan operasi 9 9 GME fitur yang didapatkan terlihat dengan jelas dan mirip dengan hasil yang menggunakan SE disk 9 9 dengan operasi GME. Namun terdapat perbedaan fitur yang signifikan pada bagian hidung, di mana bentuk dari hidung sedikit berubah menjadi sedikit kotak. SE kotak 9 9 dengan operasi GMD, hasil yang didapatkan hampir sama dengan hasil yang menggunakan SE disk 9 9 dengan operasi GMD, hanya terdapat perbedaan pada tebal garis pada hidung. Operasi menggunakan SE 15 15, hasil yang didapatkan memiliki pola yang sama dengan operasi yang menggunakan SE 9 9. SE disk atau kotak 15 15 dengan operasi GMO fitur lebih terlihat jelas dibandingkan dengan ukuran 9 9. SE disk atau kotak 15 15 dengan operasi GME fitur lebih tebal, namun untuk SE kotak bagian hidung menjadi lebih kotak dari ukuran sebelumnya. Berdasarkan dari hasil yang didapatkan dan analisis. SE disk 5 5 atau kotak 5 5 dengan operasi GMD memiliki hasil terbaik. Tepi pada bagian mata tergabung, bagian mulut tidak menjadi lebih luas. Namun pada bagian hidung menjadi lebih kecil dibandingkan citra original. Selain itu operasi menggunakan GME dengan SE disk 5 5 atau kotak 5 5 juga memberikan hasil yang baik. Tepi dari tiap komponen wajah terlihat namun tepi pada mata dan bola mata tergabung, namun memberikan hasil ekstrasi fitur bagian hidung lebih baik dibandingkan GMD. Karena luas hidung lebih sesuai pada GME dibandingkan GMD. 3.4.3.2 Pembentukan dan Pelatihan Model
Gambar 3.18 menunjukkan arsitektur dari MNN secara garis besar. Di mana terdapat 5 layer utama yang memiliki tugas masing-masing. Di mana terdapat beberapa variasi arsitektur Variasi pertama terdapat pada morphology layer, di mana akan digunakan dua jenis ekstrasi fitur berdasarkan pada hasil subbab 3.4.3.3. Variasi kedua terdaoat pada hidden layer, di mana akan digunakan beberapa kombinasi fully-connected layer. Gambar 3.18. Model MNN yang diusulkan
Input Layer adalah layer pertama dari model MNN yang bertugas untuk menerima input berupa citra. Di mana dimensi dari intput layer itu sendiri sesuai dengan ukuran citra pada dataset yaitu 160 160. Kedua adalah morphology layer. Menerima masukan dari input layer kemudian dilakukan proses morfologi. Terdapat dua jenis morfologi layer yang akan digunakan. Morfology layer pertama adalah dilation layer kemudian subtraction layer. Di mana lapisan morfologi jenis pertama menggunakan hasil terbaik dari operasi morfologi dilasi pada subbab 3.4.3.3. Morphology layer jenis kedua adalah erosion layer dilanjutkan subtraction layer. Di mana lapisan morfologi jenis kedua ini menggunakan hasil terbaik dari operasi morfologi erosi pada subab 3.4.3.3. Lapisan ketiga adalah flatten layer, lapisan yang bertugas mengubah citra menajadi feature vector. Masukan dari lapisan ini adalah hasil dari subtraction layer pada lapisan morfologi. Di mana hasil dari subtraction layer adalah citra dengan ukuran 160 160 yang kemudian diubah mnejadi satu dimensi yaitu 25600. Lapisan keempat adalah Fully Connected Layer (FC Layer) yang bertugas untuk mempelajari dan menganalisa nilai feature vector dari flatten layer. Pada lapisan ini dilakukan beberapa konfigurasi FC Layer mulai dari jumlah FC Layer, dan jumlah Neuron pada FC Layer. Konfigurasi pertama akan diuji coba 2 FC Layer dengan masing-masing neuron adalah 512, dan 256. Konfigurasi kedua akan dicoba 1024, dan 512. Kemudian dari situ akan dicoba analisis mana yang lebih baik sehingga dapat dikonfigurasi lebih lanjut. Jika konfigurasi pertama memiliki hasil lebih baik artinya memungkinkan FC Layer untuk dibuat lebih sederhana dengan mengurangi jumlah neuron. Jika konfigurasi kedua lebih baik artinya terdapat kemungkinan untuk meninkatkan performa dari model karena dataset memiliki kompleksitas tinggi. Beberapa lapisan akan dilakukan tuning paramter. Tuning pertama terdapat pada bentuk SE, ukuran SE, jenis operasi pada morphological layer. Tuning kedua terdapat pada FC-Layer yaitu fungsi aktivasi (ReLu/Sigmoid/Tanh), dan jumlah neuron pada tiap hidden layer. Selain arsitektur, pada proses pelatihan juga dilakukan tuning pada learning rate, jumlah epoch, dan batch size. Terakhir adalah Output Layer, merupakan penentuan dari ekspresi berdasarkan dari bobot hidden layer. Di mana fungsi aktivasi yang digunakan untuk output layer adalah softmax yang artinya output berupa probabilitas dari tiap ekspresi. Kemudian untuk loss function yang akan digunakan adalah categorical crossentropy, dimana fungsi loss ini digunakan jika model memprediksi multi- kelas (multi-class prediction).","3.1 Alur Penelitian
     Gambar 3.1 menunjukkan metode penelitian. Terdapat 5 tahap utama yang akan dilakukan, yang pertama adalah studi literatur untuk menyusun bab 1 dan bab
2. Tahap kedua adalah pengumpulan citra ekspresi wajah (data citra berupa data primer dan data sekunder). Tahap ketiga adalah pembentukan dataset untuk tiap model (SVM, CNN, dan MNN), skenario pembentukan dataset dilakukan berdasarkan pada penelitian (Robert, 2023). Pada tahap keempat dilakukan pembentukan model, khusus untuk SVM dan CNN menggunakan model pada penelitian (Robert, 2023), sedangkan MNN menggunakan usulan pada penelitian ini. Tahap terakhir adalah pelatihan dan pengujian untuk semua model (SVM, CNN, MNN), terdapat tahap parameter tuning untuk tiap model. Kemudian semua performa dari tiap model akan dibandingkan satu sama lain, dan juga dianalisis pada bab 4. Gambar 3.1. Metode penelitian
3.2 Pengumpulan Citra Ekspresi Wajah
Citra ekspresi wajah dikumpulkan secara langsung oleh peneliti (data primer) dan juga menggunakan data yang dikumpulkan oleh peneliti lain (data sekunder). Terdapat 7 ekspresi wajah yang akan digunakan dalam penelitian ini, yaitu: marah, jijik, menghina, senang, sedih, kaget, dan netral (tanpa ekspresi). Gambar 3.2 menunjukkan contoh 7 ekspresi wajah manusia yang digunakan penelitian ini. Gambar 3.2. Contoh 7 jenis ekspresi wajah
Dataset primer akan dilakukan pengambilan citra ekspresi wajah mahasiswa Universitas Gunadarma baik pria maupun wanita. Pengambilan akan dilakukan dari beberapa sudut pandang guna menambah variasi dataset. Gambar 3.3 menunjukan contoh dataset primer dari berbagai sudut pandang. Gambar 3.3. Contoh dataset primer
Dataset sekunder digunakan dataset yang telah digunakan umum oleh peneliti lain terkait pengenalan ekspresi wajah. Terdapat beberapa dataset yang umum digunakan dalam penelitian ekspresi wajah. Pertama, Extended Cohn- Kanade (CK+) yang berisi citra ekspresi wajah pria dan wanita dari berbagai etnis dengan resolusi tinggi (Kanade, Cohn, & Tian, 2000; Lucey et al., 2010). Kedua, Taiwanese Facial Expression Image Dataset (TFEID) yang berisi citra ekspresi wajah pria dan wanita dari etnis Taiwan (Chen & Yen, 2007). Ketiga, Japanese Female Facial Expression (JAFFE) yang terdiri dari citra ekspresi wajah wanita etnis Jepang (Lyons, 2021; Lyons, Kamachi, & Gyoba, 2020). Gambar 3.4 menunjukan contoh citra dataset CK+ (a), JAFFE (b), dan TFEID (c). Gambar 3.4. Contoh dataset sekunder
Tabel 3.1 menunjukkan detail dari tiap dataset, mulai dari jumlah citra dari tiap kelas serta ruang warna dan ukuran citra. CK+ memiliki jumlah yang tidak seimbang pada kelas neutral dan memiliki ruang warna campur antara RGB dan Gray, dengan ukuran citra dikisaran 640 490. JAFFE dataset memiliki jumlah citra pada tiap kelas yang seimbang dengan perbedaan diantara 0 hingga 2 citra, ukuran citra 256 256, dan ruang warna grayscale. TFEID juga memiliki jumlah citra yang seimbang ditiap kelas, ukuran citra dikisaran 481 600, ruang warna RGB. 3.3 Pembentukan Dataset
Secara garis besar, dalam pembuatan model AI (khususnya ML dan DL) terdapat proses yang berperan penting, yaitu preprocessing dataset seperti ekstrasi fitur, penyesuaian ukuran citra, dan augmentasi (Deshmukh et al., 2016; Franchi et al., 2020; Mohammad & Ali, 2011; Ravi et al., 2020; Sawardekar & Naik, 2018; Shan et al., 2009). Pada penelitian (Robert, 2023), dilakukan sebuah skenario pembentukan dataset menggunakan beberapa metode pengolahan citra (seperti konversi warna ke grayscale, deteksi wajah, dan ekstrasi fitur), di mana preprocessing mempengaruhi performa dari model ML dan DL. Selain itu, pada penelitian (Alam & Yao, 2019) juga dilakukan penelitian yang serupa, di mana preprocessing mempengaruhi performa model machine learning. Pada tahap ketiga, dilakukan pembentukan dataset. Gambar 3.5 menunjukkan alur pembentukan dataset untuk SVM. Pertama, dilakukan pendeteksian wajah menggunakan VJA, proses ini berguna untuk mengurangi noise pada citra. Hasil VJA membuat ukuran citra bervariasi, oleh karena itu dilakukan resizing citra untuk menyamakan semua ukuran citra dan juga menyesuaikan dengan dimensi input model. Selanjutnya dilakukan konversi warna citra dari RGB ke Grayscale dikarena fitur warna tidak dibutuhkan dan agar dapat diekstrasi fiturnya menggunakan LMP. Terakhir, terdapat dua proses ekstrasi fitur berbeda. Proses ekstrasi fitur pertama menggunakan LMP (Robert, 2023). Proses ekstrasi fitur kedua adalah usulan dari penelitian ini, di mana pertama diaplikasikan Gabor Filter terlebih dahulu kemudian diekstrasi menggunakan LMP. Gambar 3.5. Pembentukan dataset untuk SVM
Gambar 3.6 menunjukkan alur pembentukan dataset untuk CNN dan MNN. Terdapat 3 proses yang akan dilakukan. Pertama, dilakukan deteksi wajah menggunakan VJA guna mengurangi noise. Kemudian dilakukan konversi warna dari RGB ke Grayscale karena fitur warna tidak dibutuhkan untuk mengenali ekspresi wajah. Terakhir, mengubah ukuran citra untuk menyamakan semua ukuran citra dan sesuai dengan dimensi input model. Skema pembentukan dataset ini berdasarkan performa terbaik dari penelitian (Robert, 2023). Gambar 3.6. Pembentukan dataset untuk CNN dan MNN
Dataset yang sudah melalui pembentukan dataset, dilakukan augmentasi dari sisi geometris seperti membalikan flipping secara horizontal dan vertikal, dan rotasi dari 0  hingga 45 . SVM training dataset digunakan untuk melatih model, sedangkan testing dataset digunakan untuk menguji dataset. CNN dan MNN terdapat pembagian lagi pada training, di mana 90% dari training dataset digunakan untuk melatih model dan 10% dari training dataset digunakan untuk validasi, dan testing dataset digunakan untuk menguji model. Visualisasi pembagian dataset
3.3.1 Deteksi wajah
Proses deteksi wajah menggunakan VJA, VJA memanfaatkan dua komponen yaitu integral image dan Haar Basis Function. Parameter ketiga adalah nilai threshold untuk masing-masing Haar yang digunakan untuk menentukan apakah Haar tersebut merupakan fitur wajah atau bukan. Output dari algoritma ini adalah berupa koordinat wajah yang dimulai pada koordinat [? Selain itu, jika semua nilai fitur lebih besar dari threshold maka detection window tersebut merupakan wajah, dan algoritma akan memberikan empat nilai yaitu ? Gambar 3.10 menunjukkan hasil dari implementasi algoritma deteksi wajah pada sebuah citra. Baris 2 dilakukan pembuatan matriks yang digunakan untuk menyimpan hasil. Image Resize (menggunakan Bicubic Interpolation)
Gambar 3.11 menunjukkan hasil perubahan ukuran citra wajah menggunakan Algoritma 3.3. Gambar 3.15 menunjukkan contoh hasil implementasi algoritma LMP pada citra wajah. 3.4 Pembentukan Model
3.4.1 Pembentukan Model SVM
Dalam penelitian sebelumnya (Robert, 2023), telah dilakukan pengenalan ekspresi wajah menggunakan SVM dengan menggunakan 4 kernel yaitu: Linear, Polynomial, Sigmoid, dan RBF. Berdasarkan dari penelitian (Robert, 2023), didapatkan model terbaik untuk mengenal ekspresi wajah adalah menggunakan kernel Sigmoid. Operasi pertama adalah operasi morgologi opening pada citra original, kemudian dilakukan pengurangan nilai piksel antara citra original dengan hasil opening, operasi opening itu sendiri adalah operasi erosi yang dilanjutkan operasi dilasi. Operasi kedua adalah erosi pada citra original, kemudian dilakukan pengurangan citra original terhadap hasil erosi. Ketiga adalah dilasi pada citra original, kemudian dilakukan pengurangan hasil dilasi terhadap citra original. Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? Baris 12 dilakukan pengambilan nilai terkecil yang digunakan untuk sebagai hasil fitur citra berdasarkan persamaan. Baris 25 dilakukan pengambilan nilai terbesar yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.14). Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? Baris 11 dilakukan pengambilan nilai terkecil yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.10). Hasil dari algoritma ini adalah sebuah fitur dalam bentuk citra dengan ukuran (?? Baris 11 dilakukan pengambilan nilai terbesar yang digunakan untuk sebagai hasil fitur citra berdasarkan Persamaan (2.8). 3.4.3.1.3 Percobaan dan Perbandingan Hasil Operasi Morfologi
Tabel 3.2 menunjukkan hasil operasi morfologi menggunakan SE berdasarkan 3.4.3.1 dan 3.4.3.2. Operasi Morfologi dan Structure Element pada Persegi Panjang
Berdasarkan dari Tabel 3.3 dapat dilihat operasi GMO dengan SE disk 3 3 menghasilkan corner pada persegi panjang. Operasi GMD dengan SE disk 3 3 memiliki hasil garis berbentuk persegi panjang dengan nilai tiap corner terdapat hilang. Operasi GMD dengan SE kotak 3 3 menghasilkan garis yang berbentuk persegi panjang. GME dengan SE kotak 3 3 memiliki hasil yang mirip dengan GMD, namun memiliki luas yang berbeda. Dari hasil yang didapatkan, tiap ukuran memiliki hasil yang mirip dengan operasi yang digunakan. Operasi GMD dan GME memiliki hasil yang mirip dengan perbedaan tepi pada GMD menebal kearah luar persegi, sedangkan tepi pada GME menebal kearah dalam. Tabel 3.4 merupakan hasil morfologi menggunakan citra wajah dengan warna grayscale. Structure Element dan Operasi Morfologi pada Wajah
Berdasarkan Tabel 3.4, dapat dilihat hasil ekstrasi fitur menggunakan berbagai kombinasi dua bentuk dan lima ukuran. SE kotak 5 5 juga memiliki hasil yang sama dengan disk 5 5 dengan perbedaan yang tidak dapat dilihat kasat mata. SE kotak 7 7 juga memiliki hasil yang sama dengan disk 7 7 dengan perbedaan yang tidak dapat dilihat kasat mata. SE disk 9 9 dengan operasi GMO fitur yang didapatkan tidak akurat karena tidak mengekstrasi tepi (bentuk) dari ekspresi wajah. Selain itu luas pada lubang hidung dan mulut juga lebih luas dibandingkan dengan SE 5 5, dan memiliki garis tepi yang lebih tebal. SE kotak 9 9 dengan operasi GMO, fitur yang didapatkan mirip dengan disk 9 9 dengan operasi GMO namun memiliki fitur yang lebih jelas terlihat. SE kotak 9 9 dengan operasi 9 9 GME fitur yang didapatkan terlihat dengan jelas dan mirip dengan hasil yang menggunakan SE disk 9 9 dengan operasi GME. Namun terdapat perbedaan fitur yang signifikan pada bagian hidung, di mana bentuk dari hidung sedikit berubah menjadi sedikit kotak. SE kotak 9 9 dengan operasi GMD, hasil yang didapatkan hampir sama dengan hasil yang menggunakan SE disk 9 9 dengan operasi GMD, hanya terdapat perbedaan pada tebal garis pada hidung. Operasi menggunakan SE 15 15, hasil yang didapatkan memiliki pola yang sama dengan operasi yang menggunakan SE 9 9. SE disk atau kotak 15 15 dengan operasi GMO fitur lebih terlihat jelas dibandingkan dengan ukuran 9 9. SE disk atau kotak 15 15 dengan operasi GME fitur lebih tebal, namun untuk SE kotak bagian hidung menjadi lebih kotak dari ukuran sebelumnya. Berdasarkan dari hasil yang didapatkan dan analisis. SE disk 5 5 atau kotak 5 5 dengan operasi GMD memiliki hasil terbaik. Tepi pada bagian mata tergabung, bagian mulut tidak menjadi lebih luas. Namun pada bagian hidung menjadi lebih kecil dibandingkan citra original. Selain itu operasi menggunakan GME dengan SE disk 5 5 atau kotak 5 5 juga memberikan hasil yang baik. Tepi dari tiap komponen wajah terlihat namun tepi pada mata dan bola mata tergabung, namun memberikan hasil ekstrasi fitur bagian hidung lebih baik dibandingkan GMD. Karena luas hidung lebih sesuai pada GME dibandingkan GMD. 3.4.3.2 Pembentukan dan Pelatihan Model
Gambar 3.18 menunjukkan arsitektur dari MNN secara garis besar. Di mana terdapat 5 layer utama yang memiliki tugas masing-masing. Di mana terdapat beberapa variasi arsitektur Variasi pertama terdapat pada morphology layer, di mana akan digunakan dua jenis ekstrasi fitur berdasarkan pada hasil subbab 3.4.3.3. Variasi kedua terdaoat pada hidden layer, di mana akan digunakan beberapa kombinasi fully-connected layer. Gambar 3.18. Model MNN yang diusulkan
Input Layer adalah layer pertama dari model MNN yang bertugas untuk menerima input berupa citra. Di mana dimensi dari intput layer itu sendiri sesuai dengan ukuran citra pada dataset yaitu 160 160. Kedua adalah morphology layer. Menerima masukan dari input layer kemudian dilakukan proses morfologi. Terdapat dua jenis morfologi layer yang akan digunakan. Morfology layer pertama adalah dilation layer kemudian subtraction layer. Di mana lapisan morfologi jenis pertama menggunakan hasil terbaik dari operasi morfologi dilasi pada subbab 3.4.3.3. Morphology layer jenis kedua adalah erosion layer dilanjutkan subtraction layer. Di mana lapisan morfologi jenis kedua ini menggunakan hasil terbaik dari operasi morfologi erosi pada subab 3.4.3.3. Lapisan ketiga adalah flatten layer, lapisan yang bertugas mengubah citra menajadi feature vector. Masukan dari lapisan ini adalah hasil dari subtraction layer pada lapisan morfologi. Di mana hasil dari subtraction layer adalah citra dengan ukuran 160 160 yang kemudian diubah mnejadi satu dimensi yaitu 25600. Lapisan keempat adalah Fully Connected Layer (FC Layer) yang bertugas untuk mempelajari dan menganalisa nilai feature vector dari flatten layer. Pada lapisan ini dilakukan beberapa konfigurasi FC Layer mulai dari jumlah FC Layer, dan jumlah Neuron pada FC Layer. Konfigurasi pertama akan diuji coba 2 FC Layer dengan masing-masing neuron adalah 512, dan 256. Konfigurasi kedua akan dicoba 1024, dan 512. Kemudian dari situ akan dicoba analisis mana yang lebih baik sehingga dapat dikonfigurasi lebih lanjut. Jika konfigurasi pertama memiliki hasil lebih baik artinya memungkinkan FC Layer untuk dibuat lebih sederhana dengan mengurangi jumlah neuron. Jika konfigurasi kedua lebih baik artinya terdapat kemungkinan untuk meninkatkan performa dari model karena dataset memiliki kompleksitas tinggi. Beberapa lapisan akan dilakukan tuning paramter. Tuning pertama terdapat pada bentuk SE, ukuran SE, jenis operasi pada morphological layer. Tuning kedua terdapat pada FC-Layer yaitu fungsi aktivasi (ReLu/Sigmoid/Tanh), dan jumlah neuron pada tiap hidden layer. Selain arsitektur, pada proses pelatihan juga dilakukan tuning pada learning rate, jumlah epoch, dan batch size. Terakhir adalah Output Layer, merupakan penentuan dari ekspresi berdasarkan dari bobot hidden layer. Di mana fungsi aktivasi yang digunakan untuk output layer adalah softmax yang artinya output berupa probabilitas dari tiap ekspresi. Kemudian untuk loss function yang akan digunakan adalah categorical crossentropy, dimana fungsi loss ini digunakan jika model memprediksi multi- kelas (multi-class prediction)."
"Bab Metode Penelitian menjelaskan mengenai tahapan yang dilakukan dalam penelitian beserta menjelaskan mengenai jadwal dan estimasi waktu tahapan yang dilakukan pada penelitian ini serta menjelaskan mengenai kegiatan yang dilakukan selama penelitian. Tahapan penelitian dijelaskan dalam bentuk flowchart sehingga dapat menjelaskan proses yang dilakukan mulai dari Studi Literatur sampai dengan Kesimpulan, Jadwal dan Estimasi Penelitian digambarkan dalam bentuk Time Table untuk menjadwalkan dan melakukan estimasi waktu dari tiap tahap yang dilakukan. 3.1 Tahapan Penelitian
     Terdapat beberapa tahapan yang dilakukan untuk melakukan penelitian ini, beberapa tahapan yang dilakukan dapat dilihat pada Gambar 3.1. 3.1.1 Studi Literatur
       Tahap pertama yang dilakukan yaitu Studi Literature yang bertujuan untuk mencari informasi atau pengetahuan dari paper atau buku sebagai teori pendukung untuk melakukan penelitian dan mencari novelty atau gap peneltian yang sudah dilakukan. Paper dan buku yang digunakan dalam penelitian ini merupakan paper atau buku 5 tahun terakhir. 3.1.2 Pengumpulan Data
       Tahap kedua yaitu pengumpulan data, data yang digunakan pada penelitian ini adalah data citra digital Kelapa Sawit dengan Tingkat kematangan Belum matang, Setengah matang, Matang, Terlalu matang dan Tandan Buah yang kosong. Data diambil dari beberapa sumber melalui website Kaggle dan Roboflow, lalu dilakukan pemilihan gambar yang sesuai untuk dijadikan sebagai dataset. 3.1.3 Preprocessing Data
       Sebelum data digunakan pada model Machine Learning yang dibuat, data tersebut akan dilakukan preprocessing data agar data yang akan dilatih sesuai dengan keperluan yang dibutuhkan. Preprocessing yang dilakukan adalah augmentasi data, resize, dan Segmentasi Data. Gambar asli akan dilakukan resize menjadi ukuran 224x224 piksel lalu data tersebut akan di augmentasi untuk memperbanyak dan memvariasi data agar dan hasil augmentasi akan dijadikan sebagai data latih untuk model yang dibuat. Augmentasi yang dilakukan adalah rotationrange, widthshiftrange, heightshiftrange, brightnessrange, shearrange, zoomrange, horizontal_flip, vertical_flip. Segmentasi dilakukan untuk memisahkan objek gambar dengan latar belakang sehingga proses klasifikasi akan lebih terfokus pada objek yang akan diproses. 3.1.4 Pembuatan Model
       Tahap keempat yaitu pembuatan model machine learning menggunakan MobileNetV3 (Small/Large) dan menggabungkannya dengan attention module CBAM (Convolutional Block Attention Module). 3.1.5 Melatih Model
       Tahap Kelima yaitu melatih model yang sudah dibuat menggunakan Dataset yang sudah dikumpulkan pada tahap pengumpulan data. 3.1.6 Evaluasi Model
       Tahap Keenam yaitu evaluasi model, dari hasil pelatihan model akan dievaluasi atay dinilai apakah model tersebut sudah baik atau belum. Jika hasil dari evaluasi atau penilaian kinerja model kurang memuaskan maka tahap kelima akan dilakukan kembali. 3.1.7 Deploy Model
       Tahap Ketujuh yaitu deploy model, model yang sudah dievaluasi dan dikatakan baik akan di deploy menjadi tflite sehingga dapat diintegrasikan ke dalam mobile device. 3.1.8	Pembuatan Aplikasi
       Tahap kedelapan yaitu pembuatan aplikasi android, tahap ini dilakukan menggunakan Android Studio untuk membuat aplikasi android. Proses pembuatannya meliputi pembuatan tampilan user, memasukan tflite ke dalam aplikasi sehingga aplikasi dapat menggunakan Model Machine Learning untuk mengklasifikasi kematangan kelapa sawit menggunakan Kamera smartphone. 3.1.9 Pengujian Aplikasi
       Tahap kesembilan yaitu pengujian aplikasi yang sudah dibuat, aplikasi akan diuji fungsi utamanya yaitu klasifikasi kematangan kelapa sawit. 3.1.10 Kesimpulan Penelitian
       Tahap Kesepuluh yaitu menulis kesimpulan mengenai penelitian yang sudah dilakukan, kesimpulan ini mencakup kinerja model dan aplikasi yang sudah dibuat, kelebihan dan kekurangan penelitian dan penelitian selanjutnya yang akan dilakukan. 3.2 Jadwal Estimasi Penelitian
     Jadwal Estimasi Penelitian menjelaskan mengenai rancangan kegiatan yang dilakukan selama penelitian beserta estimasi waktu tiap kegiatan yang dilakukan. Gambar Jadwal Estimasi Penelitian dapat dilihat pada Gambar 3.3. Gambar diatas merupakan jadwal atau estimasi penelitian, kegiatan Studi Literatur dilakukan mulai dari bulan pertama pada tahun pertama, studi literatur dilakukan untuk mencari pengetahuan dan informasi untuk mendapatkan novelty dan gap dari penelitian yang akan dilakukan, pengumpulan data dilakukan mulai dari bulan kedua tahun pertama beriringan dengan studi literature. Pada bulan keenam ditahun pertama dilakukan kegiatan preprocessing data, preprocessing data yang dilakukan adalah pemilihan data yang akan digunakan pada penelitian dari proses pengumpulan data, melakukan resize, augmentasi, dan segmentasi data. Pembuatan model model dilakukan pada bulan keenam beriringan dengan kegiatan pelatihan model serta melakukan evaluasi kinerja model yang sudah dilatih. Bulan kesebelas melakukan kegiatan pembuatan aplikasi android, kegiatan yang dilakukan yaitu membuat aplikasi android menggunakan Android Studio, mendeploy model yang sudah dievaluasi menjadi format tflite lalu mengimplementasikannya ke dalam aplikasi android yang sudah dibuat. Pada bulan ketiga ditahun kedua akan dilakukan pengujian kinerja aplikasi dengan data baru lalu dilakukan evaluasi atau penilaian terhadap fungsi-fungsi aplikasi tersebut. Bulan keempat tahun kedua merupakan kegiatan terakhir yaitu menulis Kesimpulan penelitian. 3.3 Kegiatan Penelitian
     Kegiatan yang dilakukan untuk melakukan penelitian dari tahun pertama sampai tahun ketiga dapat dilihat pada Tabel 2 berikut. Kegiatan yang dilakukan pada tahun pertama yaitu melakukan studi literaur untuk pembuatan proposal penelitian (BAB 1 sampai BAB 3) lalu dilanjutkan dengan pengumpulan dan preprocessing data, setelah mendapatkan data kegiatan pembuatan dan pelatihan model dapat dilakukan. Pada tahun kedua, dilakukan evaluasi model dan saat hasil evaluasi model sudah cukup baik, model akan di deploy untuk dapat diimplementasi ke dalam aplikasi yang sudah dibuat. Aplikasi akan dievaluasi dan diuji kinerjanya sehingga mendapatkan Kesimpulan dari penelitian untuk ditulis dalam BAB 4 sampai BAB 5. Pada Akhir tahun kedua, setelah mendapatkan Kesimpulan penelitian, dilakukan pembuatan jurnal pertama dan dilanjutkan pada tahun ketiga untuk pembuatan jurnal kedua.","Tahapan penelitian dijelaskan dalam bentuk flowchart sehingga dapat menjelaskan proses yang dilakukan mulai dari Studi Literatur sampai dengan Kesimpulan, Jadwal dan Estimasi Penelitian digambarkan dalam bentuk Time Table untuk menjadwalkan dan melakukan estimasi waktu dari tiap tahap yang dilakukan. 3.1.2 Pengumpulan Data
       Tahap kedua yaitu pengumpulan data, data yang digunakan pada penelitian ini adalah data citra digital Kelapa Sawit dengan Tingkat kematangan Belum matang, Setengah matang, Matang, Terlalu matang dan Tandan Buah yang kosong. Gambar asli akan dilakukan resize menjadi ukuran 224x224 piksel lalu data tersebut akan di augmentasi untuk memperbanyak dan memvariasi data agar dan hasil augmentasi akan dijadikan sebagai data latih untuk model yang dibuat. 3.1.4 Pembuatan Model
       Tahap keempat yaitu pembuatan model machine learning menggunakan MobileNetV3 (Small/Large) dan menggabungkannya dengan attention module CBAM (Convolutional Block Attention Module). Proses pembuatannya meliputi pembuatan tampilan user, memasukan tflite ke dalam aplikasi sehingga aplikasi dapat menggunakan Model Machine Learning untuk mengklasifikasi kematangan kelapa sawit menggunakan Kamera smartphone. 3.1.9 Pengujian Aplikasi
       Tahap kesembilan yaitu pengujian aplikasi yang sudah dibuat, aplikasi akan diuji fungsi utamanya yaitu klasifikasi kematangan kelapa sawit. Kegiatan yang dilakukan pada tahun pertama yaitu melakukan studi literaur untuk pembuatan proposal penelitian (BAB 1 sampai BAB 3) lalu dilanjutkan dengan pengumpulan dan preprocessing data, setelah mendapatkan data kegiatan pembuatan dan pelatihan model dapat dilakukan. Pada tahun kedua, dilakukan evaluasi model dan saat hasil evaluasi model sudah cukup baik, model akan di deploy untuk dapat diimplementasi ke dalam aplikasi yang sudah dibuat. Aplikasi akan dievaluasi dan diuji kinerjanya sehingga mendapatkan Kesimpulan dari penelitian untuk ditulis dalam BAB 4 sampai BAB 5. Pada Akhir tahun kedua, setelah mendapatkan Kesimpulan penelitian, dilakukan pembuatan jurnal pertama dan dilanjutkan pada tahun ketiga untuk pembuatan jurnal kedua."
"3.1 Gambaran Umum Penelitian
     Penelitian ini digunakan untuk mengatasi sensitivitas terhadap cacat pada gambar ban dengan melibatkan penggunaan jaringan syaraf menggunakan algoritma Convolutional Neural Network (CNN) dan membangun model atau kerangka kerja menggunakan Keras. Berikut adalah Gambar 3.1 Blok Diagram Gambaran Umum Penelitian. Berdasarkan Gambar 3.1 Blok Diagram Gambaran Umum Penelitian maka dapat dijelaskan di blok tersebut terbagi menjadi 3 bagian yaitu bagian pertama adalah unit 
masukan berisikan data preparation di mana gambar ban dimuat, diubah menjadi format yang sesuai dipersiapkan untuk pelatihan model Convolutional Neural Network (CNN) 
seperti pemrosesan gambar ban, selanjutnya data augmentation di mana data dibuat lebih ber variasi dari training data yang ada sehingga dapat meningkatkan keberagaman 
training data tanpa harus mengambil data baru, mencakup (rotasi, pergeseran horizontal/vertikal, perbesar gambar, perubahan kecerahan gambar, sampai mengubah nilai pixel), selanjutnya data di mana dataset yang telah di augmentasi dan disiapkan dibagi menjadi subset yang berbeda untuk training untuk melatih model, validation untuk menyempurnakan model serta memvalidasi performanya selama pelatihan, dan testing untuk mengevaluasi kinerja model akhir. Dataset dibagi menjadi training data, validation data, dan testing data dalam proporsi tertentu. Bagian kedua adalah unit pemrosesan yang bertindak adalah model training (forward Pass, tahap di mana input diproses melalui model untuk menghasilkan prediksi), tujuannya melatih model Convolutional Neural Network (CNN) menggunakan dataset pelatihan di mana data dari unit masukan diteruskan melalui jaringan neural di lakukan transformasi linier (konvulasi) dan non-linier (fungsi aktivasi) dilakukan pada data di setiap lapisan untuk menghasilkan output prediksi yang melibatkan komputasi di setiap neuron dan lapisan jaringan, yang merupakan inti dari proses pembelajaran dalam jaringan saraf. Selanjutnya unit pemrosesan Fine-tuning tujuannya dilakukan untuk 
menyempurnakan model lebih lanjut setelah pelatihan awal dengan dataset yang lebih kecil atau lebih spesifik nantinya. Proses di dalam Fine-tuning menyesuaikan bobot 
(menggunakan kumpulan data yang lebih kecil untuk menyesuaikan bobot model untuk performa yang lebih baik), pelatihan khusus (fokus pada fitur data yang lebih relevan 
dengan objek). Bagian ketiga adalah unit keluaran yang bertindak ada proses model evaluatioan (backwardpass, tahap di mana gradien (memperbarui parameter model dalam arah yang 
akan mengurangi fungsi loss) dari fungsi loss (metrik yang mengukur seberapa baik atau buruk model melakukan prediksi dibandingkan nilai aktualnya) dihitung dan digunakan 
untuk memperbarui parameter model selama pelatihan) tujuannya mengevaluasi performa model yang dilatih dan model dievaluasi menggunakan metrik yang relevan (accuracy, 
precision, recall, dan F1- score) berdasarkan prediksi yang dihasilkan dari model terhadap validasi atau uji data. Output dari proses ini adalah tentang hasil evaluasi model, yang memberikan informasi kinerja model. Selanjutnya ada dua alur pilihan yang bisa dilakukan, alur pertama jika hasil prediksi sudah sesuai dengan keinginan maka bisa langsung masuk ke model deployment (inference), dan alur kedua jika hasil prediksi masih perlu diperbaiki pada bagian unit pemrosesan terlebih dahulu fine tuning untuk penggunaan data set lebih kecil (jika menunjukan model belum mencapai performa yang diharapkan) baru masuk ke model deployment (inference) tujuannya menerapkan model 
terlatih untuk membuat prediksi pada data baru yang belum terlihat. Model deployment (inference) yang telah dilatih digunakan untuk membuat prediksi pada data baru atau dalam situasi dunia nyata, tahap di mana model menerima input baru dan menghasilkan output berdasarkan pada pembelajaran yang dilakukan selama proses pelatihan dan merupakan output akhir dari keseluruhan proses, di mana model ""mengambil keputusan"" atau ""membuat prediksi"" berdasarkan pada pengalaman yang telah diperoleh selama pelatihan. 3.2. Tahapan Penelitian
      Penelitian ini di dalamnya terdapat tahapan-tahapan yang dilakukan untuk membentuk satu kesatuan yang utuh dari awal sampai akhir dan membentuk kerangka penelitian mengenai klasifikasi pada produk ban menggunakan algoritma Convolutional Neural Network (CNN). Berikut Gambar 3.2 Tahapan penelitian. Berdasarkan Gambar 3.2 Tahapan Penelitian maka dapat dijelaskan proses yang terlibat di dalamnya ada 8 yaitu studi literatur, data aquisition, data preprocessing, data augmentation, texture feature extraction, data splitting, model building, dan model evaluation & testing di mana tahap ke dua sampai lima merupakan tahap proses 
menyiapkan sebuah data sebelum dilakukan pemodelan. 3.2.1 Studi Literatur
      Tahap pertama adalah studi literatur di mana studi yang dilakukan berasal dari artikel ilmiah dan buku yang menunjang dalam menganalisis terkait dengan metode 
pengukuran kualitas, mengenai klasifikasi produk ban, meninjau penggunaan pembelajaran mesin algoritma Convolutional Neural Network (CNN) dari beberapa tahun ke belakang dalam konteks pengukuran kualitas untuk klasifikasi terhadap kondisi-kondisi produk ban. Sehingga dapat menemukan teknik terbaik yang dapat diaplikasikan 
pada masalah yang ada. Berikut merupakan Gambar 3.3 Tahapan Study Literature. 3.2.2 Data Aquisition
     Tahap kedua adalah data aquisition dengan mengumpulkan kumpulan data sesuai tujuan penelitian dengan target untuk kumpulan data gambar ban untuk training data, 
validation data, dan testing data, memastikan bahwa kumpulan data tersebut memiliki varian yang secara akurat memang mewakili kondisi produk ban dan diperoleh dari sumber-sumber terpercaya. Berikut merupakan Gambar 3.4 Tahapan Data Aquisition. 3.2.3 Data Preprocessing
     Tahap ketiga adalah data preprocessing melakukan pra-pemrosesan data untuk menyiapkan gambar untuk model pelatihan dan pengujian proses ini meliputi normalisasi 
dan penskalaan dengan fitur dalam program (ImageDataGenerator). Bermaksud merapikan, menata, dan menyiapkan data untuk pemeriksaan tambahan. Normalisasi data, pengkodean variabel, mengatasi nilai yang hilang, menghapus data yang tidak relevan atau hilang, dan modifikasi data lainnya untuk memenuhi persyaratan analisis adalah persiapan data. Berikut merupakan Gambar 3.5 Tahapan Data Preprocessing. 3.2.4 Data Augmentation
     Tahap keempat adalah data augmentation meningkatkan variasi dalam dataset dengan teknik augmentasi data, menggunakan operasi seperti rotasi, pergeserarn 
horizontal/vertikal, perbesar gambar, perubahan kecerahan gambar, sampai mengubah nilai pixel untuk memperkaya dataset dan mengurangi overfitting (Saat disajikan dengan 
data baru yang belum pernah dilihat sebelumnya, performa model akan menurun drastis karena model tersebut dapat menyesuaikan diri dengan kumpulan training data dengan 
sangat efektif). Augmentasi data dilakukan dengan dua cara secara statis dan dinamis yang artinya secara statis yaitu menambah data secara fisiknya dan dinamis tidak menambah secara fisik tetapi secara kuantitas dataset yang dapat diakses secara fisik di komputer tidak bertambah ketika ImageDataGenerator digunakan pada dataset. Sebaliknya, pada saat runtime hanya menghasilkan variasi dari gambar yang sudah ada dibuat secara dinamis dan cukup bagi model untuk berlatih dari berbagai kondisi gambar ban yang ada pada kenyataaanya. Secara lebih jelas nilai teknik augmentasi pertama dilakukan dengan manual menggunakan bantuan dari website roboflow dengan resize gambar menjadi 640 x 640, 
pada augmentasinya menggunakan model flip (horizontal dan vertikal), 90� pemutaran (searah jarum jam, berlawanan arah jarum jam, dan terbalik), rotasi ( -45� dan 45�), shear (�5� horizontal dan �5� vertikal), brightness (-20% sampai 20%). Data asli pada dataset berjumlah 1.028 data gambar ban setelah dilakukan augmentasi secara fisik menggunakan website roboflow ada data yang tidak dapat diidentifikasi ada 3 gambar sehingga total gambar asli yang berhasil di upload dan dijadikan data asli yang tetap berjumlah 1.025 data gambar dan setelah di augmentasi bertambah menjadi 2.050 data gambar ban. Rinciannya pada data asli training adalah 560 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 1.121 gambar. Rincian data asli pada Validation data berjumlah 140 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 279 gambar. Rincian data asli pada testing data berjumlah 328 gambar dan setelah dilakukan augmentasi bertambah menjadi sebanyak 650 gambar. Testing data pada prosesnya 
sebenarnya tidak mengalami augmentasi karena pada proses pengujian atau evaluasi model, ingin menggunakan data asli yang sebenarnya untuk melihat kinerja model pada 
kasus-kasus yang belum pernah dilihat sebelumnya. Augmentasi kedua yaitu dilakukan rotasi melakukan pemutaran gambar secara penuh dan secara acak dengan nilai 360 atau rentang nilai 0-360 derajat, kedua width shift 
range, yang menggeser gambar secara acak ke kiri atau kanan dengan nilai 0.05 atau gambar dapat digeser sampai 5% dari lebar aslinya. Ketiga height shift range gambar 
dapat digeser secara vertikal dengan nilai 0.05 atau gambar dapat digeser sampai 5% dari tinggi aslinya. Keempat shear range untuk menggeser gambar dengan sudut geser 
berlawanan arah jarum jam dengan nilai 0.05. kelima zoom range memperbesar gambar sebanyak 0.05 atau gambar dapat diperbesar sampai 5%. Keenam horizontal flip adalah memberikan variasi tambahan dengan mengubah orientasi gambar secara horizontal acak dengan keterangan nilai true. ketujuh vertikal flip adalah memberikan variasi tambahan dengan mengubah orientasi gambar secara vertikal acak dengan keterangan nilai true. Kedelapan brightness range mengubah atau menentukan kecerahan pada gambar secara acak dengan nilai rentan [0.75, 1.25] atau kecerahan dapat diubah mulai dari rentnag 75% sampai 125% dari kecerahan asli gambarnya. Kesembilan resecale mengubah nilai skala piksel 0.1 dengan membaginya setiap nilai piksel pada nilai 255, sehingga dapat membantu untuk normalisasi data. Kesepuluh validation split mengatur pembagian data untuk validasi dengan nilai 0.2 atau 20% data dari keseluruhan data untuk alokasi validation data dan 80% untuk alokasi training data. Merupakan pendekatan augmentasi awal di mana sebelum data masuk ke model untuk proses pelatihan dan akan diperbesar sebelum pembagian dataset menjadi batch untuk setiap epoch nya, sehingga model akan dilatih menggunakan dataset yang telah diaugmentsi sejak awal dan seluruh augmentasi akan diterapkan pada setiap epoch nya dengan penggunaan ukuran batch 64 dengan jumlah batch training 36 dan validasi 10. Rinciannya data asli pada training data berjumlah 1.121 gambar dan setelah dilakukan augmentasi bertambah sebanyak 1.152 gambar, sehingga data pada training data berjumlah total menjadi 2.273 gambar. Rincian data asli pada Validation data berjumlah 279 gambar dan setelah dilakukan augmentasi bertambah sebanyak 320 gambar, sehingga data pada Validation data berjumlah total menjadi 599 gambar. Testing data tidak mengalami augmentasi karena pada proses pengujian atau evaluasi model, ingin menggunakan data asli yang sebenarnya untuk melihat kinerja model pada kasus-kasus yang belum pernah dilihat sebelumnya. Berikut merupakan Gambar 3.6 Tahapan Data Augmentation. 3.2.5 Data Splitting
     Tahap kelima data splitting dengan membagi file dataset menjadi subset training data, validation data, dan testing data berisikan gambar ban normal dan gambar ban tidak normal sehingga subset training data digunakan untuk melatih model, sedangkan subset validation data digunakan untuk menguji kinerja model. Sebenarnya langkah-langkah dalam proses pra-pemrosesan data yang mempersiapkan data mentah untuk digunakan dalam pelatihan model adalah tahapan yang sudah disebutkan sebelumnya data 
aquisition, dataprerocessing, data augmentation, dan splitting data. Prosedur yang disebutkan di atas berkonsentrasi pada pengumpulan, sanitasi, pengorganisasian, dan 
penambahan jumlah data yang diperlukan untuk pelatihan model. Rinciannya yaitu file yang tersimpan di dalam komputer total data gambar sebanyak 2.050 gambar yang dibagi menjadi dua pertama adalah file testing data dengan 
jumlah data tersimpan sebanyak 650 gambar yang dibagi menjadi sub file ""crack"" berjumlah 420 dan sub file ""normal' berjumlah 230 data. kedua adalah file training data 
dengan jumlah data tersimpan sebanyak 1400 gambar yang dibagi menjadi sub file ""crack"" berjumlah 654 dan sub file ""normal' berjumlah 746 data. maka ketika dilakukan 
data splitting pada program secara otomatis yang pada data augmentasi diatur menjadi pembagian 80% untuk training data dan 20% untuk validation data yaitu untuk Train 
Data sebanyak 1.121 gambar dengan 2 kelas, validation data sebanyak 279 gambar dengan 2 kelas, dan Test Data sebanyak 650 gambar dengan 2 kelas. Testing data bernilai tetap hal ini bertujuan agar kuantitas data awal yang telah ditentukan sebelumnya tetap terjaga dan testing data tidak terpengaruh oleh prosedur pemisahan. Setelah model dilatih dan divalidasi, testing data digunakan untuk mengevaluasi performa akhir model. Akibatnya, testing data tidak terbagi, dan rincian asli 648 foto masih berlaku. Berikut merupakan Gambar 3.7 Tahapan Splitting Data. 3.2.6 Model Building
      Tahap keenam adalah model building (membangun model Convolutional Neural Network (CNN) dengan Keras) membangun arsitektur model Convolutional Neural Network (CNN) menggunakan Keras, mengatur lapisan-lapisan seperti convolutional, MaxPooling2D, Flatten, dan Dense untuk membangun model. learning rate dalam penggunaan algoritma optimasi menggunakan Adaptive Momentum (Adam) untuk menghasilkan pembelajaran yang adaptif, pemilihan penggunaan Adaptive Momentum (Adam) jika dibandingkan dengan learning rate lain seperti Stochastic Gradient Descent (SGD) karena kecepatan pembelajaran adaptif untuk Adaptive Momentum (Adam) bisa secara otomatis menyesuaikan learning rate untuk setiap parameter dalam model klasifikasi ban sedangkan Stochastic Gradient Descent (SGD) memiliki learning rate tetap selama pelatihan model klasifikasi ban yang penentuannya dari user dan tidak bisa menyesuaikan learning rate secara otomatis berdasarkan kondisi aktual dari setiap parameter. Selanjutnya secara kestabilan dan konvergensi Adaptive Momentum (Adam) menyambung dari awal dapat mengubah kecepatan pembelajaran secara adaptif, sehingga membuatnya lebih stabil dan kecil kemungkinannya terjebak pada tingkat minimum lokal (nilai yang dianggap sebagai titik terendah dari loss function dalam model) sehingga Adaptive Momentum (Adam) cenderung mencapai konvergensi (tingkat kinerja yang diharapkan) lebih cepat dan andal dalam berbagai keadaan, sedangkan Stochastic Gradient Descent (SGD) mungkin lebih stuck pada nilai minimum atau terjebak pada nilai minimum lokal yang disebabkan oleh kemungkinan bergantung pada seberapa tepat kecepatan pemelajaran dipilih, kecepatan pemelajaran yang tetap dapat membuat model mencapai konvergensi terlalu cepat atau terlalu lambat. Adapun melakukan pendekatan kedua penambahan data ketika masuk ke model building dan terjadi proses pemodelan setelah menggunakan epoch. Augmentasi data diterapkan setelah data melewati beberapa epoch selama proses pelatihan, sehingga variasi data yang dihasilkan akan berbeda-beda pada setiap epoch dan model dapat terus-menerus terlatih dengan variasi data yang lebih besar. Menggunakan 100 epoch, sehingga total training data yang diproses menjadi 230.400 gambar dan validation data menjadi 6.400 gambar. Sehingga jumlah data yang diproses selama pelatihan menjadi sangat besar dan pada akhirnya nanti akan menyiapkan model kompilasi dalam mengatur pengoptimal (Adam), fungsi kerugian (biner crossentropy), dan metrik evaluasi (akurasi). Berikut merupakan Gambar 3.8 Tahapan Building Model. Berdasarkan hasil analisis sebelumnya maka dapat diketahui untuk jumlah data asli (training data) adalah 1.121, jumlah data asli (validation data) adalah 279, jumlah data asli (training data) setelah augmentasi adalah 1152, jumlah data asli (validation data) setelah augmentasi adalah 320, jumlah epoch yang digunakan sebanyak 100, dan ukuran batch adalah 64. Berikut merupakan perhitungan manualnya ketika masuk ke model building dan terjadi proses pemodelan setelah menggunakan epoch. 1. Jumlah batch per epoch untuk training data. 2. Jumlah batch per epoch untuk validation data. 3. Total jumlah data setelah augmentasi untuk semua epoch. a. Training Data
b. Validation Data

3.2.7 Model Evaluation & Testing
      Tahap kedelapan adalah model evaluation & testing digunakan sebagai bahan terusan pada model building yang dibuat untuk melakukan evaluasi performanya dengan 
menggunakan bagian pengujian, dan parameter yang digunakan pada metrik evaluasi seperti akurasi, presisi, recall, dan Fl-score. Berikut merupakan Gambar 3.9 Tahapan 
Model Evaluation & Testing. 3.3 Arsitektur Convolutional Neural Network (CNN)
     Convolutional Neural Network (CNN) yang dibangun menggunakan model atau kerangka kerja yang pada dasarnya menggunakan Keras dan juga tensorflow dengan menambahkan beberapa model lapisan-lapisan seperti lapisan convolutional (Conv2D), laposan pooling (MaxPooling2D), Flatten, dan lapisan fully connected (Dense). Berikut merupakan Gambar 3.10 Tahapan Convolutional Neural Network (CNN) dengan Model Keras. Berdasarkan Gambar 3.10 Tahapan Convolutional Neural Network (CNN) dengan Model Keras maka dapat dijelaskan mulai dari yang mencakup lapisan-lapisan konvolusi yang telah dilatih pada dataset besar seperti ImageNet untuk mengekstrak fitur dari gambar-gambar, penggunaan image size diatur dengan (379, 379) batch size 64, kernel size 3, strides (2 untuk cov2d dan 2 untuk maxpooling2d), dan pool size 2. Selanjutnya Conv2D yang merupakan convolutional layer pertama yang berfungsi untuk mengekstrak fitur-fitur visual dari gambar. Filter convolutional layer pertama yang berfungsi untuk mengekstrak fitur-fitur visual diterapkan pada gambar untuk menghasilkan fitur-fitur yang lebih abstrak, formula untuk mengetahui jumlah training datanya dengan. Selanjutnya MaxPooling2D di mana tahap pooling digunakan untuk mengurangi dimensi spasial dari setiap feature map yang dihasilkan oleh layer sebelumnya. Max pooling memilih nilai maksimum di dalam jendela pooling untuk mengurangi ukuran fitur dan mempertahankan informasi penting. Selanjutnya Conv2D dan MaxPooling2D diulang sampai 4 layer karena untuk terus mengekstrak fitur-fitur yang semakin kompleks dari gambar. Selanjutnya flatten (digunakan untuk mengubah tensor multi-dimensi menjadi tensor satu dimensi) di mana setelah serangkaian layer konvolusi dan pooling, masukan dari layer terakhir perlu diubah menjadi vektor tunggal sebelum dimasukkan ke dalam layer dense. Flatten layer melakukan hal ini dengan mengubah matriks output menjadi array satu dimensi. Selanjutnya dense layers (lapisan dense digunakan sebagai lapisan output dalam model klasifikasi, di mana jumlah neuron dalam lapisan output sesuai dengan jumlah kelas yang harus diprediksi) di mana ada tiga lapisan dense ditambahkan, dengan fungsi pertama dan kedua menggunakan ReLu sebagai f (x) = max(0, x) yang artinya menunjukkan bahwa keluarannya nol jika masukannya negatif atau nol dan output x jika masukannya positif dengan 128 unit neuron dan pada dense kedua 64 unit neuron karena tugasnya mengurangi dimensi representasi pada lapisan dense pertama maka model dapat mempelajari pola yang lebih rumit dan mendalam dari data dengan menambahkan lapisan yang lebih padat, yang dapat meningkatkan performa model dalam tugas klasifikasi gambar. Lapisan dense ketiga dengan fungsi aktivasi sigmoid untuk output biner, dengan menunjukan kelas prediksi dari gambar yaitu normal atau crack. Di antara tiga lapisan dense di ikuti dengan lapisan dropout (untuk mencegah overfitting di mana model pembelajaran mesin terlalu menghafal pola dari training data yang tersedia, sehingga kinerjanya menurun secara signifikan saat diuji dengan data baru yang tidak dilihat sebelumnya) juga dimasukkan setelah setiap lapisan dense untuk mencegah overfitting dengan secara acak menonaktifkan sebagian unit sebanyak 0.2 atau 20% dari neuron selama pelatihan. Selanjutnya training di mana model diterapkan pada training data dengan menggunakan metode fit dan callback. Model fit digunakan untuk melatih model dengan training data dan model callback menggunakan ""modelCheckpoint"" untuk menyimpan model terbaik selama pelatihan berkaitan dengan performa pada validation data mengontrol proses pelatihan. Terakhir evaluation di mana performa model pada testing data dinilai menggunakan hasil klasifikasi dan confusion matrix untuk memahami kinerjanya testing data. Banyaknya parameter atau bobot dan jumlah data yang harus dipelajari selama pelatihan bergantung pada jumlah neuron pada lapisan. Jumlah data yang harus dipelajari 
model selama pelatihan tercermin dalam jumlah parameter ini. Berikut merupakan perhitungan dalam mengetahui total neuron yang dikerjakan oleh setiap lapisan. 1. First Conv2D
Kedalaman gambar yang diproses lapisan konvolusi sebenarnya ditunjukkan oleh jumlah saluran masukan. Tiga saluran merah, hijau, dan biru membentuk sebuah gambar jika diwarnai, artinya ada tiga saluran masukan. Karena kata grayscale digunakan untuk mendeskripsikan gambar ini, hanya ada satu saluran warna dan bernilai 1. Sehingga jumlah neuronnya 1280, yang berarti ada 1280 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan. Selanjutnya adalah dalam penentuan ukuran spasialnya setiap filter diubah menjadi setengah dari ukuran input nya (379x379) menjadi (189x189) sebagai berikut. 2. First Maxpooling2D
Tidak ada parameter baru yang ditambahkan, dan jumlah neuron (dalam contoh ini, lapisan konvolusi pertama) tetap sama. Setiap filter diubah menjadi setengah dari ukuran inputnya (189x189) menjadi (94x94) dan jumlah neuronnya 1280 mengikuti lapisan konvolusi pertama. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 3. Second Cov2D
Total Neuron = (Ukuran Filter x Jumlah Channel Input + 1) x Filter = (3 x 3 x 128 + 1)x 64 = (1153)x 128 = 73792
Jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. Sehingga jumlah neuronnya 73792, yang berarti ada 73792 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan. Selanjutnya adalah dalam penentuan ukuran spasialnya setiap filter diubah menjadi setengah dari ukuran input nya (94x94) menjadi (46x46) sebagai berikut. 4. Second Maxpooling2D
Tidak ada parameter baru yang ditambahkan, dan jumlah neuron (dalam contoh ini, lapisan konvolusi kedua) tetap sama. Setiap filter diubah menjadi setengah dari ukuran 
inputnya (46x46) menjadi (23x23) dan jumlah neuronnya 73792 mengikuti lapisan konvolusi kedua. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 5. Third Cov2D
Total Neuron = (Ukuran Filter x Jumlah Channel Input + 1) x Filter = (3 x 3 x 64 + 1)x 32 = (577)x 32 = 18464
Jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. Sehingga jumlah neuronnya 18464, yang berarti ada 18464 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan. Selanjutnya adalah dalam penentuan ukuran spasialnya setiap filter diubah menjadi setengah dari ukuran input nya (23x23) menjadi (11x11) sebagai berikut. 6. Third Maxpooling2D
Tidak ada parameter baru yang ditambahkan, dan jumlah neuron (dalam contoh ini, lapisan konvolusi ketiga) tetap sama. Setiap filter diubah menjadi setengah dari ukuran 
inputnya (11x11) menjadi (5x5) dan jumlah neuronnya 18464 mengikuti lapisan konvolusi ketiga. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 7. Fourth Cov2D
Total Neuron = (Ukuran Filter x Jumlah Channel Input + 1) x Filter = (3 x 3 x 32 + 1)x 16 = (289)x 16 = 4624
Jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. Sehingga jumlah neuronnya 4624, yang berarti ada 4624 parameter yang harus dipelajari selama pelatihan dan akan mencerminkan jumlah data yang harus dipelajari ketika pelatihan. Selanjutnya adalah dalam penentuan ukuran spasialnya setiap filter diubah menjadi setengah dari ukuran input nya (5x5) menjadi (2x2) sebagai berikut. 8. Fourth Maxpooling2D
Tidak ada parameter baru yang ditambahkan, dan jumlah neuron (dalam contoh ini, lapisan konvolusi keempat) tetap sama. Setiap filter diubah menjadi setengah dari ukuran inputnya (2x2) menjadi (1x1) dan jumlah neuronnya 4624 mengikuti lapisan konvolusi keempat. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 9. Flatten
Tidak mengubah parameter yang ada karena fungsi flatten hanya mengubah matriks multidimensi menjadi vektor tunggal berdasarkan hasil dari jumlah filter pada lapisan 
lima cov2d yaitu 8 dan maxpooling2D dengan ukuran inputnya (1x1) sehingga menjadi matriks multidimensi (1, 1, 16) diubah menjadi nilai vektor tunggal dengan panjang 16 atau menjadi jumlah neuron sebanyak 16. 10. Dense Layer 1
Total Neuron = (Jumlah Neuron Input + 1) x Jumlah Neuron Output = (16+ 1)x 128 = (17)x 128 = 2176
11. Dropout Layer 1
Menggunakan 0.2 yang artinya sebanyak 20% dari neuron dalam dense layer 1 akan dinonaktifkan secara acak. 12. Dense Layer 2
Total Neuron = (Jumlah Neuron /nput + 1) x Jumlah Neuron Output = (128 + 1)x 64 = (129)x 64 = 8256
13. Dropout Layer 2
Menggunakan 0.2 yang artinya sebanyak 20% dari neuron dalam dense layer 2 akan dinonaktifkan secara acak. 14. Dense Layer 2
Total Neuron = (Jumlah Neuron /nput + 1) x Jumlah Neuron Output = (64 + 1)x 1 = (65)x 1 = 65
Ketika dimensi spasial (tinggi dan lebar) dikurangi menggunakan operasi lapisan pooling seperti maxpooling, jumlah neuron di setiap lapisan pooling akan menurun. Misalnya, dimensi spasial setiap filter (tinggi dan lebar) di lapisan maxpooling disesuaikan menjadi setengah dari dimensi masukannya. Karena hanya separuh dari masukan yang diproses lebih lanjut, hal ini juga menyebabkan berkurangnya jumlah neuron pada lapisan tersebut. Sedangkan penurunan pada dense terjadi karena penentuan jumlah neuron.","3.1 Gambaran Umum Penelitian
     Penelitian ini digunakan untuk mengatasi sensitivitas terhadap cacat pada gambar ban dengan melibatkan penggunaan jaringan syaraf menggunakan algoritma Convolutional Neural Network (CNN) dan membangun model atau kerangka kerja menggunakan Keras. Berikut adalah Gambar 3.1 Blok Diagram Gambaran Umum Penelitian. Berdasarkan Gambar 3.1 Blok Diagram Gambaran Umum Penelitian maka dapat dijelaskan di blok tersebut terbagi menjadi 3 bagian yaitu bagian pertama adalah unit 
masukan berisikan data preparation di mana gambar ban dimuat, diubah menjadi format yang sesuai dipersiapkan untuk pelatihan model Convolutional Neural Network (CNN) 
seperti pemrosesan gambar ban, selanjutnya data augmentation di mana data dibuat lebih ber variasi dari training data yang ada sehingga dapat meningkatkan keberagaman 
training data tanpa harus mengambil data baru, mencakup (rotasi, pergeseran horizontal/vertikal, perbesar gambar, perubahan kecerahan gambar, sampai mengubah nilai pixel), selanjutnya data di mana dataset yang telah di augmentasi dan disiapkan dibagi menjadi subset yang berbeda untuk training untuk melatih model, validation untuk menyempurnakan model serta memvalidasi performanya selama pelatihan, dan testing untuk mengevaluasi kinerja model akhir. Dataset dibagi menjadi training data, validation data, dan testing data dalam proporsi tertentu. Bagian kedua adalah unit pemrosesan yang bertindak adalah model training (forward Pass, tahap di mana input diproses melalui model untuk menghasilkan prediksi), tujuannya melatih model Convolutional Neural Network (CNN) menggunakan dataset pelatihan di mana data dari unit masukan diteruskan melalui jaringan neural di lakukan transformasi linier (konvulasi) dan non-linier (fungsi aktivasi) dilakukan pada data di setiap lapisan untuk menghasilkan output prediksi yang melibatkan komputasi di setiap neuron dan lapisan jaringan, yang merupakan inti dari proses pembelajaran dalam jaringan saraf. Selanjutnya unit pemrosesan Fine-tuning tujuannya dilakukan untuk 
menyempurnakan model lebih lanjut setelah pelatihan awal dengan dataset yang lebih kecil atau lebih spesifik nantinya. Proses di dalam Fine-tuning menyesuaikan bobot 
(menggunakan kumpulan data yang lebih kecil untuk menyesuaikan bobot model untuk performa yang lebih baik), pelatihan khusus (fokus pada fitur data yang lebih relevan 
dengan objek). Bagian ketiga adalah unit keluaran yang bertindak ada proses model evaluatioan (backwardpass, tahap di mana gradien (memperbarui parameter model dalam arah yang 
akan mengurangi fungsi loss) dari fungsi loss (metrik yang mengukur seberapa baik atau buruk model melakukan prediksi dibandingkan nilai aktualnya) dihitung dan digunakan 
untuk memperbarui parameter model selama pelatihan) tujuannya mengevaluasi performa model yang dilatih dan model dievaluasi menggunakan metrik yang relevan (accuracy, 
precision, recall, dan F1- score) berdasarkan prediksi yang dihasilkan dari model terhadap validasi atau uji data. Output dari proses ini adalah tentang hasil evaluasi model, yang memberikan informasi kinerja model. Selanjutnya ada dua alur pilihan yang bisa dilakukan, alur pertama jika hasil prediksi sudah sesuai dengan keinginan maka bisa langsung masuk ke model deployment (inference), dan alur kedua jika hasil prediksi masih perlu diperbaiki pada bagian unit pemrosesan terlebih dahulu fine tuning untuk penggunaan data set lebih kecil (jika menunjukan model belum mencapai performa yang diharapkan) baru masuk ke model deployment (inference) tujuannya menerapkan model 
terlatih untuk membuat prediksi pada data baru yang belum terlihat. Model deployment (inference) yang telah dilatih digunakan untuk membuat prediksi pada data baru atau dalam situasi dunia nyata, tahap di mana model menerima input baru dan menghasilkan output berdasarkan pada pembelajaran yang dilakukan selama proses pelatihan dan merupakan output akhir dari keseluruhan proses, di mana model ""mengambil keputusan"" atau ""membuat prediksi"" berdasarkan pada pengalaman yang telah diperoleh selama pelatihan. 3.2. Tahapan Penelitian
      Penelitian ini di dalamnya terdapat tahapan-tahapan yang dilakukan untuk membentuk satu kesatuan yang utuh dari awal sampai akhir dan membentuk kerangka penelitian mengenai klasifikasi pada produk ban menggunakan algoritma Convolutional Neural Network (CNN). Berikut Gambar 3.2 Tahapan penelitian. 3.2.1 Studi Literatur
      Tahap pertama adalah studi literatur di mana studi yang dilakukan berasal dari artikel ilmiah dan buku yang menunjang dalam menganalisis terkait dengan metode 
pengukuran kualitas, mengenai klasifikasi produk ban, meninjau penggunaan pembelajaran mesin algoritma Convolutional Neural Network (CNN) dari beberapa tahun ke belakang dalam konteks pengukuran kualitas untuk klasifikasi terhadap kondisi-kondisi produk ban. 3.2.2 Data Aquisition
     Tahap kedua adalah data aquisition dengan mengumpulkan kumpulan data sesuai tujuan penelitian dengan target untuk kumpulan data gambar ban untuk training data, 
validation data, dan testing data, memastikan bahwa kumpulan data tersebut memiliki varian yang secara akurat memang mewakili kondisi produk ban dan diperoleh dari sumber-sumber terpercaya. 3.2.4 Data Augmentation
     Tahap keempat adalah data augmentation meningkatkan variasi dalam dataset dengan teknik augmentasi data, menggunakan operasi seperti rotasi, pergeserarn 
horizontal/vertikal, perbesar gambar, perubahan kecerahan gambar, sampai mengubah nilai pixel untuk memperkaya dataset dan mengurangi overfitting (Saat disajikan dengan 
data baru yang belum pernah dilihat sebelumnya, performa model akan menurun drastis karena model tersebut dapat menyesuaikan diri dengan kumpulan training data dengan 
sangat efektif). Sebaliknya, pada saat runtime hanya menghasilkan variasi dari gambar yang sudah ada dibuat secara dinamis dan cukup bagi model untuk berlatih dari berbagai kondisi gambar ban yang ada pada kenyataaanya. Data asli pada dataset berjumlah 1.028 data gambar ban setelah dilakukan augmentasi secara fisik menggunakan website roboflow ada data yang tidak dapat diidentifikasi ada 3 gambar sehingga total gambar asli yang berhasil di upload dan dijadikan data asli yang tetap berjumlah 1.025 data gambar dan setelah di augmentasi bertambah menjadi 2.050 data gambar ban. 3.2.6 Model Building
      Tahap keenam adalah model building (membangun model Convolutional Neural Network (CNN) dengan Keras) membangun arsitektur model Convolutional Neural Network (CNN) menggunakan Keras, mengatur lapisan-lapisan seperti convolutional, MaxPooling2D, Flatten, dan Dense untuk membangun model. learning rate dalam penggunaan algoritma optimasi menggunakan Adaptive Momentum (Adam) untuk menghasilkan pembelajaran yang adaptif, pemilihan penggunaan Adaptive Momentum (Adam) jika dibandingkan dengan learning rate lain seperti Stochastic Gradient Descent (SGD) karena kecepatan pembelajaran adaptif untuk Adaptive Momentum (Adam) bisa secara otomatis menyesuaikan learning rate untuk setiap parameter dalam model klasifikasi ban sedangkan Stochastic Gradient Descent (SGD) memiliki learning rate tetap selama pelatihan model klasifikasi ban yang penentuannya dari user dan tidak bisa menyesuaikan learning rate secara otomatis berdasarkan kondisi aktual dari setiap parameter. Selanjutnya secara kestabilan dan konvergensi Adaptive Momentum (Adam) menyambung dari awal dapat mengubah kecepatan pembelajaran secara adaptif, sehingga membuatnya lebih stabil dan kecil kemungkinannya terjebak pada tingkat minimum lokal (nilai yang dianggap sebagai titik terendah dari loss function dalam model) sehingga Adaptive Momentum (Adam) cenderung mencapai konvergensi (tingkat kinerja yang diharapkan) lebih cepat dan andal dalam berbagai keadaan, sedangkan Stochastic Gradient Descent (SGD) mungkin lebih stuck pada nilai minimum atau terjebak pada nilai minimum lokal yang disebabkan oleh kemungkinan bergantung pada seberapa tepat kecepatan pemelajaran dipilih, kecepatan pemelajaran yang tetap dapat membuat model mencapai konvergensi terlalu cepat atau terlalu lambat. 3.3 Arsitektur Convolutional Neural Network (CNN)
     Convolutional Neural Network (CNN) yang dibangun menggunakan model atau kerangka kerja yang pada dasarnya menggunakan Keras dan juga tensorflow dengan menambahkan beberapa model lapisan-lapisan seperti lapisan convolutional (Conv2D), laposan pooling (MaxPooling2D), Flatten, dan lapisan fully connected (Dense). Terakhir evaluation di mana performa model pada testing data dinilai menggunakan hasil klasifikasi dan confusion matrix untuk memahami kinerjanya testing data. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. Fourth Cov2D
Total Neuron = (Ukuran Filter x Jumlah Channel Input + 1) x Filter = (3 x 3 x 32 + 1)x 16 = (289)x 16 = 4624
Jumlah channel input menggunakan jumlah filter dari convolutional layer sebelumnya bukan lagi channel input di awal. Selanjutnya adalah dalam penentuan ukuran spasialnya setiap filter diubah menjadi setengah dari ukuran input nya (5x5) menjadi (2x2) sebagai berikut. Setiap filter diubah menjadi setengah dari ukuran inputnya (2x2) menjadi (1x1) dan jumlah neuronnya 4624 mengikuti lapisan konvolusi keempat. Bisa juga menggunakan rumus seperti berikut untuk mengetahui dimensi atau ukuran filter. 9. Flatten
Tidak mengubah parameter yang ada karena fungsi flatten hanya mengubah matriks multidimensi menjadi vektor tunggal berdasarkan hasil dari jumlah filter pada lapisan 
lima cov2d yaitu 8 dan maxpooling2D dengan ukuran inputnya (1x1) sehingga menjadi matriks multidimensi (1, 1, 16) diubah menjadi nilai vektor tunggal dengan panjang 16 atau menjadi jumlah neuron sebanyak 16. 10. Dense Layer 1
Total Neuron = (Jumlah Neuron Input + 1) x Jumlah Neuron Output = (16+ 1)x 128 = (17)x 128 = 2176
11. Dropout Layer 1
Menggunakan 0.2 yang artinya sebanyak 20% dari neuron dalam dense layer 1 akan dinonaktifkan secara acak. 12. Dense Layer 2
Total Neuron = (Jumlah Neuron /nput + 1) x Jumlah Neuron Output = (128 + 1)x 64 = (129)x 64 = 8256
13. Dropout Layer 2
Menggunakan 0.2 yang artinya sebanyak 20% dari neuron dalam dense layer 2 akan dinonaktifkan secara acak. 14. Dense Layer 2
Total Neuron = (Jumlah Neuron /nput + 1) x Jumlah Neuron Output = (64 + 1)x 1 = (65)x 1 = 65
Ketika dimensi spasial (tinggi dan lebar) dikurangi menggunakan operasi lapisan pooling seperti maxpooling, jumlah neuron di setiap lapisan pooling akan menurun. Misalnya, dimensi spasial setiap filter (tinggi dan lebar) di lapisan maxpooling disesuaikan menjadi setengah dari dimensi masukannya. Karena hanya separuh dari masukan yang diproses lebih lanjut, hal ini juga menyebabkan berkurangnya jumlah neuron pada lapisan tersebut. Sedangkan penurunan pada dense terjadi karena penentuan jumlah neuron."
"3.1 Gambaran Umum Penelitian
     Motivasi dari Metodologi yang diusulkan adalah membuat suatu metode peramalan yang sesuai dengan data runtun waktu yang ada serta meningkatkan akurasinya dengan tetap memperhatikan efisiensi waktu komputasinya. Langkah-langkah yang dilakukan dalam penelitian ini adalah menganalisis data jumlah kasus harian Covid 19 di Jakarta berdasarkan dataset dari situs https://corona.jakarta.go.id tanggal 6 Maret 2020 sampai 30 Juni 2021 sebagai data training dan nanti akan diprediksi untuk tanggal 1 Juli 2021 sampai dengan 31 Juli 2021 sebagai data uji dengan tahapan sebagai berikut :
1. Mempersiapkan data runtun waktu yang akan dianalisis
2. Menganalisis data runtun waktu yang ada menggunakan metode statistika ARIMA
3. Menganalisis data runtun waktu yang ada menggunakan metode Quantum Neural Network
4. Mengembangkan model Hybrid ARIMA-Quantum Neural Network
5. Menentukan model yang cocok untuk setiap variabel
6. Menguji kecocokan masing-masing model
7. Melakukan peramalan dengan menggunakan model yang cocok
8. Melakukan perbandingan tingkat akurasi hasil peramalan dengan tiap model

Untuk mendapatkan model peramalan yang diharapkan sesuai dengan data runtun waktu yang ada, maka perlu dilakukan pendekatan ilmiah yaitu dengan melihat pola data runtun waktu yang ada terlebih dahulu. Dengan melihat pola data awal yang dimiliki maka akan memudahkan dalam memilih model yang sesuai untuk data tersebut. Pendekatan lainnya adalah menggunakan tools untuk menentukan secara otomatis Bentuk model statistik ARIMA yang sesuai dengan runtun waktu yang ada, lalu model tersebut dilatih menggunakan quantum neural network agar diketahui pola-pola data yang	sudah	ada	dan	dapat	diuji	akurasinya. 3.2 Model ARIMA
2. Tahapan Analisis Time Series (ARIMA)
a. Membuat Plot Time Series
Identifikasi asumsi stasioneritas data runtun waktu. Suatu deret pengamatan dikatakan stasioner apabila proses tidak berubah seiring dengan perubahan waktu
Tidak stasioner dalam mean : jika trend tidak datar (tidak sejajar smbu waktu)
Tidak stasioner dalam varian : jika trend datar atau hampir datar, tetapi data tersebar membangun pola melebar atau menyempit (pola terompet)
Tidak stasioner dalam mean & varians : jika trend tidak datar dan data membentuk pola terompet. Augmented Dickey- Fuller (Uji Formal untuk Stasioneritas) Hipotesis :
H0 : Terdapat akar unit dan data tidak stasioner (=0)
H1 : Tidak terdapat akar unit dan data stasioner (<0 span="""">) Taraf Signifikansi : a = ... %
3.3 Model Neural Network
Dalam buku Jaringan Syaraf Tiruan dan Pemrogramannya Menggunakan MATLAB, Drs. Jong Jek Siang, M.Sc menyebutkan bahwa Jaringan Syaraf Tiruan adalah system pemroses informasi yang memiliki karakteristik mirip dengan jaringan syaraf biologi. Jaringan Syaraf Tiruan dibentuk sebagai generalisasi model matematika dari jaringan syaraf biologi, dengan asumsi bahwa Pemrosesan informasi terjadi pada banyak elemen sederhana (neuron)
a. Sinyal dikirimkan di antara neuron-neuron melalui penghubung-penghubung
b. Penghubung antar neuron memiliki bobot yang akan memperkuat atau memperlemah sinyal
c. Untuk menentukan output, setiap neuron menggunakan fungsi aktivasi (biasanya bukan fungsi linier) yang dikenakan pada jumlahan input yang diterima. Besarnya output ini selanjutnya dibandingkan dengan suatu batas ambang (treshhold)

Jaringan Syaraf Tiruan ditentukan oleh tiga hal :
a. Pola hubungan antar neuron (disebut arsitektur jaringan)
b. Metode untuk menentukan bobot penghubung (disebut metode training/learning algoritma)
c. Fungsi Aktivasi
Pemrosesan informasi dalam Jaringan Syaraf Tiruan dapat disingkat sebagai berikut : Sinyal (baik berupa aksi ataupun potensial) muncul sebagai masukan unit (sinapsis); efek dari tiap sinyal ini dinyatakan sebagai bentuk perkalian dengan sebuah nilai bobot untuk mengindikasikan kekuatan dari sinapsis. Semua sinyal yang diberi pengali bobot ini kemudian dijumlahkan satu sama lain untuk menghasilkan unit aktivasi. Jika aktivasi ini melampaui sebuah batas ambang tertentu maka unit tersebut akan memberikan keluaran dalam bentuk respon terhadap masukan. Unit aktivasi ini kemudian dibandingkan dengan sebuah nilai ambang, dan hasilnya dimasukkan kedalam fungsi transfer (fungsi non-linier) yang akan menghasilkan sebuah keluaran. Secara ringkas proses tersebut dapat digambarkan dalam gambar 3
Aktivasi dari unit masukan diatur dan diteruskan melalui jaring hingga nilai dari keluaran dapat ditentukan. Jaring berperan sebagai fungsi vektor yang mengambil satu vektor pada masukan dan mengeluarkan satu vektor lain pada keluaran. Model Jaringan Syaraf Tiruan dapat memiliki sebuah lapisan bobot, dimana masukan dihubungkan langsung dengan keluaran, atau beberapa lapisan yang didalamnya terdapat beberapa lapisan tersembunyi, karena berada tersembunyi diantara neuron masukan dan keluaran. Jaring syaraf menggunakan unit tersembunyi untuk menghasilkan representasi pola masukan secara internal didalam jaring syaraf. Fungsi transfer (non-linier) yang digunakan dalam tiap neuron (baik dilapisan masukan, keluaran, atau lapisan tersembunyi) dapat berupa fungsi nilai ambang, fungsi linier, fungsi sigmoid, ataupun fungsi gaussian, tergantung dari karakter neuron sesuai keinginan kita. Hal ini dapat dilihat pada gambar 4


3.3.1 Komponen Jaringan Syaraf
      Terdapat beberapa tipe jaringan syaraf, hampir semuanya memiliki komponen-komponen yang sama. Seperti halnya otak manusia, jaringan syaraf juga terdiri atas beberapa neuron dan ada hubungan antar neuron tersebut. Neuron-neuron tersebut akan mentransformasikan informasi yang diterima melalui sambungan keluarnya menuju ke neuron-neuron yang lain. Pada jaringan syaraf, hubungan ini dikenal dengan nama bobot. Informasi tersebut disimpan pada suatu nilai tertentu pada bobot tersebut. Neuron ini sebenarnya mirip dengan sel neuron biologis. Neuron-neuron buatan tersebut bekerja dengan cara yang sama pula dengan neuron biologis. Informasi (disebut dengan: input) akan dikirim ke neuron dengan bobot kedatangan tertentu. Input ini akan diproses oleh suatu fungsi perambatan yang akan menjumlahkan nilai-nilai semua bobot yang datang. Hasil penjumlahan ini kemudian akan dibandingkan dengan suatu nilai ambang (threshold) tertentu melalui fungsi aktivasi setiap neuron. Apabila input tersebut melewati suatu nilai ambang tertentu, maka neuron tersebut akan diaktifkan, tapi kalau tidak, maka neuron tersebut tidak akan diaktifkan. Apabila neuron tersebut diaktifkan, maka neuron tersebut akan mengirimkan output melalui bobot-bobot outputnya kesemua neuron yang berhubungan dengannnya. Pada Jaringan syaraf, neuron-neuron akan dikumpulkan dalam lapisan (layer) yang disebut dengan lapisan neuron (neuron layer). Neuron-neuron pada satu lapisan akan dihubungkan dengan lapisan-lapisan sebelum dan sesudahnya (kecuali lapisan input dan lapisan output). Informasi yang diberikan pada jaringan syaraf akan dirambatkan lapisan ke lapisan. Mulai dari lapisan input sampai ke lapisan output melalui lapisan lainnya, yang sering disebut sebagai lapisan tersembunyi (hidden layer). 3.3.2 Arsitektur Jaringan Syaraf
3.3.2.1 Jaringan dengan lapisan tunggal (single layer net)
      Jaringan dengan lapisan tunggal hanya memiliki satu lapisan dengan bobot- bobot terhubung. Jaringan ini hanya menerima input kemudian secara langsung akan mengolahnya menjadi output tanpa harus melalui lapisan tersembunyi. 3.3.3.3  Jaringan dengan banyak lapisan (multilayer net)
      Jaringan dengan banyak lapisan memiliki 1 atau lebih lapisan yang terletak diantara lapisan input dan lapisan output (memiliki 1 atau lebih lapisan tersembunyi). Umumnya, ada lapisan bobot-bobot yang terletak antara 2 lapisan yang bersebelahan. Jaringan dengan banyak lapisan ini dapat menyelesaikan permasalahan yang lebih sulit daripada jaringan dengan lapisan tunggal, tentu saja dengan pembelajaran yang lebih rumit. Namun demikian, pada banyak kasus, pembelajaran pada jaringan dengan banyak lapisan ini lebih sukses dalam menyelesaikan masalah. 3.3.3. Fungsi Aktivasi
      Ada beberapa fungsi aktivasi yang sering digunakan dalam jaringan syaraf tiruan, antara lain :
a. Fungsi Undak Biner (Hard Limit)
      Jaringan dengan lapisan tunggal sering menggunakan fungsi undak (step function) untuk mengkonversikan input dari suatu variabel yang bernilai kontinu ke suatu output biner (0 atau 1)
b. Fungsi undak biner (Threshold)
      Fungsi undak biner dengan menggunakan nilai ambang sering juga disebut dengan fungsi nilai ambang (Threshold) atau fungsi Heaviside. c. Fungsi Bipolar (Symetric Hard Limit)
      Fungsi bipolar sebenarnya hampir sama dengan fungsi undak biner, hanya saja output yang dihasilkan berupa 1, 0 atau -1
d. Fungsi Bipolar (dengan threshold)
      Fungsi bipolar sebenarnya hampir sama dengan fungsi undak biner dengan threshold. Hanya saja keluaran yang dihaslkan berupa 1, 0, atau -1
e. Fungsi Linear (Identitas)
Fungsi linear memiliki nilai output yang sama dengan nilai inputnya. f. Fungsi Saturating Linear
      Fungsi ini akan bernilai 0 jika inputnya kurang dari - 1/2, dan akan bernilai 1 jika inputnya lebih dari 1/2. Sedangkan jika nilai input terletak antara -1/2 dan 1/2, maka outputnya akan bernilai sama dengan nilai input ditambah 1/2
g. Fungsi Symetric Saturating Linear
      Fungsi ini akan bernilai -1 jika inputnya kurang dari -1, dan akan bernilai 1 jika inputnya lebih dari 1. Sedangkan jika nilai input terletak antara -1 dan 1, maka outputnya akan bernilai sama dengan nilai inputnya. h. Fungsi Sigmoid Biner
      Fungsi ini digunakan untuk jaringan syaraf yang dilatih dengan menggunakan metode backpropagation. Fungsi sigmoid biner memiliki nilai pada range 0 sampai 1. Oleh karena itu, fungsi ini sering digunakan untuk jaringan syaraf yang membutuhkan nilai output yang terletak pada interval 0 sampai 1. Namun, fungsi ini bisa juga digunakan oleh jaringan syaraf yang nilai outputnya 0 atau 1.
i. Fungsi Sigmoid Bipolar
      Fungsi sigmoid bipolar hampir sama dengan fungsi sigmoid biner, hanya saja output dari fungsi ini memiliki range antara 1 sampai -1
      Fungsi ni sangat dekat dengan fungsi hyperbolic tangent. Keduanya memiliki range antara -1 sampai 1. Untuk fungsi hyperbolic tangent,

3.4 MODEL HYBRID ARIMA NEURAL NETWORK
      Berdasarkan hasil peramalan model ARIMA, akan dilakukan proses analisis runtun waktu menggunakan metode jaringan syaraf tiruan. Dengan kata lain, output dari peramalan model ARIMA akan menjadi input pada proses pengolahan data menggunakan metode jaringan syaraf tiruan. Kemudian akan ditentukan model jaringan syaraf tiruan yang sesuai dan cocok untuk data runtun waktu tersebut. Secara matematis, hasil ramalan secara keseluruhan yang diperoleh adalah sebagai berikut :
Zt merupakan hasil peramalan yang merupakan gabungan nilai ramalan dari model ARIMA atau Exponential Smoothing dan nilai ramalan dari model JST. 3.5 MODEL QUANTUM HYBRID ARIMA NEURAL NETWORK
      Ada banyak pendekatan untuk pengembangan model Quantum Arima NN. Model-model ini fokus pada yang berbeda aspek komputasi kuantum dan pemrosesan saraf. Dalam komputasi kuantum, Sebagai unit informasi terkecil, bit kuantum atau qubit adalah sistem kuantum yang menyatakan terletak di ruang Hilbert dua dimensi. Seperti bit dalam klasik komputer, qubit berlabel dan mengekspresikan satu bit informasi: sesuai dengan bit 0 komputer klasik, dan bit 1. Keadaan qubit menyatakan superposisi keadaan yang kohere :
      Di mana dan menentukan probabilitas yang sesuai. Gerbang kuantum yang mencakup karakteristik komputasi kuantum merupakan dasar untuk implementasi fisik dari komputasi kuantum. Himpunan logika universal termasuk dalam logika kuantum. Mirip dengan bit klasik, gerbang dasar dapat membentuk gerbang kuantum bemacam-macam dan menyelesaikan keadaan kuantum dari beberapa logika transformasi. berbasis elemen pada gerbang pergeseran fasa 1 bit dan gerbang kontrol-Tidak 2 bit dalam dinamika kuantum diambil sebagai fungsi aktivasi dalam Jaringan saraf. Untuk memudahkan aplikasi, formulir berikut:
Fungsi kompleks diberikan untuk menyatakan keadaan kuantum:
adalah bilangan imaginer adalah kuantum fase

3.6 Pengukuran Kinerja
3.6.1 Mean Squared Error
      Dalam statistik, Mean Squared Error (MSE) sebuah estimator adalah nilai yang diharapkan dari kuadrat error. Error yang ada menunjukkan seberapa besar perbedaan hasil estimasi dengan nilai yang akan diestimasi. Perbedaan itu terjadi karena adanya keacakan pada data atau karena estimator tidak mengandung informasi yang dapat menghasilkan estimasi yang lebih akurat
3.6.2 Komparasi Hasil Peramalan
      Setelah nilai Mean Squared Error dari kedua metode didapatkan, maka akan dilakukan komparasi terhadap nilai MSE yang didapatkan pada periode testing (out- sample)
Jika nilai MSESTATISTIKA < MSEANN maka metode Statistika memiliki performa lebih baik dibandingkan metode ANN karena memiliki tingkat kesalahan relatif lebih kecil. Sebaliknya, jika MSESTATISTIKA > MSEANN maka metode Statistika memilki performa lebih buruk dibandingkan metode ANN karena tingkat kesalahan yang dihasilkan relatif lebih besar.","3.1 Gambaran Umum Penelitian
     Motivasi dari Metodologi yang diusulkan adalah membuat suatu metode peramalan yang sesuai dengan data runtun waktu yang ada serta meningkatkan akurasinya dengan tetap memperhatikan efisiensi waktu komputasinya. Langkah-langkah yang dilakukan dalam penelitian ini adalah menganalisis data jumlah kasus harian Covid 19 di Jakarta berdasarkan dataset dari situs https://corona.jakarta.go.id tanggal 6 Maret 2020 sampai 30 Juni 2021 sebagai data training dan nanti akan diprediksi untuk tanggal 1 Juli 2021 sampai dengan 31 Juli 2021 sebagai data uji dengan tahapan sebagai berikut :
1. Mempersiapkan data runtun waktu yang akan dianalisis
2. Menganalisis data runtun waktu yang ada menggunakan metode statistika ARIMA
3. Menganalisis data runtun waktu yang ada menggunakan metode Quantum Neural Network
4. Mengembangkan model Hybrid ARIMA-Quantum Neural Network
5. Melakukan perbandingan tingkat akurasi hasil peramalan dengan tiap model

Untuk mendapatkan model peramalan yang diharapkan sesuai dengan data runtun waktu yang ada, maka perlu dilakukan pendekatan ilmiah yaitu dengan melihat pola data runtun waktu yang ada terlebih dahulu. Pendekatan lainnya adalah menggunakan tools untuk menentukan secara otomatis Bentuk model statistik ARIMA yang sesuai dengan runtun waktu yang ada, lalu model tersebut dilatih menggunakan quantum neural network agar diketahui pola-pola data yang	sudah	ada	dan	dapat	diuji	akurasinya. Tahapan Analisis Time Series (ARIMA)
a. Membuat Plot Time Series
Identifikasi asumsi stasioneritas data runtun waktu. Suatu deret pengamatan dikatakan stasioner apabila proses tidak berubah seiring dengan perubahan waktu
Tidak stasioner dalam mean : jika trend tidak datar (tidak sejajar smbu waktu)
Tidak stasioner dalam varian : jika trend datar atau hampir datar, tetapi data tersebar membangun pola melebar atau menyempit (pola terompet)
Tidak stasioner dalam mean & varians : jika trend tidak datar dan data membentuk pola terompet. Semua sinyal yang diberi pengali bobot ini kemudian dijumlahkan satu sama lain untuk menghasilkan unit aktivasi. Unit aktivasi ini kemudian dibandingkan dengan sebuah nilai ambang, dan hasilnya dimasukkan kedalam fungsi transfer (fungsi non-linier) yang akan menghasilkan sebuah keluaran. Untuk fungsi hyperbolic tangent,

3.4 MODEL HYBRID ARIMA NEURAL NETWORK
      Berdasarkan hasil peramalan model ARIMA, akan dilakukan proses analisis runtun waktu menggunakan metode jaringan syaraf tiruan. Dengan kata lain, output dari peramalan model ARIMA akan menjadi input pada proses pengolahan data menggunakan metode jaringan syaraf tiruan. Kemudian akan ditentukan model jaringan syaraf tiruan yang sesuai dan cocok untuk data runtun waktu tersebut. Secara matematis, hasil ramalan secara keseluruhan yang diperoleh adalah sebagai berikut :
Zt merupakan hasil peramalan yang merupakan gabungan nilai ramalan dari model ARIMA atau Exponential Smoothing dan nilai ramalan dari model JST. 3.5 MODEL QUANTUM HYBRID ARIMA NEURAL NETWORK
      Ada banyak pendekatan untuk pengembangan model Quantum Arima NN. Mirip dengan bit klasik, gerbang dasar dapat membentuk gerbang kuantum bemacam-macam dan menyelesaikan keadaan kuantum dari beberapa logika transformasi. berbasis elemen pada gerbang pergeseran fasa 1 bit dan gerbang kontrol-Tidak 2 bit dalam dinamika kuantum diambil sebagai fungsi aktivasi dalam Jaringan saraf. Untuk memudahkan aplikasi, formulir berikut:
Fungsi kompleks diberikan untuk menyatakan keadaan kuantum:
adalah bilangan imaginer adalah kuantum fase

3.6 Pengukuran Kinerja
3.6.1 Mean Squared Error
      Dalam statistik, Mean Squared Error (MSE) sebuah estimator adalah nilai yang diharapkan dari kuadrat error. Error yang ada menunjukkan seberapa besar perbedaan hasil estimasi dengan nilai yang akan diestimasi. Perbedaan itu terjadi karena adanya keacakan pada data atau karena estimator tidak mengandung informasi yang dapat menghasilkan estimasi yang lebih akurat
3.6.2 Komparasi Hasil Peramalan
      Setelah nilai Mean Squared Error dari kedua metode didapatkan, maka akan dilakukan komparasi terhadap nilai MSE yang didapatkan pada periode testing (out- sample)
Jika nilai MSESTATISTIKA < MSEANN maka metode Statistika memiliki performa lebih baik dibandingkan metode ANN karena memiliki tingkat kesalahan relatif lebih kecil. Sebaliknya, jika MSESTATISTIKA > MSEANN maka metode Statistika memilki performa lebih buruk dibandingkan metode ANN karena tingkat kesalahan yang dihasilkan relatif lebih besar."
"3.1 Motivasi
       Industri manufaktur memiliki berbagai macam produk yang ada di dalamnya. Dalam upaya pemenuhan kualitas yang tinggi serta menjaga kepuasan pelanggan dan reputasi perusahaan maka mendeteksi produk yang cacat sedini mungkin merupakan aspek yang penting. Sehingga motivasi dari disertasi ini adalah sebagai berikut. 1. Pengembangan aplikasi pendeteksi cacat pada produk ini didasari keinginan peneliti untuk meningkatkan kinerja pengendalian kualitas pada industri manufaktur sehingga dapat membantu menjaga kualitas produk serta efisiensi dalam kegiatan pengendalian kualitas. 2. Untuk meminimalisir pemborosan waktu, bahan baku, biaya dan sumber daya lainnya karena deteksi cacat pada produk dilakukan sedini dan secepat mungkin. 3. Meningkatkan efisiensi pada kegiatan inspeksi produk dengan menerapkan otomatisasi melalui aplikasi yang dikembangkan. 4. Mengintegrasikan teknologi yang sedang berkembang seperti artificial intelligence dengan industri manufaktur sehingga tercipta manufaktur cerdas yang akan berakibat pendapatan profit perusahaan yang optimal. 5. Memberikan kontribusi pemahaman dan pengembangan teknologi baru dalam deteksi objek sehingga bisa menjadi referensi untuk pembaca serta penelitian selanjutnya. 3.2 Alur Kerja Riset
       Alur kerja riset digambarkan melalui diagram alir. Tujuannya agar penelitian dapat terstruktur sehingga tidak ada tahapan penelitian yang terlewat. Secara umum berikut ini merupakan diagram alir penelitian ini. Diagram alir penelitian di atas menggambarkan alur penelitian yang akan dilakukan. Berikut ini adalah penjelasan dari diagram alir penelitian di atas. 1. Tahap Awal
Kegiatan yang dilakukan pada tahap awal ini adalah merancang dan membuat prototype alat deteksi cacat dan pengumpulan data cacat objek. Prototype alat ini menggunakan ban berjalan dengan motor listrik sebagai penggeraknya dengan alat pencahayaan yang cukup. Alat ini nantinya digunakan untuk mengumpulkan data yang akan digunakan untuk melakukan perancangan dan pelatihan model deteksi cacat objek. Pengumpulan data dilakukan untuk memperoleh data yang dibutuhkan pada penelitian ini. Data bisa berupa data primer dan data sekunder ataupun keduanya bergantung pada kebutuhan penelitian yang akan dilakukan. Data primer dikumpulan dengan memotret objek pada ban berjalan menjadi citra baik citra bergerak maupun citra tak bergerak yang akan menjadi satu kesatuan yaitu dataset. Data primer dikumpulkan menggunakan alat yang dirancang seperti di bawah ini. Gambar 3.2 di atas menggambarkan rancangan alat yang akan dikembangkan. Alat tersebut pertama digunakan sebagai media untuk pengambilan data primer yaitu data citra dari objek yang akan dideteksi. Objek berupa sekrup akan berjalan melalui ban berjalan (conveyor) yang nantinya akan ditangkap gambarnya oleh webcam atau kamera yang terhubung dengan komputer untuk menyimpan gambar tersebut untuk kebutuhan pelatihan model. Sedangkan data sekunder dikumpulkan melalui website haggle maupun website atau jurnal lain yang sejenis. Hasil dari akuisisi citra ini akan digunakan untuk pelatihan dan pengujian data. Sampel yang diambil adalah objek berupa sekrup yang terdapat kecacatan. Data tersebut kemudian dikumpulkan menjadi sebuah dataset yang akan digunakan untuk melatih model. Data-data yang diambil kemudian dikelompokkan menjadi beberapa kelas sesuai dengan jenis cacat yang ada pada sekrup tersebut. Luaran pada tahap ini adalah pengajuan HKI untuk prototype alat pendeteksi cacat objek yang dirancang. 2. Tahap Pengembangan
Tahap ini terdapat beberapa kegiatan yang dilakukan. Pertama adalah melakukan uji coba prototype alat deteksi cacat objek yang digambarkan pada gambar 3.2 di atas. Uji coba dilakukan dengan menyesuaikan tinggi kamera, tingkat pencahayaan, kecepatan ban berjalan serta pengaturan tempat ban berjalan untuk menjaga efektivitas dan efisiensi dalam mendeteksi objek. Kedua adalah merancang model untuk mendeteksi cacat objek dengan menggunakan deep learning. Sebelum melatih data dilakukan preprocessing terlebih dahulu. Kegiatan ini dilakukan dengan menggunakan website roboflow. Preprocessing dilakukan mengoptimalkan pelatihan dengan menganotasi citra untuk menandai bagian penting dari citra (region of interest), menyamakan orientasi citra, mengubah ukuran citra agar sama, memperbanyak variasi data dengan augmentasi, dan generalisir data sehingga menjadi satu kesatuan dataset yang lebih siap untuk dilatih. Setelah preprocessing dilakukan maka diharapkan pelatihan data yang dilakukan lebih optimal. Pelatihan data dilakukan untuk melatih model mengenali citra yang akan dideteksi sehingga pada penerapannya mendapatkan hasil deteksi yang akurat dan optimal. Pelatihan data dilakukan dengan menggunakan salah satu algoritma dari teknologi kecerdasan artifisial yaitu deep learning dengan bahasa pemrograman yang digunakan adalah python. Pada pelatihan data ini juga akan mendapatkan nilai pengukuran evaluasi (measurment evaluation) berupa accuracy, recall and precision, dan mean average precision (MAP). Pada umumnya pelatihan harus memiliki jumlah data (dalam hal ini adalah citra) yang lebih banyak dibandingkan pengujian. 3. Tahap Optimasi
Tahap pengembangan telah dilakukan kemudian masuk ke tahap optimasi. Tahap ini terdapat kegiatan yaitu evaluasi dan penyempurnaan model deteksi cacat objek. Evaluasi dan penyempurnaan dilakukan agar ftur yang ada pada aplikasi yang akan dikembangkan dapat ditampilkan dengan maksimal. Fitur yang akan ditambahkan pada model pendeteksi objek berupa kemampuan komputer untuk secara otomatis menyimpan hasil deteksi menjadi sebuah basis data. Sehingga nantinya data tersebut dapat menjadi acuan bagi departemen terkait untuk inovasi ke depannya. Setelah pelatihan data dilakukan, maka selanjutnya adalah pengujian data. Pengujian data dilakukan untuk menguji model sejauh mana dapat mendeteksi cacat dari suatu produk. Pada pengujian data dilakukan dengan mengunggah data secara acak selain data yang digunakan pada pelatihan. Pada akhirnya akan menampilkan output model dalam mendeteksi cacat pada produk. Setelah itu maka dibangun aplikasi yang mampu mendeteksi cacat produk pada industri secara real time. Aplikasi ini nantinya akan menampilkan hasil deteksi dari produk yang bergerak. Informasi yang disampaikan antara lain kondisi dari produk cacat atau tidak serta bagian mana yang cacat akan ditandai oleh bounding box. Hal ini akan dengan cepat membantu operator mengetahui cacat jenis apa yang terjadi. Sehingga dapat ditindaklanjuti sesegera mungkin yang secara tidak langsung juga membantu dalam pengambilan keputusan. Target penelitian ini adalah pengajuan HKI serta publikasi artikel/jurnal ilmiah internasional bereputasi (Q1 ; IEEE Access). 3.3 Pendekatan
       Pendekatan yang dilakukan adalah dengan menggunakan teknologi artificial intelligence dalam mengadopsi kemampuan manusia dalam mendeteksi objek. Pendekatan ini menggabungkan antara pengolahan citra dan deep learning dengan memanfaatkan salah satu arsitektur yang dimilikinya. Selain itu diterapkan juga pengukuran evaluasi seperti precision, recall, dan mean average precision (MAP) untuk memastikan model yang dikembangkan dapat digunakan dengan optimal. Nantinya akan dikembangkan sebuah aplikasi yang kemungkinan berbasis web untuk mempermudah pengguna untuk mengambil gambar (bergerak maupun tak bergerak) yang kemudian mengirimnya ke sistem pendeteksi cacat dan menerima hasil deteksi secara real time. Hasil deteksi secara real time dikehendaki agar produk dapat diperiksa selama proses produksi berlangsung sehingga cacat dapat dideteksi secepat dan seakurat mungkin. Hal ini akan membantu operator untuk melakukan kegiatan inspeksi produk dengan efisien. 3.4 Rencana Jadwal Kegiatan
       Rencana jadwal kegiatan dibuat bertujuan untuk menentukan rencana waktu suatu kegiatan yang menunjang penelitian ini dilakukan. Berikut ini merupakan tabel rencana jadwal kegiatan pada penelitian ini.","3.1 Motivasi
       Industri manufaktur memiliki berbagai macam produk yang ada di dalamnya. Dalam upaya pemenuhan kualitas yang tinggi serta menjaga kepuasan pelanggan dan reputasi perusahaan maka mendeteksi produk yang cacat sedini mungkin merupakan aspek yang penting. Pengembangan aplikasi pendeteksi cacat pada produk ini didasari keinginan peneliti untuk meningkatkan kinerja pengendalian kualitas pada industri manufaktur sehingga dapat membantu menjaga kualitas produk serta efisiensi dalam kegiatan pengendalian kualitas. Untuk meminimalisir pemborosan waktu, bahan baku, biaya dan sumber daya lainnya karena deteksi cacat pada produk dilakukan sedini dan secepat mungkin. Meningkatkan efisiensi pada kegiatan inspeksi produk dengan menerapkan otomatisasi melalui aplikasi yang dikembangkan. Mengintegrasikan teknologi yang sedang berkembang seperti artificial intelligence dengan industri manufaktur sehingga tercipta manufaktur cerdas yang akan berakibat pendapatan profit perusahaan yang optimal. Memberikan kontribusi pemahaman dan pengembangan teknologi baru dalam deteksi objek sehingga bisa menjadi referensi untuk pembaca serta penelitian selanjutnya. Tahap Awal
Kegiatan yang dilakukan pada tahap awal ini adalah merancang dan membuat prototype alat deteksi cacat dan pengumpulan data cacat objek. Gambar 3.2 di atas menggambarkan rancangan alat yang akan dikembangkan. Tahap ini terdapat kegiatan yaitu evaluasi dan penyempurnaan model deteksi cacat objek. Evaluasi dan penyempurnaan dilakukan agar ftur yang ada pada aplikasi yang akan dikembangkan dapat ditampilkan dengan maksimal. Fitur yang akan ditambahkan pada model pendeteksi objek berupa kemampuan komputer untuk secara otomatis menyimpan hasil deteksi menjadi sebuah basis data. Pengujian data dilakukan untuk menguji model sejauh mana dapat mendeteksi cacat dari suatu produk. Pada akhirnya akan menampilkan output model dalam mendeteksi cacat pada produk. Setelah itu maka dibangun aplikasi yang mampu mendeteksi cacat produk pada industri secara real time. Aplikasi ini nantinya akan menampilkan hasil deteksi dari produk yang bergerak. Informasi yang disampaikan antara lain kondisi dari produk cacat atau tidak serta bagian mana yang cacat akan ditandai oleh bounding box. Selain itu diterapkan juga pengukuran evaluasi seperti precision, recall, dan mean average precision (MAP) untuk memastikan model yang dikembangkan dapat digunakan dengan optimal. Nantinya akan dikembangkan sebuah aplikasi yang kemungkinan berbasis web untuk mempermudah pengguna untuk mengambil gambar (bergerak maupun tak bergerak) yang kemudian mengirimnya ke sistem pendeteksi cacat dan menerima hasil deteksi secara real time. Hasil deteksi secara real time dikehendaki agar produk dapat diperiksa selama proses produksi berlangsung sehingga cacat dapat dideteksi secepat dan seakurat mungkin. Hal ini akan membantu operator untuk melakukan kegiatan inspeksi produk dengan efisien."
"3.1 KONSEP PENELITIAN
Untuk mempermudah dalam melakukan penelitian, maka dibuat sebuah flowchart agar penelitian tidak menyimpang dan salah. Berikut flowchart penelitian untuk rangkaian SRAM 6T Low power dan High read stability dengan metode m- GDI. Dalam memulai desain SRAM 6T dengan menggunakan metode m-GDI, diperlukan studi literatur terkait beberapa penelitian dengan metode atau hasil serupa. Setelah mempelajari seluruh penelitian terkait, maka dilakukan desain
rangkaian SRAM dengan metode konvensional sebagai referensi untuk dilakukan proses m-GDI. Referensi rangkaian diperlukan untuk melihat hasil sebagai pembanding dengan rangkaian baru yang didesain dengan metode m-GDI. Dipilih desain berdasarkan penelitian sebelumnya yang dilakukan oleh Ebrahim Abiri dan Abdolreza Darabi (2015) SRAM 8T dengan Low Power dan High Read Stability menggunakan metode m-GDI. Berdasarkan penelitian yang dilakukan, rangkaian SRAM ini terbagi menjadi 3 Blok, yakni Write Block, SRAM 8T Block dan Read Block, seperti pada gambar 11 berikut,
Proses write pada rangkaian SRAM 8T dimulai ketika word line (WL) mencapai tegangan tinggi, yang menyebabkan transistor akses (access transistors) menjadi aktif (on). Pada saat itu, data disimpan dengan cepat pada node q dan qb yang terhubung ke gerbang dari transistor pusat sel (ENR dan ENL). Setelah itu, data
mencapai keadaan permanen dengan bantuan sel-sel m-GDI . Selama siklus write, sel m-GDI berperan dalam memastikan bahwa data yang ditulis ke dalam sel memori SRAM disimpan dengan stabil dan cepat, yang merupakan bagian penting dari desain SRAM low power
Dalam blok rangkaian SRAM 8T, input utama termasuk word line (WL), bit lines (BL dan BLB), dan sinyal-sinyal kontrol untuk operasi pembacaan dan penulisan. Output dari rangkaian ini adalah data yang dibaca dari sel memori (D_Out dan D_Outb). Ketika melakukan operasi penulisan, data yang akan ditulis ke dalam sel memori disuplai melalui bit lines (BL dan BLB). Sinyal WL diaktifkan untuk menghubungkan sel memori dengan bit lines, memungkinkan data untuk ditransfer ke dalam sel. Setelah data ditulis, WL dinonaktifkan untuk mengisolasi sel dari bit lines dan menjaga data yang telah disimpan. Selama operasi pembacaan, WL diaktifkan untuk menghubungkan sel memori dengan bit lines, memungkinkan data yang tersimpan di dalam sel untuk ditransfer keluar. Data yang dibaca kemudian muncul pada output D_Out dan D_Outb dengan swing tegangan maksimum pada output inverter.","Berikut flowchart penelitian untuk rangkaian SRAM 6T Low power dan High read stability dengan metode m- GDI. Dalam memulai desain SRAM 6T dengan menggunakan metode m-GDI, diperlukan studi literatur terkait beberapa penelitian dengan metode atau hasil serupa. Setelah mempelajari seluruh penelitian terkait, maka dilakukan desain
rangkaian SRAM dengan metode konvensional sebagai referensi untuk dilakukan proses m-GDI. Referensi rangkaian diperlukan untuk melihat hasil sebagai pembanding dengan rangkaian baru yang didesain dengan metode m-GDI."
"3.1 Tahapan Penelitian
    Dalam penelitian mengenai pengembangan algoritma DBSCAN dengan kuantum terdapat langkah-langkah yang dilakukan, seperti pada gambar 3.1. Langkah-langkah yang dilaukan diantaranya yaitu, pengumpulan data, definisi qubits kriteria, inisialisasi sistem kuantum, hingga evaluasi klaster. 1. Data
Tahap awal dalam penelitian di awali dengan pembuatan data, dimana data yang digunakan pada penelitian ini adalah data sintetik. Data sintetik digunakan untuk mendapatkan jumlah data yang besar, selain itu data sintetik juga bersifat fleksibel karena jumlah data yang digunakan dapat ditentukan sesuai dengan kebutuhan pengujian algoritma yang dikembangkan. Data sintetik yang dibuat berisikan nama supplier, harga, kualitas, dan waktu pengiriman. 2. Definisi Qubits Kriteria
Pada tahap ini kriteria yang digunakan untuk pengelompokan supplier diubah menjadi representasi kuantum menggunakan qubits. Setiap kriteria mungkin diwakili oleh satu atau lebih qubits tergantung pada kompleksitas yang diperlukan. Kriteria yang digunakan dalam pengelompokan supplier yaitu, Harga, Kualitas, dan Waktu Pengiriman. 3. Inisialisasi Sistem Kuantum
Pada tahapan ini melakukan persiapan awal dari komputer kuantum yaitu mengatur qubits ke state awal dan memastikan semua qubits berada dalam keadaan awal sebelum operasi kuantum dijalankan. Pada tahapan ini juga menentukkan jumlah qubits yang digunakan. 4. Implementasi Quantum Distance Measure
Pada tahapan ini melakukan penerapan metode untuk mengukur jarak antar supplier dalam ruang kuantum dengan menggunakan prinsip-prinsip mekanika kuantum. Tahapan ini digunakan dalam proses pengelompokkan data menggunakan Quantum DBSCAN karena jarak antar supplier akan digunakan untuk menentukan klaster
5. Penentuan Eps dan MinPts Kuantum
Pada tahap ini menentukan nilai nilai Epsilon atau Eps dan minimum points (MinPts) dalam konteks kuantum untuk menentukan batas-batas klaster. Epsilon atau Eps digunakan untuk menentukan radius yang menentukan lingkungan di sekitar setiap titik data. Dua titik dianggap bertetangga jika jarak antara mereka kurang dari nilai Eps. Minimum Points atau MinPts untuk menentukan jumlah minimum titik yang diperlukan untuk membentuk sebuah klaster. 6. Identifikasi Core Supplier dengan Kuantum Sirkuit
Pada tahapan ini menggunakan rangkaian kuantum untuk mengidentifikasi supplier ini (core supplier). Supplier inti adalah supplier yang memiliki cukup banyak tetangga yang sesuai dengan minpts dalam radius epsilon yang telah ditentukan. 7. Identifikasi Noise Supplier dengan Kuantum Sirkuit
Pada tahapan ini mengidentifikasi supplier noise atau outlier yang memiliki jarak tidak cukup dekat atau memiliki jarak yang jauh dengan supplier lain untuk dianggap bagian dari klaster. 8. Penanganan Noise dengan Quantum State
Pada tahapan ini Mengelola supplier noise yang telah diidentifikasi menggunakan teknik kuantum untuk memisahkan atau mengelompokkan noise secara terpisah. Dalam DBSCAN klasik, noise adalah titik data yang tidak termasuk dalam klaster apa pun. Titik-titik ini tidak memiliki cukup tetangga dalam radius Epsilon (Eps) atau tidak terhubung ke core point. 9. Identifikasi Core Supplier dengan Quantum Circuit
Pada tahapan ini mengidentifikasi titik-titik data yang berada dalam jarak Epsilon atau Eps dari titik inti tetapi tidak memiliki cukup tetanga untuk masuk ke dalam klaster dengan menggunakan kuantum sirkuit. 10. Formasi Kluster Supplier dengan Quantum Measurement
Pada tahapan ini membentukan klaster supplier dengan mengukur state kuantum yang telah diubah melalui interaksi antar qubits yang mewakili supplier. 11. Evaluasi Kluster
Tahap terakhir di mana kualitas dan keefektifan kluster yang terbentuk dievaluasi. Tahapan ini bertujuan untuk menilai seberapa baik kluster yang terbentuk menggunakan. 3.2 Rangkuman Langkah-Langkah Penelitian
      Setelah mengembangkan algoritma kuantum DBSCAN selanjutnya membandingkannya dengan algoritma DBSCAN untuk mengetahui seberapa baik algoritma DBSCAN jika dibandingkan dengan algoritma klasiknya. Langkah-langkah tersebut dapat dilihat pada Gambar 3.2 Rangkuman Langkah-Langkah Prosedur Penelitian. 3.3 Jadwal Penelitian
    Jadwal Penelitian digunakan untuk merencanakan, mengatur, dan mengelola waktu, sumber daya, dan tugas-tugas dalam rangka mencapai tujuan yang telah ditetapkan. Berikut merupakan rencana kegiatan pada penelitian ini.","3.1 Tahapan Penelitian
    Dalam penelitian mengenai pengembangan algoritma DBSCAN dengan kuantum terdapat langkah-langkah yang dilakukan, seperti pada gambar 3.1. Langkah-langkah yang dilaukan diantaranya yaitu, pengumpulan data, definisi qubits kriteria, inisialisasi sistem kuantum, hingga evaluasi klaster. 1. Data
Tahap awal dalam penelitian di awali dengan pembuatan data, dimana data yang digunakan pada penelitian ini adalah data sintetik. Data sintetik digunakan untuk mendapatkan jumlah data yang besar, selain itu data sintetik juga bersifat fleksibel karena jumlah data yang digunakan dapat ditentukan sesuai dengan kebutuhan pengujian algoritma yang dikembangkan. Data sintetik yang dibuat berisikan nama supplier, harga, kualitas, dan waktu pengiriman. Definisi Qubits Kriteria
Pada tahap ini kriteria yang digunakan untuk pengelompokan supplier diubah menjadi representasi kuantum menggunakan qubits. Formasi Kluster Supplier dengan Quantum Measurement
Pada tahapan ini membentukan klaster supplier dengan mengukur state kuantum yang telah diubah melalui interaksi antar qubits yang mewakili supplier. 3.2 Rangkuman Langkah-Langkah Penelitian
      Setelah mengembangkan algoritma kuantum DBSCAN selanjutnya membandingkannya dengan algoritma DBSCAN untuk mengetahui seberapa baik algoritma DBSCAN jika dibandingkan dengan algoritma klasiknya. Langkah-langkah tersebut dapat dilihat pada Gambar 3.2 Rangkuman Langkah-Langkah Prosedur Penelitian. 3.3 Jadwal Penelitian
    Jadwal Penelitian digunakan untuk merencanakan, mengatur, dan mengelola waktu, sumber daya, dan tugas-tugas dalam rangka mencapai tujuan yang telah ditetapkan. Berikut merupakan rencana kegiatan pada penelitian ini."
"Bab ini akan menjelaskan tentang metodologi penelitian yang digunakan sebagai gambaran dari langkah-langkah yang akan dilakukan untuk menyelesaikan penelitian ini. 3.1 Tahapan Penelitian
Penelitian ini melakukan pengembangan model klasifikasi toksisitas pada platform sosial media. Tahapan penelitian yang digunakan dapat dilihat pada gambar 3.1. Tahapan metode penelitian pada gambar 3.1 terdiri dari beberapa langkah, yaitu :
1. Tahap Literature Review
Pada tahap ini dimulai dengan melakukan kajian dari berbagai sumber tertulis dalam bentuk buku, artikel dan jurnal serta penelitian-peneltiian terkait guna memahami dan mengidentifikasi kesenjangan dalam topik penelitian serta menemukan kelemahan dan kelebihan dalam penelitian. Selain itu juga untuk menentukan dan membandingkan metode serta algoritma yang sudah digunakan pada penelitian sebelumnya, yang nantinya akan mengembangkan atau menciptakan suatu metode atau algoritma terbaru. 2. Tahap Pengumpulan Data
Pada tahap ini dilakukan pengumpulan data yang akan digunakan untuk melatih dan menguji model. Data ini dapat berupa konten-konten pada sosial media yang akan dikategorikan ke dalam 3 kategori toksisitas yaitu toxic, non-toxic, dan netral. Data tersebut harus mencakup berbagai jenis media, seperti teks, gambar dan video, untuk memungkinkan model mengenali toksisitas dari berbagai jenis konten yang ada pada platform sosial media. 3. Large Language Model (LLM)
Large Language Model merupakan jenis model kecerdasan buatan (Artificial Intelligence) yang dilatih untuk memahami, menghasilkan dan memproses bahasa alami (Natural Languange) dalam skala besar. Large Language Model dilatih menggunakan dataset yang besar, terdiri dari teks yang diambil dari berbagai sumber seperti artikel, buku, situs web dan lainnya. 4. Large Language Model (LLM) Multimodal
Large Language Model pada penelitian yang dilakukan untuk memproses tidak hanya dalam bentuk jenis media teks, melainkan gambar dan juga video. Large Language Model (LLM) untuk klasifikasi multimodal melibatkan teks, gambar dan juga video. Meskipun LLM berfokus pada teks, model ini dapat diadaptasi atau dikombinasikan dengan model lain yang mendukung modalitas non-teks seperti gambar dan video, melalui pendekatan yang disebut dengan model multimodal. Pada tahap ini dilakukan pre-processing dari masing-masing jenis media yang digunakan. * Untuk representasi teks menggunakan teknik-teknik pemrosesan bahasa alami seperti tokenisasi, vektorisasi data (word embedding) dan penggunaan model bahasa pre-trained seperti BERT untuk mewakili teks dalam bentuk vektor numerik yang dapat dimengerti oleh model. * Untuk representasi gambar dan video menggunakan teknik-teknik pemrosesan gambar seperti ekstraksi fitur dengan convolutional neural networks (CNN) atau menggunakan model pre-trained seperti ResNet atau VGG untuk mewakili gambar dalam bentuk vektor numerik. 5. Generate Caption
Pada tahapan ini menggunakan model LLM, seperti BLIP atau Flamingo untuk menggabungkan kemampuan visual dan bahasa dalam menghasilkan teks/captioning dari representasi gambar dan video. 6. Model Klasifikasi Toksisitas
Pada tahapan ini dilakukan pengembangan model dari hasil penggabungan ketiga representasi tersebut, dengan menggunakan teknik fusion, seperti concatenation atau attention mechanism untuk menghasilkan hasil klasifikasi akhir. Model klasifikasi yang digunakan adalah Convolutional Neural Network (CNN). 7. Evaluasi Model
Pada tahapan ini dilakukan evaluasi untuk mengetahui kinerja terhadap model yang dikembangkan dengan menggunakan pengukuran akurasi, seperti precision, recall dan juga F1-score untuk klasifikasi teks, dan mengukur akurasi dengan confusion matrix untuk gambar dan video. 8. Hasil
Tahapan ini menghasilkan klasifikasi sesuai dengan label yang sudah dikategorikan ke dalam 3 kategori toksisitas yaitu toxic, non-toxic, dan netral.","3.1 Tahapan Penelitian
Penelitian ini melakukan pengembangan model klasifikasi toksisitas pada platform sosial media. Selain itu juga untuk menentukan dan membandingkan metode serta algoritma yang sudah digunakan pada penelitian sebelumnya, yang nantinya akan mengembangkan atau menciptakan suatu metode atau algoritma terbaru. Data tersebut harus mencakup berbagai jenis media, seperti teks, gambar dan video, untuk memungkinkan model mengenali toksisitas dari berbagai jenis konten yang ada pada platform sosial media. Large Language Model (LLM)
Large Language Model merupakan jenis model kecerdasan buatan (Artificial Intelligence) yang dilatih untuk memahami, menghasilkan dan memproses bahasa alami (Natural Languange) dalam skala besar. Generate Caption
Pada tahapan ini menggunakan model LLM, seperti BLIP atau Flamingo untuk menggabungkan kemampuan visual dan bahasa dalam menghasilkan teks/captioning dari representasi gambar dan video. Model Klasifikasi Toksisitas
Pada tahapan ini dilakukan pengembangan model dari hasil penggabungan ketiga representasi tersebut, dengan menggunakan teknik fusion, seperti concatenation atau attention mechanism untuk menghasilkan hasil klasifikasi akhir. Evaluasi Model
Pada tahapan ini dilakukan evaluasi untuk mengetahui kinerja terhadap model yang dikembangkan dengan menggunakan pengukuran akurasi, seperti precision, recall dan juga F1-score untuk klasifikasi teks, dan mengukur akurasi dengan confusion matrix untuk gambar dan video. Hasil
Tahapan ini menghasilkan klasifikasi sesuai dengan label yang sudah dikategorikan ke dalam 3 kategori toksisitas yaitu toxic, non-toxic, dan netral."
"3.1   Tahapan Penelitian
      Tahapan penelitian dibagi atas beberapa tahapan yang dilakukan dari awal sampai akhir. Tahapan dimulai dari studi literatur sampai analisis yang membentuk alur secara sistematis. Tahapan penelitian ini terpada pada Gambar 3.1


3.2   Desain Algoritma
      Penelitian yang terdahulu menggunakan metode yang memiliki keamanan tinggi yang dibuktikan dengan beberapa parameter pengujian. Pada penelitian ini mengajukan pengembangan algoritma kriptografi citra digital dengan mengkombinasi teknik konfusi dengan algoritma Cat Map dan Henon Map serta teknik difusi dengan algoritma Logistic Map. Pengembangan pada algoritma ini diharapkan dapat memiliki keamanan yang lebih tinggi dengan melalui beberapa parameter pengujian. Diagram alur proses enkripsi dapat dilihat pada 3.2. 3.3   Pengujian
      Tahapan pengujian dilakukan untuk mengetahui hasil pada proses enkripsi dan dekripsi beberapa pengujian yang dilakukan yaitu:
1. Histogram
Histogram merupakan analisis statistik yang menunjukkan penyebaran atau distribusi piksel pada citra. Histogram sering digunakan untuk pada pengolahan citra untuk melihat kualitas citra. Kriptografi pada citra digital yang ideal memiliki distribusi nilai piksel yang beragam (Benlashram et al., 2020). 2. PSNR (Peak Signal Noise to Ratio)
PSNR digunakan untuk pengukuran kualitas citra antara citra asli dan noise yang terjadi pada citra terenkripsi. Nilai PSNR = 30 dB membuktikan kualitas yang baik pada citra asli atau citra terdekripsi (Lone et al., 2021). Berikut persamaan PSNR terdapat pada persamaan 3.1.","3.1   Tahapan Penelitian
      Tahapan penelitian dibagi atas beberapa tahapan yang dilakukan dari awal sampai akhir. Pada penelitian ini mengajukan pengembangan algoritma kriptografi citra digital dengan mengkombinasi teknik konfusi dengan algoritma Cat Map dan Henon Map serta teknik difusi dengan algoritma Logistic Map. Pengembangan pada algoritma ini diharapkan dapat memiliki keamanan yang lebih tinggi dengan melalui beberapa parameter pengujian. Nilai PSNR = 30 dB membuktikan kualitas yang baik pada citra asli atau citra terdekripsi (Lone et al., 2021)."
3.1 tahapan penelitian tahapan penelitian merupakan gambaran dari langkah langkah atau proses yang akan dilakukan dalam suatu penelitian. penelitian ini terdiri dari lima tahap an. tahap pertama adalah mengidentifikasi permasalahan pihak terkait interaksi tujuan dan tinjauan pustaka . tahap an kedua adalah membuat model manajemen persediaan beras perum bulog berdasarkan hasil wawancara awal dengan pihak terkait. tahap ketiga adalah melakukan analisis terhadap model manajemen persediaan beras perum bulog untuk mengidentifikasi area yang perlu ditingkatkan . hasil analisis ini akan digunakan untuk merumuskan solusi terhadap permasalahan yang ada . tahap keempat adalah pengembangan solusi berbasis teknologi yang terdiri dari pengembangan be rbagai model dan prototype sistem yang akan diuji . usulan yang pertama adalah model generative ai untuk menghasilkan data sintetis yang realistis yang dapat digunakan sebagai data pelatihan untuk model prediksi . model ini kemudian diintegrasikan ke dalam model ml prediksi produksi hasil panen . usulan yang kedua adalah model prediksi permintaan beras yang merupakan model yang mirip dengan model prediksi produksi hasil panen beras dengan beberapa penyesuaian agar sesuai dengan karakteristik data untuk prediksi permintaan beras. usulan yang ketiga adalah pengembangan prototype decision support system yang mengintegrasikan model prediksi dan optimasi untuk mendukung kebijakan terkait pengadaan cadangan beras . tahap kelima adalah uji coba terhadap prototype decision support system . gambar 3. 1 adalah tahapan pada penelitian ini. 59 gambar 3. 1 tahapan penelitian 3.2 pemodelan manajemen persediaan beras perum bulog manajemen persediaan cadangan beras nasional telah menjadi perhatian penting dalam beberapa tahun terakhir karena meningkatnya permintaan pangan global perubahan iklim dan ketidakstabilan ekonomi. cadangan ini merupakan stok strategis yang diawasi oleh pemerintah untuk menstabilkan persediaan dan harga beras memberikan bantuan saat terjadi kekurangan pangan dan mendukung tujuan ketahanan pangan nasional . manajemen persed iaan cadangan beras yang efektif sangat penting untuk memitigasi risiko yang terkait dengan gangguan persediaan dan fluktuasi harga beras yulianis rachman 2021 yang pada akhirnya akan menjamin ketahanan pangan dan stabilitas ekonomi octania 2021 usdianto setiyowati 2023 . 60 para pemangku kepentingan yang terlibat dalam manajemen cadangan beras ini termasuk kementerian pertanian kementerian perdagangan kem enteri an keuangan dan kem enteri badan usaha milik negara sebagai regulator serta perum bulog yang bertanggung jawab untuk mengelola persediaan beras pemerintah dan stabilisasi harga di tingkat produsen dan konsumen octania 2021 . perum bulog bertanggung jawab untuk menjaga stabilitas harga dengan membeli gabah dan beras dari petani dengan harga yang ditentukan pemerintah ketika harga beli gabah turun sehingga melindungi petani dari kerugian dan menjual beras dengan harga yang lebih rendah daripada harga pasar ketika terjadi kenaikan harga beras untuk memastikan keterjangkauan harga beras bagi masyarakat octania 2021 . lembaga ini bertanggung jawab atas manajemen salah satu komponen cadangan beras nasional yaitu cadangan beras pemerintah cbp termasuk pada pengadaan dalam negeri dan impor penyimpanan dan penyaluran beras untuk kebutuhan stabilisasi harga bantuan pangan dan keadaan darurat fang chen zhang pei gao wang 2020 octania 2021 . beberapa penelitian telah menekankan peran penting perum bulog dalam manajemen persediaan cadangan beras di indonesia . melalui manajemen cbp perum bulog memainkan peran penting dalam menjaga ketahanan pangan nasional terutama saat terjadi fluktuasi harga atau gangguan p ersediaan . keberadaan cbp yang dikelola perum bulog tidak hanya menstabilkan harga beras di pasar tetapi juga menjamin ketersediaan beras bagi masyarakat sehingga berkontribusi terhadap stabilitas ekonomi nasional octania 2021 putro purwaningsih sensuse suryono 2022 silalahi et al. 2019 . mengingat peran penting ini penerapan teknologi ai dapat membantu perum bulog dalam mengoptimalkan berbagai aspek dalam manajemen cadangan beras pemerintah seperti prediksi permintaan dan produksi hasil panen optimasi cadangan beras dan pengambilan keputusan yang lebih baik. ai dapat digunakan untuk menganalisis data historis dan realtime guna menghasilkan prediksi yang akurat mengenai permintaan dan produksi hasil panen beras mehmood et al. 2023 rai et al. 2021 sehingga memungkinkan 61 perum bulog untuk mengoptimalkan cadangan beras menghindari kelebihan atau kekurangan cadangan beras dan mengambil keputusan yang lebih baik dalam manajemen cadangan beras pemerintah h. qin 2023 . berdasarkan kajian model manajemen persediaan beras perum bulog maka penelitian ini berfokus pada pemanfaatan teknologi ai untuk efektivitas manajemen cadangan beras pemerintah terutama pada proses pengadaan . gambar 3.1 menggambarkan model manajemen persediaan beras perum bulog . gambar 3. 2 model manajemen persediaan perum bulog 3.3. analisis analisis ini bertujuan untuk mengungkap kelemahan dan proses yang kompleks dalam manajemen persediaan cadangan beras pemerintah di perum bulog . penelitian ini menggunakan metode analisis swot untuk mengidentifikasi titik titik lemah yang krusial dalam manajemen persediaan cadangan beras pemerintah dan mengembangkan strategi untuk meningkatkan efisiensi dan efektivitas pengelolaan persediaan cadangan beras di indonesia analisis swot bertujuan untuk mengetahui kekuatan kelemahan peluang dan ancaman bisnis. analisis lingkungan internal di fokuskan untuk mengetahui kekuatan dan kelemahan sedangkan analisis lingkungan eksternal difokuskan untuk mengetahui peluang dan ancaman putra pujangkoro situmorang 2022 . 62 1. analisis swot 1. strengths kekuatan s1 dukungan pemerintah perum bulog didukung oleh berbagai kebijakan pemerintah yang bertujuan menjaga stabilitas harga dan ketersediaan beras seperti yang diatur dalam uu no. 18 tahun 2012 tentang pangan dan perpres no. 48 tahun 2016 tentang penugasan kepada perum bulog . hal ini memberikan akses terhadap dukun gan kebijakan dan finansial yang kuat termasuk alokasi anggaran khusus untuk pengadaan cbp anggraini faqih sangadji kadarisman revany 2021 octania 2021 utomo 2020 . s2 infrastruktur logistik yang memadai perum bulog memiliki infrastruktur logistik yang cukup baik termasuk gudang penyimpanan yang tersebar di berbagai daerah yang berperan penting dalam menjaga kecukupan persediaan cadangan beras octania 2021 utomo 2020 s3 pengalaman dan keahlian perum bulog memiliki pengalaman puluhan tahun dalam manajemen persediaan beras mulai dari pengadaan hingga distribusi yang menjadi keunggulan dalam menjaga stabilitas harga dan persediaan beras anggraini et al. 2021 utomo 2020 . 2. weaknesses kelemahan w1 ketidakmampuan untuk bersaing dengan sektor swasta perum bulog sering menghadapi tantangan dalam bersaing dengan sektor swasta yang mampu menawarkan harga lebih tinggi kepada petan i octania 2021 . gambar 3.3 menggambarkan perbedaan yang signifikan antara harga gabah di tingkat petani dengan harga beli yang ditetapkan pemerintah. kon disi ini menyebabkan petani lebih memilih menjual hasil panennya ke sektor swasta. 63 gambar 3. 3 perbandingan harga gabah bps 2023 w2 ketergantungan pada impor meskipun perum bulog memprioritaskan pengadaan dalam negeri untuk memenuhi cbp tetapi masih terdapat ketergantungan pada impor beras terutama selama periode penurunan produksi dalam negeri yang membuat persediaan cbp rentan terhadap fluktuasi harga dan kebijakan perdagangan internasional octania 2021 utomo 2020 . tabel 3.1 dan gambar 3.4 menunjukkan banyaknya jumlah impor beras yang dilakukan perum bulog setiap tahunnya. tabel 3. 1 jumlah impor beras bps 2024 negara asal 2017 2018 2019 2020 2021 2022 2023 berat bersih ton india 32209.7 337999 7973.3 10594.4 215386.46 178533.57 69715.7 thailand 108944.8 795600.1 53278 88593.1 69360.037 80182.506 1381921.2 vietnam 16599.9 767180.9 33133.1 88716.4 65692.874 81828.039 1147705.3 pakistan 87500 310990 182564.9 110516.5 52479.011 84407 309309.7 myanmar 57475 41820 166700.6 57841.4 3790 3830 141204 jepang 72.1 0.2 90 0.3 230.291 56.087 61.5 tiongkok 2419 227.7 24.3 23.8 42.601 6 7 lainnya 54.3 6.5ssss 744.6 0.3 760.146 364.065 12933.3 total 305274.8 2253824.4 444508.8 356286.2 407741.42 429207.27 3062857.6 02000400060008000 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024perbandingan harga rata rata gkp di tingkat petani dan harga pembelian pemerintah tingkat petani kelompok kualitas gkp gabah kering panen tingkat petani hpp harga pembelian pemerintah gkp gkp gabah kering panen64 gambar 3. 4 jumlah impor beras bps 2024 w3 manajemen persediaan yang tidak efisien terdapat ketidakmampuan dalam memprediksi permintaan dan persediaan secara akurat dan realtime yang menyebabkan kelebihan dan kekurangan persediaan anggraini et al. 2021 utomo 2020 . w4 keterbatasan teknologi dan transparansi sistem yang ada saat ini tidak memiliki kapasitas untuk mencatat transaksi secara transparan yang mengakibatkan meningkatnya risiko kebocoran dan penipuan di sepanjang rantai pasok anggraini et al. 2021 utomo 2020 . w5 koordinasi antar lembaga koordinasi antara perum bulog dan lembaga pemerintah lainnya seperti kementerian pertanian dan kementerian perdagangan terkadang menghadapi tantangan karena adanya perbedaan data sehingga berdampak pada kelancaran pengambilan keputusan kebijakan impor siahaan 2023 . 3. opportunities peluang o1 memanfaatkan teknologi canggih integrasi teknologi canggih seperti blockchain dan ai berpotensi meningkatkan presisi transparansi dan efisiensi dalam mengelola persediaan cbp anggraini et al. 2021 putro et al. 2022 utomo 2020 . 0500000100000015000002000000250000030000003500000 2017 2018 2019 2020 2021 2022 2023berat bersih tonimpor beras tahun 2017 2023 impor beras65 o2 pengembangan sistem yang terintegrasi terdapat peluang untuk mengembangkan sistem yang lebih terintegrasi dan canggih yang dapat memfasilitasi pengelolaan cbp yang lebih baik anggraini et al. 2021 putro et al. 2022 utomo 2020 . 4. threats ancaman t1 ketidakstabilan harga dan persediaan global ketidakstabilan harga dan persediaan beras di pasar global dapat mempengaruhi kemampuan perum bulog untuk mengimpor beras dalam jumlah yang cukup dan dengan harga yang stabil octania 2021 . t2 dampak perubahan iklim perubahan iklim yang ekstrim dapat mengganggu produksi beras dalam negeri sehingga meningkatkan risiko ketidakcukupan cbp dan fluktuasi harga beras di tingkat konsumen octania 2021 saud wang fahad alharby bamagoos mjrashi alabdallah alzahrani abdelgawad adnan 2022 . 2. strategi setelah dilakukan analisis swot maka dapat dirumuskan strategi yang dapat digunakan perum bulog untuk meningkatkan kekuatan mengatasi kelemahan memanfaatkan peluang dan meminimalkan potensi ancaman . berikut adalah rincian strategi strategi tersebut . a. strategi s o leveraging strengths to optimize opportunities so1 pemanfaatan teknologi blockchain untuk transparansi dan efisiensi memanfaatkan dukungan pemerintah untuk mengadopsi teknologi blockchain dalam manajemen pangan yang dapat meningkatkan transparansi efisiensi dan keamanan dalam transaksi dan pencatatan. teknologi ini membantu dalam pencatatan mengotomatisasi kontrak memverifikasi transaksi dan melacak keaslian produk dari produsen ke konsumen s1 o1. 66 so2 pengembangan sistem terintegrasi meningkatkan infrastruktur yang sudah ada dengan mengembangkan sistem manajemen data yang terintegrasi dan canggih untuk memperkuat pengelolaan cbp secara lebih efektif s2 o2. so3 pengembangan model prediksi permintaan dan produksi hasil panen memanfaatkan ai untuk mengembangkan model prediksi permintaan dan produksi hasil panen beras yang akurat sehingga memungkinkan pengambilan keputusan pengadaan yang lebih tepat untuk menghindari kelebihan atau kekurangan persediaan s3 o1 o2. so4 optimalisasi program peningkatan produksi pangan memanfaatkan tenaga ahli dan infrastruktur yang ada untuk mendukung program pemerintah dalam meningkatkan produksi pangan lokal mengurangi ketergantungan impor serta memperkuat stabilitas harga dan persediaan beras s1 s3 o3. b. strategi s t using strengths to counter threats st1 pengembangan alat pendukung keputusan untuk pengadaan impor mengembangkan decision support tool dst dengan menggunakan input dari model prediksi untuk membantu menentukan kebijakan impo r beras. ds t akan membantu mengidentifikasi jumlah impor yang optimal berdasarkan analisis kebutuhan dan persediaan beras dalam negeri sehingga mengurangi risiko ketidakstabilan persediaan dan harga beras s2 t1. st2 optimalisasi manajemen krisis dengan prediksi produksi hasil panen menerapkan model prediksi hasil panen untuk mempersiapkan dan merespons secara efektif dampak perubahan iklim pada produksi beras . model prediksi ini dimanfaatkan untuk meningkatkan ketahanan pangan dan kesiapan dalam menghadapi fluktuasi hasil panen yang tidak terduga s3 t2. 67 c. strategi wo minimizing weaknesses by seizing opportunities wo1 mengadopsi teknologi blockchain untuk meningkatkan kepercayaan dan efisiensi mengadopsi teknologi blockchain untuk mengatasi keterbatasan teknologi saat ini seperti sistem yang kurang transparan . blockchain akan meningkatkan kepercayaan dan kredibilitas dalam operasi perum bulog memfasilitasi transaksi yang lebih aman dan audit yang dapat diverifikasi w1 w4 o1. wo2 peningkatan koordinasi antar lembaga melalui sistem terintegrasi membangun sistem terintegrasi yang melibatkan semua lembaga terkait untuk mengatasi masalah kurangnya koordinasi dan perbedaan data serta memfasilitasi pengambilan keputusan yang lebih cepat dan akurat w5 o2. d. strategi wt minimizing weaknesses and avoiding threats wt1 mengoptimalkan kebijakan impor dengan model prediksi menggunakan model prediksi untuk mengurangi ketergantungan pada impor dengan mengidentifikasi jumlah produksi hasil panen dalam negeri yang dapat memenuhi permintaan sekaligus mengetahui perlu atau tidaknya dilakukan impor w2 w3 t1. wt2 peningkatan manajemen persediaan melalui analisis tingkat lanjut meningkatkan sistem manajemen persediaan dengan model prediksi agar lebih responsif terhadap perubahan permintaan dan kondisi darurat serta mengurangi risiko kekurangan persediaan dan mengatasi fluktuasi harga w3 t2. setelah melakukan analisis swot da n merumuskan strategi maka hasilnya akan dirangkum dalam bentuk matriks swot yang terlampir pada gambar 3.5. 68 gambar 3. 5 matriks swot berdasarkan analisis swot yang dilakukan terhadap perum bulog telah diidentifikasi beberapa strategi penting yang menjadi fokus penelitian ini . strategi tersebut meliputi pengembangan model prediksi permintaan dan produksi optimasi manajemen krisis dengan prediksi produksi pembuatan alat pendukung keputusan untuk pengadaan optimasi kebijakan impor dengan model prediksi dan peningkatan manajemen persediaan melalui analisis lanjutan. strategi strategi ini bertujuan untuk meningkatkan kinerja perum bulog dalam manajemen cadangan beras pemerintah cbp dan dapat memberikan rekomendasi berbasis data untuk mendukung kebijakan pengadaan cadangan beras . 3.3.1 regulasi terkait manajemen cadangan beras pemerintah pemerintah telah menetapkan regulasi dalam manajemen cadangan beras untuk menjamin ketahanan pangan dan menangani potensi keadaan darurat. 69 regulasi regulasi ini bertujuan untuk menjamin ketersediaan beras selama keadaan darurat dan memanfaatkan cadangan beras yang di kelola pemerintah . berikut adalah regulasi regulasi utama yang berkaitan dengan cadangan beras pemerinta h. 1. peraturan presiden nomor 125 tahun 2022 tentang penyelenggaraan cadangan pangan pemerintah pasal 2 ayat 1 pemerintah menetapkan jenis dan jumlah cadangan pangan pemerintah cpp untuk menjamin ketersediaan pangan di seluruh wilayah indonesia. pasal 4 ayat 1 penugasan kepada badan usaha milik negara untuk mengelola cpp termasuk perum bulog danatau bumn pangan. pasal 8 ayat 1 pendanaan untuk penyelenggaraan cpp bersumber dari apbn danatau sumber pendanaan lain yang sah dan tidak mengikat . 2. peraturan badan pangan nasional nomor 12 tahun 2022 tentang penyelenggaraan cadangan beras pemerintah pasal 2 penetapan jumlah cbp dilakukan dengan mempertimbangkan a. produksi beras danatau gabah secara nasional b. penanggulangan keadaan darurat dan kerawanan pangan c. pengendalian dan stabilisasi harga dan pasokan beras danatau gabah pada tingkat produsen dan konsumen d. pelaksanaan perjanjian internasional dan bantuan pangan kerja sama internasional e. angka kecukupan gizi yang dianjurkan pasal 3 ayat 1 penetapan jumlah cbp sebagaimana dimaksud dalam pasal 2 ditetapkan oleh kepala badan . pasal 3 ayat 4 penetapan jumlah cbp sebagaimana dimaksud pada ayat 1 dilakukan paling sedikit satu kali dalam 1 satu tahaun . pasal 4 ayat 3 target pengadaan cbp terdiri atas volume pengadaa dalam negeri danatau pengadaan luar negeri. pasal 5 ayat 1 penyelenggaraan cbp dilakukan melalui pengadaan pengelolaan dan penyaluran . 70 pasal 5 ayat 2 penyelenggaraan cbp sebagaimana dimaksud pada ayat 1 dilaksanakan oleh badan pangan nasional melalui penugasan kepada perum bulog . pasal 10 ayat 1 dalam hal pengadaan cbp melalui produksi dalam negeri tidak emncukupi untuk pemenuhan cadangan menjaga stabilitas harga dalam negeri danatau memenuhi kebutuhan pemerintah dapat dilakukan pengadaan cbp dari luar negeri dengan tetap menjaga kepentingan produsen dan konsumen dalam negeri. 3. peraturan menteri perdagangan nomor 127 tahun 2018 tentang pengelolaan cadangan beras pemerintah pasal 2 ayat 1 pengelolaan cadangan beras pemerintah untuk menjamin ketersediaan pasokan dan stabilisasi harga. pasal 5 ayat 1 penugasan kepada perum bulog untuk melakukan pengadaan beras dari produksi dalam negeri berdasarkan harga pembelian pemerintah hpp. 4. peraturan menteri koordinator bidang perekonomian republik indonesia nomor 5 tahun 2018 tentang koordinasi pengelolaan cadangan beras pemerintah untuk stabilisasi harga pasal 1 ayat 3 rapat koordinasi adalah rapat yang dipimpin oleh menteri koordinator bidang perekonomian. pasal 2 ayat 2 rapat koordinasi sebagaimana dimaksud pada ayat 1 paling sedikit melibatkan menteri pertanian menteri perdagangan menteri keuangan menteri badan usaha milik negara dan direktur utama perum bulog. 5. keputusan kepala badan pangan nasional nomor 379.1ts.03.03k112023 menetapkan jumlah persediaan minimal berbagai komoditas pangan yang harus dimiliki pemerintah hingga akhir tahun 2024. untuk beras jumlah minimal yang harus dimiliki adalah 24 juta ton dengan persediaan akhir tahun minimal 12 juta ton. 71 3.3.2 parameter yang dipertimbangkan dalam menentukan jumlah cbp berdasarkan pasal 2 dari peraturan badan pangan nasional nomor 12 tahun 2022 terdapat beberapa parameter kunci yang dipertimbangkan dalam penetapan jumlah cbp . berikut adalah parameter parameter tersebut 1. produksi beras danatau gabah secara nasional merupakan total hasil produksi beras atau gabah di seluruh negeri. 2. penanggulangan keadaan darurat dan kerawanan pangan merupakan potensi dan realitas situasi darurat atau krisis pangan yang memerlukan penggunaan cadangan. 3. pengendalian dan stabilisasi harga dan pasokan beras danatau gabah merupakan kemampuan cadangan untuk mengontrol fluktuasi harga dan pasokan beras di pasar. 4. pelaksanaan perjanjian internasional dan bantuan pangan kerja sama internasional merupakan kewajiban dan komitmen internasional yang mempengaruhi jumlah beras yang perlu disimpan. 5. angka kecukupan gizi yang dianjurkan merupakan kebutuhan gizi yang harus dipenuhi melalui konsumsi beras oleh populasi. 3.4. pengembangan model prediksi produksi hasil panen beras gambar 3. 6 menggambarkan delapan tahapan yang dijalani dalam pengembangan model prediksi untuk produksi hasil panen beras pada penelitian ini. setiap tahap dirancang untuk memastikan keakuratan dan efektivitas model dalam memprediksi hasil panen . tahap pertama adalah pengumpulan data yang meliputi data historis produksi padi dan data explanatory variable . tahap kedua adalah explorasi data eda yang meliputi penanganan data duplikat penanganan nilai yang hilang missing value analisis statistik deskriptif analisis univariat dan analisis multivariat . 72 gambar 3. 6 tahapan penelitian prediksi permintaan yang diusulkan 73 tahap ketiga adalah preprocessing data prapemprosesan yang meliputi penanganan outlier dan skewness pengkodean encoding fitur kategorikal serta normalisasi normalize . tahap ke empat adalah feature engineering rekayasa fitur yang melibatkan kontruksi fitur dan seleksi fitur . tahap kelima adalah membagi data split data menjadi data pelatihan training dan data pengujian testing . tahap ke enam adalah augmentasi data. tahap ketujuh adalah pemodelan menggunakan algoritma xgboost yang dilatih menggunakan dataset pelatihan. tahap kedelapan adalah evaluasi hasil yang meliputi pengukuran performa model yang dapat dilihat dari nilai r2 root mean squared error rmse dan mean absolute error mae. jika performa model perlu ditingkatkan hyperparameter dapat disesuaikan kembali di tahap k etujuh . setelah performa terbaik tercapai model dapat diintegrasikan ke dalam model decision support system . 3.4.1 pengumpulan data pengembangan model prediksi produksi hasil panen ini akan menggunakan data yang dikumpulkan dari website badan pusat statistik bps dan bmkg . dataset ini berisi variabel provinsi tahun produksi luas panen curah hujan kelembapan dan suhu rata rata untuk periode 1993 2020. penjelasan mengenai masing masing variabel dapat dilihat di tabel 3 .2 dan sampel dari dataset ini dapat dilihat di lampiran 1. tabel 3. 2 deskripsi variabel variabel deskripsi tipe data provinsi informasi mengenai nama provinsi yang menghasilkan padiberas categorical tahun tahun produksi panen date produksi jumlah hasil produksi hasil panen dalam satu tahun numerical luas panen ukuran luas lahan yang dipanen di provinsi tersebut numerical curah hujan curah hujan yang diterima provinsi tersebut dalam satu tahun numerical kelembapan tingkat kelembapan udara rata rata di provinsi tersebut dalam satu tahun numerical suhu rata rata suhu udara rata rata di provinsi tersebut dalam satu tahun numerical dataset yang digunakan dalam penelitian ini mencakup berbagai informasi yang berkaitan dengan hasil panen dari berbagai provinsi . setiap baris mencakup beberapa variabel yang penting untuk analisis prediksi produksi hasil panen . 74 variabel provinsi menunjukkan nama provinsi di indonesia di mana produksi padi beras diukur sementara variabel tahun menunjukkan tahun pengumpulan data produksi padi beras. variabel produksi menunjukkan jumlah total produksi panen beras dalam ton yang diproduksi di provinsi tersebut pada tahun tertentu. variabel luas panen menunjukkan luas lahan pertanian di provinsi tersebut yang ditanami padi dan dipanen untuk menghasilkan beras dalam satuan hektar. variabel curah hujan menunjukkan rata rata curah hujan dalam milimeter yang diterima oleh provinsi tersebut pada tahun tersebut sedangkan variabel kelembaban menampilkan tingkat kelembaban udara rata rata di provinsi tersebut dalam persentase. terakhir variabel suhu rata rata menunjukkan suhu udara rata rata di provinsi tersebut dalam derajat celcius yang diukur selama satu tahun. 3.4.2 exploratory data analysis eda exploratory data analysis eda adalah tahapan penting dalam pemodelan predi ksi. hal ini melibatkan analisis data historis secara menyeluruh menggunakan statistik deskriptif visualisasi data univariat dan multivariat seperti plot untuk mengidentifikasi pola tren dan korelasi. eda memainkan peran penting dalam mengelola kualitas data. eda juga memandu seleksi dan rekayasa fitur yang sangat penting untuk mengembangkan model prediksi yang akurat santhoshkumar vanila 2024 . eda memungkinkan prediksi produksi hasil panen yang tepat dengan membangun fondasi yang kuat untuk pemodelan tingkat lanjut . penelitian ini menggunakan python untuk eda . a. deteksi data duplikat sangat penting untuk mendeteksi dan menghapus data duplikat untuk menjaga keakuratan analisis dan pengembangan model prediktif. data duplikat dapat menyebabkan hasil yang bias dan tidak dapat diandalkan koumarelas jiang naumann 2020 . pada penelitian ini tidak ditemukan adanya data duplikat dalam dataset yang digunakan. 75 b. deteksi nilai yang hilang missing value proses deteksi dan penanganan data hilang missing value dilakukan untuk memastikan keberlanjutan dalam data deret waktu time series . hal ini penting untuk prediksi produksi hasil panen karena memungkinkan model untuk menangkap pola dan trend dengan akurat atau dengan kata lain tanpa melakukan pengisian nilai nol ini analisis yang dilakukan kemungkinan besar akan menghasilkan hasil yang tidak akurat atau bias vallés pérez et al. 2022 . pada penelitian ini tidak dilakukan proses replace missing value karena dataset yang digunakan tidak memuat missing value seperti yang terlihat pada tabel 3. 3. tabel 3. 3 missing value variabel missing value provinsi 0 tahun 0 produksi 0 luas panen 0 curah hujan 0 kelembapan 0 suhu ratarata 0 c. analisis statistik deskriptif analisis statistik deskriptif dilakukan untuk mendapatkan wawasan awal mengenai properti statistik dari dataset . hal ini termasuk penghitungan rata rata median standar deviasi dan lainnya yang memberikan gambaran umum tentang bentuk dan distribusi data. tabel 3.4 adalah ringkasan analisis statistik deskriptif dataset hasil panen. tabel 3. 4 ringkasan statistik tahun produksi luas panen curah hujan kelembapan suhu ratarata count 224 224 224 224 224 224 mean 2006 5 1679701e06 374349 96692 2452 .490759 80948705 26801964 std 8095838 1161387e06 232751 161987 1031 972625 487868 1197041 min 1993 4293800e04 63142 04 2225 542 2219 25 1999 75 5488570e05 146919 5 1703 525 78975 261775 50 2006 5 1667773e06 373551 5 2315 7 82375 2673 75 2013 25 2436851e06 514570 25 3039 7 84 272 max 2020 4881089e06 872737 5522 906 2985 berdasarkan tabel 3.4 diketahui bahwa d istribusi data cenderung normal karena perbedaan antara nilai mean dan median tidak terlalu besar . produksi dan luas panen menunjukkan variasi yang besar dengan adanya outlier yang 76 ditunjukkan oleh perbedaan antara mean dan median yang cukup besar. curah hujan menunjukkan rentang nilai yang luas namun perbedaan antara mean dan median relatif kecil. kelembapan menunjukkan variasi yang cukup stabil dengan perbedaan antara mean dan median yang tidak besar . suhu rata rata menunjukkan data yang konsisten dengan perbedaan minimal antara mean dan median . d. analisis univariat analisis univariat dilakukan untuk setiap variabel dalam dataset menggunakan teknik box plots density plot violin plots dan count plot . hal ini membantu dalam memvisualisasikan distribusi mendeteksi outliers dan memahami variasi dalam data shabdin yaacob sjarif 2020 . gambar 3. 7 a box plots b density plot c violin plots dan d count plot box plot memberikan visualisasi distribusi berbasis waktu dari berbagai variabel seperti produksi area panen curah hujan kelembaban dan suhu ratarata. produksi dan luas panen menunjukkan tren yang meningkat dari waktu ke waktu sementara curah hujan kelembaban dan suhu rata rata 77 relatif stabil dengan outlier yang terlihat pada variabel curah hujan kelembaban dan suhu rata rata. density plot menggambarkan distribusi probabilitas dari setiap variabel. distribusi produksi dan luas panen menunjukkan pola miring ke kanan right skewed yang mengindikasikan adanya konsentrasi nilai di batas bawah kisaran dan beberapa nilai yang sangat tinggi. distribusi curah hujan dan suhu terlihat lebih simetris sementara distribusi kelembaban menunjukkan bukti bimodalitas dengan dua puncak yang berbeda. hal serupa juga terjadi pada violin plot yang memberikan wawasan tentang distribusi probabilitas data. variabel produksi dan luas panen menunjukkan distribusi yang condong ke kanan. sebaliknya variabel curah hujan dan suhu menunjukkan distribusi yang lebih simetris sedangkan variabel kelembaban menunjukkan pola bimodal. terakhir count plot menunjukkan jumlah pengamatan terlihat bahwa semua provinsi memiliki jumlah observasi yang sama. e. analisis multivariat analisis multivariat melibatkan visualisasi data multivariat menggunakan pair plot dan heatmap korelasi untuk mengeksplorasi hubungan antar variabel. visualisasi ini membantu mengidentifikasi pola dan hubungan yang mungkin tidak terlihat ketika menganalisis variabel secara terpisah ratilal reddy 2023 . gambar 3.8 menunjukkan pair plot dan gambar 3.9 menunjukkan heatmap korelasi. pair plot menggambarkan distribusi dan hubungan antara variabel dengan fokus pada korelasi yang kuat antara produksi dan luas panen yang mengindikasikan bahwa luas panen secara signifikan mempengaruhi tingkat produksi. hubungan ini diperkuat oleh heatmap korelasi yang menunjukkan koefisien korelasi sebesar 091 yang menandakan adanya korelasi yang kuat antara kedua variabel tersebut. 78 gambar 3. 8 pair plot gambar 3. 9 heatmap korelasi 79 3.4.3 preprocessing data tahap preprocessing dilakukan untuk mempersiapkan dataset untuk keperluan analisis machine learning . berdasarkan hasil eda yang telah dilakukan maka tahapan preprocessing yang dilakukan meliputi penanganan outlier dan skewness pengkodean encoding fitur kategorikal serta normalisasi normalize . tahap ini sangat penting untuk menghasilkan model prediksi berbasis machine learning yang akurat dan efektif. a. penanganan outliers dan skewness berdasarkan analisis box plots dan violin plots terlihat bahwa dataset yang digunakan mengandung banyak outlier sehingga perlu menerapkan metode untuk mengatasinya. di sisi lain analisis density plot menunjukkan bahwa terdapat data yang skew atau miring pada dataset yang dapat mempengaruhi kinerja model. untuk menangani keduanya transformasi log akan digunakan dalam penelitian ini choi et al. 2022 . b. pengkodean encoding fitur kategorikal untuk memproses variabel input kategorikal dengan cara yang dapat dipahami dan digunakan untuk analisis maka digunakan representasi label encoding . representasi ini mengubah variabel kategorikal menjadi representasi numerik berdasarkan penomoran kategori secara berurutan di mana setiap kategori diberi nilai numerik yang unik dahouda joe 2021 . pada penelitian ini label encoding diterapkan pada variabel provinsi. c. normalisasi normalize pada penelitian ini proses normalisasi data dilakukan dengan menggunakan metode interquartile range iqr atau robust normalization yang melibatkan penghitungan rentang interkuartil data dan menggunakannya untuk menormalkan data. iqr adalah rentang antara kuartil pertama q1 dan kuartil ketiga q3 yang mewakili 50 bagian tengah data. dengan menormalkan data metode iqr dapat membantu meningkatkan akurasi prediksi vaitheeshwari sathieshkumar 2019 . 80 3.4.4 features engineering feature engineering dilakukan untuk meningkatkan interpretabilitas model dengan membuat fitur fitur yang lebih mudah dimengerti. fitur fitur yang terstruktur dengan baik dapat membantu dalam menjelaskan alasan dibalik prediksi yang dibuat oleh model. proses feature engineering terdiri dari lima teknik utama yaitu feature improvement feature construction feature selection feature extraction dan feature learning ozdemir 2022 . pada penelitian ini dilakukan dua perlakuan dimana model dibangun dengan dan tanpa proses feature engineering. hal ini dilakukan untuk membuktikan hasil penelitian swaminathan and venkitasubramony 2023 yang menyatakan bahwa penggunaan feature engineering sanga t diperlukan untuk pemodelan prediksi untuk menghasilkan prediksi yang lebih akurat dan dapat diandalkan. selain itu penggunaan features engineering keterlibatan explanatory variabel dan data yang lebih banyak akan menjadi salah satu fokus utama pada penelitian in. penelitian ini melibatkan feature construction dan feature selection seperti yang terlihat pada gambar 3. 10. gambar 3. 10 flowchart feature engineering feature construction melibatkan proses pembuatan fitur baru dari data yang sudah ada. hal i ni melibatkan penggabungan data untuk membuat fitur yang lebih informatif dan relevan dengan masalah yang sedang dihadapi ozdemir 2022 . pada penelitian ini dibuat fitur efisiensi produksi dari produksi dan luas panen . seleksi fitur feature selection berperan penting dalam meningkatkan akurasi dalam model prediksi . teknik ini melibatkan identifikasi fitur yang paling berpengaruh terhadap hasil panen . pemilihan fitur yang tepat dapat memfasilitasi prediksi hasil panen padiberas masa depan. proses seleksi ini esensial untuk menyesuaikan model prediksi dengan dinamika hasil panen yang berubah ubah bedi 2023 . 81 3.4.5 pembagian data split data pada p roses pembuatan model machine learning data dapat dibagi menjadi dua set yaitu data pelatihan training dan data pengujian testing . mesin dilatih dengan data pelatihan dan diuji dengan data pengujian untuk mengevaluasi keandalan dan performanya dalam mencapai tujuan penelitian . untuk menentukan rasio pembagian data yang optimal penelitian ini menggunakan tiga rasio yang umum digunakan yaitu 7030 8020 dan 9010 untuk data pelatihan dan pengujian grigorev 2021 nguyen ly ho al ansari le tran prakash pham 2021 . gambar 3. 11 membagi data menjadi pelatihan dan pengujian grigorev 2021 3.4.6 augmentasi data augmentasi data data augmentation da adalah teknik yang digunakan dalam machine learning dan deep learning untuk meningkatkan ukuran dataset pelatihan dengan membuat versi modifikasi dari data yang ada. proses ini sangat penting untuk meningkatkan performa model terutama dalam skenario di mana jumlah data pelatihan yang tersedia terbatas bayer et al. 2022 moreno barea et al. 2020 onishi meguro 2023 . generative ai genai akan dimanfaatkan untuk menambah data training dalam penelitian ini khususnya dengan menggunakan generative adversarial networks gan. 3.4.7 pemodelan dengan gridsearchcv xgboost merupakan model machine learning canggih yang membangun ansambel pohon keputusan secara berurutan mengoptimalkan loss function dan menangani ketidakseimbangan data missing value serta overfitting . prediksi produksi beras menggunakan algoritma xgboost membutuhkan penyetingan 82 beberapa hyperparameter sebagai penunjang pemodelan . proses untuk memilih hyperparameter terbaik dalam model machine learning untuk meningkatkan kinerja dikenal sebagai penyetelan hyperparameter tuning soleymani mohammadzadeh 2023 . model prediksi pada penelitian ini menggunakan parameter n_estimator max_depth dan learning_rate yang diadopsi dari penelitian ørebæk and geitle 2021 . range parameter yang dipertimbangkan pada penelitian ini dapat dilihat di tabel 3. 5. tabel 3. 5 parameter yang dipilih dan nilainya ørebæk geitle 2021 hyperparameter v alue range learning _rate 011 n_estimator 100250 max_depth 1 14 uji coba satu persatu kombinasi yang telah diatur dan ditambah dengan gridsearchcv akan mampu menentukan kombinasi mana yang menghasilkan performa terbaiknya. pada konteks machine learning 10fold cross validation seringkali lebih disukai karena memberikan estimasi yang lebih dapat diandalkan untuk kinerja model terutama ketika ukuran dataset terbatas marcot hanea 2021 . untuk itu pada penelitian ini nilai cv disetting 10 yang mana setiap kombinasi model dan parameter divalidasi sebanyak 10 kali dan data akan dibagi menjadi 10 bagian sama besar secara acak yang mana 9 bagian akan digunakan untuk training dan 1 bagian untuk validasi . dari hasil uji coba secara acak didapatkan kombinasi terbaik yaitu n_estimator 115 learning_rate 028 serta max_depth 1. kombinasi ini yang dimasukan ke dalam algoritma xgboost seperti yang terlihat di gambar 3. 12. gambar 3. 12 flowchart pemodelan jika ingin meningkatkan performa model maka dapat dilakukan penyetelan ulang atau tuning model. proses ini melibatkan penyesuaian hyperparameter dan konfigurasi model untuk mencapai hasil yang lebih baik. dengan penyetelan ulang dapat mengeksplorasi kombinasi hyperparameter yang berbeda atau menggunakan teknik optimasi lanjut seperti gridserchcv 83 liyanage basnayake gamage prabhashi kasthuriarachchi abeywardhana 2023 . meskipun memerlukan upaya yang cukup besar penyetelan ulang dapat menghasilkan peningkatan signifikan dalam performa prediksi. untuk itu dalam konteks penelitian ini akan terus dilakukan juga evaluasi dan penelitian pada tahap pemodelan ak an terus dilakukan untuk memastikan bahwa model sesuai dengan tujuan penelitian. 3.4.8 evaluasi model setelah model dilatih model tersebut menjalani tahap evaluasi di mana tingkat kesalahan diuji untuk memastikan model berfungsi dengan baik. penelitian ini menggunakan rsquared r2 root mean squared error rmse dan mean absolute error mae untuk menguji tingkat kesalahan. r2 mengukur proporsi varians dalam variabel dan berkisar antara 0 1 dimana semakin mendekati 1 maka semakin layak suatu model untuk digunakan . rmse mengukur besarnya tingkat kesalahan prediksi dimana semakin rendah nilainya mendekati nol maka hasil prediksi akan semakin akurat. di sisi lain mae menghitung rata rata kesalahan kuadrat antara nilai aktual dan nilai yang diramalkan dengan nilai yang rendah atau mendekati nol menunjukkan kecocokan yang lebih baik antara hasil permintaan dan data aktual chicco et al. 2021 . gambar 3. 13 adalah skema evaluasi model pada penelitian ini. gambar 3. 13 flowchart evaluasi model pada tahap evaluasi model juga diuji dengan dua rancangan yang berbeda yaitu model tanpa features engineering serta pada model dengan features engineering. tabel 3. 6 adalah hasil evaluasi model pada penelitian ini. tabel 3. 6 hasil evaluasi model dataset kondisi tanpa feature engineering kondisi dengan feature engineering r2 rmse mae r2 rmse mae 7030 0773 0441 0176 0933 0209 0115 8020 0795 0418 0155 0956 0163 0099 9010 0976 0111 0079 0976 0109 0076 84 hasil evaluasi menunjukkan pengaruh positif dari penggunaan feature engineering fe terhadap kinerja model machine learning dalam skema pembagian dataset yang berbeda 7030 8020 dan 9010. khususnya penggunaan feature engineering membantu meningkatkan nilai r² yang menandakan peningkatan kemampuan model dalam menjelaskan variabilitas data yang diamati. selanjutnya nilai rmse dan mae menurun dengan implementasi fe mengindikasikan bahwa kesalahan pada model prediksi menjadi lebih kecil dan prediksi menjadi lebih akurat secara umum. analisis lebih lanjut pada perbedaan skema pembagian data menunjukkan bahwa model dengan proporsi data pelatihan yang lebih besar 9010 menunjukkan hasil yang paling stabil dan akurat. hal i ni menyoroti pentingnya fe dalam meningkatkan efektivitas model dan menunjukkan keuntungan dari alokasi yang lebih besar pada data pelatihan dalam pengembangan model prediksi berbasis machine learning . nilai rmse terendah yang diperoleh adalah 0109 yang me nunjukkan potensi model untuk menghasilkan prediksi produksi yang sangat akurat jika terus dikembangkan. nosratabadi et al. 2021 mendapatkan nilai rmse 33.575.59574 untuk model prediksi nya hal ini menunjukkan bahwa model prediksi yang diusulkan memiliki potensi untuk menyaingi model yang ada dalam hal keakuratan hasil prediksi . meskipun hal ini tidak mutlak karena model ini dibangun dengan algoritma dan data yang berbeda. namun potensi model ini menghasilkan prediksi yang sangat akurat masih terlihat. berdasarkan hasil ini penelitian ini akan fokus pada penambahan data pelatihan menggunakan generative adversarial network gan dan pengembangan lebih lanjut pada teknik feature engineering untuk meningkatkan kinerja prediksi. 3.5 pengembangan model prediksi permintaan model prediksi yang digunakan untuk prediksi permintaan adalah model yang sama dengan yang digunakan untuk memprediksi produksi atau hasil panen . namun beberapa penyesuaian dilakukan untuk memastikan bahwa model sesuai dengan karakteristik data permintaan beras terutama pada tahapan feature s engineering augmentasi data dan pemodelan dengan gridsearchcv . gambar 85 3.14 adalah tahapan pengembangan model prediksi permintaan pada penelitian ini. gambar 3. 14 tahapan penelitian prediksi permintaan yang diusulkan 86 pengembangan model prediksi permintaan beras ini terdiri dari delapan tahap. tahap pertama adalah pengumpulan data yang meliputi data historis konsumsi beras bashir yuliana 2019 dan data explanatory variable seperti harga beras iqbal 2019 sossou igue 2019 pendapatan per kapita bashir yuliana 2019 yusuf et al. 2020 populasi bashir yuliana 2019 rasio kemiskinan qian ito zhao 2020 dan peristiwa khusus seperti bulan ramadhan dan hari raya hasanah 2020 . tahap kedua adalah explorasi data eda yang meliputi penanganan data duplikat penanganan nilai yang hilang missing value analisis statistik deskriptif analisis univariat dan analisis multivariat. tahap ketiga adalah preprocessing data prapemprosesan yang meliputi penanganan outlier dan skewness pengkodean encoding fitur kategorikal dan normalisasi normalize . tahap ke empat adalah feature engineering rekayasa fitur yang melibatkan konstruksi fitur dan seleksi fitur . tahap kelima adalah membagi data split data menjadi data pelatihan training dan data pengujian testing . tahap ke enam adalah augmentasi data. tahap ketujuh adalah pemodelan menggunakan algoritma xgboost yang dilatih menggunakan dataset pelatihan. tahap kedelapan adalah evaluasi hasil yang meliputi pengukuran performa model yang dapat dilihat dari nilai r2 root mean squared error rmse dan mean absolute error mae. jika performa model perlu ditingkatkan hyperparameter dapat disesuaikan kembali di tahap k etujuh . setelah performa terbaik tercapai model dapat diintegrasikan ke dalam model decision support system . 3.6 prototype decision support system dss untuk pengadaan cadangan beras pemerintah decision support system dss pada penelitian ini akan digunakan untuk mendukung proses pengambilan keputusan internal terkait kebijakan pengadaan cadangan beras pemerintah . dss ini akan mengintegrasikan data hasil prediksi produksi hasil panen permintaan dan persediaan aktual untuk menentukan variabel keputusan pengadaan yang optimal. alat ini memungkinkan perum 87 bulog untuk merencanakan dan mengelola cadangan beras pemerintah cbp secara efisien sesuai dengan kebutuhan masyarakat . gambar 3.1 5 adalah ilustrasi dari proses pemenuhan cbp. gambar 3. 15 ilustrasi pemenuhan cbp gambar 3.1 5 menggambarkan alur proses pemenuhan cbp yang dilakukan oleh perum bulog dan lembaga terkait . proses ini dimulai dengan perum bulog memeriksa tingkat persediaan cbp. jika ditemukan adanya kekurangan persediaan maka langkah selanjutnya adalah menyusun strategi untuk mengatasi kekurangan tersebut. terdapat beberapa opsi untuk mengatasi kekurangan tersebut meliputi pengadaan dalam negeri kombinasi antara pengadaan dalam negeri dan impor atau impor penuh jika pengadaan dalam negeri tidak mencukupi. gambar 3.1 6 menguraikan proses pengambilan keputusan untuk mengelola persediaan cbp. proses ini dimulai dengan penerimaan data tentang prediksi produksi hasil panen dalam negeri dan kebutuhan beras mendatang. selanjutnya evaluasi kondisi pasar beras internasional kebijakan pemerintah dan perubahan iklim global . lalu dilakukan perhitungan terhadap jumlah cadangan beras pemerintah cbp yang optimal yang menentukan jumlah beras yang harus disimpan untuk kebutuhan darurat. untuk mencapai optimasi ini penelitian ini akan menggunakan metode deep reinforcement learning drl. berbeda dengan metode heuristik drl lebih kuat dengan hasil konvergensi yang stabil dan lebih cocok untuk masalah pengambilan keputusan z. zhang zhang qiu 2019 . tahap berikutnya adalah memeriksa jumlah persediaan aktual . 88 gambar 3. 16 diagram alir skenario dasar pengambilan keputusan 89 berdasarkan data persediaan aktual dilakukan evaluasi apakah jumlah tersebut sudah mencukupi untuk memenuhi prediksi permintaan atau kebutuhan beras mendatang . jika persediaan cukup maka tidak diperlukan tindakan pengadaan tambahan . sebaliknya jika persediaan dinilai belum mencukupi kebutuhan maka akan dilakukan penyerapan atau pengadaan dari produksi hasil panen dalam negeri . pada kondisi di mana produksi hasil panen dalam negeri yang ada pada petani atau mitra kerja perum bulog sangat minim maka akan dilakukan pengadaan dari sumber dalam negeri dan impor untuk mengisi kekurangan tersebut. jika kondisi di mana produksi hasil panen dalam negeri kosong. penentuan jumlah pengadaan didasarkan pada penghitungan jumlah cbp yang diperlukan untuk memenuhi kekurangan antara kebutuhan dan persediaan yang tersedia dengan turut mempertimbangkan ketersediaan persediaan global kebijakan perdagangan dan kejadian luar biasa seperti bencana alam atau gangguan politik yang bisa mempengaruhi pengadaan. output dari dss ini adalah sebuah rekomendasi untuk strategi dan jumlah pengadaan beras. rekomendasi ini dapat digunakan oleh perum bulog untuk membuat kebijakan pengadaan beras khususnya dalam menentukan kebutuhan akan impor beras. keseluruhan proses ini penting untuk memastikan ketersediaan cbp dan menghindari potensi krisis pangan. dss yang diusulkan adalah sebuah platform yang mengintegrasikan dua model prediksi dan proses optimasi untuk cadangan persediaan beras pemerintah . sistem ini membantu perum bulog dan lembaga terkait dalam me ngambil kebij akan strategis terkait pengadaan cadangan beras pemerintah . hal ini untuk memastikan ketersediaan cadangan beras pemerintah cbp sehingga mendukung stabilitas harga dan ketahanan pangan n asional. gambar 3. 17 adalah proto type dss yang diusulkan . 90 gambar 3. 17 prototype dss yang diusulkan 3.7 uji coba uji coba sistem bertujuan untuk memvalidasi apakah sistem yang dikembangkan sesuai dengan tujuan awal dan layak untuk digunakan. pada tahap ini prototype sistem akan menjalani evaluasi secara menyeluruh untuk memastikan bahwa fungsionalitasnya telah memenuhi standar yang diharapkan dan untuk mengidentifikasi potensi error sutiah supriyono 2021 . tahap uji coba pada penelitian ini akan menggunakan metode black box testing yang dapat menemukan error secara cepat dan efisien di berbagai kategori . hal ini termasuk fungsi yang salah atau hilang kesalahan interface masalah dengan struktur data atau akses database eksternal kesalahan kinerja serta kesalahan yang terkait dengan operasi dan shutdown sistem corso moss koren lee kochenderfer 2021 .,3.1 tahapan penelitian tahapan penelitian merupakan gambaran dari langkah langkah atau proses yang akan dilakukan dalam suatu penelitian. tahap an kedua adalah membuat model manajemen persediaan beras perum bulog berdasarkan hasil wawancara awal dengan pihak terkait. tahap ketiga adalah melakukan analisis terhadap model manajemen persediaan beras perum bulog untuk mengidentifikasi area yang perlu ditingkatkan . hasil analisis ini akan digunakan untuk merumuskan solusi terhadap permasalahan yang ada . tahap keempat adalah pengembangan solusi berbasis teknologi yang terdiri dari pengembangan be rbagai model dan prototype sistem yang akan diuji . usulan yang pertama adalah model generative ai untuk menghasilkan data sintetis yang realistis yang dapat digunakan sebagai data pelatihan untuk model prediksi . model ini kemudian diintegrasikan ke dalam model ml prediksi produksi hasil panen . usulan yang kedua adalah model prediksi permintaan beras yang merupakan model yang mirip dengan model prediksi produksi hasil panen beras dengan beberapa penyesuaian agar sesuai dengan karakteristik data untuk prediksi permintaan beras. usulan yang ketiga adalah pengembangan prototype decision support system yang mengintegrasikan model prediksi dan optimasi untuk mendukung kebijakan terkait pengadaan cadangan beras . tahap kelima adalah uji coba terhadap prototype decision support system . 1 tahapan penelitian 3.2 pemodelan manajemen persediaan beras perum bulog manajemen persediaan cadangan beras nasional telah menjadi perhatian penting dalam beberapa tahun terakhir karena meningkatnya permintaan pangan global perubahan iklim dan ketidakstabilan ekonomi. cadangan ini merupakan stok strategis yang diawasi oleh pemerintah untuk menstabilkan persediaan dan harga beras memberikan bantuan saat terjadi kekurangan pangan dan mendukung tujuan ketahanan pangan nasional . manajemen persed iaan cadangan beras yang efektif sangat penting untuk memitigasi risiko yang terkait dengan gangguan persediaan dan fluktuasi harga beras yulianis rachman 2021 yang pada akhirnya akan menjamin ketahanan pangan dan stabilitas ekonomi octania 2021 usdianto setiyowati 2023 . 60 para pemangku kepentingan yang terlibat dalam manajemen cadangan beras ini termasuk kementerian pertanian kementerian perdagangan kem enteri an keuangan dan kem enteri badan usaha milik negara sebagai regulator serta perum bulog yang bertanggung jawab untuk mengelola persediaan beras pemerintah dan stabilisasi harga di tingkat produsen dan konsumen octania 2021 . perum bulog bertanggung jawab untuk menjaga stabilitas harga dengan membeli gabah dan beras dari petani dengan harga yang ditentukan pemerintah ketika harga beli gabah turun sehingga melindungi petani dari kerugian dan menjual beras dengan harga yang lebih rendah daripada harga pasar ketika terjadi kenaikan harga beras untuk memastikan keterjangkauan harga beras bagi masyarakat octania 2021 . lembaga ini bertanggung jawab atas manajemen salah satu komponen cadangan beras nasional yaitu cadangan beras pemerintah cbp termasuk pada pengadaan dalam negeri dan impor penyimpanan dan penyaluran beras untuk kebutuhan stabilisasi harga bantuan pangan dan keadaan darurat fang chen zhang pei gao wang 2020 octania 2021 . beberapa penelitian telah menekankan peran penting perum bulog dalam manajemen persediaan cadangan beras di indonesia . melalui manajemen cbp perum bulog memainkan peran penting dalam menjaga ketahanan pangan nasional terutama saat terjadi fluktuasi harga atau gangguan p ersediaan . keberadaan cbp yang dikelola perum bulog tidak hanya menstabilkan harga beras di pasar tetapi juga menjamin ketersediaan beras bagi masyarakat sehingga berkontribusi terhadap stabilitas ekonomi nasional octania 2021 putro purwaningsih sensuse suryono 2022 silalahi et al. mengingat peran penting ini penerapan teknologi ai dapat membantu perum bulog dalam mengoptimalkan berbagai aspek dalam manajemen cadangan beras pemerintah seperti prediksi permintaan dan produksi hasil panen optimasi cadangan beras dan pengambilan keputusan yang lebih baik. ai dapat digunakan untuk menganalisis data historis dan realtime guna menghasilkan prediksi yang akurat mengenai permintaan dan produksi hasil panen beras mehmood et al. 2021 sehingga memungkinkan 61 perum bulog untuk mengoptimalkan cadangan beras menghindari kelebihan atau kekurangan cadangan beras dan mengambil keputusan yang lebih baik dalam manajemen cadangan beras pemerintah h. qin 2023 . berdasarkan kajian model manajemen persediaan beras perum bulog maka penelitian ini berfokus pada pemanfaatan teknologi ai untuk efektivitas manajemen cadangan beras pemerintah terutama pada proses pengadaan . gambar 3.1 menggambarkan model manajemen persediaan beras perum bulog . 2 model manajemen persediaan perum bulog 3.3. analisis analisis ini bertujuan untuk mengungkap kelemahan dan proses yang kompleks dalam manajemen persediaan cadangan beras pemerintah di perum bulog . penelitian ini menggunakan metode analisis swot untuk mengidentifikasi titik titik lemah yang krusial dalam manajemen persediaan cadangan beras pemerintah dan mengembangkan strategi untuk meningkatkan efisiensi dan efektivitas pengelolaan persediaan cadangan beras di indonesia analisis swot bertujuan untuk mengetahui kekuatan kelemahan peluang dan ancaman bisnis. 62 1. analisis swot 1. strengths kekuatan s1 dukungan pemerintah perum bulog didukung oleh berbagai kebijakan pemerintah yang bertujuan menjaga stabilitas harga dan ketersediaan beras seperti yang diatur dalam uu no. s2 infrastruktur logistik yang memadai perum bulog memiliki infrastruktur logistik yang cukup baik termasuk gudang penyimpanan yang tersebar di berbagai daerah yang berperan penting dalam menjaga kecukupan persediaan cadangan beras octania 2021 utomo 2020 s3 pengalaman dan keahlian perum bulog memiliki pengalaman puluhan tahun dalam manajemen persediaan beras mulai dari pengadaan hingga distribusi yang menjadi keunggulan dalam menjaga stabilitas harga dan persediaan beras anggraini et al. gambar 3.3 menggambarkan perbedaan yang signifikan antara harga gabah di tingkat petani dengan harga beli yang ditetapkan pemerintah. kon disi ini menyebabkan petani lebih memilih menjual hasil panennya ke sektor swasta. 1 jumlah impor beras bps 2024 negara asal 2017 2018 2019 2020 2021 2022 2023 berat bersih ton india 32209.7 337999 7973.3 10594.4 215386.46 178533.57 69715.7 thailand 108944.8 795600.1 53278 88593.1 69360.037 80182.506 1381921.2 vietnam 16599.9 767180.9 33133.1 88716.4 65692.874 81828.039 1147705.3 pakistan 87500 310990 182564.9 110516.5 52479.011 84407 309309.7 myanmar 57475 41820 166700.6 57841.4 3790 3830 141204 jepang 72.1 0.2 90 0.3 230.291 56.087 61.5 tiongkok 2419 227.7 24.3 23.8 42.601 6 7 lainnya 54.3 6.5ssss 744.6 0.3 760.146 364.065 12933.3 total 305274.8 2253824.4 444508.8 356286.2 407741.42 429207.27 3062857.6 02000400060008000 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024perbandingan harga rata rata gkp di tingkat petani dan harga pembelian pemerintah tingkat petani kelompok kualitas gkp gabah kering panen tingkat petani hpp harga pembelian pemerintah gkp gkp gabah kering panen64 gambar 3. 3. opportunities peluang o1 memanfaatkan teknologi canggih integrasi teknologi canggih seperti blockchain dan ai berpotensi meningkatkan presisi transparansi dan efisiensi dalam mengelola persediaan cbp anggraini et al. 0500000100000015000002000000250000030000003500000 2017 2018 2019 2020 2021 2022 2023berat bersih tonimpor beras tahun 2017 2023 impor beras65 o2 pengembangan sistem yang terintegrasi terdapat peluang untuk mengembangkan sistem yang lebih terintegrasi dan canggih yang dapat memfasilitasi pengelolaan cbp yang lebih baik anggraini et al. 2. strategi setelah dilakukan analisis swot maka dapat dirumuskan strategi yang dapat digunakan perum bulog untuk meningkatkan kekuatan mengatasi kelemahan memanfaatkan peluang dan meminimalkan potensi ancaman . a. strategi s o leveraging strengths to optimize opportunities so1 pemanfaatan teknologi blockchain untuk transparansi dan efisiensi memanfaatkan dukungan pemerintah untuk mengadopsi teknologi blockchain dalam manajemen pangan yang dapat meningkatkan transparansi efisiensi dan keamanan dalam transaksi dan pencatatan. 66 so2 pengembangan sistem terintegrasi meningkatkan infrastruktur yang sudah ada dengan mengembangkan sistem manajemen data yang terintegrasi dan canggih untuk memperkuat pengelolaan cbp secara lebih efektif s2 o2. so3 pengembangan model prediksi permintaan dan produksi hasil panen memanfaatkan ai untuk mengembangkan model prediksi permintaan dan produksi hasil panen beras yang akurat sehingga memungkinkan pengambilan keputusan pengadaan yang lebih tepat untuk menghindari kelebihan atau kekurangan persediaan s3 o1 o2. so4 optimalisasi program peningkatan produksi pangan memanfaatkan tenaga ahli dan infrastruktur yang ada untuk mendukung program pemerintah dalam meningkatkan produksi pangan lokal mengurangi ketergantungan impor serta memperkuat stabilitas harga dan persediaan beras s1 s3 o3. b. strategi s t using strengths to counter threats st1 pengembangan alat pendukung keputusan untuk pengadaan impor mengembangkan decision support tool dst dengan menggunakan input dari model prediksi untuk membantu menentukan kebijakan impo r beras. st2 optimalisasi manajemen krisis dengan prediksi produksi hasil panen menerapkan model prediksi hasil panen untuk mempersiapkan dan merespons secara efektif dampak perubahan iklim pada produksi beras . model prediksi ini dimanfaatkan untuk meningkatkan ketahanan pangan dan kesiapan dalam menghadapi fluktuasi hasil panen yang tidak terduga s3 t2. wo2 peningkatan koordinasi antar lembaga melalui sistem terintegrasi membangun sistem terintegrasi yang melibatkan semua lembaga terkait untuk mengatasi masalah kurangnya koordinasi dan perbedaan data serta memfasilitasi pengambilan keputusan yang lebih cepat dan akurat w5 o2. wt2 peningkatan manajemen persediaan melalui analisis tingkat lanjut meningkatkan sistem manajemen persediaan dengan model prediksi agar lebih responsif terhadap perubahan permintaan dan kondisi darurat serta mengurangi risiko kekurangan persediaan dan mengatasi fluktuasi harga w3 t2. strategi tersebut meliputi pengembangan model prediksi permintaan dan produksi optimasi manajemen krisis dengan prediksi produksi pembuatan alat pendukung keputusan untuk pengadaan optimasi kebijakan impor dengan model prediksi dan peningkatan manajemen persediaan melalui analisis lanjutan. strategi strategi ini bertujuan untuk meningkatkan kinerja perum bulog dalam manajemen cadangan beras pemerintah cbp dan dapat memberikan rekomendasi berbasis data untuk mendukung kebijakan pengadaan cadangan beras . 3.4. pengembangan model prediksi produksi hasil panen beras gambar 3. 6 menggambarkan delapan tahapan yang dijalani dalam pengembangan model prediksi untuk produksi hasil panen beras pada penelitian ini. setiap tahap dirancang untuk memastikan keakuratan dan efektivitas model dalam memprediksi hasil panen . 3.4.1 pengumpulan data pengembangan model prediksi produksi hasil panen ini akan menggunakan data yang dikumpulkan dari website badan pusat statistik bps dan bmkg . eda memungkinkan prediksi produksi hasil panen yang tepat dengan membangun fondasi yang kuat untuk pemodelan tingkat lanjut . tahap ini sangat penting untuk menghasilkan model prediksi berbasis machine learning yang akurat dan efektif. proses untuk memilih hyperparameter terbaik dalam model machine learning untuk meningkatkan kinerja dikenal sebagai penyetelan hyperparameter tuning soleymani mohammadzadeh 2023 . meskipun memerlukan upaya yang cukup besar penyetelan ulang dapat menghasilkan peningkatan signifikan dalam performa prediksi. rmse mengukur besarnya tingkat kesalahan prediksi dimana semakin rendah nilainya mendekati nol maka hasil prediksi akan semakin akurat. 13 flowchart evaluasi model pada tahap evaluasi model juga diuji dengan dua rancangan yang berbeda yaitu model tanpa features engineering serta pada model dengan features engineering. 6 adalah hasil evaluasi model pada penelitian ini. 6 hasil evaluasi model dataset kondisi tanpa feature engineering kondisi dengan feature engineering r2 rmse mae r2 rmse mae 7030 0773 0441 0176 0933 0209 0115 8020 0795 0418 0155 0956 0163 0099 9010 0976 0111 0079 0976 0109 0076 84 hasil evaluasi menunjukkan pengaruh positif dari penggunaan feature engineering fe terhadap kinerja model machine learning dalam skema pembagian dataset yang berbeda 7030 8020 dan 9010. khususnya penggunaan feature engineering membantu meningkatkan nilai r² yang menandakan peningkatan kemampuan model dalam menjelaskan variabilitas data yang diamati. analisis lebih lanjut pada perbedaan skema pembagian data menunjukkan bahwa model dengan proporsi data pelatihan yang lebih besar 9010 menunjukkan hasil yang paling stabil dan akurat. hal i ni menyoroti pentingnya fe dalam meningkatkan efektivitas model dan menunjukkan keuntungan dari alokasi yang lebih besar pada data pelatihan dalam pengembangan model prediksi berbasis machine learning . nilai rmse terendah yang diperoleh adalah 0109 yang me nunjukkan potensi model untuk menghasilkan prediksi produksi yang sangat akurat jika terus dikembangkan. 2021 mendapatkan nilai rmse 33.575.59574 untuk model prediksi nya hal ini menunjukkan bahwa model prediksi yang diusulkan memiliki potensi untuk menyaingi model yang ada dalam hal keakuratan hasil prediksi . meskipun hal ini tidak mutlak karena model ini dibangun dengan algoritma dan data yang berbeda. namun potensi model ini menghasilkan prediksi yang sangat akurat masih terlihat. berdasarkan hasil ini penelitian ini akan fokus pada penambahan data pelatihan menggunakan generative adversarial network gan dan pengembangan lebih lanjut pada teknik feature engineering untuk meningkatkan kinerja prediksi. 3.5 pengembangan model prediksi permintaan model prediksi yang digunakan untuk prediksi permintaan adalah model yang sama dengan yang digunakan untuk memprediksi produksi atau hasil panen . gambar 85 3.14 adalah tahapan pengembangan model prediksi permintaan pada penelitian ini. 14 tahapan penelitian prediksi permintaan yang diusulkan 86 pengembangan model prediksi permintaan beras ini terdiri dari delapan tahap. tahap kedelapan adalah evaluasi hasil yang meliputi pengukuran performa model yang dapat dilihat dari nilai r2 root mean squared error rmse dan mean absolute error mae. jika performa model perlu ditingkatkan hyperparameter dapat disesuaikan kembali di tahap k etujuh . setelah performa terbaik tercapai model dapat diintegrasikan ke dalam model decision support system . 3.6 prototype decision support system dss untuk pengadaan cadangan beras pemerintah decision support system dss pada penelitian ini akan digunakan untuk mendukung proses pengambilan keputusan internal terkait kebijakan pengadaan cadangan beras pemerintah . dss ini akan mengintegrasikan data hasil prediksi produksi hasil panen permintaan dan persediaan aktual untuk menentukan variabel keputusan pengadaan yang optimal. alat ini memungkinkan perum 87 bulog untuk merencanakan dan mengelola cadangan beras pemerintah cbp secara efisien sesuai dengan kebutuhan masyarakat . proses ini dimulai dengan perum bulog memeriksa tingkat persediaan cbp. proses ini dimulai dengan penerimaan data tentang prediksi produksi hasil panen dalam negeri dan kebutuhan beras mendatang. selanjutnya evaluasi kondisi pasar beras internasional kebijakan pemerintah dan perubahan iklim global . lalu dilakukan perhitungan terhadap jumlah cadangan beras pemerintah cbp yang optimal yang menentukan jumlah beras yang harus disimpan untuk kebutuhan darurat. berbeda dengan metode heuristik drl lebih kuat dengan hasil konvergensi yang stabil dan lebih cocok untuk masalah pengambilan keputusan z. zhang zhang qiu 2019 . 16 diagram alir skenario dasar pengambilan keputusan 89 berdasarkan data persediaan aktual dilakukan evaluasi apakah jumlah tersebut sudah mencukupi untuk memenuhi prediksi permintaan atau kebutuhan beras mendatang . sebaliknya jika persediaan dinilai belum mencukupi kebutuhan maka akan dilakukan penyerapan atau pengadaan dari produksi hasil panen dalam negeri . pada kondisi di mana produksi hasil panen dalam negeri yang ada pada petani atau mitra kerja perum bulog sangat minim maka akan dilakukan pengadaan dari sumber dalam negeri dan impor untuk mengisi kekurangan tersebut. jika kondisi di mana produksi hasil panen dalam negeri kosong. output dari dss ini adalah sebuah rekomendasi untuk strategi dan jumlah pengadaan beras. rekomendasi ini dapat digunakan oleh perum bulog untuk membuat kebijakan pengadaan beras khususnya dalam menentukan kebutuhan akan impor beras. dss yang diusulkan adalah sebuah platform yang mengintegrasikan dua model prediksi dan proses optimasi untuk cadangan persediaan beras pemerintah . sistem ini membantu perum bulog dan lembaga terkait dalam me ngambil kebij akan strategis terkait pengadaan cadangan beras pemerintah . hal ini untuk memastikan ketersediaan cadangan beras pemerintah cbp sehingga mendukung stabilitas harga dan ketahanan pangan n asional. 17 prototype dss yang diusulkan 3.7 uji coba uji coba sistem bertujuan untuk memvalidasi apakah sistem yang dikembangkan sesuai dengan tujuan awal dan layak untuk digunakan. hal ini termasuk fungsi yang salah atau hilang kesalahan interface masalah dengan struktur data atau akses database eksternal kesalahan kinerja serta kesalahan yang terkait dengan operasi dan shutdown sistem corso moss koren lee kochenderfer 2021 .
"Penelitian ini bertujuan untuk mengembangkan aplikasi berbasis Large Language Model (LLM) dengan arsitektur GPT-4 yang mampu melakukan telaah sejawat(peer review) secara otomatis pada artikel ilmiah dari jurnal komputer. Data utama yang digunakan adalah artikel ilmiah berbahasa Indonesia dalam bidang ilmu komputer dari berbagai jurnal akademik. Sebelum digunakan, data akan diperiksa untuk menghilangkan informasi pribadi yang dapat mengidentifikasi penulis atau reviewer. Aplikasi ini diharapkan dapat membantu para peneliti dan editor jurnal dalam menganalisis dan memperoleh wawasan dari artikel yang seringkali bersifat kompleks dan teknis. Untuk melakukan penelitian ini perlu dilakukan beberapa tahapan hingga penelitian selesai, tahapan yang dilaukan mulai dari pengumpulan data, preprocessing data, melakukan pemodelan untuk telaah sejawat, mengevaluasi model dan validasi ahli. Untuk tahapan penelitian dapat dilihat pada gambar 3.1. 3.1.1 Pengumpulan data
Proses pengumpulan data dilakukan dengan cara mengumpulkan artikel ilmiah dari berbagai sumber terbuka dengan topik artikel ilmu computer. Pengumpulan data menggunakan teknik webscraping, artikel yang telah dikumpulkan akan diproses melalui tahap preprocessing. 3.1.2 Preprocessing data
Proses preprocessing data merupakan langkah yang sangat penting dalam persiapan data untuk pemodelan LLM. Proses ini melibatkan beberapa tahap penting yang bertujuan untuk membersihkan dan menyiapkan data teks agar sesuai dengan kebutuhan model serta meningkatkan kualitas dan konsistensi representasi teks. Proses preprocessing dilakukan melalui beberapa tahap seperti tokenisasi, pembersihan teks ,normalisasi, token encoding, penghapusan stopword, stemming, segmentasi kalimat dan pemisahan dataset. Gambar 3. 2 Tahapan Preprocessing
Tahap pertama adalah tokenisasi, di mana teks dipecah menjadi unit-unit yang lebih kecil yang dikenal sebagai token, memungkinkan model untuk menganalisis teks pada tingkat yang lebih granular. Selanjutnya, dilakukan pembersihan teks untuk menghilangkan karakter atau simbol yang tidak diinginkan seperti tanda baca, angka, dan karakter khusus lainnya, serta penghapusan spasi berlebih dan karakter yang tidak relevan. Normalisasi juga dilakukan untuk mengubah teks menjadi bentuk standar, termasuk mengubah semua huruf menjadi huruf kecil, menghapus aksen dari huruf, dan menangani variasi penulisan yang berbeda untuk kata yang sama. Setelah itu, token yang dihasilkan dari tokenisasi perlu diubah menjadi representasi numerik melalui token encoding, menggunakan teknik embeddings dari model transformer. Penghapusan stopword, yaitu kata-kata umum yang sering muncul dalam teks tetapi tidak memiliki makna khusus yang penting untuk analisis, juga dilakukan untuk mengurangi dimensi data dan fokus pada kata- kata yang lebih bermakna. Proses selanjutnya adalah stemming dan lemmatisasi yang bertujuan untuk mengurangi kata-kata ke bentuk dasar atau akar katanya, dengan stemming memotong akhiran kata dan lemmatisasi menggunakan kamus bahasa untuk mengembalikan kata ke bentuk dasar yang benar secara gramatikal. Selanjutnya segmentasi kalimat dilakukan untuk memisahkan teks menjadi kalimat-kalimat individu yang bisa dianalisis lebih lanjut secara terpisah. Tahap terakhir dalam preprocessing adalah pemisahan dataset menjadi bagian-bagian yang berbeda, seperti data latih, data validasi, dan data uji, yang penting untuk mengevaluasi kinerja model secara adil dan menghindari overfitting. Melalui proses preprocessing yang cermat dan terstruktur, data teks menjadi lebih bersih, terorganisir, dan siap digunakan dalam pemodelan, sehingga tidak hanya meningkatkan efisiensi pemrosesan data tetapi juga memungkinkan model untuk belajar dan melakukan prediksi dengan lebih akurat. 3.1.3 Pembuatan Model LLM
Setelah dataset yang di kumpulkan dan melalui proses preprocessing maka dilanjutkan tahap pemodelan dengan menggunakan LLM. Pada tahap ini dilakukan pemodelan dengan arsitektur GPT-4 untuk platform tinjauan artikel ilmiah. Proses pemodelan dimulai dengan fine-tuning GPT-4 menggunakan dataset yang telah dipreproccesing sebelumnya. Fine-tuning dilakukan untuk menyesuaikan model dengan gaya penulisan dan terminologi spesifik yang digunakan dalam artikel ilmiah. Selama fase pelatihan, model dievaluasi secara berkala untuk memastikan kinerjanya sesuai dengan harapan, dan parameter model dioptimalkan untuk meningkatkan kualitas output. Penggunaan GPT-4 untuk platform tinjauan artikel ilmiah dapat menyediakan analisis yang mendalam dan komprehensif, membantu reviewer untuk lebih cepat dan efisien dalam menilai kualitas dan kontribusi sebuah artikel. Hal ini tidak hanya meningkatkan produktivitas tetapi juga memastikan bahwa artikel yang dipublikasikan memenuhi standar ilmiah yang tinggi. 3.1.4 Evaluasi Model LLM
Evaluasi model merupakan langkah yang penting dalam pengembangan sistem kecerdasan buatan, karena memungkinkan untuk menilai kinerja dan efektivitas model dalam menyelesaikan tugas tertentu. Proses evaluasi membantu mengidentifikasi kelemahan dan kekuatan model, serta memberikan wawasan tentang seberapa baik model dapat digunakan. Tanpa evaluasi yang tepat, model yang dikembangkan dapat menghasilkan prediksi yang tidak akurat atau tidak dapat diandalkan, yang berpotensi menyebabkan kinerja sistem yang buruk secara keseluruhan. Pada penelitian ini dilakukan evaluasi model dengan melihat nilai akurasi, presisi, recall dan F1-Score. 1. Akurasi memberikan gambaran umum tentang seberapa baik model klasifikasi melakukan prediksi secara keseluruhan. 2. Presisi memberikan informasi tentang seberapa banyak prediksi positif yang sebenarnya benar dari semua prediksi positif yang dilakukan oleh model. 3. Recall memberikan informasi tentang seberapa banyak instance positif yang berhasil diidentifikasi oleh model dari semua instance positif yang
4. sebenarnya dalam dataset. 5. F1-Score berguna ketika kelas target tidak seimbang dalam dataset, karena mencakup baik presisi maupun recall dalam perhitungannya. 3.1.5 Validasi Ahli
Proses validasi ahli ini memastikan bahwa model GPT-4 yang digunakan untuk telaah sejawat mampu memberikan evaluasi yang akurat, relevan, dan sesuai dengan standar akademik, dengan masukan berharga dari para ahli di bidangnya. 3.2 Jadwal Penelitian
Jadwal penelitian bertujuan untuk mengatasi target waktu penelitian, memastikan bahwa penelitian ini dapat diselesaikan sesuai dengan batas waktu yang telah ditetapkan. Adanya jadwal penelitian, diharapkan penelitian dapat berjalan secara efisien dan sesuai rencana, sehingga memberikan kepastian bahwa semua tahapan penelitian dapat diselesaikan tepat pada waktunya. Table jadwal penelitian dapat dilihat pada table 3.1","Penelitian ini bertujuan untuk mengembangkan aplikasi berbasis Large Language Model (LLM) dengan arsitektur GPT-4 yang mampu melakukan telaah sejawat(peer review) secara otomatis pada artikel ilmiah dari jurnal komputer. Data utama yang digunakan adalah artikel ilmiah berbahasa Indonesia dalam bidang ilmu komputer dari berbagai jurnal akademik. Sebelum digunakan, data akan diperiksa untuk menghilangkan informasi pribadi yang dapat mengidentifikasi penulis atau reviewer. Aplikasi ini diharapkan dapat membantu para peneliti dan editor jurnal dalam menganalisis dan memperoleh wawasan dari artikel yang seringkali bersifat kompleks dan teknis. Proses ini melibatkan beberapa tahap penting yang bertujuan untuk membersihkan dan menyiapkan data teks agar sesuai dengan kebutuhan model serta meningkatkan kualitas dan konsistensi representasi teks. 2 Tahapan Preprocessing
Tahap pertama adalah tokenisasi, di mana teks dipecah menjadi unit-unit yang lebih kecil yang dikenal sebagai token, memungkinkan model untuk menganalisis teks pada tingkat yang lebih granular. 3.1.4 Evaluasi Model LLM
Evaluasi model merupakan langkah yang penting dalam pengembangan sistem kecerdasan buatan, karena memungkinkan untuk menilai kinerja dan efektivitas model dalam menyelesaikan tugas tertentu. Tanpa evaluasi yang tepat, model yang dikembangkan dapat menghasilkan prediksi yang tidak akurat atau tidak dapat diandalkan, yang berpotensi menyebabkan kinerja sistem yang buruk secara keseluruhan. Recall memberikan informasi tentang seberapa banyak instance positif yang berhasil diidentifikasi oleh model dari semua instance positif yang
4. sebenarnya dalam dataset. 3.1.5 Validasi Ahli
Proses validasi ahli ini memastikan bahwa model GPT-4 yang digunakan untuk telaah sejawat mampu memberikan evaluasi yang akurat, relevan, dan sesuai dengan standar akademik, dengan masukan berharga dari para ahli di bidangnya. 3.2 Jadwal Penelitian
Jadwal penelitian bertujuan untuk mengatasi target waktu penelitian, memastikan bahwa penelitian ini dapat diselesaikan sesuai dengan batas waktu yang telah ditetapkan. Adanya jadwal penelitian, diharapkan penelitian dapat berjalan secara efisien dan sesuai rencana, sehingga memberikan kepastian bahwa semua tahapan penelitian dapat diselesaikan tepat pada waktunya. Table jadwal penelitian dapat dilihat pada table 3.1"
"Bab ini menyajikan desain yang digunakan dalam penelitian ini. Desain penelitian adalah rencana umum bagaimana penelitian akan dilakukan untuk menjawab pertanyaan dan pernyataan dalam penelitian. Hal ini menentukan sumber dari mana data akan dikumpulkan dan bagaimana mengumpulkan dan menganalisis data ini. Selanjutnya membahas masalah etika dan beberapa kendala yang dapat ditemui peneliti. Ini menunjukkan bahwa peneliti telah memikirkan elemen-elemen desain penelitian tertentu (Saunders, Lewis, & Thornhill, 2011). Pada bab ini akan dibahas mengenai filosofi keilmuan dari data governance, konsep teknolgi BLockchain dan penerapan data governance dalam teknologi blockchain di bidang pendidikan, yang akan memberikan pandangan utama saat melakukan penelitian. Selanjutnya akan dijelaskan pendekatan yang digunakan penelitian dalam pengumpulan data, menganalisis data yang digunakan serta etika lain yang akan dipatuhi terutama terkait kerahasiaan data yang digunakan. Jadi metodologi penelitian memberikan gambaran jelas mengenai strategi penelitian, pengambilan data, pengumpulan, pengolahan dan analisis dan serta keterbatasan penelitian. 3.1 Filosofi Keilmuan
Pengkajian ilmiah (penelitian) menurut aliran positivistik banyak dianut peneliti ilmu komputer merupakan upaya sistematis, investigatif, objektif, logis, hati-hati dan terencana dengan selalu berusaha mencari kebenaran. Penelitian dengan pendekatan positivistik adalah memiliki karakteristik: analitik, nomotetik, dedikatif, laboratorik, pembuktian dengan logika , kebenaran universal, dan bersifat bebas nilainya. (Jazi Eko Istiyanto, 2009). 3.2. Skema Penelitian
Untuk menyelesaikan penelitian dirancang kerangka pikir yang menggambarkan langkah-langkah yang harus ditempuh, dapat dilihat penjelasan dan urutannya sebagai berikut:

3.2.1 Mendefinisikan Tata Kelola Data untuk Organisasi
Upaya Tata Kelola Data harus mendukung strategi dan tujuan bisnis. Strategi dan sasaran bisnis organisasi menginformasikan strategi data perusahaan dan bagaimana tata kelola data dan aktivitas manajemen data perlu dioperasionalkan dalam organisasi. Tata kelola data memungkinkan tanggung jawab bersama untuk keputusan terkait data. Kegiatan tata kelola data melintasi batasbatas organisasi dan sistem untuk mendukung tampilan data yang terintegrasi. Tata kelola data membutuhkan pemahaman yang jelas tentang apa yang diatur dan siapa yang diatur, serta siapa yang mengatur
Uraian lebih detil tentang proses transformasi warna, ruang warna yang digunakan, algoritma trasnformasinya. Setiap univeritas merupakan node, dimana masing-masing node mengajukan beberapa kesepatan yang diturunkan dalam fungsional requirement yang nantinya akan dituangkan dalam consensus yang terdalam di dalam smartcard

3.2.2 Mengidentikasi fungsional Requirement
Berdasarkan kesepakatan fungsional requirement akan diusulkan smart contract
Penilaian yang menggambarkan keadaan saat ini dari kemampuan manajemen informasi organisasi, kematangan, dan efektivitas sangat penting untuk merencanakan program unit bisnis. Karena dapat digunakan untuk mengukur efektivitas program, penilaian juga berharga dalam mengelola dan mempertahankan program unit binis. Penilaian khas meliputi:
Kematangan pengelolaan data: Memahami apa yang dilakukan organisasi dengan data; mengukur kemampuan dan kapasitas manajemen datanya saat ini. Fokusnya adalah pada kesan yang dimiliki personel bisnis tentang seberapa baik perusahaan mengelola data dan menggunakan data untuk keuntungannya, serta pada kriteria objektif, seperti penggunaan alat, tingkat pelaporan, dll. Kesiapan kolaboratif: Penilaian ini mencirikan kemampuan organisasi untuk berkolaborasi dalam pengelolaan dan penggunaan data. Karena penatalayanan menurut definisi melintasi area fungsional, itu bersifat kolaboratif. Jika sebuah organisasi tidak tahu bagaimana berkolaborasi, budaya akan menjadi hambatan bagi penatalayanan. Jangan pernah berasumsi bahwa sebuah organisasi tahu bagaimana berkolaborasi. Ketika
dilakukan bersama dengan kapasitas perubahan, penilaian ini menawarkan wawasan tentang kapasitas budaya untuk melaksanakan Ditjen. Penyelarasan bisnis: Terkadang disertakan dengan kapasitas perubahan, penilaian keselarasan bisnis memeriksa seberapa baik organisasi menyelaraskan penggunaan data dengan strategi bisnis. Seringkali mengejutkan untuk mengetahui bagaimana aktivitas terkait data ad hoc dapat terjadi. 3.2.3 Membuat kerangka lapisan data logis
Dalam tahap ini setelah setiap node mnyepakati proses bisnis yang akan dipakai bersama dalam aplikasi blockchain, menetapkan skema pada lapisan data logis. Hasilnya ada bagaimana kerangka komunikasi dijelaskan dalam gambar 3.3. Pedoman Membuat kerangka logis:
1. Mekanismen Identitas Mangement, otoritas dan Autentifikasi
Identitas Mangement, otorisasi, dan mekanisme otentikasi sangat penting dalam sistem manajemen data karena hal tersebut terkait langsung dengan keamanan dan privasi sistem. Dalam konsep desain, entitas dalam jaringan Blockchain harus diidentifikasi secara unik menggunakan kunci publik (atau hash kunci publik) dalam pasangan kunci kriptografi asimetris; proses otentikasi dan otorisasi harus diterapkan dengan memanfaatkan teknik kriptografi kunci publik (misalnya, tanda tangan digital dan enkripsi). Dalam hal izin BC, lapisan kontrol akses tambahan dikonsolidasikan dengan menggunakan Otoritas Sertifikat (CA) dan Penyedia Layanan Keanggotaan (MSP). 2. Desain Buku Besar Terdistribusi:
Konten terdistribusi buku besar mencerminkan keadaan historis dan informasi terkini yang dicatat dalam buku besar yang dikelola oleh jaringan blockchain. Platform manajemen data pribadi harus mengklarifikasi informasi apa dan model data terkait yang akan disimpan dalam buku besar. (i) Informasi yang diperlukan agar tahan terhadap kerusakan, transparan dan dapat dilacak harus dicatat dalam buku besar yang didistribusikan. Setiap kumpulan data pribadi harus ditentukan oleh data subjek dan data controller menggunakan tanda tangan digital dalam buku besar yang didistribusikan;
Kebijakan Penggunaan Data harus ditetapkan dengan jelas dan dicatat dalam buku besar yang didistribusikan;
Aktivitas data harus dicatat dalam buku besar yang didistribusikan. Log harus berisi informasi tentang 'siapa', 'mengapa', 'kapan', 'apa' dan 'bagaimana' data pribadi diproses;
Hash data pribadi dapat dicatat dalam buku besar terdistribusi untuk pemeriksaan integritas data. (ii) Desain buku besar yang didistribusikan harus memastikan:
Node yang ditunjuk dalam jaringan blockchain dapat memverifikasi apakah suatu entitas adalah data subjeck atau data controller dari kumpulan data;
Node yang ditunjuk dalam jaringan blockchain harus dapat memverifikasi apakah aktivitas entitas memenuhi kebijakan penggunaan data seperti yang dicatat dalam buku besar terdistribusi
3. Kebijakan Penggunaan Data: Kebijakan tersebut menentukan tindakan tata kelola data termasuk hak, izin, dan kondisi. Kebijakan penggunaan harus didefinisikan secara halus dan ekspresif menggunakan bahasa kebijakan seperti eXtensible Access Control Markup Language (XACML) dan Model-based Security Toolkit (SecKit) yang ditujukan untuk domain IoT. Secara alami, manajemen data pribadi berbasis blockchain mengikuti konsep desain yang diusulkan memberikan kemampuan kontrol akses yang halus karena pengguna individu dapat menyesuaikan kebijakannya sendiri pada setiap kumpulan data dengan memaksakan preferensi kontrol akses yang dicatat ke buku besar. 4. Penyimpanan Data Off-chain: Data pribadi harus disimpan off-chain untuk skalabilitas yang lebih baik dan efisiensi yang lebih tinggi. Selain itu, menyimpan data pribadi langsung ke clockchain, bahkan dalam bentuk terenkripsi, dapat menimbulkan potensi kebocoran privasi dan mengakibatkan ketidakpatuhan terhadap GDPR. Tergantung pada skenario tertentu, DBMS konvensional (misalnya, Oracle atau MongoDB), layanan penyimpanan awan (misalnya, S3, AWS atau Azure), atau sistem penyimpanan dapat digunakan untuk penyimpanan data. Hanya referensi ke data yang disimpan secara on-chain (yaitu, disimpan dalam buku besar terdistribusi). Referensi disebut penunjuk data itu bisa menjadi hash, string koneksi, jalur absolut, atau pengidentifikasi yang merujuk ke kumpulan data; tergantung pada sistem penyimpanan off-chain tertentu yang digunakan dalam platform.","Bab ini menyajikan desain yang digunakan dalam penelitian ini. Hal ini menentukan sumber dari mana data akan dikumpulkan dan bagaimana mengumpulkan dan menganalisis data ini. Pada bab ini akan dibahas mengenai filosofi keilmuan dari data governance, konsep teknolgi BLockchain dan penerapan data governance dalam teknologi blockchain di bidang pendidikan, yang akan memberikan pandangan utama saat melakukan penelitian. Skema Penelitian
Untuk menyelesaikan penelitian dirancang kerangka pikir yang menggambarkan langkah-langkah yang harus ditempuh, dapat dilihat penjelasan dan urutannya sebagai berikut:

3.2.1 Mendefinisikan Tata Kelola Data untuk Organisasi
Upaya Tata Kelola Data harus mendukung strategi dan tujuan bisnis. Strategi dan sasaran bisnis organisasi menginformasikan strategi data perusahaan dan bagaimana tata kelola data dan aktivitas manajemen data perlu dioperasionalkan dalam organisasi. Tata kelola data memungkinkan tanggung jawab bersama untuk keputusan terkait data. Kegiatan tata kelola data melintasi batasbatas organisasi dan sistem untuk mendukung tampilan data yang terintegrasi. Fokusnya adalah pada kesan yang dimiliki personel bisnis tentang seberapa baik perusahaan mengelola data dan menggunakan data untuk keuntungannya, serta pada kriteria objektif, seperti penggunaan alat, tingkat pelaporan, dll. Hasilnya ada bagaimana kerangka komunikasi dijelaskan dalam gambar 3.3. Kebijakan Penggunaan Data: Kebijakan tersebut menentukan tindakan tata kelola data termasuk hak, izin, dan kondisi. Penyimpanan Data Off-chain: Data pribadi harus disimpan off-chain untuk skalabilitas yang lebih baik dan efisiensi yang lebih tinggi. Selain itu, menyimpan data pribadi langsung ke clockchain, bahkan dalam bentuk terenkripsi, dapat menimbulkan potensi kebocoran privasi dan mengakibatkan ketidakpatuhan terhadap GDPR. Tergantung pada skenario tertentu, DBMS konvensional (misalnya, Oracle atau MongoDB), layanan penyimpanan awan (misalnya, S3, AWS atau Azure), atau sistem penyimpanan dapat digunakan untuk penyimpanan data. Hanya referensi ke data yang disimpan secara on-chain (yaitu, disimpan dalam buku besar terdistribusi). Referensi disebut penunjuk data itu bisa menjadi hash, string koneksi, jalur absolut, atau pengidentifikasi yang merujuk ke kumpulan data; tergantung pada sistem penyimpanan off-chain tertentu yang digunakan dalam platform."
