{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harry\\anaconda3\\envs\\torch-nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch_directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'model/80/model_penilaian'\n",
    "DEVICE = torch_directml.device()\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 3\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, nama_dokumen, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.nama_dokumen = nama_dokumen\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "        nama_dokumen = self.nama_dokumen[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'nama_dokumen': nama_dokumen\n",
    "        }\n",
    "\n",
    "# Fungsi untuk memuat dan memproses data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    texts = df['summary'].tolist()\n",
    "    nama_dokumen = df['nama_dokumen'].tolist()  # Tambahkan baris ini\n",
    "    label_columns = [\n",
    "        'nilai_isi_disertasi',\n",
    "        'nilai_penguasaan_materi_dan_metode_penelitian',\n",
    "        'nilai_kontribusi_hasil_penelitian_bagi_ilmu_pengetahuan',\n",
    "        'nilai_kontribusi_hasil_penelitian_bagi_masyarakat',\n",
    "        'nilai_wawasan_pengetahuan_konsep_ilmu_komputer',\n",
    "        'nilai_kemampuan_untuk_menjawab_pertanyaan'\n",
    "    ]\n",
    "\n",
    "    # Inisialisasi LabelEncoder\n",
    "    label_encoders = {col: LabelEncoder() for col in label_columns}\n",
    "\n",
    "    # Mengonversi label kategorikal menjadi numerik\n",
    "    encoded_labels = []\n",
    "    for col in label_columns:\n",
    "        encoded_labels.append(label_encoders[col].fit_transform(df[col]))\n",
    "\n",
    "    # Menggabungkan label yang telah dienkode\n",
    "    labels = np.column_stack(encoded_labels)\n",
    "\n",
    "    return texts, labels, label_encoders, label_columns, nama_dokumen  # Tambahkan nama_dokumen di sini\n",
    "\n",
    "# Fungsi untuk melatih model\n",
    "def train_model(model, train_data_loader, val_data_loader, epochs, optimizer, device):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_data_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = 0\n",
    "            for i in range(labels.shape[1]):\n",
    "                loss += criterion(outputs.logits[:, i*3:(i+1)*3], labels[:, i])\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_data_loader)\n",
    "        print(f\"Average train loss: {avg_train_loss}\")\n",
    "\n",
    "        val_loss = evaluate_model(model, val_data_loader, device)\n",
    "        print(f\"Validation loss: {val_loss}\")\n",
    "\n",
    "# Fungsi untuk evaluasi model\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = 0\n",
    "            for i in range(labels.shape[1]):\n",
    "                loss += criterion(outputs.logits[:, i*3:(i+1)*3], labels[:, i])\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Fungsi untuk prediksi\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    nama_dokumen_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels']\n",
    "            nama_dokumen = batch['nama_dokumen']\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits.view(-1, 6, 3), dim=2).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            actual_labels.extend(labels.numpy())\n",
    "            nama_dokumen_list.extend(nama_dokumen)\n",
    "\n",
    "    return np.array(predictions), np.array(actual_labels), nama_dokumen_list\n",
    "\n",
    "    return np.array(predictions), np.array(actual_labels)\n",
    "\n",
    "def save_model(model, tokenizer, output_dir):\n",
    "    # Pastikan direktori output ada\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Simpan model\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    # Simpan tokenizer\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    print(f\"Model dan tokenizer telah disimpan di {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=18).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts, test_labels, _, _, test_nama_dokumen = load_data('data/penilaian-data/80/final-data-penilaian.csv')\n",
    "train_texts, train_labels, label_encoders, label_columns, train_nama_dokumen = load_data('data/penilaian-data/80/final-data-penilaian.csv')\n",
    "test_dataset = CustomDataset(test_texts, test_labels, test_nama_dokumen, tokenizer, MAX_LEN)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.8355111360549925\n",
      "F1 Score: 0.8263962673191882\n",
      "Accuracy: 0.8277777777777777\n",
      "Predictions saved to 'data/output-bert-penilaian/pred-80.csv'\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss = evaluate_model(model, test_data_loader, DEVICE)\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions, actual_labels, nama_dokumen_list = predict(model, test_data_loader, DEVICE)\n",
    "\n",
    "# Calculate metrics\n",
    "f1 = f1_score(actual_labels.flatten(), predictions.flatten(), average='weighted')\n",
    "accuracy = accuracy_score(actual_labels.flatten(), predictions.flatten())\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "original_predictions = {}\n",
    "for i, col in enumerate(label_columns):\n",
    "    original_predictions[col] = label_encoders[col].inverse_transform(predictions[:, i])\n",
    "\n",
    "# Create DataFrame with original labels\n",
    "results_df = pd.DataFrame(original_predictions)\n",
    "results_df['nama_dokumen'] = nama_dokumen_list\n",
    "results_df = results_df[['nama_dokumen'] + [col for col in label_columns]]\n",
    "results_df.to_csv('data/output-bert-penilaian/pred-80.csv', index=False)\n",
    "print(\"Predictions saved to 'data/output-bert-penilaian/pred-80.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
