{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat summary BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harry\\anaconda3\\envs\\torch-nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import EncoderDecoderModel, BertTokenizer\n",
    "import torch_directml\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gunakan torch_directml untuk akselerasi GPU\n",
    "device = torch_directml.device()\n",
    "\n",
    "def load_bert_model(model_path):\n",
    "    model = EncoderDecoderModel.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_bert_summary(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        num_beams=8,\n",
    "        max_length=256,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def process_section(section, model, tokenizer, df):\n",
    "    print(f\"Processing {section}...\")\n",
    "    \n",
    "    summaries = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Summarizing {section}\"):\n",
    "        summary = generate_bert_summary(row['kalimat'], model, tokenizer)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        'nama_dokumen': df['nama_dokumen'],\n",
    "        section: summaries\n",
    "    })\n",
    "    \n",
    "    print(f\"Completed {section}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing latarbelakang...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing latarbelakang: 100%|██████████| 30/30 [08:55<00:00, 17.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed latarbelakang\n",
      "Processing rumusanmasalah...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing rumusanmasalah: 100%|██████████| 30/30 [02:44<00:00,  5.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed rumusanmasalah\n",
      "Processing tujuanpenelitian...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing tujuanpenelitian: 100%|██████████| 30/30 [02:12<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed tujuanpenelitian\n",
      "Processing rangkumanpenelitianterkait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing rangkumanpenelitianterkait: 100%|██████████| 30/30 [02:33<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed rangkumanpenelitianterkait\n",
      "Processing metodologipenelitian...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing metodologipenelitian: 100%|██████████| 30/30 [08:57<00:00, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed metodologipenelitian\n",
      "All sections processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    sections = ['latarbelakang', 'rumusanmasalah', 'tujuanpenelitian', 'rangkumanpenelitianterkait', 'metodologipenelitian']\n",
    "    results = []\n",
    "\n",
    "    for section in sections:\n",
    "        df = pd.read_csv(f'data/final-data/{section}.csv')\n",
    "        model, tokenizer = load_bert_model(f\"model/80/model_{section}\")\n",
    "        \n",
    "        result = process_section(section, model, tokenizer, df)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Gabungkan hasil dari semua bagian\n",
    "    output_df = pd.concat(results, axis=1)\n",
    "    output_df = output_df.loc[:,~output_df.columns.duplicated()]  # Hapus kolom duplikat\n",
    "    \n",
    "    # Simpan output-bert.csv\n",
    "    output_df.to_csv('data/penilaian-data/80/output-bert.csv', index=False)\n",
    "    \n",
    "    # Buat summary akhir\n",
    "    output_df['summary'] = output_df[sections].agg(' '.join, axis=1)\n",
    "    \n",
    "    # Simpan final-output-bert.csv\n",
    "    final_output_df = output_df[['nama_dokumen', 'summary']]\n",
    "    final_output_df.to_csv('data/penilaian-data/80/merged-summary.csv', index=False)\n",
    "    \n",
    "    print(\"All sections processed and saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menggabungkan data penilaian dan summary BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File hasil penggabungan telah disimpan di: data/penilaian-data/80/final-data-penilaian.csv\n",
      "Jumlah baris dalam file Excel: 30\n",
      "Jumlah baris dalam file CSV: 30\n",
      "Jumlah baris dalam file hasil penggabungan: 30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_assessment_data(excel_path, csv_path, output_path):\n",
    "    # Baca file Excel\n",
    "    excel_df = pd.read_excel(excel_path)\n",
    "    \n",
    "    # Baca file CSV\n",
    "    csv_df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Gabungkan DataFrame berdasarkan 'nama_dokumen'\n",
    "    merged_df = pd.merge(excel_df, csv_df, on='nama_dokumen', how='outer')\n",
    "    \n",
    "    # Periksa apakah ada data yang tidak cocok\n",
    "    unmatched_excel = excel_df[~excel_df['nama_dokumen'].isin(csv_df['nama_dokumen'])]\n",
    "    unmatched_csv = csv_df[~csv_df['nama_dokumen'].isin(excel_df['nama_dokumen'])]\n",
    "    \n",
    "    if not unmatched_excel.empty:\n",
    "        print(\"Data dari Excel yang tidak cocok:\")\n",
    "        print(unmatched_excel['nama_dokumen'])\n",
    "    \n",
    "    if not unmatched_csv.empty:\n",
    "        print(\"Data dari CSV yang tidak cocok:\")\n",
    "        print(unmatched_csv['nama_dokumen'])\n",
    "    \n",
    "    # Simpan hasil gabungan ke file CSV baru\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"File hasil penggabungan telah disimpan di: {output_path}\")\n",
    "\n",
    "    # Tampilkan informasi tentang hasil penggabungan\n",
    "    print(f\"Jumlah baris dalam file Excel: {len(excel_df)}\")\n",
    "    print(f\"Jumlah baris dalam file CSV: {len(csv_df)}\")\n",
    "    print(f\"Jumlah baris dalam file hasil penggabungan: {len(merged_df)}\")\n",
    "\n",
    "# Jalankan fungsi\n",
    "excel_path = 'data/penilaian-xlsx/data-penilaian.xlsx'\n",
    "csv_path = 'data/penilaian-data/80/merged-summary.csv'\n",
    "output_path = 'data/penilaian-data/80/final-data-penilaian.csv'\n",
    "\n",
    "merge_assessment_data(excel_path, csv_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 24\n",
      "Validation set size: 3\n",
      "Test set size: 3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_dataset(input_file, output_dir, val_docs, test_docs):\n",
    "    # Baca file input\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Fungsi untuk menentukan set berdasarkan nama dokumen\n",
    "    def get_set(doc_name):\n",
    "        if doc_name in val_docs:\n",
    "            return 'val'\n",
    "        elif doc_name in test_docs:\n",
    "            return 'test'\n",
    "        else:\n",
    "            return 'train'\n",
    "    \n",
    "    # Tambahkan kolom 'set' ke DataFrame\n",
    "    df['set'] = df['nama_dokumen'].apply(get_set)\n",
    "    \n",
    "    # Pisahkan DataFrame berdasarkan set\n",
    "    train_df = df[df['set'] == 'train'].drop('set', axis=1)\n",
    "    val_df = df[df['set'] == 'val'].drop('set', axis=1)\n",
    "    test_df = df[df['set'] == 'test'].drop('set', axis=1)\n",
    "    \n",
    "    # Simpan file CSV\n",
    "    train_df.to_csv(f\"{output_dir}/train.csv\", index=False)\n",
    "    val_df.to_csv(f\"{output_dir}/val.csv\", index=False)\n",
    "    test_df.to_csv(f\"{output_dir}/test.csv\", index=False)\n",
    "    \n",
    "    print(f\"Train set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")\n",
    "    print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Definisikan dokumen validasi dan test\n",
    "val_docs = [\n",
    "    'Utami Lestari_Kualifikasi.txt',\n",
    "    'Kualifikasi Witta Listiya Ningrum.txt',\n",
    "    'Kualifikasi_Remigius.txt'\n",
    "]\n",
    "\n",
    "test_docs = [\n",
    "    'Robert_Kualifikasi.txt',\n",
    "    'MetaMeysawati_KUALIFIKASI(99216026).txt',\n",
    "    'Kualifikasi_Rama Dian Syah.txt'\n",
    "]\n",
    "\n",
    "# Jalankan fungsi\n",
    "input_file = 'data/penilaian-data/80/final-data-penilaian.csv'\n",
    "output_dir = 'data/model-data-penilaian/80'\n",
    "\n",
    "split_dataset(input_file, output_dir, val_docs, test_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
